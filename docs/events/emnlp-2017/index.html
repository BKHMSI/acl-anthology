<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Conference on Empirical Methods in Natural Language Processing (and forerunners) (2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Conference on Empirical Methods in Natural Language Processing (and forerunners) (2017)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#d17-1>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">265&nbsp;papers</span></li><li><a class=align-middle href=#d17-2>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">18&nbsp;papers</span></li><li><a class=align-middle href=#d17-3>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li></ul></div></div><div id=d17-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D17-1/>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1000/>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</a></strong><br><a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/r/rebecca-hwa/>Rebecca Hwa</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1001.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234373 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1001/>Monolingual Phrase Alignment on Parse Forests</a></strong><br><a href=/people/y/yuki-arase/>Yuki Arase</a>
|
<a href=/people/j/junichi-tsujii/>Junichi Tsujii</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1001><div class="card-body p-3 small">We propose an efficient method to conduct phrase alignment on <a href=https://en.wikipedia.org/wiki/Parse_forest>parse forests</a> for <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a>. Unlike previous studies, our method identifies syntactic paraphrases under linguistically motivated grammar. In addition, it allows phrases to non-compositionally align to handle <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> with non-homographic phrase correspondences. A <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that provides gold parse trees and their phrase alignments is created. The experimental results confirm that the proposed method conducts highly accurate phrase alignment compared to human performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238228823 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1005" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1005/>Heterogeneous Supervision for Relation Extraction : A Representation Learning Approach</a></strong><br><a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/q/qi-zhu/>Qi Zhu</a>
|
<a href=/people/s/shi-zhi/>Shi Zhi</a>
|
<a href=/people/h/huan-gui/>Huan Gui</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1005><div class="card-body p-3 small">Relation extraction is a fundamental task in <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. Most existing methods have heavy reliance on <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task : how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238228743 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1007/>Entity Linking for Queries by Searching Wikipedia Sentences<span class=acl-fixed-case>W</span>ikipedia Sentences</a></strong><br><a href=/people/c/chuanqi-tan/>Chuanqi Tan</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/p/pengjie-ren/>Pengjie Ren</a>
|
<a href=/people/w/weifeng-lv/>Weifeng Lv</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1007><div class="card-body p-3 small">We present a simple yet effective approach for linking entities in queries. The key idea is to search sentences similar to a query from Wikipedia articles and directly use the human-annotated entities in the similar sentences as candidate entities for the query. Then, we employ a rich set of features, such as link-probability, context-matching, word embeddings, and relatedness among candidate entities as well as their related entities, to rank the candidates under a regression based framework. The advantages of our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> lie in two aspects, which contribute to the ranking process and final linking result. First, it can greatly reduce the number of candidate entities by filtering out irrelevant entities with the words in the query. Second, we can obtain the query sensitive prior probability in addition to the static link-probability derived from all Wikipedia articles. We conduct experiments on two benchmark datasets on <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> for queries, namely the ERD14 dataset and the GERDAQ dataset. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art systems</a> and yields 75.0 % in F1 on the ERD14 dataset and 56.9 % on the GERDAQ dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1008.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236475 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1008/>Train-O-Matic : Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data<span class=acl-fixed-case>O</span>-<span class=acl-fixed-case>M</span>atic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data</a></strong><br><a href=/people/t/tommaso-pasini/>Tommaso Pasini</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1008><div class="card-body p-3 small">Annotating large numbers of sentences with senses is the heaviest requirement of current <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a>. We present Train-O-Matic, a language-independent method for generating millions of sense-annotated training instances for virtually all meanings of words in a language&#8217;s vocabulary. The approach is fully automatic : no human intervention is required and the only type of <a href=https://en.wikipedia.org/wiki/Knowledge>human knowledge</a> used is a WordNet-like resource. Train-O-Matic achieves consistently state-of-the-art performance across gold standard datasets and languages, while at the same time removing the burden of manual annotation. All the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is available for research purposes at.<url>http://trainomatic.org</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1009.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236598 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1009/>Universal Semantic Parsing</a></strong><br><a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/o/oscar-tackstrom/>Oscar Täckström</a>
|
<a href=/people/s/slav-petrov/>Slav Petrov</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1009><div class="card-body p-3 small">Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and can not process <a href=https://en.wikipedia.org/wiki/Dependency_graph>dependency graphs</a>, which allow handling complex phenomena such as <a href=https://en.wikipedia.org/wiki/Control_flow>control</a>. In this work, we introduce UDepLambda, a semantic interface for UD, which maps <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> against <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDepLambda outperforms strong baselines across languages and datasets. For <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> achieves a 4.9 F1 point improvement over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on GraphQuestions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1010.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234299 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1010/>Mimicking Word Embeddings using Subword RNNs<span class=acl-fixed-case>RNN</span>s</a></strong><br><a href=/people/y/yuval-pinter/>Yuval Pinter</a>
|
<a href=/people/r/robert-guthrie/>Robert Guthrie</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1010><div class="card-body p-3 small">Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data. However, the effectiveness of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist. In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>function</a> from spellings to distributional embeddings. Unlike prior work, MIMICK does not require re-training on the original word embedding corpus ; instead, <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> is performed at the type level. Intrinsic and extrinsic evaluations demonstrate the power of this simple approach. On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes. It is competitive with (and complementary to) a supervised character-based model in low resource settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234769 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1012/>Neural Machine Translation with Source-Side Latent Graph Parsing</a></strong><br><a href=/people/k/kazuma-hashimoto/>Kazuma Hashimoto</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1012><div class="card-body p-3 small">This paper presents a novel neural machine translation model which jointly learns <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, and thus the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is optimized according to the translation objective. In experiments, we first show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> compares favorably with state-of-the-art sequential and pipelined syntax-based NMT models. We also show that the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be further improved by pre-training it with a small amount of treebank annotations. Our final <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble model</a> significantly outperforms the previous best models on the standard English-to-Japanese translation dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1013/>Neural Machine Translation with Word Predictions</a></strong><br><a href=/people/r/rongxiang-weng/>Rongxiang Weng</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1013><div class="card-body p-3 small">In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Decoder>decoder</a> carry the crucial information about the sentence. These <a href=https://en.wikipedia.org/wiki/Euclidean_vector>vectors</a> are generated by parameters which are updated by back-propagation of translation errors through time. We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors. In this paper, we propose to use <a href=https://en.wikipedia.org/wiki/Word_prediction>word predictions</a> as a mechanism for direct supervision. More specifically, we require these <a href=https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)>vectors</a> to be able to predict the vocabulary in target sentence. Our simple mechanism ensures better representations in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Code>decoder</a> without using any extra data or annotation. It is also helpful in reducing the target side vocabulary and improving the decoding efficiency. Experiments on Chinese-English machine translation task show an average BLEU improvement by 4.53, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1014.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236392 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1014/>Towards Decoding as Continuous Optimisation in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/c/cong-duy-vu-hoang/>Cong Duy Vu Hoang</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1014><div class="card-body p-3 small">We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. We reformulate decoding, a discrete optimization problem, into a continuous problem, such that <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a> can make use of efficient gradient-based techniques. Our powerful decoding framework allows for more accurate decoding for standard neural machine translation models, as well as enabling decoding in intractable models such as intersection of several different NMT models. Our empirical results show that our decoding framework is effective, and can leads to substantial improvements in translations, especially in situations where <a href=https://en.wikipedia.org/wiki/Greedy_search>greedy search</a> and <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> are not feasible. Finally, we show how the <a href=https://en.wikipedia.org/wiki/Methodology>technique</a> is highly competitive with, and complementary to, <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238230308 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1015" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1015/>Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space</a></strong><br><a href=/people/n/nikita-kitaev/>Nikita Kitaev</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1015><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for locating regions in space based on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language descriptions</a>. Starting with a 3D scene and a sentence, our model is able to associate words in the sentence with regions in the scene, interpret relations such as &#8216;on top of&#8217; or &#8216;next to,&#8217; and finally locate the region described in the sentence. All <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> form a single <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> that is trained end-to-end without prior knowledge of object segmentation. To evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, we construct and release a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of Minecraft scenes with <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourced natural language descriptions</a>. We achieve a 32 % relative error reduction compared to a strong neural baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238228698 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1016/>Continuous Representation of Location for <a href=https://en.wikipedia.org/wiki/Geolocation>Geolocation</a> and Lexical Dialectology using Mixture Density Networks</a></strong><br><a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1016><div class="card-body p-3 small">We propose a method for embedding two-dimensional locations in a continuous vector space using a neural network-based model incorporating mixtures of Gaussian distributions, presenting two model variants for text-based geolocation and lexical dialectology. Evaluated over <a href=https://en.wikipedia.org/wiki/Twitter>Twitter data</a>, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms conventional regression-based geolocation and provides a better <a href=https://en.wikipedia.org/wiki/Measurement_uncertainty>estimate of uncertainty</a>. We also show the effectiveness of the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a> for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238230148 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1017/>Obj2Text : Generating Visually Descriptive Language from Object Layouts<span class=acl-fixed-case>O</span>bj2<span class=acl-fixed-case>T</span>ext: Generating Visually Descriptive Language from Object Layouts</a></strong><br><a href=/people/x/xuwang-yin/>Xuwang Yin</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1017><div class="card-body p-3 small">Generating captions for <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> is a task that has recently received considerable attention. Another type of visual inputs are abstract scenes or object layouts where the only information provided is a set of objects and their locations. This type of <a href=https://en.wikipedia.org/wiki/Imagery>imagery</a> is commonly found in many applications in <a href=https://en.wikipedia.org/wiki/Computer_graphics>computer graphics</a>, <a href=https://en.wikipedia.org/wiki/Virtual_reality>virtual reality</a>, and <a href=https://en.wikipedia.org/wiki/Storyboard>storyboarding</a>. We explore in this paper OBJ2TEXT, a sequence-to-sequence model that encodes a set of objects and their locations as an input sequence using an LSTM network, and decodes this representation using an LSTM language model. We show in our paper that this model despite using a sequence encoder can effectively represent complex spatial object-object relationships and produce descriptions that are globally coherent and semantically relevant. We test our approach for the task of describing object layouts in the MS-COCO dataset by producing sentences given only object annotations. We additionally show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> combined with a state-of-the-art object detector can improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of an image captioning model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238232979 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1018/>End-to-end Neural Coreference Resolution</a></strong><br><a href=/people/k/kenton-lee/>Kenton Lee</a>
|
<a href=/people/l/luheng-he/>Luheng He</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1018><div class="card-body p-3 small">We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234840 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1019/>Neural Net Models of Open-domain Discourse Coherence</a></strong><br><a href=/people/j/jiwei-li/>Jiwei Li</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1019><div class="card-body p-3 small">Discourse coherence is strongly associated with text quality, making it important to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language generation and understanding</a>. Yet existing models of coherence focus on measuring individual aspects of <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> (lexical overlap, rhetorical structure, entity centering) in narrow domains. In this paper, we describe domain-independent neural models of discourse coherence that are capable of measuring multiple aspects of <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> in existing sentences and can maintain <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> while generating new sentences. We study both discriminative models that learn to distinguish coherent from incoherent discourse, and generative models that produce coherent text, including a novel neural latent-variable Markovian generative model that captures the latent discourse dependencies between sentences in a text. Our work achieves state-of-the-art performance on multiple coherence evaluations, and marks an initial step in generating coherent texts given discourse contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231488 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1020/>Affinity-Preserving Random Walk for Multi-Document Summarization</a></strong><br><a href=/people/k/kexiang-wang/>Kexiang Wang</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1020><div class="card-body p-3 small">Multi-document summarization provides users with a short text that summarizes the information in a set of related documents. This paper introduces affinity-preserving random walk to the summarization task, which preserves the affinity relations of sentences by an absorbing <a href=https://en.wikipedia.org/wiki/Random_walk_model>random walk model</a>. Meanwhile, we put forward adjustable affinity-preserving random walk to enforce the diversity constraint of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> in the random walk process. The ROUGE evaluations on DUC 2003 topic-focused summarization task and DUC 2004 generic summarization task show the good performance of our method, which has the best ROUGE-2 recall among the graph-based ranking methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1022/>Hierarchical Embeddings for Hypernymy Detection and Directionality</a></strong><br><a href=/people/k/kim-anh-nguyen/>Kim Anh Nguyen</a>
|
<a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1022><div class="card-body p-3 small">We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> have shown limitations on prototypical hypernyms, HyperVec represents an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised measure</a> where <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are learned in a specific order and capture the hypernymhyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state-of-the-art unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1024" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1024/>Dict2vec : Learning Word Embeddings using Lexical Dictionaries<span class=acl-fixed-case>D</span>ict2vec : Learning Word Embeddings using Lexical Dictionaries</a></strong><br><a href=/people/j/julien-tissier/>Julien Tissier</a>
|
<a href=/people/c/christophe-gravier/>Christophe Gravier</a>
|
<a href=/people/a/amaury-habrard/>Amaury Habrard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1024><div class="card-body p-3 small">Learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> on large unlabeled corpus has been shown to be successful in improving many <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language tasks</a>. The most efficient and popular approaches learn or retrofit such <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> using additional <a href=https://en.wikipedia.org/wiki/Data>external data</a>. Resulting embeddings are generally better than their corpus-only counterparts, although such resources cover a fraction of words in the vocabulary. In this paper, we propose a new approach, Dict2vec, based on one of the largest yet refined datasource for describing words natural language dictionaries. Dict2vec builds new word pairs from dictionary entries so that semantically-related words are moved closer, and negative sampling filters out pairs whose words are unrelated in dictionaries. We evaluate the word representations obtained using Dict2vec on eleven <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for the word similarity task and on four <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for a text classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1026/>Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext</a></strong><br><a href=/people/j/john-wieting/>John Wieting</a>
|
<a href=/people/j/jonathan-mallinson/>Jonathan Mallinson</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1026><div class="card-body p-3 small">We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al. (2016b). We use <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> to generate sentential paraphrases via back-translation of bilingual sentence pairs. We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embeddings. We find that the data quality is stronger than prior work based on bitext and on par with manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains. We experiment with several language pairs and <a href=https://en.wikipedia.org/wiki/Data_source>data sources</a>, and develop a variety of data filtering techniques. In the process, we explore how neural machine translation output differs from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>human-written sentences</a>, finding clear differences in length, the amount of <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>repetition</a>, and the use of rare words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1028/>Exploiting Morphological Regularities in Distributional Word Representations</a></strong><br><a href=/people/a/arihant-gupta/>Arihant Gupta</a>
|
<a href=/people/s/syed-sarfaraz-akhtar/>Syed Sarfaraz Akhtar</a>
|
<a href=/people/a/avijit-vajpayee/>Avijit Vajpayee</a>
|
<a href=/people/a/arjit-srivastava/>Arjit Srivastava</a>
|
<a href=/people/m/madan-gopal-jhanwar/>Madan Gopal Jhanwar</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1028><div class="card-body p-3 small">We present an unsupervised, language agnostic approach for exploiting morphological regularities present in high dimensional vector spaces. We propose a novel method for generating embeddings of words from their morphological variants using morphological transformation operators. We evaluate this approach on MSR word analogy test set with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 85 % which is 12 % higher than the previous best known system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1029/>Exploiting Word Internal Structures for Generic Chinese Sentence Representation<span class=acl-fixed-case>C</span>hinese Sentence Representation</a></strong><br><a href=/people/s/shaonan-wang/>Shaonan Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1029><div class="card-body p-3 small">We introduce a novel mixed characterword architecture to improve Chinese sentence representations, by utilizing rich semantic information of word internal structures. Our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> uses two key strategies. The first is a mask gate on characters, learning the relation among characters in a word. The second is a maxpooling operation on words, adaptively finding the optimal mixture of the atomic and compositional word representations. Finally, the proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> is applied to various sentence composition models, which achieves substantial performance gains over baseline models on sentence similarity task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1030.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1030/>High-risk learning : acquiring new word vectors from tiny data</a></strong><br><a href=/people/a/aurelie-herbelot/>Aurélie Herbelot</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1030><div class="card-body p-3 small">Distributional semantics models are known to struggle with <a href=https://en.wikipedia.org/wiki/Small_data>small data</a>. It is generally accepted that in order to learn &#8216;a good vector&#8217; for a word, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences&#8217; worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1032/>VecShare : A Framework for Sharing Word Representation Vectors<span class=acl-fixed-case>V</span>ec<span class=acl-fixed-case>S</span>hare: A Framework for Sharing Word Representation Vectors</a></strong><br><a href=/people/j/jared-fernandez/>Jared Fernandez</a>
|
<a href=/people/z/zhaocheng-yu/>Zhaocheng Yu</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1032><div class="card-body p-3 small">Many Natural Language Processing (NLP) models rely on distributed vector representations of words. Because the process of training word vectors can require large amounts of data and computation, NLP researchers and practitioners often utilize pre-trained embeddings downloaded from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>. However, finding the best <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for a given <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> is difficult, and can be computationally prohibitive. We present a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, called VecShare, that makes it easy to share and retrieve <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> leverages a public data-sharing infrastructure to host embedding sets, and provides automated mechanisms for retrieving the embeddings most similar to a given corpus. We perform an experimental evaluation of VecShare&#8217;s similarity strategies, and show that they are effective at efficiently retrieving <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> that boost <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in a document classification task. Finally, we provide an open-source Python library for using the VecShare framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1033.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1033/>Word Re-Embedding via Manifold Dimensionality Retention</a></strong><br><a href=/people/s/souleiman-hasan/>Souleiman Hasan</a>
|
<a href=/people/e/edward-curry/>Edward Curry</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1033><div class="card-body p-3 small">Word embeddings seek to recover a <a href=https://en.wikipedia.org/wiki/Euclidean_space>Euclidean metric space</a> by mapping words into vectors, starting from words co-occurrences in a corpus. Word embeddings may underestimate the similarity between nearby words, and overestimate it between distant words in the <a href=https://en.wikipedia.org/wiki/Euclidean_space>Euclidean metric space</a>. In this paper, we re-embed pre-trained <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> with a stage of <a href=https://en.wikipedia.org/wiki/Manifold_learning>manifold learning</a> which retains <a href=https://en.wikipedia.org/wiki/Dimensionality>dimensionality</a>. We show that this approach is theoretically founded in the metric recovery paradigm, and empirically show that it can improve on state-of-the-art embeddings in word similarity tasks 0.5-5.0 % points depending on the original space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1035" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1035/>Reporting Score Distributions Makes a Difference : Performance Study of LSTM-networks for Sequence Tagging<span class=acl-fixed-case>LSTM</span>-networks for Sequence Tagging</a></strong><br><a href=/people/n/nils-reimers/>Nils Reimers</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1035><div class="card-body p-3 small">In this paper we show that reporting a single performance score is insufficient to compare <a href=https://en.wikipedia.org/wiki/Non-deterministic_algorithm>non-deterministic approaches</a>. We demonstrate for common sequence tagging tasks that the seed value for the <a href=https://en.wikipedia.org/wiki/Random_number_generation>random number generator</a> can result in statistically significant (p &lt; 10 ^ -4) differences for state-of-the-art systems. For two recent <a href=https://en.wikipedia.org/wiki/System>systems</a> for NER, we observe an absolute difference of one percentage point <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> depending on the selected seed value, making these <a href=https://en.wikipedia.org/wiki/System>systems</a> perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present <a href=https://en.wikipedia.org/wiki/Network_architecture>network architectures</a> that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.<tex-math>p &lt; 10^{-4}</tex-math>) differences for state-of-the-art systems. For two recent systems for NER, we observe an absolute difference of one percentage point F&#8321;-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre. Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1036.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1036/>Learning What’s Easy : Fully Differentiable Neural Easy-First Taggers</a></strong><br><a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1036><div class="card-body p-3 small">We introduce a novel neural easy-first decoder that learns to solve sequence tagging tasks in a flexible order. In contrast to previous easy-first decoders, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are end-to-end differentiable. The <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> iteratively updates a sketch of the predictions over the sequence. At its core is an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> that controls which parts of the input are strategically the best to process next. We present a new constrained softmax transformation that ensures the same cumulative attention to every word, and show how to efficiently evaluate and backpropagate over it. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> compare favourably to BILSTM taggers on three sequence tagging tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1037.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1037.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1037/>Incremental Skip-gram Model with Negative Sampling</a></strong><br><a href=/people/n/nobuhiro-kaji/>Nobuhiro Kaji</a>
|
<a href=/people/h/hayato-kobayashi/>Hayato Kobayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1037><div class="card-body p-3 small">This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SGNS, are multi-pass algorithms and thus can not perform incremental model update. To address this problem, we present a simple incremental extension of SGNS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1038/>Learning to select data for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> with Bayesian Optimization<span class=acl-fixed-case>B</span>ayesian Optimization</a></strong><br><a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1038><div class="card-body p-3 small">Domain similarity measures can be used to gauge adaptability and select suitable <a href=https://en.wikipedia.org/wiki/Data>data</a> for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to learn data selection measures using <a href=https://en.wikipedia.org/wiki/Bayesian_optimization>Bayesian Optimization</a> and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks : <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. We show the importance of complementing <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> with diversity, and that learned measures areto some degreetransferable across models, domains, and even tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1039.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1039/>Unsupervised Pretraining for Sequence to Sequence Learning</a></strong><br><a href=/people/p/prajit-ramachandran/>Prajit Ramachandran</a>
|
<a href=/people/p/peter-j-liu/>Peter Liu</a>
|
<a href=/people/q/quoc-le/>Quoc Le</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1039><div class="card-body p-3 small">This work presents a general unsupervised learning method to improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and abstractive summarization and find that it significantly improves the subsequent <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a>. Our main result is that pretraining improves the <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> of seq2seq models. We achieve state-of-the-art results on the WMT EnglishGerman task, surpassing a range of methods using both phrase-based machine translation and <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. Our method achieves a significant improvement of 1.3 <a href=https://en.wikipedia.org/wiki/British_thermal_unit>BLEU</a> from th previous best models on both WMT&#8217;14 and WMT&#8217;15 EnglishGerman. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1040/>Efficient Attention using a Fixed-Size Memory Representation</a></strong><br><a href=/people/d/denny-britz/>Denny Britz</a>
|
<a href=/people/m/melody-guan/>Melody Guan</a>
|
<a href=/people/m/minh-thang-luong/>Minh-Thang Luong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1040><div class="card-body p-3 small">The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step. In this work, we propose an alternative <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> based on a fixed size memory representation that is more efficient. Our technique predicts a compact set of K attention contexts during <a href=https://en.wikipedia.org/wiki/Encoding_(memory)>encoding</a> and lets the <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a> compute an efficient <a href=https://en.wikipedia.org/wiki/Lookup_table>lookup</a> that does not need to consult the <a href=https://en.wikipedia.org/wiki/Memory>memory</a>. We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20 % for real-world translation tasks and more for tasks with longer sequences. By visualizing <a href=https://en.wikipedia.org/wiki/Attention>attention scores</a> we demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> learn distinct, meaningful alignments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1041" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1041/>Rotated Word Vector Representations and their Interpretability</a></strong><br><a href=/people/s/sungjoon-park/>Sungjoon Park</a>
|
<a href=/people/j/jinyeong-bak/>JinYeong Bak</a>
|
<a href=/people/a/alice-oh/>Alice Oh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1041><div class="card-body p-3 small">Vector representation of words improves performance in various NLP tasks, but the high dimensional word vectors are very difficult to interpret. We apply several <a href=https://en.wikipedia.org/wiki/Rotation_(mathematics)>rotation algorithms</a> to the vector representation of words to improve the <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. Unlike previous approaches that induce sparsity, the rotated vectors are interpretable while preserving the expressive performance of the original vectors. Furthermore, any prebuilt word vector representation can be rotated for improved <a href=https://en.wikipedia.org/wiki/Interpreter_(computing)>interpretability</a>. We apply <a href=https://en.wikipedia.org/wiki/Rotation_(mathematics)>rotation</a> to skipgrams and glove and compare the expressive power and interpretability with the original <a href=https://en.wikipedia.org/wiki/Euclidean_vector>vectors</a> and the sparse overcomplete vectors. The results show that the rotated vectors outperform the original and the sparse overcomplete vectors for interpretability and expressiveness tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1042.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1042/>A causal framework for explaining the predictions of black-box sequence-to-sequence models</a></strong><br><a href=/people/d/david-alvarez-melis/>David Alvarez-Melis</a>
|
<a href=/people/t/tommi-jaakkola/>Tommi Jaakkola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1042><div class="card-body p-3 small">We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an explanation consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with perturbed inputs, generating a graph over tokens from the responses, and solving a <a href=https://en.wikipedia.org/wiki/Partition_of_a_set>partitioning problem</a> to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1043 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1043.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1043/>Piecewise Latent Variables for Neural Variational Text Processing</a></strong><br><a href=/people/i/iulian-vlad-serban/>Iulian Vlad Serban</a>
|
<a href=/people/a/alexander-g-ororbia/>Alexander G. Ororbia</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a>
|
<a href=/people/a/aaron-courville/>Aaron Courville</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1043><div class="card-body p-3 small">Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> will learn to represent rich, multi-modal latent factors in real-world data, such as <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language text</a>. However, current models often assume simplistic priors on the latent variables-such as the uni-modal Gaussian distribution-which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This <a href=https://en.wikipedia.org/wiki/Probability_distribution>distribution</a> has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> for <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1044/>Learning the Structure of Variable-Order CRFs : a finite-state perspective<span class=acl-fixed-case>CRF</span>s: a finite-state perspective</a></strong><br><a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1044><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> of linear-chain Conditional Random Fields (CRFs) makes it difficult to deal with very large label sets and long range dependencies. Such situations are not rare and arise when dealing with <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphologically rich languages</a> or joint labelling tasks. We extend here recent proposals to consider variable order CRFs. Using an effective finite-state representation of variable-length dependencies, we propose new ways to perform <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection</a> at large scale and report experimental results where we outperform strong baselines on a tagging task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1047 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1047.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1047" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1047/>Recurrent Attention Network on Memory for Aspect Sentiment Analysis</a></strong><br><a href=/people/p/peng-chen/>Peng Chen</a>
|
<a href=/people/z/zhongqian-sun/>Zhongqian Sun</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/w/wei-yang/>Wei Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1047><div class="card-body p-3 small">We propose a novel framework based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to identify the sentiment of opinion targets in a comment / review. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> adopts multiple-attention mechanism to capture <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment features</a> separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>, which strengthens the expressive power of our model for handling more complications. The weighted-memory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence. We examine the merit of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on four <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> : two are from SemEval2014, i.e. reviews of restaurants and laptops ; a twitter dataset, for testing its performance on social media data ; and a Chinese news comment dataset, for testing its language sensitivity. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> consistently outperforms the state-of-the-art methods on different types of data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1049/>Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions from Reviews</a></strong><br><a href=/people/l/lahari-poddar/>Lahari Poddar</a>
|
<a href=/people/w/wynne-hsu/>Wynne Hsu</a>
|
<a href=/people/m/mong-li-lee/>Mong Li Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1049><div class="card-body p-3 small">User generated content about products and services in the form of reviews are often diverse and even contradictory. This makes it difficult for users to know if an opinion in a review is prevalent or biased. We study the problem of searching for supporting opinions in the context of reviews. We propose a framework called SURF, that first identifies opinions expressed in a review, and then finds similar opinions from other reviews. We design a novel probabilistic graphical model that captures opinions as a combination of aspect, topic and sentiment dimensions, takes into account the preferences of individual authors, as well as the quality of the entity under review, and encodes the flow of thoughts in a review by constraining the aspect distribution dynamically among successive review segments. We derive a <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measure</a> that considers both lexical and semantic similarity to find supporting opinions. Experiments on TripAdvisor hotel reviews and Yelp restaurant reviews show that our model outperforms existing methods for modeling opinions, and the proposed framework is effective in finding supporting opinions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1050" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1050/>Magnets for <a href=https://en.wikipedia.org/wiki/Sarcasm>Sarcasm</a> : Making Sarcasm Detection Timely, Contextual and Very Personal</a></strong><br><a href=/people/a/aniruddha-ghosh/>Aniruddha Ghosh</a>
|
<a href=/people/t/tony-veale/>Tony Veale</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1050><div class="card-body p-3 small">Sarcasm is a pervasive phenomenon in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, permitting the concise communication of meaning, affect and <a href=https://en.wikipedia.org/wiki/Attitude_(psychology)>attitude</a>. Concision requires wit to produce and wit to understand, which demands from each party knowledge of norms, context and a speaker&#8217;s mindset. Insight into a speaker&#8217;s psychological profile at the time of production is a valuable source of context for sarcasm detection. Using a neural architecture, we show significant gains in detection accuracy when knowledge of the speaker&#8217;s mood at the time of production can be inferred. Our focus is on <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm detection</a> on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, and show that the mood exhibited by a speaker over tweets leading up to a new post is as useful a cue for <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> as the topical context of the post itself. The work opens the door to an empirical exploration not just of <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> in text but of the <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcastic state of mind</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1051/>Identifying Humor in Reviews using Background Text Sources</a></strong><br><a href=/people/a/alex-morales/>Alex Morales</a>
|
<a href=/people/c/chengxiang-zhai/>Chengxiang Zhai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1051><div class="card-body p-3 small">We study the problem of automatically identifying humorous text from a new kind of <a href=https://en.wikipedia.org/wiki/Text_file>text data</a>, i.e., <a href=https://en.wikipedia.org/wiki/Online_review>online reviews</a>. We propose a generative language model, based on the theory of incongruity, to model humorous text, which allows us to leverage background text sources, such as Wikipedia entry descriptions, and enables construction of multiple features for identifying humorous reviews. Evaluation of these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> using <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> for classifying reviews into humorous and non-humorous reviews shows that the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> constructed based on the proposed generative model are much more effective than the major <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> proposed in the existing literature, allowing us to achieve almost 86 % accuracy. These humorous review predictions can also supply good indicators for identifying helpful reviews.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1052/>Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision</a></strong><br><a href=/people/l/leyi-wang/>Leyi Wang</a>
|
<a href=/people/r/rui-xia/>Rui Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1052><div class="card-body p-3 small">Sentiment lexicon is an important tool for identifying the sentiment polarity of words and texts. How to automatically construct sentiment lexicons has become a research topic in the field of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a>. Recently there were some attempts to employ <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning algorithms</a> to construct a sentiment lexicon with sentiment-aware word embedding. However, these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> were normally trained under document-level sentiment supervision. In this paper, we develop a neural architecture to train a sentiment-aware word embedding by integrating the sentiment supervision at both document and word levels, to enhance the quality of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> as well as the sentiment lexicon. Experiments on the SemEval 2013-2016 datasets indicate that the sentiment lexicon generated by our approach achieves the state-of-the-art performance in both supervised and unsupervised sentiment classification, in comparison with several strong sentiment lexicon construction methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1053/>Towards a Universal Sentiment Classifier in Multiple languages</a></strong><br><a href=/people/k/kui-xu/>Kui Xu</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1053><div class="card-body p-3 small">Existing sentiment classifiers usually work for only one specific language, and different classification models are used in different languages. In this paper we aim to build a universal sentiment classifier with a single <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a> in multiple different languages. In order to achieve this goal, we propose to learn multilingual sentiment-aware word embeddings simultaneously based only on the labeled reviews in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and unlabeled parallel data available in a few language pairs. It is not required that the parallel data exist between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and any other language, because the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> can be transferred into any language via pivot languages. We present the evaluation results of our universal sentiment classifier in five languages, and the results are very promising even when the parallel data between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and the target languages are not used. Furthermore, the universal single classifier is compared with a few cross-language sentiment classifiers relying on direct parallel data between the source and target languages, and the results show that the performance of our universal sentiment classifier is very promising compared to that of different cross-language classifiers in multiple target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1054/>Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network</a></strong><br><a href=/people/z/zi-yi-dou/>Zi-Yi Dou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1054><div class="card-body p-3 small">Document-level sentiment classification is a fundamental problem which aims to predict a user&#8217;s overall sentiment about a product in a document. Several methods have been proposed to tackle the problem whereas most of them fail to consider the influence of users who express the sentiment and products which are evaluated. To address the issue, we propose a deep memory network for document-level sentiment classification which could capture the user and product information at the same time. To prove the effectiveness of our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, we conduct experiments on IMDB and Yelp datasets and the results indicate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can achieve better performance than several existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1055/>Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters</a></strong><br><a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/j/jincheng-mei/>Jincheng Mei</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/z/zhou-zhao/>Zhou Zhao</a>
|
<a href=/people/x/xiaojun-chen/>Xiaojun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1055><div class="card-body p-3 small">We study the problem of identifying the topics and sentiments and tracking their shifts from social media texts in different geographical regions during emergencies and disasters. We propose a location-based dynamic sentiment-topic model (LDST) which can jointly model topic, sentiment, time and Geolocation information. The experimental results demonstrate that LDST performs very well at discovering topics and sentiments from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and tracking their shifts in different geographical regions during emergencies and disasters. We will release the data and source code after this work is published.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1056/>Refining Word Embeddings for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/l/liang-chih-yu/>Liang-Chih Yu</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/k/k-robert-lai/>K. Robert Lai</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1056><div class="card-body p-3 small">Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks. However, existing methods for learning context-based word embeddings typically fail to capture sufficient sentiment information. This may result in words with similar <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>vector representations</a> having an opposite sentiment polarity (e.g., good and bad), thus degrading <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> performance. Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., <a href=https://en.wikipedia.org/wiki/Word2vec>Word2vec</a> and GloVe). The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1058/>Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word Embeddings</a></strong><br><a href=/people/r/raksha-sharma/>Raksha Sharma</a>
|
<a href=/people/a/arpan-somani/>Arpan Somani</a>
|
<a href=/people/l/lakshya-kumar/>Lakshya Kumar</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1058><div class="card-body p-3 small">Identification of intensity ordering among polar (positive or negative) words which have the same <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> can lead to a fine-grained sentiment analysis. For example, &#8216;master&#8217;, &#8216;seasoned&#8217; and &#8216;familiar&#8217; point to different intensity levels, though they all convey the same meaning (semantics), i.e., expertise : having a good knowledge of. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised technique</a> that uses sentiment bearing word embeddings to produce a continuous ranking among adjectives that share common semantics. Our system demonstrates a strong Spearman&#8217;s rank correlation of 0.83 with the gold standard ranking. We show that sentiment bearing word embeddings facilitate a more accurate intensity ranking system than other standard word embeddings (word2vec and GloVe). Word2vec is the state-of-the-art for intensity ordering task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1059/>Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary Lookup, and Polarity Association<span class=acl-fixed-case>PU</span> Learning, Double Dictionary Lookup, and Polarity Association</a></strong><br><a href=/people/y/yasheng-wang/>Yasheng Wang</a>
|
<a href=/people/y/yang-zhang/>Yang Zhang</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1059><div class="card-body p-3 small">Although many sentiment lexicons in different languages exist, most are not comprehensive. In a recent sentiment analysis application, we used a large Chinese sentiment lexicon and found that it missed a large number of sentiment words in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This prompted us to make a new attempt to study sentiment lexicon expansion. This paper first poses the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a PU learning problem, which is a new <a href=https://en.wikipedia.org/wiki/Formulation>formulation</a>. It then proposes a new PU learning method suitable for our problem using a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. The results are enhanced further with a new dictionary-based technique and a novel polarity classification technique. Experimental results show that the proposed approach outperforms baseline methods greatly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238232302 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1060" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1060/>DeepPath : A Reinforcement Learning Method for Knowledge Graph Reasoning<span class=acl-fixed-case>D</span>eep<span class=acl-fixed-case>P</span>ath: A Reinforcement Learning Method for Knowledge Graph Reasoning</a></strong><br><a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/t/thien-hoang/>Thien Hoang</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1060><div class="card-body p-3 small">We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths : we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a <a href=https://en.wikipedia.org/wiki/Reward_system>reward function</a> that takes the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, <a href=https://en.wikipedia.org/wiki/Diversity_(business)>diversity</a>, and <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a> into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235599 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1062/>Sentence Simplification with Deep Reinforcement Learning</a></strong><br><a href=/people/x/xingxing-zhang/>Xingxing Zhang</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1062><div class="card-body p-3 small">Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms competitive simplification systems.<b>D</b>eep\n <b>RE</b>inforcement <b>S</b>entence <b>S</b>implification), explores the space of possible\n simplifications while learning to optimize a reward function that\n encourages outputs which are simple, fluent, and preserve the meaning of\n the input. Experiments on three datasets demonstrate that our model\n outperforms competitive simplification systems.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234005 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1063" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1063/>Learning how to Active Learn : A Deep Reinforcement Learning Approach</a></strong><br><a href=/people/m/meng-fang/>Meng Fang</a>
|
<a href=/people/y/yuan-li/>Yuan Li</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1063><div class="card-body p-3 small">Active learning aims to select a small subset of data for <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> such that a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> as a reinforcement learning problem and explicitly learning a data selection policy, where the <a href=https://en.wikipedia.org/wiki/Policy>policy</a> takes the role of the <a href=https://en.wikipedia.org/wiki/Active_learning>active learning heuristic</a>. Importantly, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> allows the selection policy learned using simulation to one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238230265 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1064" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1064/>Split and Rephrase</a></strong><br><a href=/people/s/shashi-narayan/>Shashi Narayan</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1064><div class="card-body p-3 small">We propose a new sentence simplification task (Split-and-Rephrase) where the aim is to split a complex sentence into a meaning preserving sequence of shorter sentences. Like <a href=https://en.wikipedia.org/wiki/Sentence_simplification>sentence simplification</a>, splitting-and-rephrasing has the potential of benefiting both <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and societal applications. Because shorter sentences are generally better processed by <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>, it could be used as a preprocessing step which facilitates and improves the performance of <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>, <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labellers</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>. It should also be of use for people with reading disabilities because <a href=https://en.wikipedia.org/wiki/Italian_language>it</a> allows the conversion of longer sentences into shorter ones. This paper makes two contributions towards this new task. First, we create and make available a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> consisting of 1,066,115 tuples mapping a single complex sentence to a sequence of sentences expressing the same meaning. Second, we propose five <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> (vanilla sequence-to-sequence to semantically-motivated models) to understand the difficulty of the proposed task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1065 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238230011 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1065/>Neural Response Generation via <a href=https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units>GAN</a> with an Approximate Embedding Layer<span class=acl-fixed-case>GAN</span> with an Approximate Embedding Layer</a></strong><br><a href=/people/z/zhen-xu/>Zhen Xu</a>
|
<a href=/people/b/bingquan-liu/>Bingquan Liu</a>
|
<a href=/people/b/baoxun-wang/>Baoxun Wang</a>
|
<a href=/people/c/cheng-jie-sun/>Chengjie Sun</a>
|
<a href=/people/x/xiaolong-wang/>Xiaolong Wang</a>
|
<a href=/people/z/zhuoran-wang/>Zhuoran Wang</a>
|
<a href=/people/c/chao-qi/>Chao Qi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1065><div class="card-body p-3 small">This paper presents a Generative Adversarial Network (GAN) to model single-turn short-text conversations, which trains a sequence-to-sequence (Seq2Seq) network for response generation simultaneously with a discriminative classifier that measures the differences between human-produced responses and machine-generated ones. In addition, the proposed method introduces an approximate embedding layer to solve the non-differentiable problem caused by the sampling-based output decoding procedure in the Seq2Seq generative model. The GAN setup provides an effective way to avoid noninformative responses (a.k.a safe responses), which are frequently observed in traditional neural response generators. The experimental results show that the proposed approach significantly outperforms existing neural response generation models in diversity metrics, with slight increases in <a href=https://en.wikipedia.org/wiki/Relevance_(information_retrieval)>relevance scores</a> as well, when evaluated on both a <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin corpus</a> and an <a href=https://en.wikipedia.org/wiki/English_language>English corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1066 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1066.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238230365 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1066" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1066/>A Hybrid Convolutional Variational Autoencoder for Text Generation</a></strong><br><a href=/people/s/stanislau-semeniuta/>Stanislau Semeniuta</a>
|
<a href=/people/a/aliaksei-severyn/>Aliaksei Severyn</a>
|
<a href=/people/e/erhardt-barth/>Erhardt Barth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1066><div class="card-body p-3 small">In this paper we explore the effect of architectural choices on learning a variational autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid the issue of the VAE collapsing to a deterministic model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234243 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1068" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1068/>Measuring Thematic Fit with Distributional Feature Overlap</a></strong><br><a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a>
|
<a href=/people/p/philippe-blache/>Philippe Blache</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1068><div class="card-body p-3 small">In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional method</a> for modeling predicate-argument thematic fit judgments. We use a syntax-based DSM to build a prototypical representation of verb-specific roles : for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes. Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-the-art <a href=https://en.wikipedia.org/wiki/System>system</a>, and achieves better or comparable results to those reported in the literature for the other <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised systems</a>. Moreover, it provides an explicit representation of the <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> characterizing verb-specific semantic roles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1070 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236002 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1070" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1070/>Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</a></strong><br><a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/h/holger-schwenk/>Holger Schwenk</a>
|
<a href=/people/l/loic-barrault/>Loïc Barrault</a>
|
<a href=/people/a/antoine-bordes/>Antoine Bordes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1070><div class="card-body p-3 small">Many modern NLP systems rely on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, previously trained in an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised manner</a> on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised data</a> of the Stanford Natural Language Inference datasets can consistently outperform <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> like SkipThought vectors on a wide range of transfer tasks. Much like how <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> uses <a href=https://en.wikipedia.org/wiki/ImageNet>ImageNet</a> to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to other NLP tasks. Our <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238232412 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1071" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1071/>Determining Semantic Textual Similarity using Natural Deduction Proofs</a></strong><br><a href=/people/h/hitomi-yanaka/>Hitomi Yanaka</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a>
|
<a href=/people/p/pascual-martinez-gomez/>Pascual Martínez-Gómez</a>
|
<a href=/people/d/daisuke-bekki/>Daisuke Bekki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1071><div class="card-body p-3 small">Determining semantic textual similarity is a core research subject in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Since vector-based models for sentence representation often use shallow information, capturing accurate <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> is difficult. By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of textual similarity. We propose a method for determining semantic textual similarity by combining shallow features with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higher-order automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. Experiments show that our <a href=https://en.wikipedia.org/wiki/System>system</a> was able to outperform other logic-based systems and that <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> derived from the proofs are effective for learning textual similarity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1072/>Multi-Grained Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/c/chen-gong/>Chen Gong</a>
|
<a href=/people/z/zhenghua-li/>Zhenghua Li</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/x/xinzhou-jiang/>Xinzhou Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1072><div class="card-body p-3 small">Traditionally, word segmentation (WS) adopts the single-grained formalism, where a sentence corresponds to a single word sequence. However, Sproat et al. (1997) show that the inter-native-speaker consistency ratio over Chinese word boundaries is only 76 %, indicating single-grained WS (SWS) imposes unnecessary challenges on both manual annotation and statistical modeling. Moreover, WS results of different <a href=https://en.wikipedia.org/wiki/Granularity>granularities</a> can be complementary and beneficial for <a href=https://en.wikipedia.org/wiki/High-level_programming_language>high-level applications</a>. This work proposes and addresses multi-grained WS (MWS). We build a large-scale pseudo MWS dataset for model training and tuning by leveraging the annotation heterogeneity of three SWS datasets. Then we manually annotate 1,500 test sentences with true MWS annotations. Finally, we propose three benchmark approaches by casting MWS as constituent parsing and <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. Experiments and analysis lead to many interesting findings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1073/>Do n’t Throw Those Morphological Analyzers Away Just Yet : Neural Morphological Disambiguation for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a><span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1073><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for Arabic morphological disambiguation based on Recurrent Neural Networks (RNN). We train Long Short-Term Memory (LSTM) cells in several configurations and embedding levels to model the various morphological features. Our experiments show that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> outperform state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> without explicit use of <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. However, adding learning features from a <a href=https://en.wikipedia.org/wiki/Morphological_analysis>morphological analyzer</a> to model the space of possible analyses provides additional improvement. We make use of the resulting <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological models</a> for scoring and ranking the analyses of the morphological analyzer for morphological disambiguation. The results show significant gains in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> across several evaluation metrics. Our <a href=https://en.wikipedia.org/wiki/System>system</a> results in 4.4 % absolute increase over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in full morphological analysis accuracy (30.6 % relative error reduction), and 10.6 % (31.5 % relative error reduction) for out-of-vocabulary words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1074/>Paradigm Completion for Derivational Morphology</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/c/christo-kirov/>Christo Kirov</a>
|
<a href=/people/d/david-yarowsky/>David Yarowsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1074><div class="card-body p-3 small">The generation of complex derived word forms has been an overlooked problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> ; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivational morphology</a>, and introduce the task of <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivational paradigm completion</a> as a parallel to <a href=https://en.wikipedia.org/wiki/Morphological_derivation>inflectional paradigm completion</a>. State-of-the-art neural models adapted from the inflection task are able to learn the range of derivation patterns, and outperform a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>non-neural baseline</a> by 16.4 %. However, due to semantic, historical, and lexical considerations involved in <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivational morphology</a>, future work will be needed to achieve performance parity with inflection-generating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1076/>Do LSTMs really work so well for PoS tagging? A replication study<span class=acl-fixed-case>LSTM</span>s really work so well for <span class=acl-fixed-case>P</span>o<span class=acl-fixed-case>S</span> tagging? – A replication study</a></strong><br><a href=/people/t/tobias-horsmann/>Tobias Horsmann</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1076><div class="card-body p-3 small">A recent study by Plank et al. (2016) found that LSTM-based PoS taggers considerably improve over the current state-of-the-art when evaluated on the corpora of the Universal Dependencies project that use a coarse-grained tagset. We replicate this study using a fresh collection of 27 corpora of 21 languages that are annotated with fine-grained tagsets of varying size. Our replication confirms the result in general, and we additionally find that the advantage of <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> is even bigger for larger tagsets. However, we also find that for the very large tagsets of morphologically rich languages, hand-crafted morphological lexicons are still necessary to reach state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1077/>The Labeled Segmentation of Printed Books</a></strong><br><a href=/people/l/lara-mcconnaughey/>Lara McConnaughey</a>
|
<a href=/people/j/jennifer-dai/>Jennifer Dai</a>
|
<a href=/people/d/david-bamman/>David Bamman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1077><div class="card-body p-3 small">We introduce the task of book structure labeling : segmenting and assigning a fixed category (such as Table of Contents, Preface, Index) to the document structure of printed books. We manually annotate the page-level structural categories for a large dataset totaling 294,816 pages in 1,055 books evenly sampled from 1750-1922, and present empirical results comparing the performance of several classes of models. The best-performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, a bidirectional LSTM with rich features, achieves an overall <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 95.8 and a class-balanced macro F-score of 71.4.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1079/>Word-Context Character Embeddings for Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/z/zhenting-yu/>Zhenting Yu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1079><div class="card-body p-3 small">Neural parsers have benefited from automatically labeled data via dependency-context word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method improves state-of-the-art neural word segmentation models significantly, beating tri-training baselines for leveraging auto-segmented data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1080 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1080/>Segmentation-Free Word Embedding for Unsegmented Languages</a></strong><br><a href=/people/t/takamasa-oshikiri/>Takamasa Oshikiri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1080><div class="card-body p-3 small">In this paper, we propose a new pipeline of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> for unsegmented languages, called segmentation-free word embedding, which does not require <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> as a preprocessing step. Unlike space-delimited languages, unsegmented languages, such as <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, require <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> as a preprocessing step. However, <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>, that often requires manually annotated resources, is difficult and expensive, and unavoidable errors in <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> affect downstream tasks. To avoid these problems in learning word vectors of unsegmented languages, we consider word co-occurrence statistics over all possible candidates of segmentations based on frequent character n-grams instead of segmented sentences provided by conventional word segmenters. Our experiments of noun category prediction tasks on raw Twitter, <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Weibo</a>, and Wikipedia corpora show that the proposed method outperforms the conventional approaches that require <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmenters</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1081/>From Textbooks to Knowledge : A Case Study in Harvesting Axiomatic Knowledge from <a href=https://en.wikipedia.org/wiki/Textbook>Textbooks</a> to Solve Geometry Problems</a></strong><br><a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a>
|
<a href=/people/k/kumar-dubey/>Kumar Dubey</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1081><div class="card-body p-3 small">Textbooks are rich sources of information. Harvesting structured knowledge from <a href=https://en.wikipedia.org/wiki/Textbook>textbooks</a> is a key challenge in many <a href=https://en.wikipedia.org/wiki/Educational_technology>educational applications</a>. As a case study, we present an approach for harvesting structured axiomatic knowledge from math textbooks. Our approach uses rich contextual and typographical features extracted from raw textbooks. It leverages the <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundancy</a> and shared ordering across multiple textbooks to further refine the harvested axioms. These <a href=https://en.wikipedia.org/wiki/Axiom>axioms</a> are then parsed into <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> that are used to improve the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in solving <a href=https://en.wikipedia.org/wiki/Geometry>geometry problems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1083/>Beyond Sentential Semantic Parsing : Tackling the Math SAT with a Cascade of Tree Transducers<span class=acl-fixed-case>SAT</span> with a Cascade of Tree Transducers</a></strong><br><a href=/people/m/mark-hopkins/>Mark Hopkins</a>
|
<a href=/people/c/cristian-petrescu-prahova/>Cristian Petrescu-Prahova</a>
|
<a href=/people/r/roie-levin/>Roie Levin</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/a/alvaro-herrasti/>Alvaro Herrasti</a>
|
<a href=/people/v/vidur-joshi/>Vidur Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1083><div class="card-body p-3 small">We present an approach for answering questions that span multiple sentences and exhibit sophisticated cross-sentence anaphoric phenomena, evaluating on a rich source of such questions the math portion of the Scholastic Aptitude Test (SAT). By using a tree transducer cascade as its basic architecture, our <a href=https://en.wikipedia.org/wiki/System>system</a> propagates uncertainty from multiple sources (e.g. coreference resolution or verb interpretation) until it can be confidently resolved. Experiments show the first-ever results 43 % <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and 91 % <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a>) on SAT algebra word problems. We also apply our system to the public Dolphin algebra question set, and improve the state-of-the-art F1-score from 73.9 % to 77.0 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1084/>Learning Fine-Grained Expressions to Solve Math Word Problems</a></strong><br><a href=/people/d/danqing-huang/>Danqing Huang</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1084><div class="card-body p-3 small">This paper presents a novel template-based method to solve math word problems. This method learns the mappings between math concept phrases in math word problems and their math expressions from training data. For each equation template, we automatically construct a rich template sketch by aggregating information from various problems with the same <a href=https://en.wikipedia.org/wiki/Template_processor>template</a>. Our approach is implemented in a two-stage system. It first retrieves a few relevant equation system templates and aligns numbers in math word problems to those <a href=https://en.wikipedia.org/wiki/Template_processor>templates</a> for candidate equation generation. It then does a fine-grained inference to obtain the final answer. Experiment results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 28.4 % on the linear Dolphin18 K benchmark, which is 10 % (54 % relative) higher than previous state-of-the-art systems while achieving an accuracy increase of 12 % (59 % relative) on the TS6 benchmark subset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1085/>Structural Embedding of Syntactic Trees for Machine Comprehension</a></strong><br><a href=/people/r/rui-liu/>Rui Liu</a>
|
<a href=/people/j/junjie-hu/>Junjie Hu</a>
|
<a href=/people/w/wei-wei/>Wei Wei</a>
|
<a href=/people/z/zi-yang/>Zi Yang</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1085><div class="card-body p-3 small">Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1086/>World Knowledge for Reading Comprehension : Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions<span class=acl-fixed-case>LSTM</span>s Using External Descriptions</a></strong><br><a href=/people/t/teng-long/>Teng Long</a>
|
<a href=/people/e/emmanuel-bengio/>Emmanuel Bengio</a>
|
<a href=/people/r/ryan-lowe/>Ryan Lowe</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/d/doina-precup/>Doina Precup</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1086><div class="card-body p-3 small">Humans interpret texts with respect to some background information, or <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>, and we would like to develop automatic reading comprehension systems that can do the same. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and several <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to drive progress towards this goal. In particular, we propose the task of rare entity prediction : given a web document with several entities removed, models are tasked with predicting the correct missing entities conditioned on the document context and the lexical resources. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is challenging due to the diversity of language styles and the extremely large number of rare entities. We propose two recurrent neural network architectures which make use of <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>external knowledge</a> in the form of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity descriptions</a>. Our experiments show that our hierarchical LSTM model performs significantly better at the rare entity prediction task than those that do not make use of external resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1087" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1087/>Two-Stage Synthesis Networks for <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a> in Machine Comprehension</a></strong><br><a href=/people/d/david-golub/>David Golub</a>
|
<a href=/people/p/po-sen-huang/>Po-Sen Huang</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/l/li-deng/>Li Deng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1087><div class="card-body p-3 small">We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network. Given a high performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs. Using the proposed synthesis network with a pretrained model on the SQuAD dataset, we achieve an <a href=https://en.wikipedia.org/wiki/F-number>F1 measure</a> of 46.6 % on the challenging NewsQA dataset, approaching performance of in-domain models (F1 measure of 50.0 %) and outperforming the out-of-domain baseline by 7.6 %, without use of provided annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1088/>Deep Neural Solver for Math Word Problems</a></strong><br><a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1088><div class="card-body p-3 small">This paper presents a deep neural solver to automatically solve math word problems. In contrast to previous statistical learning approaches, we directly translate math word problems to equation templates using a recurrent neural network (RNN) model, without sophisticated <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. We further design a hybrid model that combines the RNN model and a similarity-based retrieval model to achieve additional performance improvement. Experiments conducted on a large dataset show that the RNN model and the hybrid model significantly outperform state-of-the-art statistical learning methods for math word problem solving.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1090 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1090/>Question Generation for Question Answering</a></strong><br><a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/p/peng-chen/>Peng Chen</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1090><div class="card-body p-3 small">This paper presents how to generate questions from given passages using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, where large scale QA pairs are automatically crawled and processed from Community-QA website, and used as training data. The contribution of the paper is 2-fold : First, two types of question generation approaches are proposed, one is a retrieval-based method using convolution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN) ; Second, we show how to leverage the generated questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> improvement can be achieved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1091 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1091/>Learning to Paraphrase for Question Answering</a></strong><br><a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/j/jonathan-mallinson/>Jonathan Mallinson</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1091><div class="card-body p-3 small">Question answering (QA) systems are sensitive to the many different ways <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> expresses the same information need. In this paper we turn to <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is trained end-to-end using question-answer pairs as a <a href=https://en.wikipedia.org/wiki/Signal_processing>supervision signal</a>. A question and its <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers. We evaluate our approach on <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> over <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> and answer sentence selection. Experimental results on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> consistently improves performance, achieving competitive results despite the use of simple <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1092 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1092/>Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture<span class=acl-fixed-case>LSTM</span>-based Architecture</a></strong><br><a href=/people/y/yuanliang-meng/>Yuanliang Meng</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1092><div class="card-body p-3 small">In this paper, we propose to use a set of simple, uniform in architecture LSTM-based models to recover different kinds of temporal relations from text. Using the shortest dependency path between entities as input, the same architecture is used to extract intra-sentence, cross-sentence, and document creation time relations. A double-checking technique reverses entity pairs in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, boosting the <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> of positive cases and reducing misclassifications between opposite classes. An efficient pruning algorithm resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-of-the-art methods by a large margin. We also conduct intrinsic evaluation and post state-of-the-art results on Timebank-Dense.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1093 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1093/>Ranking Kernels for Structures and Embeddings : A Hybrid Preference and Classification Model</a></strong><br><a href=/people/k/kateryna-tymoshenko/>Kateryna Tymoshenko</a>
|
<a href=/people/d/daniele-bonadiman/>Daniele Bonadiman</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1093><div class="card-body p-3 small">Recent work has shown that Tree Kernels (TKs) and Convolutional Neural Networks (CNNs) obtain the state of the art in answer sentence reranking. Additionally, their combination used in Support Vector Machines (SVMs) is promising as it can exploit both the syntactic patterns captured by TKs and the embeddings learned by CNNs. However, the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are constructed according to a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification function</a>, which is not directly exploitable in the preference ranking algorithm of SVMs. In this work, we propose a new hybrid approach combining preference ranking applied to TKs and pointwise ranking applied to CNNs. We show that our approach produces better results on two well-known and rather different datasets : WikiQA for answer sentence selection and SemEval cQA for comment selection in Community Question Answering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1094 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1094.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1094/>Recovering Question Answering Errors via Query Revision</a></strong><br><a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/i/izzeddin-gur/>Izzeddin Gur</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1094><div class="card-body p-3 small">The existing factoid QA systems often lack a post-inspection component that can help models recover from their own mistakes. In this work, we propose to crosscheck the corresponding <a href=https://en.wikipedia.org/wiki/Binary_relation>KB relations</a> behind the predicted answers and identify potential inconsistencies. Instead of developing a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that accepts evidences collected from these relations, we choose to plug them back to the original questions directly and check if the revised question makes sense or not. A bidirectional LSTM is applied to encode revised questions. We develop a scoring mechanism over the revised question encodings to refine the predictions of a base QA system. This approach can improve the <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a> of <a href=https://en.wikipedia.org/wiki/STAGG>STAGG</a> (Yih et al., 2015), one of the leading QA systems, from 52.5 % to 53.9 % on WEBQUESTIONS data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1095 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1095/>An empirical study on the effectiveness of <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> in Multimodal Neural Machine Translation</a></strong><br><a href=/people/j/jean-benoit-delbrouck/>Jean-Benoit Delbrouck</a>
|
<a href=/people/s/stephane-dupont/>Stéphane Dupont</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1095><div class="card-body p-3 small">In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a>. At every step, the <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a> uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multi-modal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multi-modal translation task (English, image German) and evaluate the ability of the model to make use of images to improve <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while <a href=https://en.wikipedia.org/wiki/Translation>translating</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1096/>Sound-Word2Vec : Learning Word Representations Grounded in Sounds<span class=acl-fixed-case>W</span>ord2<span class=acl-fixed-case>V</span>ec: Learning Word Representations Grounded in Sounds</a></strong><br><a href=/people/a/ashwin-vijayakumar/>Ashwin Vijayakumar</a>
|
<a href=/people/r/ramakrishna-vedantam/>Ramakrishna Vedantam</a>
|
<a href=/people/d/devi-parikh/>Devi Parikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1096><div class="card-body p-3 small">To be able to interact better with humans, it is crucial for machines to understand sound a primary modality of human perception. Previous works have used <a href=https://en.wikipedia.org/wiki/Sound>sound</a> to learn <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for improved generic semantic similarity assessment. In this work, we treat <a href=https://en.wikipedia.org/wiki/Sound>sound</a> as a first-class citizen, studying downstream 6textual tasks which require aural grounding. To this end, we propose sound-word2vec a new embedding scheme that learns specialized word embeddings grounded in sounds. For example, we learn that two seemingly (semantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make. Our embeddings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering <a href=https://en.wikipedia.org/wiki/Foley_(filmmaking)>Foley sound effects</a> (used in movies). Moreover, our embedding space captures interesting dependencies between words and <a href=https://en.wikipedia.org/wiki/Onomatopoeia>onomatopoeia</a> and outperforms prior work on aurally-relevant word relatedness datasets such as AMEN and ASLex.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1097/>The Promise of Premise : Harnessing Question Premises in Visual Question Answering</a></strong><br><a href=/people/a/aroma-mahendru/>Aroma Mahendru</a>
|
<a href=/people/v/viraj-prabhu/>Viraj Prabhu</a>
|
<a href=/people/a/akrit-mohapatra/>Akrit Mohapatra</a>
|
<a href=/people/d/dhruv-batra/>Dhruv Batra</a>
|
<a href=/people/s/stefan-lee/>Stefan Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1097><div class="card-body p-3 small">In this paper, we make a simple observation that questions about images often contain premises objects and relationships implied by the question and that reasoning about premises can help Visual Question Answering (VQA) models respond more intelligently to irrelevant or previously unseen questions. When presented with a question that is irrelevant to an <a href=https://en.wikipedia.org/wiki/Image>image</a>, state-of-the-art VQA models will still answer purely based on learned language biases, resulting in non-sensical or even misleading answers. We note that a visual question is irrelevant to an <a href=https://en.wikipedia.org/wiki/Image>image</a> if at least one of its premises is false (i.e. not depicted in the image). We leverage this observation to construct a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for Question Relevance Prediction and Explanation (QRPE) by searching for false premises. We train novel question relevance detection models and show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that reason about premises consistently outperform <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that do not. We also find that forcing standard VQA models to reason about premises during training can lead to improvements on tasks requiring compositional reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1098.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1098/>Guided Open Vocabulary Image Captioning with Constrained Beam Search</a></strong><br><a href=/people/p/peter-anderson/>Peter Anderson</a>
|
<a href=/people/b/basura-fernando/>Basura Fernando</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a>
|
<a href=/people/s/stephen-gould/>Stephen Gould</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1098><div class="card-body p-3 small">Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the <a href=https://en.wikipedia.org/wiki/Machine_learning>learning algorithm</a>. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1099" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1099/>Zero-Shot Activity Recognition with Verb Attribute Induction</a></strong><br><a href=/people/r/rowan-zellers/>Rowan Zellers</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1099><div class="card-body p-3 small">In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs. For example, the verb salute has several properties, such as being a light movement, a <a href=https://en.wikipedia.org/wiki/Social_actions>social act</a>, and short in duration. We use these attributes as the internal mapping between visual and textual representations to reason about a previously unseen action. In contrast to much prior work that assumes access to gold standard attributes for zero-shot classes and focuses primarily on object attributes, our model uniquely learns to infer action attributes from dictionary definitions and distributed word representations. Experimental results confirm that action attributes inferred from <a href=https://en.wikipedia.org/wiki/Language>language</a> can provide a predictive signal for zero-shot prediction of previously unseen activities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1100 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1100/>Deriving continous grounded meaning representations from referentially structured multimodal contexts</a></strong><br><a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1100><div class="card-body p-3 small">Corpora of referring expressions paired with their visual referents are a good source for learning word meanings directly grounded in visual representations. Here, we explore additional ways of extracting from them word representations linked to multi-modal context : through expressions that refer to the same object, and through expressions that refer to different objects in the same scene. We show that continuous meaning representations derived from these contexts capture complementary aspects of similarity,, even if not outperforming textual embeddings trained on very large amounts of raw text when tested on standard similarity benchmarks. We propose a new task for evaluating grounded meaning representationsdetection of potentially co-referential phrasesand show that it requires precise denotational representations of attribute meanings, which our method provides.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1102.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1102/>Video Highlight Prediction Using Audience Chat Reactions</a></strong><br><a href=/people/c/cheng-yang-fu/>Cheng-Yang Fu</a>
|
<a href=/people/j/joon-lee/>Joon Lee</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/a/alexander-berg/>Alexander Berg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1102><div class="card-body p-3 small">Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis. We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and traditional Chinese. We present a novel dataset based on <a href=https://en.wikipedia.org/wiki/League_of_Legends_World_Championship>League of Legends championships</a> recorded from North American and Taiwanese Twitch.tv channels (will be released for further research), and demonstrate strong results on these using multimodal, character-level CNN-RNN model architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1104.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1104/>Evaluating Hierarchies of Verb Argument Structure with <a href=https://en.wikipedia.org/wiki/Hierarchical_clustering>Hierarchical Clustering</a></a></strong><br><a href=/people/j/jesse-mu/>Jesse Mu</a>
|
<a href=/people/j/joshua-k-hartshorne/>Joshua K. Hartshorne</a>
|
<a href=/people/t/timothy-odonnell/>Timothy O’Donnell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1104><div class="card-body p-3 small">Verbs can only be used with a few specific arrangements of their arguments (syntactic frames). Most theorists note that verbs can be organized into a hierarchy of verb classes based on the frames they admit. Here we show that such a hierarchy is objectively well-supported by the patterns of verbs and frames in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, since a systematic hierarchical clustering algorithm converges on the same structure as the handcrafted taxonomy of VerbNet, a broad-coverage verb lexicon. We also show that the <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchies</a> capture meaningful psychological dimensions of generalization by predicting novel verb coercions by human participants. We discuss limitations of a simple hierarchical representation and suggest similar approaches for identifying the representations underpinning verb argument structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1105 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1105/>Incorporating Global Visual Features into Attention-based Neural Machine Translation.</a></strong><br><a href=/people/i/iacer-calixto/>Iacer Calixto</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1105><div class="card-body p-3 small">We introduce multi-modal, attention-based neural machine translation (NMT) models which incorporate visual features into different parts of both the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>. Global image features are extracted using a pre-trained convolutional neural network and are incorporated (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate translations into <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, how different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional <a href=https://en.wikipedia.org/wiki/Data>data</a> have a positive impact on multi-modal NMT models. We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1106.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1106" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1106/>Mapping Instructions and Visual Observations to Actions with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/d/dipendra-misra/>Dipendra Misra</a>
|
<a href=/people/j/john-langford/>John Langford</a>
|
<a href=/people/y/yoav-artzi/>Yoav Artzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1106><div class="card-body p-3 small">We propose to directly map raw visual observations and <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text input</a> to actions for <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>instruction execution</a>. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to jointly reason about linguistic and visual input. We use <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> in a contextual bandit setting to train a neural network agent. To guide the agent&#8217;s exploration, we use reward shaping with different forms of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>. Our approach does not require intermediate representations, planning procedures, or training different <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. We evaluate in a simulated environment, and show significant improvements over <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> and common reinforcement learning variants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1107/>An analysis of <a href=https://en.wikipedia.org/wiki/Eye_movement>eye-movements</a> during reading for the detection of mild cognitive impairment</a></strong><br><a href=/people/k/kathleen-c-fraser/>Kathleen C. Fraser</a>
|
<a href=/people/k/kristina-lundholm-fors/>Kristina Lundholm Fors</a>
|
<a href=/people/d/dimitrios-kokkinakis/>Dimitrios Kokkinakis</a>
|
<a href=/people/a/arto-nordlund/>Arto Nordlund</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1107><div class="card-body p-3 small">We present a machine learning analysis of eye-tracking data for the detection of mild cognitive impairment, a decline in cognitive abilities that is associated with an increased risk of developing dementia. We compare two experimental configurations (reading aloud versus reading silently), as well as two methods of combining information from the two trials (concatenation and merging). Additionally, we annotate the words being read with information about their frequency and <a href=https://en.wikipedia.org/wiki/Syntactic_category>syntactic category</a>, and use these annotations to generate new <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a>. Ultimately, we are able to distinguish between participants with and without <a href=https://en.wikipedia.org/wiki/Cognitive_deficit>cognitive impairment</a> with up to 86 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231266 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1108/>A Structured Learning Approach to Temporal Relation Extraction</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/z/zhili-feng/>Zhili Feng</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1108><div class="card-body p-3 small">Identifying temporal relations between events is an essential step towards <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. However, the temporal relation between two events in a story depends on, and is often dictated by, relations among other events. Consequently, effectively identifying temporal relations between events is a challenging problem even for human annotators. This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge. As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a>. As we show, the proposed approach results in significant improvements on the two commonly used <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> for this problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233745 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1109/>Importance sampling for unbiased on-demand evaluation of knowledge base population</a></strong><br><a href=/people/a/arun-chaganty/>Arun Chaganty</a>
|
<a href=/people/a/ashwin-paranjape/>Ashwin Paranjape</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1109><div class="card-body p-3 small">Knowledge base population (KBP) systems take in a large document corpus and extract entities and their relations. Thus far, KBP evaluation has relied on judgements on the pooled predictions of existing systems. We show that this <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is problematic : when a new system predicts a previously unseen relation, it is penalized even if it is correct. This leads to significant bias against new systems, which counterproductively discourages innovation in the field. Our first contribution is a new importance-sampling based evaluation which corrects for this bias by annotating a new system&#8217;s predictions on-demand via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We show this eliminates bias and reduces variance using data from the 2015 TAC KBP task. Our second contribution is an implementation of our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> made publicly available as an online KBP evaluation service. We pilot the service by testing diverse state-of-the-art systems on the TAC KBP 2016 corpus and obtain accurate scores in a cost effective manner.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235171 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1110" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1110/>PACRR : A Position-Aware Neural IR Model for Relevance Matching<span class=acl-fixed-case>PACRR</span>: A Position-Aware Neural <span class=acl-fixed-case>IR</span> Model for Relevance Matching</a></strong><br><a href=/people/k/kai-hui/>Kai Hui</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/k/klaus-berberich/>Klaus Berberich</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1110><div class="card-body p-3 small">In order to adopt <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> for <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are needed that can capture all relevant information required to assess the relevance of a document to a given user query. While previous works have successfully captured unigram term matches, how to fully employ position-dependent information such as proximity and term dependencies has been insufficiently explored. In this work, we propose a novel neural IR model named PACRR aiming at better modeling position-dependent interactions between a query and a document. Extensive experiments on six years&#8217; TREC Web Track data confirm that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields better results under multiple benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233236 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1111/>Globally Normalized Reader</a></strong><br><a href=/people/j/jonathan-raiman/>Jonathan Raiman</a>
|
<a href=/people/j/john-miller/>John Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1111><div class="card-body p-3 small">Rapid progress has been made towards question answering (QA) systems that can extract answers from text. Existing neural approaches make use of expensive bi-directional attention mechanisms or score all possible answer spans, limiting <a href=https://en.wikipedia.org/wiki/Scalability>scalability</a>. We propose instead to cast extractive QA as an iterative search problem : select the answer&#8217;s sentence, start word, and end word. This representation reduces the space of each search step and allows computation to be conditionally allocated to promising search paths. We show that globally normalizing the <a href=https://en.wikipedia.org/wiki/Decision-making>decision process</a> and back-propagating through <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> makes this <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> viable and learning efficient. We empirically demonstrate the benefits of this approach using our model, Globally Normalized Reader (GNR), which achieves the second highest single model performance on the Stanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow. We also introduce a data-augmentation method to produce semantically valid examples by aligning named entities to a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> and swapping them with new entities of the same type. This method improves the performance of all <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> considered in this work and is of independent interest for a variety of NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1112.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235902 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1112/>Speech segmentation with a neural encoder model of working memory</a></strong><br><a href=/people/m/micha-elsner/>Micha Elsner</a>
|
<a href=/people/c/cory-shain/>Cory Shain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1112><div class="card-body p-3 small">We present the first unsupervised LSTM speech segmenter as a <a href=https://en.wikipedia.org/wiki/Cognitive_model>cognitive model</a> of the acquisition of words from unsegmented input. Cognitive biases toward phonological and syntactic predictability in <a href=https://en.wikipedia.org/wiki/Speech>speech</a> are rooted in the limitations of <a href=https://en.wikipedia.org/wiki/Memory>human memory</a> (Baddeley et al., 1998) ; compressed representations are easier to acquire and retain in memory. To model the biases introduced by these memory limitations, our system uses an LSTM-based encoder-decoder with a small number of hidden units, then searches for a segmentation that minimizes autoencoding loss. Linguistically meaningful segments (e.g. words) should share regular patterns of features that facilitate decoder performance in comparison to random segmentations, and we show that our learner discovers these <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> when trained on either phoneme sequences or raw acoustics. To our knowledge, ours is the first fully <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised system</a> to be able to segment both symbolic and acoustic representations of speech.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235824 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1113/>Speaking, Seeing, Understanding : Correlating semantic models with conceptual representation in the brain</a></strong><br><a href=/people/l/luana-bulat/>Luana Bulat</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1113><div class="card-body p-3 small">Research in <a href=https://en.wikipedia.org/wiki/Computational_semantics>computational semantics</a> is increasingly guided by our understanding of human semantic processing. However, semantic models are typically studied in the context of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing system</a> performance. In this paper, we present a systematic evaluation and comparison of a range of widely-used, state-of-the-art semantic models in their ability to predict patterns of conceptual representation in the human brain. Our results provide new insights both for the design of computational semantic models and for further research in <a href=https://en.wikipedia.org/wiki/Cognitive_neuroscience>cognitive neuroscience</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1114 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234442 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1114/>Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video</a></strong><br><a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/j/junnan-zhu/>Junnan Zhu</a>
|
<a href=/people/c/cong-ma/>Cong Ma</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1114><div class="card-body p-3 small">The rapid increase of the multimedia data over the <a href=https://en.wikipedia.org/wiki/Internet>Internet</a> necessitates multi-modal summarization from collections of text, image, audio and video. In this work, we propose an extractive Multi-modal Summarization (MMS) method which can automatically generate a textual summary given a set of <a href=https://en.wikipedia.org/wiki/Document>documents</a>, <a href=https://en.wikipedia.org/wiki/Image>images</a>, audios and <a href=https://en.wikipedia.org/wiki/Video>videos</a> related to a specific topic. The key idea is to bridge the semantic gaps between multi-modal contents. For <a href=https://en.wikipedia.org/wiki/Audio_signal>audio information</a>, we design an approach to selectively use its <a href=https://en.wikipedia.org/wiki/Transcription_(biology)>transcription</a>. For vision information, we learn joint representations of texts and images using a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. Finally, all the multi-modal aspects are considered to generate the textural summary by maximizing the salience, <a href=https://en.wikipedia.org/wiki/Redundancy_(engineering)>non-redundancy</a>, <a href=https://en.wikipedia.org/wiki/Readability>readability</a> and <a href=https://en.wikipedia.org/wiki/Coverage_(statistics)>coverage</a> through budgeted optimization of submodular functions. We further introduce an <a href=https://en.wikipedia.org/wiki/Multimedia_Messaging_Service>MMS corpus</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. The experimental results on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms other competitive baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236114 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1115/>Tensor Fusion Network for Multimodal Sentiment Analysis</a></strong><br><a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/m/minghai-chen/>Minghai Chen</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1115><div class="card-body p-3 small">Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of <a href=https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis>multimodal sentiment analysis</a> as modeling intra-modality and inter-modality dynamics. We introduce a novel <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, termed Tensor Fusion Networks, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in <a href=https://en.wikipedia.org/wiki/Online_video_platform>online videos</a> as well as accompanying gestures and <a href=https://en.wikipedia.org/wiki/Human_voice>voice</a>. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1116.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231805 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1116/>ConStance : Modeling Annotation Contexts to Improve Stance Classification<span class=acl-fixed-case>C</span>on<span class=acl-fixed-case>S</span>tance: Modeling Annotation Contexts to Improve Stance Classification</a></strong><br><a href=/people/k/kenneth-joseph/>Kenneth Joseph</a>
|
<a href=/people/l/lisa-friedland/>Lisa Friedland</a>
|
<a href=/people/w/william-hobbs/>William Hobbs</a>
|
<a href=/people/d/david-lazer/>David Lazer</a>
|
<a href=/people/o/oren-tsur/>Oren Tsur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1116><div class="card-body p-3 small">Manual annotations are a prerequisite for many applications of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. However, weaknesses in the <a href=https://en.wikipedia.org/wiki/Annotation>annotation process</a> itself are easy to overlook. In particular, scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, and stance detection, such choices can impact results. Here, for the task of political stance detection on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, we show that providing too little context can result in noisy and uncertain annotations, whereas providing too strong a <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> may cause it to outweigh other signals. To characterize and reduce these biases, we develop ConStance, a general <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for reasoning about <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> across information conditions. Given conflicting labels produced by multiple annotators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> for new instances. We show that the classifier learned by ConStance outperforms a variety of baselines at predicting political stance, while the model&#8217;s interpretable parameters shed light on the effects of each context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1117 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238232251 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1117/>Deeper Attention to Abusive User Content Moderation</a></strong><br><a href=/people/j/john-pavlopoulos/>John Pavlopoulos</a>
|
<a href=/people/p/prodromos-malakasiotis/>Prodromos Malakasiotis</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1117><div class="card-body p-3 small">Experimenting with a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 1.6 M user comments from a news portal and an existing <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 115 K Wikipedia talk page comments, we show that an RNN operating on word embeddings outpeforms the previous state of the art in moderation, which used <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> or an MLP classifier with character or word n-grams. We also compare against a <a href=https://en.wikipedia.org/wiki/CNN>CNN</a> operating on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, and a word-list baseline. A novel, deep, classificationspecific attention mechanism improves the performance of the <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>RNN</a> further, and can also highlight suspicious words for free, without including highlighted words in the training data. We consider both fully automatic and semi-automatic moderation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235126 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1118/>Outta Control : Laws of Semantic Change and Inherent Biases in Word Representation Models</a></strong><br><a href=/people/h/haim-dubossarsky/>Haim Dubossarsky</a>
|
<a href=/people/d/daphna-weinshall/>Daphna Weinshall</a>
|
<a href=/people/e/eitan-grossman/>Eitan Grossman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1118><div class="card-body p-3 small">This article evaluates three proposed laws of semantic change. Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition, in which no change can possibly have taken place. Our analysis shows that the effects reported in recent literature must be substantially revised : (i) the proposed negative correlation between meaning change and <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequency</a> is shown to be largely an artefact of the models of word representation used ; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art ; and (iii) the proposed positive correlation between meaning change and <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> is largely an artefact of <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequency</a>. These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequency</a>, and thus <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequency</a> can not be evaluated as an independent factor with these representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233454 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1119/>Human Centered NLP with User-Factor Adaptation<span class=acl-fixed-case>NLP</span> with User-Factor Adaptation</a></strong><br><a href=/people/v/veronica-lynn/>Veronica Lynn</a>
|
<a href=/people/y/youngseo-son/>Youngseo Son</a>
|
<a href=/people/v/vivek-kulkarni/>Vivek Kulkarni</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1119><div class="card-body p-3 small">We pose the general task of user-factor adaptation adapting supervised learning models to real-valued user factors inferred from a background of their language, reflecting the idea that a piece of text should be understood within the context of the user that wrote it. We introduce a continuous adaptation technique, suited for real-valued user factors that are common in <a href=https://en.wikipedia.org/wiki/Social_science>social science</a> and bringing us closer to personalized NLP, adapting to each user uniquely. We apply this technique with known user factors including age, gender, and personality traits, as well as latent factors, evaluating over five tasks : POS tagging, PP-attachment, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, sarcasm detection, and stance detection. Adaptation provides statistically significant benefits for 3 of the 5 tasks : up to +1.2 points for PP-attachment, +3.4 points for <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, and +3.0 points for <a href=https://en.wikipedia.org/wiki/List_of_human_positions>stance</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1120 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1120/>Neural Sequence Learning Models for Word Sense Disambiguation</a></strong><br><a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/c/claudio-delli-bovi/>Claudio Delli Bovi</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1120><div class="card-body p-3 small">Word Sense Disambiguation models exist in many flavors. Even though supervised ones tend to perform best in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, they often lose ground to more flexible knowledge-based solutions, which do not require training by a word expert for every <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>disambiguation target</a>. To bridge this gap we adopt a different perspective and rely on <a href=https://en.wikipedia.org/wiki/Sequence_learning>sequence learning</a> to frame the disambiguation problem : we propose and study in depth a series of end-to-end neural architectures directly tailored to the task, from bidirectional Long Short-Term Memory to encoder-decoder models. Our extensive evaluation over standard benchmarks and in multiple languages shows that <a href=https://en.wikipedia.org/wiki/Sequence_learning>sequence learning</a> enables more versatile all-words models that consistently lead to state-of-the-art results, even against word experts with engineered features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1121.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1121" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1121/>Learning Word Relatedness over Time</a></strong><br><a href=/people/g/guy-d-rosin/>Guy D. Rosin</a>
|
<a href=/people/e/eytan-adar/>Eytan Adar</a>
|
<a href=/people/k/kira-radinsky/>Kira Radinsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1121><div class="card-body p-3 small">Search systems are often focused on providing relevant results for the now, assuming both corpora and user needs that focus on the present. However, many corpora today reflect significant longitudinal collections ranging from 20 years of the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a> to hundreds of years of digitized newspapers and books. Understanding the temporal intent of the user and retrieving the most relevant historical content has become a significant challenge. Common search features, such as <a href=https://en.wikipedia.org/wiki/Query_expansion>query expansion</a>, leverage the relationship between terms but can not function well across all times when relationships vary temporally. In this work, we introduce a temporal relationship model that is extracted from <a href=https://en.wikipedia.org/wiki/Longitudinal_study>longitudinal data collections</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> supports the task of identifying, given two words, when they relate to each other. We present an algorithmic framework for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and show its application for the task of <a href=https://en.wikipedia.org/wiki/Query_expansion>query expansion</a>, achieving high gain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1122 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1122/>Inter-Weighted Alignment Network for Sentence Pair Modeling</a></strong><br><a href=/people/g/gehui-shen/>Gehui Shen</a>
|
<a href=/people/y/yunlun-yang/>Yunlun Yang</a>
|
<a href=/people/z/zhi-hong-deng/>Zhi-Hong Deng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1122><div class="card-body p-3 small">Sentence pair modeling is a crucial problem in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to measure the similarity of a sentence pair focusing on the <a href=https://en.wikipedia.org/wiki/Interaction_information>interaction information</a>. We utilize the word level similarity matrix to discover fine-grained alignment of two sentences. It should be emphasized that each word in a sentence has a different importance from the perspective of semantic composition, so we exploit two novel and efficient strategies to explicitly calculate a weight for each word. Although the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> only use a sequential LSTM for sentence modeling without any external resource such as syntactic parser tree and additional lexicon features, experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on three datasets of two tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1123 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1123/>A Short Survey on Taxonomy Learning from Text Corpora : Issues, Resources and Recent Advances</a></strong><br><a href=/people/c/chengyu-wang/>Chengyu Wang</a>
|
<a href=/people/x/xiaofeng-he/>Xiaofeng He</a>
|
<a href=/people/a/aoying-zhou/>Aoying Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1123><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a> is a semantic hierarchy, consisting of concepts linked by is-a relations. While a large number of <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomies</a> have been constructed from <a href=https://en.wikipedia.org/wiki/Web_of_Science>human-compiled resources</a> (e.g., Wikipedia), learning <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomies</a> from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> has received a growing interest and is essential for long-tailed and domain-specific knowledge acquisition. In this paper, we overview recent advances on <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomy construction</a> from free texts, reorganizing relevant subtasks into a complete <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>. We also overview resources for evaluation and discuss challenges for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1124 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1124/>Idiom-Aware Compositional Distributed Semantics</a></strong><br><a href=/people/p/pengfei-liu/>Pengfei Liu</a>
|
<a href=/people/k/kaiyu-qian/>Kaiyu Qian</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1124><div class="card-body p-3 small">Idioms are peculiar linguistic constructions that impose great challenges for representing the <a href=https://en.wikipedia.org/wiki/Semantics>semantics of language</a>, especially in current prevailing end-to-end neural models, which assume that the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of a phrase or sentence can be literally composed from its constitutive words. In this paper, we propose an idiom-aware distributed semantic model to build representation of sentences on the basis of understanding their contained idioms. Our models are grounded in the literal-first psycholinguistic hypothesis, which can adaptively learn semantic compositionality of a phrase literally or idiomatically. To better evaluate our models, we also construct an idiom-enriched sentiment classification dataset with considerable scale and abundant peculiarities of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a>. The qualitative and quantitative experimental analyses demonstrate the efficacy of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1125 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1125.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1125" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1125/>Macro Grammars and Holistic Triggering for Efficient Semantic Parsing</a></strong><br><a href=/people/y/yuchen-zhang/>Yuchen Zhang</a>
|
<a href=/people/p/panupong-pasupat/>Panupong Pasupat</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1125><div class="card-body p-3 small">To learn a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> from denotations, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WikiTableQuestions dataset, we first expand the search space of an existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to improve the state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> from 38.7 % to 42.7 %, and then use <a href=https://en.wikipedia.org/wiki/Macro_(computer_science)>macro grammars</a> and holistic triggering to achieve an 11x speedup and an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 43.7 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1126/>A Continuously Growing Dataset of Sentential Paraphrases</a></strong><br><a href=/people/w/wuwei-lan/>Wuwei Lan</a>
|
<a href=/people/s/siyu-qiu/>Siyu Qiu</a>
|
<a href=/people/h/hua-he/>Hua He</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1126><div class="card-body p-3 small">A major challenge in paraphrase research is the lack of <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a>. In this paper, we present a new method to collect large-scale sentential paraphrases from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> by linking tweets through shared URLs. The main advantage of our method is its simplicity, as it gets rid of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> or human in the loop needed to select data before <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> and subsequent application of paraphrase identification algorithms in the previous work. We present the largest human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification. In addition, we show that more than 30,000 new sentential paraphrases can be easily and continuously captured every month at ~70 % precision, and demonstrate their utility for downstream NLP tasks through phrasal paraphrase extraction. We make our code and data freely available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1128 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1128/>A Joint Sequential and Relational Model for Frame-Semantic Parsing</a></strong><br><a href=/people/b/bishan-yang/>Bishan Yang</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1128><div class="card-body p-3 small">We introduce a new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> for frame-semantic parsing that significantly improves the prior state of the art. Our model leverages the advantages of a deep bidirectional LSTM network which predicts semantic role labels word by word and a relational network which predicts semantic roles for individual text expressions in relation to a predicate. The two networks are integrated into a single <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> via knowledge distillation, and a unified graphical model is employed to jointly decode frames and semantic roles during <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. Experiments on the standard FrameNet data show that our model significantly outperforms existing neural and non-neural approaches, achieving a 5.7 F1 gain over the current state of the art, for full frame structure extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1129 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1129/>Getting the Most out of AMR Parsing<span class=acl-fixed-case>AMR</span> Parsing</a></strong><br><a href=/people/c/chuan-wang/>Chuan Wang</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1129><div class="card-body p-3 small">This paper proposes to tackle the AMR parsing bottleneck by improving two components of an AMR parser : concept identification and alignment. We first build a Bidirectional LSTM based concept identifier that is able to incorporate richer contextual information to learn sparse AMR concept labels. We then extend an HMM-based word-to-concept alignment model with graph distance distortion and a rescoring method during decoding to incorporate the structural information in the AMR graph. We show integrating the two components into an existing AMR parser results in consistently better performance over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> on various datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1131 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1131/>An End-to-End Deep Framework for Answer Triggering with a Novel Group-Level Objective</a></strong><br><a href=/people/j/jie-zhao/>Jie Zhao</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/z/ziyu-guan/>Ziyu Guan</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1131><div class="card-body p-3 small">Given a question and a set of answer candidates, answer triggering determines whether the candidate set contains any correct answers. If yes, it then outputs a correct one. In contrast to existing pipeline methods which first consider individual candidate answers separately and then make a prediction based on a threshold, we propose an end-to-end deep neural network framework, which is trained by a novel group-level objective function that directly optimizes the answer triggering performance. Our <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> penalizes three potential types of error and allows training the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> in an end-to-end manner. Experimental results on the WikiQA benchmark show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> outperforms the <a href=https://en.wikipedia.org/wiki/State_(polity)>state</a> of the arts by a 6.6 % absolute gain under F1 measure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1132 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1132/>Predicting Word Association Strengths</a></strong><br><a href=/people/a/andrew-cattle/>Andrew Cattle</a>
|
<a href=/people/x/xiaojuan-ma/>Xiaojuan Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1132><div class="card-body p-3 small">This paper looks at the task of predicting word association strengths across three datasets ; WordNet Evocation (Boyd-Graber et al., 2006), University of Southern Florida Free Association norms (Nelson et al., 2004), and Edinburgh Associative Thesaurus (Kiss et al., 1973). We achieve results of r=0.357 and p=0.379, r=0.344 and p=0.300, and r=0.292 and p=0.363, respectively. We find Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) cosine similarities, as well as vector offsets, to be the highest performing features. Furthermore, we examine the usefulness of Gaussian embeddings (Vilnis and McCallum, 2014) for predicting word association strength, the first work to do so.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1133 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1133/>Learning Contextually Informed Representations for Linear-Time Discourse Parsing</a></strong><br><a href=/people/y/yang-liu-edinburgh/>Yang Liu</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1133><div class="card-body p-3 small">Recent advances in RST discourse parsing have focused on two modeling paradigms : (a) high order parsers which jointly predict the tree structure of the discourse and the relations it encodes ; or (b) linear-time parsers which are efficient but mostly based on local features. In this work, we propose a linear-time parser with a novel way of representing <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>discourse constituents</a> based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> which takes into account global contextual information and is able to capture long-distance dependencies. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> obtains state-of-the art performance on benchmark datasets, while being efficient (with <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a> linear in the number of sentences in the document) and requiring minimal feature engineering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1134 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1134/>Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification</a></strong><br><a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/j/jianxiang-wang/>Jianxiang Wang</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/z/zheng-yu-niu/>Zheng-Yu Niu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1134><div class="card-body p-3 small">We present a novel multi-task attention based neural network model to address implicit discourse relationship representation and identification through two types of <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>, an attention based neural network for learning discourse relationship representation with two arguments and a multi-task framework for learning knowledge from annotated and unannotated corpora. The extensive experiments have been performed on two benchmark corpora (i.e., PDTB and CoNLL-2016 datasets). Experimental results show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art systems on benchmark corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1135 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1135/>Chinese Zero Pronoun Resolution with Deep Memory Network<span class=acl-fixed-case>C</span>hinese Zero Pronoun Resolution with Deep Memory Network</a></strong><br><a href=/people/q/qingyu-yin/>Qingyu Yin</a>
|
<a href=/people/y/yu-zhang/>Yu Zhang</a>
|
<a href=/people/w/weinan-zhang/>Weinan Zhang</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1135><div class="card-body p-3 small">Existing approaches for Chinese zero pronoun resolution typically utilize only syntactical and lexical features while ignoring semantic information. The fundamental reason is that zero pronouns have no descriptive information, which brings difficulty in explicitly capturing their semantic similarities with antecedents. Meanwhile, representing zero pronouns is challenging since they are merely gaps that convey no actual content. In this paper, we address this issue by building a deep memory network that is capable of encoding zero pronouns into vector representations with information obtained from their contexts and potential antecedents. Consequently, our resolver takes advantage of <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> by using these continuous distributed representations. Experiments on the OntoNotes 5.0 dataset show that the proposed memory network could substantially outperform the state-of-the-art systems in various experimental settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1136 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1136/>How much progress have we made on RST discourse parsing? A replication study of recent results on the RST-DT<span class=acl-fixed-case>RST</span> discourse parsing? A replication study of recent results on the <span class=acl-fixed-case>RST</span>-<span class=acl-fixed-case>DT</span></a></strong><br><a href=/people/m/mathieu-morey/>Mathieu Morey</a>
|
<a href=/people/p/philippe-muller/>Philippe Muller</a>
|
<a href=/people/n/nicholas-asher/>Nicholas Asher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1136><div class="card-body p-3 small">This article evaluates purported progress over the past years in RST discourse parsing. Several studies report a relative error reduction of 24 to 51 % on all metrics that authors attribute to the introduction of distributed representations of discourse units. We replicate the standard evaluation of 9 <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>, 5 of which use distributed representations, from 8 studies published between 2013 and 2017, using their predictions on the test set of the RST-DT. Our main finding is that most recently reported increases in RST discourse parser performance are an artefact of differences in implementations of the evaluation procedure. We evaluate all these <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> with the standard Parseval procedure to provide a more accurate picture of the actual RST discourse parsers performance in standard evaluation settings. Under this more stringent procedure, the gains attributable to <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> represent at most a 16 % relative error reduction on fully-labelled structures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1137/>What is it? Disambiguating the different readings of the pronoun ‘it’</a></strong><br><a href=/people/s/sharid-loaiciga/>Sharid Loáiciga</a>
|
<a href=/people/l/liane-guillou/>Liane Guillou</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1137><div class="card-body p-3 small">In this paper, we address the problem of predicting one of three functions for the English pronoun &#8216;it&#8217; : <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphoric</a>, event reference or <a href=https://en.wikipedia.org/wiki/Pleonasm>pleonastic</a>. This <a href=https://en.wikipedia.org/wiki/Disambiguation>disambiguation</a> is valuable in the context of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. We present experiments using a MAXENT classifier trained on <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold-standard data</a> and self-training experiments of an RNN trained on silver-standard data, annotated using the MAXENT classifier. Lastly, we report on an analysis of the strengths of these two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1138 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1138/>Revisiting Selectional Preferences for Coreference Resolution</a></strong><br><a href=/people/b/benjamin-heinzerling/>Benjamin Heinzerling</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1138><div class="card-body p-3 small">Selectional preferences have long been claimed to be essential for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. However, they are modeled only implicitly by current <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolvers</a>. We propose a dependency-based embedding model of selectional preferences which allows fine-grained compatibility judgments with high coverage. Incorporating our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves performance, matching state-of-the-art results of a more complex system. However, it comes with a cost that makes it debatable how worthwhile are such improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1139/>Learning to Rank Semantic Coherence for Topic Segmentation</a></strong><br><a href=/people/l/liang-wang/>Liang Wang</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a>
|
<a href=/people/y/yajuan-lu/>Yajuan Lv</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1139><div class="card-body p-3 small">Topic segmentation plays an important role for discourse parsing and <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>. Due to the absence of training data, previous work mainly adopts <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> to rank semantic coherence between paragraphs for <a href=https://en.wikipedia.org/wiki/Topic_segmentation>topic segmentation</a>. In this paper, we present an intuitive and simple idea to automatically create a quasi training dataset, which includes a large amount of text pairs from the same or different documents with different semantic coherence. With the training corpus, we design a symmetric CNN neural network to model text pairs and rank the semantic coherence within the learning to rank framework. Experiments show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is able to achieve competitive performance over strong baselines on several real-world datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1140.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1140/>GRASP : Rich Patterns for Argumentation Mining<span class=acl-fixed-case>GRASP</span>: Rich Patterns for Argumentation Mining</a></strong><br><a href=/people/e/eyal-shnarch/>Eyal Shnarch</a>
|
<a href=/people/r/ran-levy/>Ran Levy</a>
|
<a href=/people/v/vikas-raykar/>Vikas Raykar</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1140><div class="card-body p-3 small">GRASP (GReedy Augmented Sequential Patterns) is an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for automatically extracting patterns that characterize subtle linguistic phenomena. To that end, GRASP augments each term of input text with multiple <a href=https://en.wikipedia.org/wiki/Linguistic_description>layers of linguistic information</a>. These different facets of the text terms are systematically combined to reveal rich patterns. We report highly promising experimental results in several challenging text analysis tasks within the field of Argumentation Mining. We believe that GRASP is general enough to be useful for other domains too. For example, each of the following sentences includes a claim for a [ topic ] : 1. Opponents often argue that the open primary is unconstitutional. [ Open Primaries ] 2. Prof. Smith suggested that <a href=https://en.wikipedia.org/wiki/Affirmative_action>affirmative action</a> devalues the accomplishments of the chosen. [ Affirmative Action ] 3. The majority stated that the First Amendment does not guarantee the right to offend others. [ Freedom of Speech ] These sentences share almost no words in common, however, they are similar at a more abstract level. A human observer may notice the following underlying common structure, or pattern : [ someone][argue / suggest / state][that][topic term][sentiment term ]. GRASP aims to automatically capture such underlying structures of the given data. For the above examples it finds the pattern [ noun][express][that][noun, topic][sentiment ], where [ express ] stands for all its (in)direct hyponyms, and [ noun, topic ] means a noun which is also related to the topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1141 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1141/>Patterns of Argumentation Strategies across Topics</a></strong><br><a href=/people/k/khalid-al-khatib/>Khalid Al-Khatib</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/m/matthias-hagen/>Matthias Hagen</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1141><div class="card-body p-3 small">This paper presents an analysis of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation strategies</a> in <a href=https://en.wikipedia.org/wiki/Editorial>news editorials</a> within and across topics. Given nearly 29,000 argumentative editorials from the <a href=https://en.wikipedia.org/wiki/The_New_York_Times>New York Times</a>, we develop two machine learning models, one for determining an editorial&#8217;s topic, and one for identifying evidence types in the editorial. Based on the distribution and structure of the identified types, we analyze the usage patterns of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation strategies</a> among 12 different topics. We detect several common patterns that provide insights into the manifestation of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation strategies</a>. Also, our experiments reveal clear correlations between the topics and the detected patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1142 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1142.Attachment.rar data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1142/>Using Argument-based Features to Predict and Analyse Review Helpfulness</a></strong><br><a href=/people/h/haijing-liu/>Haijing Liu</a>
|
<a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/p/pin-lv/>Pin Lv</a>
|
<a href=/people/m/mengxue-li/>Mengxue Li</a>
|
<a href=/people/s/shiqiang-geng/>Shiqiang Geng</a>
|
<a href=/people/m/minglan-li/>Minglan Li</a>
|
<a href=/people/h/hao-wang/>Hao Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1142><div class="card-body p-3 small">We study the helpful product reviews identification problem in this paper. We observe that the evidence-conclusion discourse relations, also known as arguments, often appear in product reviews, and we hypothesise that some argument-based features, e.g. the percentage of argumentative sentences, the evidences-conclusions ratios, are good indicators of helpful reviews. To validate this hypothesis, we manually annotate arguments in 110 hotel reviews, and investigate the effectiveness of several combinations of argument-based features. Experiments suggest that, when being used together with the argument-based features, the state-of-the-art baseline features can enjoy a performance boost (in terms of F1) of 11.01 % in average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1143 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1143/>Here’s My Point : Joint Pointer Architecture for Argument Mining</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1143><div class="card-body p-3 small">In order to determine argument structure in text, one must understand how individual components of the overall argument are linked. This work presents the first neural network-based approach to link extraction in <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>. Specifically, we propose a novel architecture that applies Pointer Network sequence-to-sequence attention modeling to structural prediction in discourse parsing tasks. We then develop a joint model that extends this <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> to simultaneously address the link extraction task and the classification of argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, showing far superior performance than the previously proposed corpus-specific and heavily feature-engineered models. Furthermore, our results demonstrate that jointly optimizing for both <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> is crucial for high performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1144 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1144/>Identifying attack and support argumentative relations using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a></a></strong><br><a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/f/francesca-toni/>Francesca Toni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1144><div class="card-body p-3 small">We propose a deep learning architecture to capture argumentative relations of attack and support from one piece of text to another, of the kind that naturally occur in a debate. The architecture uses two (unidirectional or bidirectional) Long Short-Term Memory networks and (trained or non-trained) word embeddings, and allows to considerably improve upon existing techniques that use syntactic features and supervised classifiers for the same form of (relation-based) argument mining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1146 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1146/>Memory-augmented Neural Machine Translation</a></strong><br><a href=/people/y/yang-feng/>Yang Feng</a>
|
<a href=/people/s/shiyue-zhang/>Shiyue Zhang</a>
|
<a href=/people/a/andi-zhang/>Andi Zhang</a>
|
<a href=/people/d/dong-wang/>Dong Wang</a>
|
<a href=/people/a/andrew-abel/>Andrew Abel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1146><div class="card-body p-3 small">Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs. This paper presents a novel memory-augmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model. We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework. Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture outperformed the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>NMT baseline</a> by 9.0 and 2.7 BLEU points on the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, respectively. Additionally, we found this <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> resulted in a much more effective OOV treatment compared to competitive methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1147 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1147" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1147/>Dynamic Data Selection for Neural Machine Translation</a></strong><br><a href=/people/m/marlies-van-der-wees/>Marlies van der Wees</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1147><div class="card-body p-3 small">Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance for phrase-based machine translation (PBMT). With the recent increase in popularity of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a>, we explore in this paper to what extent and how <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>NMT</a> can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for <a href=https://en.wikipedia.org/wiki/Pharmacodynamics>PBMT</a>, we show that gains are substantially lower for <a href=https://en.wikipedia.org/wiki/Pharmacodynamics>NMT</a>. Next, we introduce &#8216;dynamic data selection&#8217; for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a <a href=https://en.wikipedia.org/wiki/Scientific_technique>technique</a> we call &#8216;gradual fine-tuning&#8217;, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1148 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1148/>Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search</a></strong><br><a href=/people/l/leonard-dahlmann/>Leonard Dahlmann</a>
|
<a href=/people/e/evgeny-matusov/>Evgeny Matusov</a>
|
<a href=/people/p/pavel-petrushkov/>Pavel Petrushkov</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1148><div class="card-body p-3 small">In this paper, we introduce a hybrid search for attention-based neural machine translation (NMT). A target phrase learned with statistical MT models extends a hypothesis in the NMT beam search when the attention of the NMT model focuses on the source words translated by this phrase. Phrases added in this way are scored with the NMT model, but also with SMT features including phrase-level translation probabilities and a target language model. Experimental results on German-to-English news domain and English-to-Russian e-commerce domain translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3 % BLEU absolute as compared to a strong NMT baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1149 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1149/>Translating Phrases in Neural Machine Translation</a></strong><br><a href=/people/x/xing-wang/>Xing Wang</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1149><div class="card-body p-3 small">Phrases play an important role in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> (Sag et al., 2002 ; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>. At each decoding step, the phrase memory is first re-written by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make <a href=https://en.wikipedia.org/wiki/Probability_estimation>probability estimations</a> for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese to English translation show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves significant improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on various <a href=https://en.wikipedia.org/wiki/Test_set>test sets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1151 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1151" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1151/>Massive Exploration of Neural Machine Translation Architectures</a></strong><br><a href=/people/d/denny-britz/>Denny Britz</a>
|
<a href=/people/a/anna-goldie/>Anna Goldie</a>
|
<a href=/people/m/minh-thang-luong/>Minh-Thang Luong</a>
|
<a href=/people/q/quoc-le/>Quoc Le</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1151><div class="card-body p-3 small">Neural Machine Translation (NMT) has shown remarkable progress over the past few years, with production systems now being deployed to end-users. As the field is moving rapidly, it has become unclear which elements of NMT architectures have a significant impact on translation quality. In this work, we present a large-scale analysis of the sensitivity of NMT architectures to common hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on a WMT English to German translation task. Our experiments provide practical insights into the relative importance of factors such as embedding size, network depth, RNN cell type, residual connections, attention mechanism, and decoding heuristics. As part of this contribution, we also release an open-source NMT framework in <a href=https://en.wikipedia.org/wiki/TensorFlow>TensorFlow</a> to make it easy for others to reproduce our results and perform their own experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1153 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1153.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1153" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1153/>Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback</a></strong><br><a href=/people/k/khanh-nguyen/>Khanh Nguyen</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1153><div class="card-body p-3 small">Machine translation is a natural candidate problem for <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> from human feedback : users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning algorithm</a> that improves neural machine translation systems from simulated human feedback. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder architecture (Luong et al., 2015). This <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1154 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1154/>Towards Compact and Fast Neural Machine Translation Using a Combined Method</a></strong><br><a href=/people/x/xiaowei-zhang/>Xiaowei Zhang</a>
|
<a href=/people/w/wei-chen/>Wei Chen</a>
|
<a href=/people/f/feng-wang/>Feng Wang</a>
|
<a href=/people/s/shuang-xu/>Shuang Xu</a>
|
<a href=/people/b/bo-xu/>Bo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1154><div class="card-body p-3 small">Neural Machine Translation (NMT) lays intensive burden on computation and memory cost. It is a challenge to deploy NMT models on the devices with limited computation and memory budgets. This paper presents a four stage pipeline to compress <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and speed up the <a href=https://en.wikipedia.org/wiki/Code>decoding</a> for <a href=https://en.wikipedia.org/wiki/Nuclear_magnetic_resonance_spectroscopy>NMT</a>. Our method first introduces a compact architecture based on convolutional encoder and weight shared embeddings. Then weight pruning is applied to obtain a sparse model. Next, we propose a fast sequence interpolation approach which enables the greedy decoding to achieve performance on par with the <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>. Hence, the time-consuming <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> can be replaced by simple greedy decoding. Finally, vocabulary selection is used to reduce the computation of softmax layer. Our final <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 10 times <a href=https://en.wikipedia.org/wiki/Speedup>speedup</a>, 17 times parameters reduction, less than 35 MB storage size and comparable performance compared to the baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1155 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1155" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1155/>Instance Weighting for Neural Machine Translation Domain Adaptation</a></strong><br><a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a>
|
<a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1155><div class="card-body p-3 small">Instance weighting has been widely applied to phrase-based machine translation domain adaptation. However, it is challenging to be applied to <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation (NMT)</a> directly, because <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>NMT</a> is not a <a href=https://en.wikipedia.org/wiki/Linear_model>linear model</a>. In this paper, two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT English-German / French tasks show that the proposed methods can substantially improve <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> performance by up to 2.7-6.7 BLEU points, outperforming the existing <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by up to 1.6-3.6 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1156 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1156/>Regularization techniques for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a></a></strong><br><a href=/people/a/antonio-valerio-miceli-barone/>Antonio Valerio Miceli Barone</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1156><div class="card-body p-3 small">We investigate techniques for supervised domain adaptation for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> is a major challenge. We investigate a number of techniques to reduce <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and improve <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, obtaining improvements on IWSLT datasets for <a href=https://en.wikipedia.org/wiki/German_language>EnglishGerman</a> and EnglishRussian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1158 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1158" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1158/>Using Target-side Monolingual Data for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> through <a href=https://en.wikipedia.org/wiki/Multi-task_learning>Multi-task Learning</a></a></strong><br><a href=/people/t/tobias-domhan/>Tobias Domhan</a>
|
<a href=/people/f/felix-hieber/>Felix Hieber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1158><div class="card-body p-3 small">The performance of Neural Machine Translation (NMT) models relies heavily on the availability of sufficient amounts of <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>, and an efficient and effective way of leveraging the vastly available amounts of <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> has yet to be found. We propose to modify the decoder in a neural sequence-to-sequence model to enable <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> for two strongly related tasks : target-side language modeling and <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. The decoder predicts the next target word through two channels, a target-side language model on the lowest layer, and an attentional recurrent model which is conditioned on the source representation. This architecture allows joint training on both large amounts of monolingual and moderate amounts of bilingual data to improve <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> performance. Initial results in the news domain for three language pairs show moderate but consistent improvements over a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> trained on <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual data</a> only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1161 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233841 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1161/>Joint Concept Learning and Semantic Parsing from Natural Language Explanations</a></strong><br><a href=/people/s/shashank-srivastava/>Shashank Srivastava</a>
|
<a href=/people/i/igor-labutov/>Igor Labutov</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1161><div class="card-body p-3 small">Natural language constitutes a predominant medium for much of <a href=https://en.wikipedia.org/wiki/Pedagogy>human learning and pedagogy</a>. We consider the problem of <a href=https://en.wikipedia.org/wiki/Concept>concept learning</a> from <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language explanations</a>, and a small number of labeled examples of the <a href=https://en.wikipedia.org/wiki/Concept>concept</a>. For example, in learning the concept of a phishing email, one might say &#8216;this is a phishing email because it asks for your bank account number&#8217;. Solving this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> involves both learning to interpret open ended natural language statements, and learning the <a href=https://en.wikipedia.org/wiki/Concept>concept</a> itself. We present a joint model for (1) language interpretation (semantic parsing) and (2) concept learning (classification) that does not require labeling statements with logical forms. Instead, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> prefers discriminative interpretations of statements in context of observable features of the data as a weak signal for <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. On a dataset of email-related concepts, our approach yields across-the-board improvements in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance, with a 30 % relative improvement in F1 score over competitive methods in the low data regime.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1162 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233405 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1162/>Grasping the Finer Point : A Supervised Similarity Network for Metaphor Detection</a></strong><br><a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/l/luana-bulat/>Luana Bulat</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1162><div class="card-body p-3 small">The ubiquity of <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a> in our everyday communication makes <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> an important problem for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. Yet, the majority of metaphor processing systems to date rely on hand-engineered features and there is still no consensus in the field as to which features are optimal for this task. In this paper, we present the first deep learning architecture designed to capture metaphorical composition. Our results demonstrate that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> outperforms the existing <a href=https://en.wikipedia.org/wiki/Psychological_evaluation>approaches</a> in the metaphor identification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1163 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233559 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1163/>Identifying civilians killed by police with distantly supervised entity-event extraction</a></strong><br><a href=/people/k/katherine-keith/>Katherine Keith</a>
|
<a href=/people/a/abram-handler/>Abram Handler</a>
|
<a href=/people/m/michael-pinkham/>Michael Pinkham</a>
|
<a href=/people/c/cara-magliozzi/>Cara Magliozzi</a>
|
<a href=/people/j/joshua-mcduffie/>Joshua McDuffie</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1163><div class="card-body p-3 small">We propose a new, socially-impactful task for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> : from a <a href=https://en.wikipedia.org/wiki/Text_corpus>news corpus</a>, extract names of persons who have been killed by police. We present a newly collected police fatality corpus, which we release publicly, and present a model to solve this problem that uses EM-based distant supervision with <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> and convolutional neural network classifiers. Our model outperforms two off-the-shelf event extractor systems, and it can suggest candidate victim names in some cases faster than one of the major manually-collected police fatality databases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1164 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1164/>Asking too much? The rhetorical role of questions in political discourse</a></strong><br><a href=/people/j/justine-zhang/>Justine Zhang</a>
|
<a href=/people/a/arthur-spirling/>Arthur Spirling</a>
|
<a href=/people/c/cristian-danescu-niculescu-mizil/>Cristian Danescu-Niculescu-Mizil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1164><div class="card-body p-3 small">Questions play a prominent role in <a href=https://en.wikipedia.org/wiki/Social_relation>social interactions</a>, performing <a href=https://en.wikipedia.org/wiki/Rhetoric>rhetorical functions</a> that go beyond that of simple <a href=https://en.wikipedia.org/wiki/Information_exchange>informational exchange</a>. The surface form of a question can signal the intention and background of the person asking it, as well as the nature of their relation with the interlocutor. While the informational nature of questions has been extensively examined in the context of question-answering applications, their rhetorical aspects have been largely understudied. In this work we introduce an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methodology</a> for extracting surface motifs that recur in questions, and for grouping them according to their latent rhetorical role. By applying this framework to the setting of question sessions in the <a href=https://en.wikipedia.org/wiki/Parliament_of_the_United_Kingdom>UK parliament</a>, we show that the resulting typology encodes key aspects of the political discoursesuch as the bifurcation in questioning behavior between government and opposition partiesand reveals new insights into the effects of a legislator&#8217;s tenure and political career ambitions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1165 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238232359 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1165" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1165/>Detecting Perspectives in Political Debates</a></strong><br><a href=/people/d/david-vilares/>David Vilares</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1165><div class="card-body p-3 small">We explore how to detect people&#8217;s perspectives that occupy a certain proposition. We propose a Bayesian modelling approach where topics (or propositions) and their associated perspectives (or viewpoints) are modeled as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>. Words associated with topics or perspectives follow different <a href=https://en.wikipedia.org/wiki/Generative_grammar>generative routes</a>. Based on the extracted perspectives, we can extract the top associated sentences from text to generate a succinct summary which allows a quick glimpse of the main viewpoints in a document. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is evaluated on debates from the House of Commons of the UK Parliament, revealing perspectives from the debates without the use of labelled data and obtaining better results than previous related solutions under a variety of evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1166 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236876 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1166/>i have a feeling trump will win.................. : Forecasting Winners and Losers from User Predictions on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/s/sandesh-swamy/>Sandesh Swamy</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1166><div class="card-body p-3 small">Social media users often make explicit predictions about upcoming events. Such statements vary in the degree of certainty the author expresses toward the outcome : Leonardo DiCaprio will win Best Actor vs. Leonardo DiCaprio may win or No way Leonardo wins !. Can popular beliefs on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> predict who will win? To answer this question, we build a corpus of tweets annotated for <a href=https://en.wikipedia.org/wiki/Veridicality>veridicality</a> on which we train a log-linear classifier that detects positive veridicality with high precision. We then forecast uncertain outcomes using the <a href=https://en.wikipedia.org/wiki/Wisdom_of_crowds>wisdom of crowds</a>, by aggregating users&#8217; explicit predictions. Our method for forecasting winners is fully automated, relying only on a set of contenders as input. It requires no training data of past outcomes and outperforms sentiment and tweet volume baselines on a broad range of contest prediction tasks. We further demonstrate how our approach can be used to measure the reliability of individual accounts&#8217; predictions and retrospectively identify surprise outcomes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1168 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235959 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1168/>Story Comprehension for Predicting What Happens Next</a></strong><br><a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a>
|
<a href=/people/h/haoruo-peng/>Haoruo Peng</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1168><div class="card-body p-3 small">Automatic story comprehension is a fundamental challenge in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a>, and can enable computers to learn about <a href=https://en.wikipedia.org/wiki/Social_norm>social norms</a>, <a href=https://en.wikipedia.org/wiki/Human_behavior>human behavior</a> and <a href=https://en.wikipedia.org/wiki/Commonsense>commonsense</a>. In this paper, we present a story comprehension model that explores three distinct semantic aspects : (i) the sequence of events described in the story, (ii) its emotional trajectory, and (iii) its plot consistency. We judge the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s understanding of real-world stories by inquiring if, like humans, it can develop an expectation of what will happen next in a given story. Specifically, we use <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> to predict the correct ending of a given short story from possible alternatives. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> uses a <a href=https://en.wikipedia.org/wiki/Hidden-variable_theory>hidden variable</a> to weigh the <a href=https://en.wikipedia.org/wiki/Semantics>semantic aspects</a> in the context of the story. Our experiments demonstrate the potential of our approach to characterize these semantic aspects, and the strength of the hidden variable based approach. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art approaches and achieves best results on a publicly available dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1171 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1171/>CRF Autoencoder for Unsupervised Dependency Parsing<span class=acl-fixed-case>CRF</span> Autoencoder for Unsupervised Dependency Parsing</a></strong><br><a href=/people/j/jiong-cai/>Jiong Cai</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1171><div class="card-body p-3 small">Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> focuses on learning <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a>. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> as well as a tractable learning algorithm. We evaluated the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on eight multilingual treebanks and found that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved comparable performance with state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1172 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1172/>Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence</a></strong><br><a href=/people/c/caio-corro/>Caio Corro</a>
|
<a href=/people/j/joseph-le-roux/>Joseph Le Roux</a>
|
<a href=/people/m/mathieu-lacroix/>Mathieu Lacroix</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1172><div class="card-body p-3 small">We present a new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> for the joint task of tagging and non-projective dependency parsing. We demonstrate its usefulness with an application to discontinuous phrase-structure parsing where decoding lexicalized spines and syntactic derivations is performed jointly. The main contributions of this paper are (1) a reduction from joint tagging and non-projective dependency parsing to the Generalized Maximum Spanning Arborescence problem, and (2) a novel decoding algorithm for this problem through <a href=https://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field>Lagrangian relaxation</a>. We evaluate this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and obtain state-of-the-art results despite strong independence assumptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1173/>Incremental Graph-based Neural Dependency Parsing</a></strong><br><a href=/people/x/xiaoqing-zheng/>Xiaoqing Zheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1173><div class="card-body p-3 small">Very recently, some studies on neural dependency parsers have shown advantage over the traditional ones on a wide variety of languages. However, for graph-based neural dependency parsing systems, they either count on the <a href=https://en.wikipedia.org/wiki/Long-term_memory>long-term memory</a> and attention mechanism to implicitly capture the high-order features or give up the global exhaustive inference algorithms in order to harness the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> over a rich history of parsing decisions. The former might miss out the important <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for specific headword predictions without the help of the explicit structural information, and the latter may suffer from the <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a> as false early structural constraints are used to create <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> when making future predictions. We explore the feasibility of explicitly taking high-order features into account while remaining the main advantage of global inference and <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> for graph-based parsing. The proposed <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> first forms an initial <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a> by head-modifier predictions based on the <a href=https://en.wikipedia.org/wiki/First-order_logic>first-order factorization</a>. High-order features (such as grandparent, sibling, and uncle) then can be defined over the initial <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree</a>, and used to refine the parse tree in an iterative fashion. Experimental results showed that our model (called INDP) archived competitive performance to existing benchmark parsers on both English and Chinese datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1174/>Neural Discontinuous Constituency Parsing</a></strong><br><a href=/people/m/milos-stanojevic/>Miloš Stanojević</a>
|
<a href=/people/r/raquel-g-alhama/>Raquel G. Alhama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1174><div class="card-body p-3 small">One of the most pressing issues in discontinuous constituency transition-based parsing is that the relevant information for parsing decisions could be located in any part of the <a href=https://en.wikipedia.org/wiki/Call_stack>stack</a> or the <a href=https://en.wikipedia.org/wiki/Data_buffer>buffer</a>. In this paper, we propose a solution to this problem by replacing the structured perceptron model with a recursive neural model that computes a global representation of the configuration, therefore allowing even the most remote parts of the configuration to influence the parsing decisions. We also provide a detailed analysis of how this representation should be built out of sub-representations of its core elements (words, <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a> and stack). Additionally, we investigate how different types of swap oracles influence the results. Our model is the first neural discontinuous constituency parser, and it outperforms all the previously published models on three out of four datasets while on the fourth it obtains second place by a tiny difference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1175 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1175/>Stack-based Multi-layer Attention for Transition-based Dependency Parsing</a></strong><br><a href=/people/z/zhirui-zhang/>Zhirui Zhang</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/e/enhong-chen/>Enhong Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1175><div class="card-body p-3 small">Although sequence-to-sequence (seq2seq) network has achieved significant success in many NLP tasks such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, simply applying this approach to transition-based dependency parsing can not yield a comparable performance gain as in other state-of-the-art methods, such as stack-LSTM and head selection. In this paper, we propose a stack-based multi-layer attention model for seq2seq learning to better leverage structural linguistics information. In our method, two binary vectors are used to track the decoding stack in transition-based parsing, and multi-layer attention is introduced to capture multiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1176 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1176.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1176/>Dependency Grammar Induction with Neural Lexicalization and Big Training Data</a></strong><br><a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1176><div class="card-body p-3 small">We study the impact of <a href=https://en.wikipedia.org/wiki/Big_data>big models</a> (in terms of the degree of lexicalization) and <a href=https://en.wikipedia.org/wiki/Big_data>big data</a> (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016). We find that L-DMV only benefits from very small degrees of <a href=https://en.wikipedia.org/wiki/Lexicalization>lexicalization</a> and moderate sizes of training corpora. L-NDMV can benefit from <a href=https://en.wikipedia.org/wiki/Big_data>big training data</a> and <a href=https://en.wikipedia.org/wiki/Lexicalization>lexicalization</a> of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1177 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1177/>Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition</a></strong><br><a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/w/wenjuan-han/>Wenjuan Han</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1177><div class="card-body p-3 small">Unsupervised dependency parsing aims to learn a <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parser</a> from unannotated sentences. Existing work focuses on either learning <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> using the <a href=https://en.wikipedia.org/wiki/Expectation&#8211;maximization_algorithm>expectation-maximization algorithm</a> and its variants, or learning <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative models</a> using the discriminative clustering algorithm. In this paper, we propose a new learning strategy that learns a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> and a <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative model</a> jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and improve their learning results. We tested our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> on the UD treebank and achieved a state-of-the-art performance on thirty languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1178/>Effective Inference for Generative Neural Parsing</a></strong><br><a href=/people/m/mitchell-stern/>Mitchell Stern</a>
|
<a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1178><div class="card-body p-3 small">Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a>. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> while exploring significantly less of the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a>. Applied to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> of Choe and Charniak (2016), our <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference procedure</a> obtains 92.56 F1 on section 23 of the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a>, surpassing prior state-of-the-art results for single-model systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1179 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1179" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1179/>Semi-supervised Structured Prediction with Neural CRF Autoencoder<span class=acl-fixed-case>CRF</span> Autoencoder</a></strong><br><a href=/people/x/xiao-zhang/>Xiao Zhang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/h/hao-peng/>Hao Peng</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1179><div class="card-body p-3 small">In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning of sequential structured prediction problems. Our NCRF-AE consists of two parts : an encoder which is a CRF model enhanced by <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>, and a decoder which is a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> trying to reconstruct the input. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has a unified structure with different <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a> for labeled and unlabeled data with shared parameters. We developed a variation of the EM algorithm for optimizing both the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> simultaneously by decoupling their parameters. Our Experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that our model can outperform competitive systems in both supervised and semi-supervised scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1180 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1180/>TAG Parsing with <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> and Vector Representations of Supertags<span class=acl-fixed-case>TAG</span> Parsing with Neural Networks and Vector Representations of Supertags</a></strong><br><a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/b/bob-frank/>Bob Frank</a>
|
<a href=/people/t/tom-mccoy/>Tom McCoy</a>
|
<a href=/people/o/owen-rambow/>Owen Rambow</a>
|
<a href=/people/a/alexis-nasr/>Alexis Nasr</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1180><div class="card-body p-3 small">We present supertagging-based models for Tree Adjoining Grammar parsing that use neural network architectures and dense vector representation of supertags (elementary trees) to achieve state-of-the-art performance in unlabeled and labeled attachment scores. The shift-reduce parsing model eschews lexical information entirely, and uses only the 1-best supertags to parse a sentence, providing further support for the claim that supertagging is almost <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. We demonstrate that the embedding vector representations the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> induces for supertags possess linguistically interpretable structure, supporting analogies between grammatical structures like those familiar from recent work in <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>. This dense representation of supertags overcomes the drawbacks for statistical models of TAG as compared to CCG parsing, raising the possibility that <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>TAG</a> is a viable alternative for NLP tasks that require the assignment of richer structural descriptions to sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1181 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1181/>Global Normalization of <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for Joint Entity and Relation Classification</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1181><div class="card-body p-3 small">We introduce globally normalized convolutional neural networks for joint entity classification and <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. In particular, we propose a way to utilize a linear-chain conditional random field output layer for predicting entity types and relations between entities at the same time. Our experiments show that global normalization outperforms a locally normalized softmax layer on a benchmark dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1182/>End-to-End Neural Relation Extraction with Global Optimization</a></strong><br><a href=/people/m/meishan-zhang/>Meishan Zhang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/g/guohong-fu/>Guohong Fu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1182><div class="card-body p-3 small">Neural networks have shown promising results for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. State-of-the-art models cast the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> as an <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end problem</a>, solved incrementally using a local classifier. Yet previous work using <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a> have demonstrated that <a href=https://en.wikipedia.org/wiki/Global_optimization>global optimization</a> can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is highly effective, achieving the best performances on two standard benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1183 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1183.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1183/>KGEval : Accuracy Estimation of Automatically Constructed Knowledge Graphs<span class=acl-fixed-case>KGE</span>val: Accuracy Estimation of Automatically Constructed Knowledge Graphs</a></strong><br><a href=/people/p/prakhar-ojha/>Prakhar Ojha</a>
|
<a href=/people/p/partha-talukdar/>Partha Talukdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1183><div class="card-body p-3 small">Automatic construction of large knowledge graphs (KG) by mining web-scale text datasets has received considerable attention recently. Estimating accuracy of such automatically constructed KGs is a challenging problem due to their size and diversity. This important problem has largely been ignored in prior research we fill this gap and propose KGEval. KGEval uses coupling constraints to bind facts and crowdsources those few that can infer large parts of the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. We demonstrate that the <a href=https://en.wikipedia.org/wiki/Loss_function>objective</a> optimized by KGEval is submodular and NP-hard, allowing guarantees for our <a href=https://en.wikipedia.org/wiki/Approximation_algorithm>approximation algorithm</a>. Through experiments on real-world datasets, we demonstrate that KGEval best estimates KG accuracy compared to other baselines, while requiring significantly lesser number of human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1184 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1184" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1184/>Sparsity and Noise : Where Knowledge Graph Embeddings Fall Short</a></strong><br><a href=/people/j/jay-pujara/>Jay Pujara</a>
|
<a href=/people/e/eriq-augustine/>Eriq Augustine</a>
|
<a href=/people/l/lise-getoor/>Lise Getoor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1184><div class="card-body p-3 small">Knowledge graph (KG) embedding techniques use structured relationships between entities to learn low-dimensional representations of entities and relations. One prominent goal of these approaches is to improve the quality of <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> by removing errors and adding <a href=https://en.wikipedia.org/wiki/Missing_data>missing facts</a>. Surprisingly, most embedding techniques have been evaluated on benchmark datasets consisting of dense and reliable subsets of human-curated KGs, which tend to be fairly complete and have few errors. In this paper, we consider the problem of applying embedding techniques to KGs extracted from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, which are often incomplete and contain errors. We compare the sparsity and unreliability of different KGs and perform empirical experiments demonstrating how embedding approaches degrade as sparsity and <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>unreliability</a> increase.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1185 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1185.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1185/>Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1185><div class="card-body p-3 small">Detection of lexico-semantic relations is one of the central tasks of <a href=https://en.wikipedia.org/wiki/Computational_semantics>computational semantics</a>. Although some fundamental relations (e.g., hypernymy) are asymmetric, most existing models account for asymmetry only implicitly and use the same concept representations to support detection of symmetric and asymmetric relations alike. In this work, we propose the Dual Tensor model, a neural architecture with which we explicitly model the asymmetry and capture the translation between unspecialized and specialized word embeddings via a pair of <a href=https://en.wikipedia.org/wiki/Tensor>tensors</a>. Although our Dual Tensor model needs only unspecialized embeddings as input, our experiments on <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a> and <a href=https://en.wikipedia.org/wiki/Meronymy>meronymy detection</a> suggest that it can outperform more complex and resource-intensive models. We further demonstrate that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can account for <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a> and that it exhibits stable performance across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1186 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1186" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1186/>Incorporating Relation Paths in Neural Relation Extraction</a></strong><br><a href=/people/w/wenyuan-zeng/>Wenyuan Zeng</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1186><div class="card-body p-3 small">Distantly supervised relation extraction has been widely used to find novel relational facts from <a href=https://en.wikipedia.org/wiki/Plain_text>plain text</a>. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which also provide rich useful information but not yet employed by <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with strong baselines. The source code of this paper can be obtained from.<url>https://github.com/thunlp/PathNRE</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1187 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1187/>Adversarial Training for Relation Extraction</a></strong><br><a href=/people/y/yi-wu/>Yi Wu</a>
|
<a href=/people/d/david-bamman/>David Bamman</a>
|
<a href=/people/s/stuart-russell/>Stuart Russell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1187><div class="card-body p-3 small">Adversarial training is a mean of <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizing classification algorithms</a> by generating adversarial noise to the training data. We apply <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial training</a> in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> within the multi-instance multi-label learning framework. We evaluate various neural network architectures on two different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Experimental results demonstrate that adversarial training is generally effective for both CNN and RNN models and significantly improves the <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> of predicted relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1188 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1188/>Context-Aware Representations for Knowledge Base Relation Extraction</a></strong><br><a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1188><div class="card-body p-3 small">We demonstrate that for sentence-level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation. Our architecture uses an LSTM-based encoder to jointly learn representations for all relations in a single sentence. We combine the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context representations</a> with an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to make the final prediction. We use the <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata knowledge base</a> to construct a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of multiple relations per sentence and to evaluate our approach. Compared to a baseline system, our method results in an average error reduction of 24 on a held-out set of relations. The code and the dataset to replicate the experiments are made available at.<url>https://github.com/ukplab/</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1189 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1189/>A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction</a></strong><br><a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/k/kexiang-wang/>Kexiang Wang</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1189><div class="card-body p-3 small">Distant-supervised relation extraction inevitably suffers from wrong labeling problems because it heuristically labels relational facts with <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. Previous sentence level denoise models do n&#8217;t achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable during training. To this end, we introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During <a href=https://en.wikipedia.org/wiki/Training>training</a>, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms other state-of-the-art systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1190 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1190/>A <a href=https://en.wikipedia.org/wiki/Sequential_model>Sequential Model</a> for Classifying Temporal Relations between Intra-Sentence Events</a></strong><br><a href=/people/p/prafulla-kumar-choubey/>Prafulla Kumar Choubey</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1190><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential model</a> for temporal relation classification between intra-sentence events. The key observation is that the overall <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a> and compositional meanings of the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>multi-word context</a> between events are important for distinguishing among fine-grained temporal relations. Specifically, our approach first extracts a sequence of context words that indicates the temporal relation between two events, which well align with the dependency path between two event mentions. The context word sequence, together with a parts-of-speech tag sequence and a dependency relation sequence that are generated corresponding to the word sequence, are then provided as input to bidirectional recurrent neural network (LSTM) models. The <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural nets</a> learn compositional syntactic and semantic representations of contexts surrounding the two events and predict the temporal relation between them. Evaluation of the proposed approach on TimeBank corpus shows that sequential modeling is capable of accurately recognizing temporal relations between events, which outperforms a neural net model using various discrete features as input that imitates previous feature based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1191 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1191/>Deep Residual Learning for Weakly-Supervised Relation Extraction</a></strong><br><a href=/people/y/yi-yao-huang/>Yi Yao Huang</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1191><div class="card-body p-3 small">Deep residual learning (ResNet) is a new method for training very deep neural networks using <a href=https://en.wikipedia.org/wiki/Identity_mapping>identity mapping</a> for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolutional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that <a href=https://en.wikipedia.org/wiki/ResNet>ResNet</a> only works well for very deep networks, we found that even with 9 layers of CNNs, using <a href=https://en.wikipedia.org/wiki/Identity_mapping>identity mapping</a> could significantly improve the performance for distantly-supervised relation extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1192/>Noise-Clustered Distant Supervision for <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a> : A Nonparametric Bayesian Perspective<span class=acl-fixed-case>B</span>ayesian Perspective</a></strong><br><a href=/people/q/qing-zhang/>Qing Zhang</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1192><div class="card-body p-3 small">For the task of <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, distant supervision is an efficient approach to generate labeled data by aligning <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> with free texts. The essence of it is a challenging incomplete multi-label classification problem with sparse and noisy features. To address the challenge, this work presents a novel nonparametric Bayesian formulation for the task. Experiment results show substantially higher top precision improvements over the traditional state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1194 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1194/>Temporal dynamics of semantic relations in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> : an application to predicting armed conflict participants</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1194><div class="card-body p-3 small">This paper deals with using word embedding models to trace the temporal dynamics of semantic relations between pairs of words. The set-up is similar to the well-known analogies task, but expanded with a <a href=https://en.wikipedia.org/wiki/Spacetime>time dimension</a>. To this end, we apply incremental updating of the models with new training texts, including incremental vocabulary expansion, coupled with learned <a href=https://en.wikipedia.org/wiki/Transformation_matrix>transformation matrices</a> that let us map between members of the relation. The proposed approach is evaluated on the task of predicting insurgent armed groups based on <a href=https://en.wikipedia.org/wiki/Geographic_coordinate_system>geographical locations</a>. The gold standard data for the time span 19942010 is extracted from the UCDP Armed Conflicts dataset. The results show that the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is feasible and outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, but also that important work still remains to be done.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1195 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1195" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1195/>Dynamic Entity Representations in Neural Language Models</a></strong><br><a href=/people/y/yangfeng-ji/>Yangfeng Ji</a>
|
<a href=/people/c/chenhao-tan/>Chenhao Tan</a>
|
<a href=/people/s/sebastian-martschat/>Sebastian Martschat</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1195><div class="card-body p-3 small">Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>, EntityNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is generative and flexible ; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity prediction</a>. Experimental results with all these tasks demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> consistently outperforms strong baselines and prior work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1197 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1197.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1197/>Reference-Aware Language Models</a></strong><br><a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/w/wang-ling/>Wang Ling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1197><div class="card-body p-3 small">We propose a general class of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> that treat <a href=https://en.wikipedia.org/wiki/Reference>reference</a> as discrete stochastic latent variables. This decision allows for the creation of entity mentions by accessing external databases of referents (required by, e.g., dialogue generation) or past internal state (required to explicitly model coreferentiality). Beyond simple copying, our coreference model can additionally refer to a referent using <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>varied mention forms</a> (e.g., a reference to Jane can be realized as she), a characteristic feature of reference in <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a>. Experiments on three representative applications show our model variants outperform models based on deterministic attention and standard language modeling baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1198 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1198/>A Simple <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a> based on PMI Matrix Approximations<span class=acl-fixed-case>PMI</span> Matrix Approximations</a></strong><br><a href=/people/o/oren-melamud/>Oren Melamud</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/j/jacob-goldberger/>Jacob Goldberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1198><div class="card-body p-3 small">In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired <a href=https://en.wikipedia.org/wiki/Conditional_probability>conditional probabilities</a> from PMI at test time. Specifically, we show that with minor modifications to word2vec&#8217;s algorithm, we get principled <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> to learn word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1200 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1200.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1200/>Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels</a></strong><br><a href=/people/l/lea-frermann/>Lea Frermann</a>
|
<a href=/people/g/gyorgy-szarvas/>György Szarvas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1200><div class="card-body p-3 small">Automatically understanding the <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>plot of novels</a> is important both for informing <a href=https://en.wikipedia.org/wiki/Literary_criticism>literary scholarship</a> and applications such as summarization or <a href=https://en.wikipedia.org/wiki/Book_review>recommendation</a>. Various <a href=https://en.wikipedia.org/wiki/Scientific_modelling>models</a> have addressed this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, but their evaluation has remained largely intrinsic and qualitative. Here, we propose a principled and scalable <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> leveraging expert-provided semantic tags (e.g., mystery, pirates) to evaluate plot representations in an extrinsic fashion, assessing their ability to produce locally coherent groupings of novels (micro-clusters) in model space. We present a deep recurrent autoencoder model that learns richly structured multi-view plot representations, and show that they i) yield better micro-clusters than less structured representations ; and ii) are interpretable, and thus useful for further literary analysis or labeling of the emerging micro-clusters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1201.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1201/>Initializing Convolutional Filters with <a href=https://en.wikipedia.org/wiki/Semantic_feature>Semantic Features</a> for Text Classification</a></strong><br><a href=/people/s/shen-li/>Shen Li</a>
|
<a href=/people/z/zhe-zhao/>Zhe Zhao</a>
|
<a href=/people/t/tao-liu/>Tao Liu</a>
|
<a href=/people/r/renfen-hu/>Renfen Hu</a>
|
<a href=/people/x/xiaoyong-du/>Xiaoyong Du</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1201><div class="card-body p-3 small">Convolutional Neural Networks (CNNs) are widely used in NLP tasks. This paper presents a novel weight initialization method to improve the CNNs for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. Instead of randomly initializing the convolutional filters, we encode semantic features into them, which helps the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> focus on learning useful features at the beginning of the training. Experiments demonstrate the effectiveness of the initialization technique on seven text classification tasks, including <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Topic_and_comment>topic classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1202 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1202/>Shortest-Path Graph Kernels for Document Similarity</a></strong><br><a href=/people/g/giannis-nikolentzos/>Giannis Nikolentzos</a>
|
<a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/f/francois-rousseau/>François Rousseau</a>
|
<a href=/people/y/yannis-stavrakas/>Yannis Stavrakas</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1202><div class="card-body p-3 small">In this paper, we present a novel document similarity measure based on the definition of a <a href=https://en.wikipedia.org/wiki/Graph_kernel>graph kernel</a> between pairs of documents. The proposed measure takes into account both the terms contained in the documents and the relationships between them. By representing each document as a graph-of-words, we are able to model these relationships and then determine how similar two documents are by using a modified shortest-path graph kernel. We evaluate our approach on two tasks and compare it against several baseline approaches using various <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a> such as DET curves and macro-average F1-score. Experimental results on a range of <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> showed that our proposed approach outperforms traditional techniques and is capable of measuring more accurately the similarity between two documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1203/>Adapting Topic Models using <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Associations</a> with Tree Priors</a></strong><br><a href=/people/w/weiwei-yang/>Weiwei Yang</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1203><div class="card-body p-3 small">Models work best when they are optimized taking into account the evaluation criteria that people care about. For <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a>, people often care about <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>, which can be approximated using measures of lexical association. We integrate lexical association into topic optimization using tree priors, which provide a flexible framework that can take advantage of both first order word associations and the higher-order associations captured by word embeddings. Tree priors improve topic interpretability without hurting <a href=https://en.wikipedia.org/wiki/Intrinsic_and_extrinsic_properties>extrinsic performance</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1204 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1204/>Finding Patterns in Noisy Crowds : Regression-based Annotation Aggregation for Crowdsourced Data</a></strong><br><a href=/people/n/natalie-parde/>Natalie Parde</a>
|
<a href=/people/r/rodney-nielsen/>Rodney Nielsen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1204><div class="card-body p-3 small">Crowdsourcing offers a convenient means of obtaining <a href=https://en.wikipedia.org/wiki/Data_(computing)>labeled data</a> quickly and inexpensively. However, crowdsourced labels are often noisier than expert-annotated data, making it difficult to aggregate them meaningfully. We present an aggregation approach that learns a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> from crowdsourced annotations to predict aggregated labels for instances that have no expert adjudications. The predicted labels achieve a correlation of 0.594 with expert labels on our <a href=https://en.wikipedia.org/wiki/Data>data</a>, outperforming the best alternative aggregation method by 11.9 %. Our approach also outperforms the alternatives on third-party datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1205 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1205/>CROWD-IN-THE-LOOP : A Hybrid Approach for Annotating Semantic Roles<span class=acl-fixed-case>CROWD</span>-<span class=acl-fixed-case>IN</span>-<span class=acl-fixed-case>THE</span>-<span class=acl-fixed-case>LOOP</span>: A Hybrid Approach for Annotating Semantic Roles</a></strong><br><a href=/people/c/chenguang-wang/>Chenguang Wang</a>
|
<a href=/people/a/alan-akbik/>Alan Akbik</a>
|
<a href=/people/l/laura-chiticariu/>Laura Chiticariu</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/a/anbang-xu/>Anbang Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1205><div class="card-body p-3 small">Crowdsourcing has proven to be an effective method for generating <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a> for a range of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. However, multiple recent attempts of using <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> to generate gold-labeled training data for semantic role labeling (SRL) reported only modest results, indicating that SRL is perhaps too difficult a task to be effectively crowdsourced. In this paper, we postulate that while producing SRL annotation does require expert involvement in general, a large subset of SRL labeling tasks is in fact appropriate for the crowd. We present a novel <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a> in which we employ a classifier to identify difficult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1207 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238232779 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1207/>Earth Mover’s Distance Minimization for Unsupervised Bilingual Lexicon Induction</a></strong><br><a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a>
|
<a href=/people/h/huanbo-luan/>Huanbo Luan</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1207><div class="card-body p-3 small">Cross-lingual natural language processing hinges on the premise that there exists invariance across languages. At the word level, researchers have identified such <a href=https://en.wikipedia.org/wiki/Invariant_(mathematics)>invariance</a> in the word embedding semantic spaces of different languages. However, in order to connect the separate spaces, cross-lingual supervision encoded in parallel data is typically required. In this paper, we attempt to establish the cross-lingual connection without relying on any cross-lingual supervision. By viewing word embedding spaces as <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributions</a>, we propose to minimize their earth mover&#8217;s distance, a measure of divergence between distributions. We demonstrate the success on the unsupervised bilingual lexicon induction task. In addition, we reveal an interesting finding that the earth mover&#8217;s distance shows potential as a measure of <a href=https://en.wikipedia.org/wiki/Language>language difference</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1209 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1209/>Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</a></strong><br><a href=/people/j/jasmijn-bastings/>Jasmijn Bastings</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/w/wilker-aziz/>Wilker Aziz</a>
|
<a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/k/khalil-simaan/>Khalil Sima’an</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1209><div class="card-body p-3 small">We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We rely on graph-convolutional networks (GCNs), a recent class of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> developed for modeling <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph-structured data</a>. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representations</a> as input and produce <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representations</a> as output, so they can easily be incorporated as layers into standard <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236435 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1210/>Trainable Greedy Decoding for Neural Machine Translation</a></strong><br><a href=/people/j/jiatao-gu/>Jiatao Gu</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/v/victor-o-k-li/>Victor O.K. Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1210><div class="card-body p-3 small">Recent research in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> has largely focused on two aspects ; neural network architectures and end-to-end learning algorithms. The problem of decoding, however, has received relatively little attention from the research community. In this paper, we solely focus on the problem of decoding given a trained neural machine translation model. Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective. More specifically, we design an <a href=https://en.wikipedia.org/wiki/Actor>actor</a> that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient. We extensively evaluate the proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> using four language pairs and two decoding objectives and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>computational overhead</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1211 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235500 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1211" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1211/>Satirical News Detection and Analysis using <a href=https://en.wikipedia.org/wiki/Attentional_control>Attention Mechanism</a> and <a href=https://en.wikipedia.org/wiki/Linguistic_description>Linguistic Features</a></a></strong><br><a href=/people/f/fan-yang/>Fan Yang</a>
|
<a href=/people/a/arjun-mukherjee/>Arjun Mukherjee</a>
|
<a href=/people/e/eduard-dragut/>Eduard Dragut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1211><div class="card-body p-3 small">Satirical news is considered to be entertainment, but <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> is potentially deceptive and harmful. Despite the embedded genre in the article, not everyone can recognize the satirical cues and therefore believe the news as true news. We observe that satirical cues are often reflected in certain paragraphs rather than the whole document. Existing works only consider document-level features to detect the <a href=https://en.wikipedia.org/wiki/Satire>satire</a>, which could be limited. We consider paragraph-level linguistic features to unveil the <a href=https://en.wikipedia.org/wiki/Satire>satire</a> by incorporating <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> and <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. We investigate the difference between paragraph-level features and document-level features, and analyze them on a large satirical news dataset. The evaluation shows that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> detects <a href=https://en.wikipedia.org/wiki/News_satire>satirical news</a> effectively and reveals what <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are important at which level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1212 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233167 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1212/>Fine Grained Citation Span for References in Wikipedia<span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/b/besnik-fetahu/>Besnik Fetahu</a>
|
<a href=/people/k/katja-markert/>Katja Markert</a>
|
<a href=/people/a/avishek-anand/>Avishek Anand</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1212><div class="card-body p-3 small">Verifiability is one of the core editing principles in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, where editors are encouraged to provide <a href=https://en.wikipedia.org/wiki/Citation>citations</a> for the added content. For a Wikipedia article determining what content is covered by a <a href=https://en.wikipedia.org/wiki/Citation>citation</a> or the citation span is not trivial, an important aspect for automated citation finding for uncovered content, or fact assessments. We address the problem of determining the citation span in Wikipedia articles. We approach this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> by classifying which textual fragments in an article are covered or hold true given a <a href=https://en.wikipedia.org/wiki/Citation>citation</a>. We propose a sequence classification approach where for a paragraph and a citation, we determine the citation span at a fine-grained level. We provide a thorough experimental evaluation and compare our approach against baselines adopted from the scientific domain, where we show improvement for all evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1213 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233635 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1213/>Identifying Semantic Edit Intentions from Revisions in Wikipedia<span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/a/aaron-halfaker/>Aaron Halfaker</a>
|
<a href=/people/r/robert-kraut/>Robert Kraut</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1213><div class="card-body p-3 small">Most studies on human editing focus merely on syntactic revision operations, failing to capture the intentions behind revision changes, which are essential for facilitating the single and collaborative writing process. In this work, we develop in collaboration with Wikipedia editors a 13-category taxonomy of the semantic intention behind edits in Wikipedia articles. Using labeled article edits, we build a computational classifier of intentions that achieved a micro-averaged F1 score of 0.621. We use this model to investigate edit intention effectiveness : how different types of edits predict the retention of newcomers and changes in the quality of articles, two key concerns for Wikipedia today. Our analysis shows that the types of <a href=https://en.wikipedia.org/wiki/Wikipedia_community>edits</a> that users make in their first session predict their subsequent survival as <a href=https://en.wikipedia.org/wiki/Wikipedia_community>Wikipedia editors</a>, and articles in different stages need different types of <a href=https://en.wikipedia.org/wiki/Wikipedia_community>edits</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1214 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231359 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1214/>Accurate Supervised and Semi-Supervised Machine Reading for Long Documents</a></strong><br><a href=/people/d/daniel-hewlett/>Daniel Hewlett</a>
|
<a href=/people/l/llion-jones/>Llion Jones</a>
|
<a href=/people/a/alexandre-lacoste/>Alexandre Lacoste</a>
|
<a href=/people/i/izzeddin-gur/>Izzeddin Gur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1214><div class="card-body p-3 small">We introduce a hierarchical architecture for <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a> capable of extracting precise information from long documents. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> divides the document into small, overlapping windows and encodes all windows in parallel with an RNN. It then attends over these window encodings, reducing them to a single <a href=https://en.wikipedia.org/wiki/Code>encoding</a>, which is decoded into an answer using a sequence decoder. This hierarchical approach allows the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to scale to longer documents without increasing the number of sequential steps. In a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised setting</a>, our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves state of the art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 76.8 on the WikiReading dataset. We also evaluate the model in a semi-supervised setting by downsampling the WikiReading training set to create increasingly smaller amounts of supervision, while leaving the full unlabeled document corpus to train a sequence autoencoder on document windows. We evaluate <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that can reuse autoencoder states and outputs without fine-tuning their weights, allowing for more efficient <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1215 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231419 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1215" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1215/>Adversarial Examples for Evaluating Reading Comprehension Systems</a></strong><br><a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1215><div class="card-body p-3 small">Standard accuracy metrics indicate that <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension systems</a> are making rapid progress, but the extent to which these <a href=https://en.wikipedia.org/wiki/System>systems</a> truly understand <a href=https://en.wikipedia.org/wiki/Language>language</a> remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of sixteen published <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> drops from an average of 75 % F1 score to 36 % ; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7 %. We hope our insights will motivate the development of new <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that understand language more precisely.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235420 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1216/>Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension</a></strong><br><a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1216><div class="card-body p-3 small">Reasoning with <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> is critical for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. Traditional methods for commonsense machine comprehension mostly only focus on one specific kind of knowledge, neglecting the fact that <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> requires simultaneously considering different kinds of commonsense knowledge. In this paper, we propose a multi-knowledge reasoning method, which can exploit heterogeneous knowledge for commonsense machine comprehension. Specifically, we first mine different kinds of knowledge (including event narrative knowledge, entity semantic knowledge and sentiment coherent knowledge) and encode them as inference rules with costs. Then we propose a multi-knowledge reasoning model, which selects <a href=https://en.wikipedia.org/wiki/Rule_of_inference>inference rules</a> for a specific reasoning context using attention mechanism, and reasons by summarizing all valid <a href=https://en.wikipedia.org/wiki/Rule_of_inference>inference rules</a>. Experiments on RocStories show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms traditional <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1218 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1218.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1218" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1218/>What is the Essence of a Claim? Cross-Domain Claim Identification</a></strong><br><a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/c/christian-stab/>Christian Stab</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1218><div class="card-body p-3 small">Argument mining has become a popular research area in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent conceptualization of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1219 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1219.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1219/>Identifying Where to Focus in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>Reading Comprehension</a> for Neural Question Generation</a></strong><br><a href=/people/x/xinya-du/>Xinya Du</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1219><div class="card-body p-3 small">A first step in the task of automatically generating questions for testing <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> is to identify question-worthy sentences, i.e. sentences in a text passage that humans find it worthwhile to ask questions about. We propose a hierarchical neural sentence-level sequence tagging model for this task, which existing approaches to question generation have ignored. The approach is fully data-driven with no sophisticated NLP pipelines or any hand-crafted rules / features and compares favorably to a number of baselines when evaluated on the SQuAD data set. When incorporated into an existing neural question generation system, the resulting end-to-end system achieves state-of-the-art performance for paragraph-level question generation for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>.<i>question-worthy</i> sentences, i.e.\n sentences in a text passage that humans find it worthwhile to ask\n questions about. We propose a hierarchical neural sentence-level sequence\n tagging model for this task, which existing approaches to question\n generation have ignored. The approach is fully data-driven &#8212; with no\n sophisticated NLP pipelines or any hand-crafted rules/features &#8212; and\n compares favorably to a number of baselines when evaluated on the SQuAD\n data set. When incorporated into an existing neural question generation\n system, the resulting end-to-end system achieves state-of-the-art\n performance for paragraph-level question generation for reading\n comprehension.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1221 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1221/>Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization</a></strong><br><a href=/people/p/piji-li/>Piji Li</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/w/weiwei-guo/>Weiwei Guo</a>
|
<a href=/people/h/hang-li/>Hang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1221><div class="card-body p-3 small">When people recall and digest what they have read for writing summaries, the important content is more likely to attract their attention. Inspired by this observation, we propose a cascaded attention based unsupervised model to estimate the <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>salience information</a> from the text for compressive multi-document summarization. The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience. By adding sparsity constraints on the number of output vectors, we can generate condensed information which can be treated as word salience. Fine-grained and coarse-grained sentence compression strategies are incorporated to produce compressive summaries. Experiments on some benchmark data sets show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves better results than the state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1222 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1222/>Deep Recurrent Generative Decoder for Abstractive Text Summarization</a></strong><br><a href=/people/p/piji-li/>Piji Li</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/z/zihao-wang/>Zihao Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1222><div class="card-body p-3 small">We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1224 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1224/>Towards Automatic Construction of News Overview Articles by News Synthesis</a></strong><br><a href=/people/j/jianmin-zhang/>Jianmin Zhang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1224><div class="card-body p-3 small">In this paper we investigate a new task of automatically constructing an overview article from a given set of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> about a news event. We propose a news synthesis approach to address this task based on passage segmentation, <a href=https://en.wikipedia.org/wiki/Ranking>ranking</a>, selection and merging. Our proposed approach is compared with several typical multi-document summarization methods on the Wikinews dataset, and achieves the best performance on both automatic evaluation and manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1227 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1227/>When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size)</a></strong><br><a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/k/kai-zhao/>Kai Zhao</a>
|
<a href=/people/m/mingbo-ma/>Mingbo Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1227><div class="card-body p-3 small">In neural text generation such as <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, and image captioning, <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> is widely used to improve the output text quality. However, in the neural generation setting, hypotheses can finish in different steps, which makes it difficult to decide when to end <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> to ensure <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimality</a>. We propose a provably optimal beam search algorithm that will always return the optimal-score complete hypothesis (modulo beam size), and finish as soon as the optimality is established. To counter neural generation&#8217;s tendency for shorter hypotheses, we also introduce a bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal. Experiments on <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> demonstrate that our principled beam search algorithm leads to improvement in BLEU score over previously proposed alternatives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1228 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1228.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1228" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1228/>Steering Output Style and Topic in Neural Response Generation</a></strong><br><a href=/people/d/di-wang/>Di Wang</a>
|
<a href=/people/n/nebojsa-jojic/>Nebojsa Jojic</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1228><div class="card-body p-3 small">We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems : a faithfulness model and a decoding method based on <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>selective-sampling</a>. We also describe <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and sampling algorithms</a> that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to to restrict style and topic without degrading output quality in conversational tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1230" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1230/>Adversarial Learning for Neural Dialogue Generation</a></strong><br><a href=/people/j/jiwei-li/>Jiwei Li</a>
|
<a href=/people/w/will-monroe/>Will Monroe</a>
|
<a href=/people/t/tianlin-shi/>Tianlin Shi</a>
|
<a href=/people/s/sebastien-jean/>Sébastien Jean</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1230><div class="card-body p-3 small">We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems : a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> to produce response sequences, and a discriminatoranalagous to the human evaluator in the <a href=https://en.wikipedia.org/wiki/Turing_test>Turing test</a> to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> are used to encourage the <a href=https://en.wikipedia.org/wiki/System>system</a> towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1233 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1233/>Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems</a></strong><br><a href=/people/l/lili-yao/>Lili Yao</a>
|
<a href=/people/y/yaoyuan-zhang/>Yaoyuan Zhang</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1233><div class="card-body p-3 small">The study on human-computer conversation systems is a hot research topic nowadays. One of the prevailing methods to build the system is using the generative Sequence-to-Sequence (Seq2Seq) model through <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. However, the standard Seq2Seq model is prone to generate <a href=https://en.wikipedia.org/wiki/Triviality_(mathematics)>trivial responses</a>. In this paper, we aim to generate a more meaningful and informative reply when answering a given question. We propose an implicit content-introducing method which incorporates additional information into the Seq2Seq model in a flexible way. Specifically, we fuse the general decoding and the auxiliary cue word information through our proposed hierarchical gated fusion unit. Experiments on real-life data demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> consistently outperforms a set of competitive baselines in terms of BLEU scores and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1234 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1234.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1234/>Affordable On-line Dialogue Policy Learning</a></strong><br><a href=/people/c/cheng-chang/>Cheng Chang</a>
|
<a href=/people/r/runzhe-yang/>Runzhe Yang</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/x/xiang-zhou/>Xiang Zhou</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1234><div class="card-body p-3 small">The key to building an evolvable dialogue system in real-world scenarios is to ensure an affordable on-line dialogue policy learning, which requires the on-line learning process to be safe, efficient and economical. But in reality, due to the scarcity of real interaction data, the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> usually grows slowly. Besides, the poor initial dialogue policy easily leads to bad user experience and incurs a failure of attracting users to contribute training data, so that the learning process is unsustainable. To accurately depict this, two <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>quantitative metrics</a> are proposed to assess safety and efficiency issues. For solving the unsustainable learning problem, we proposed a complete companion teaching framework incorporating the guidance from the human teacher. Since the human teaching is expensive, we compared various teaching schemes answering the question how and when to teach, to economically utilize teaching budget, so that make the online learning process affordable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1235 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1235.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1235/>Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models</a></strong><br><a href=/people/y/yuanlong-shao/>Yuanlong Shao</a>
|
<a href=/people/s/stephan-gouws/>Stephan Gouws</a>
|
<a href=/people/d/denny-britz/>Denny Britz</a>
|
<a href=/people/a/anna-goldie/>Anna Goldie</a>
|
<a href=/people/b/brian-strope/>Brian Strope</a>
|
<a href=/people/r/ray-kurzweil/>Ray Kurzweil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1235><div class="card-body p-3 small">Sequence-to-sequence models have been applied to the conversation response generation problem where the source sequence is the conversation history and the target sequence is the response. Unlike <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, conversation responding is inherently creative. The generation of long, informative, coherent, and diverse responses remains a hard task. In this work, we focus on the <a href=https://en.wikipedia.org/wiki/Turn_(geometry)>single turn setting</a>. We add self-attention to the decoder to maintain <a href=https://en.wikipedia.org/wiki/Coherence_(statistics)>coherence</a> in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1236 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1236/>Bootstrapping incremental dialogue systems from minimal data : the generalisation power of dialogue grammars</a></strong><br><a href=/people/a/arash-eshghi/>Arash Eshghi</a>
|
<a href=/people/i/igor-shalyminov/>Igor Shalyminov</a>
|
<a href=/people/o/oliver-lemon/>Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1236><div class="card-body p-3 small">We investigate an <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end method</a> for automatically inducing task-based dialogue systems from small amounts of unannotated dialogue data. It combines an incremental semantic grammar-Dynamic Syntax and Type Theory with Records (DS-TTR)-with Reinforcement Learning (RL), where language generation and dialogue management are a joint decision problem. The systems thus produced are incremental : dialogues are processed word-by-word, shown previously to be essential in supporting natural, spontaneous dialogue. We hypothesised that the rich linguistic knowledge within the grammar should enable a combinatorially large number of dialogue variations to be processed, even when trained on very few dialogues. Our experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can process 74 % of the Facebook AI bAbI dataset even when trained on only 0.13 % of the <a href=https://en.wikipedia.org/wiki/Data>data</a> (5 dialogues). It can in addition process 65 % of bAbI+, a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> we created by systematically adding incremental dialogue phenomena such as restarts and <a href=https://en.wikipedia.org/wiki/Self-reference>self-corrections</a> to bAbI. We compare our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with a state-of-the-art retrieval model, MEMN2N. We find that, in terms of semantic accuracy, the MEMN2N model shows very poor robustness to the bAbI+ transformations even when trained on the full bAbI dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1239 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1239.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1239" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1239/>Challenges in Data-to-Document Generation</a></strong><br><a href=/people/s/sam-wiseman/>Sam Wiseman</a>
|
<a href=/people/s/stuart-m-shieber/>Stuart Shieber</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1239><div class="card-body p-3 small">Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of <a href=https://en.wikipedia.org/wiki/Record_(computer_science)>database records</a>. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1240 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1240.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1240/>All that is English may be Hindi : Enhancing <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a> through automatic ranking of the likeliness of <a href=https://en.wikipedia.org/wiki/Loanword>word borrowing</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a><span class=acl-fixed-case>E</span>nglish may be <span class=acl-fixed-case>H</span>indi: Enhancing language identification through automatic ranking of the likeliness of word borrowing in social media</a></strong><br><a href=/people/j/jasabanta-patro/>Jasabanta Patro</a>
|
<a href=/people/b/bidisha-samanta/>Bidisha Samanta</a>
|
<a href=/people/s/saurabh-singh/>Saurabh Singh</a>
|
<a href=/people/a/abhipsa-basu/>Abhipsa Basu</a>
|
<a href=/people/p/prithwish-mukherjee/>Prithwish Mukherjee</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/a/animesh-mukherjee/>Animesh Mukherjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1240><div class="card-body p-3 small">n this paper, we present a set of computational methods to identify the likeliness of a word being borrowed, based on the signals from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In terms of Spearman&#8217;s correlation values, our methods perform more than two times better (0.62) in predicting the borrowing likeliness compared to the best performing baseline (0.26) reported in literature. Based on this likeliness estimate we asked annotators to re-annotate the language tags of foreign words in predominantly native contexts. In 88 % of cases the annotators felt that the foreign language tag should be replaced by native language tag, thus indicating a huge scope for improvement of automatic language identification systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1241 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1241/>Multi-View Unsupervised User Feature Embedding for Social Media-based Substance Use Prediction</a></strong><br><a href=/people/t/tao-ding/>Tao Ding</a>
|
<a href=/people/w/warren-k-bickel/>Warren K. Bickel</a>
|
<a href=/people/s/shimei-pan/>Shimei Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1241><div class="card-body p-3 small">In this paper, we demonstrate how the state-of-the-art machine learning and text mining techniques can be used to build effective social media-based substance use detection systems. Since a substance use ground truth is difficult to obtain on a large scale, to maximize system performance, we explore different unsupervised feature learning methods to take advantage of a large amount of unsupervised social media data. We also demonstrate the benefit of using multi-view unsupervised feature learning to combine heterogeneous user information such as <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook likes</a> and status updates to enhance system performance. Based on our evaluation, our best models achieved 86 % <a href=https://en.wikipedia.org/wiki/Analysis_of_covariance>AUC</a> for predicting <a href=https://en.wikipedia.org/wiki/Tobacco_smoking>tobacco use</a>, 81 % for <a href=https://en.wikipedia.org/wiki/Alcoholic_drink>alcohol use</a> and 84 % for <a href=https://en.wikipedia.org/wiki/Substance_abuse>illicit drug use</a>, all of which significantly outperformed existing methods. Our investigation has also uncovered interesting relations between a <a href=https://en.wikipedia.org/wiki/User-generated_content>user&#8217;s social media behavior</a> (e.g., word usage) and <a href=https://en.wikipedia.org/wiki/Substance_use_disorder>substance use</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1242 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1242/>Demographic-aware word associations</a></strong><br><a href=/people/a/aparna-garimella/>Aparna Garimella</a>
|
<a href=/people/c/carmen-banea/>Carmen Banea</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1242><div class="card-body p-3 small">Variations of <a href=https://en.wikipedia.org/wiki/Word_association>word associations</a> across different groups of people can provide insights into people&#8217;s psychologies and their world views. To capture these variations, we introduce the task of demographic-aware word associations. We build a new gold standard dataset consisting of word association responses for approximately 300 stimulus words, collected from more than 800 respondents of different gender (male / female) and from different locations (India / United States), and show that there are significant variations in the word associations made by these groups. We also introduce a new demographic-aware word association model based on a neural net skip-gram architecture, and show how computational methods for measuring word associations that specifically account for writer demographics can outperform generic methods that are agnostic to such information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1243 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1243" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1243/>A Factored Neural Network Model for Characterizing Online Discussions in Vector Space</a></strong><br><a href=/people/h/hao-cheng/>Hao Cheng</a>
|
<a href=/people/h/hao-fang/>Hao Fang</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1243><div class="card-body p-3 small">We develop a novel factored neural model that learns comment embeddings in an unsupervised way leveraging the structure of distributional context in online discussion forums. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> links different context with related language factors in the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>embedding space</a>, providing a way to interpret the factored embeddings. Evaluated on a community endorsement prediction task using a large collection of topic-varying Reddit discussions, the factored embeddings consistently achieve improvement over other text representations. Qualitative analysis shows that the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> captures community style and topic, as well as response trigger patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1244.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1244 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1244 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1244/>Dimensions of Interpersonal Relationships : Corpus and Experiments</a></strong><br><a href=/people/f/farzana-rashid/>Farzana Rashid</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1244><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and experiments to determine dimensions of interpersonal relationships. We define a set of <a href=https://en.wikipedia.org/wiki/Dimension_(data_warehouse)>dimensions</a> heavily inspired by work in <a href=https://en.wikipedia.org/wiki/Social_science>social science</a>. We create a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by retrieving pairs of people, and then annotating <a href=https://en.wikipedia.org/wiki/Dimension_(data_warehouse)>dimensions</a> for their relationships. A corpus analysis shows that <a href=https://en.wikipedia.org/wiki/Dimension_(data_warehouse)>dimensions</a> can be annotated reliably. Experimental results show that given a pair of people, values to <a href=https://en.wikipedia.org/wiki/Dimension_(data_warehouse)>dimensions</a> can be assigned automatically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1245.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1245 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1245 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1245/>Argument Mining on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> : Arguments, Facts and Sources<span class=acl-fixed-case>T</span>witter: Arguments, Facts and Sources</a></strong><br><a href=/people/m/mihai-dusmanu/>Mihai Dusmanu</a>
|
<a href=/people/e/elena-cabrio/>Elena Cabrio</a>
|
<a href=/people/s/serena-villata/>Serena Villata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1245><div class="card-body p-3 small">Social media collect and spread on the Web personal opinions, facts, fake news and all kind of information users may be interested in. Applying argument mining methods to such heterogeneous data sources is a challenging open research issue, in particular considering the peculiarities of the language used to write textual messages on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In addition, new issues emerge when dealing with arguments posted on such platforms, such as the need to make a distinction between personal opinions and actual facts, and to detect the source disseminating information about such facts to allow for provenance verification. In this paper, we apply <a href=https://en.wikipedia.org/wiki/Supervised_classification>supervised classification</a> to identify arguments on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, and we present two new tasks for <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>, namely facts recognition and source identification. We study the feasibility of the approaches proposed to address these tasks on a set of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> related to the Grexit and Brexit news topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1246 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1246/>Distinguishing Japanese Non-standard Usages from Standard Ones<span class=acl-fixed-case>J</span>apanese Non-standard Usages from Standard Ones</a></strong><br><a href=/people/t/tatsuya-aoki/>Tatsuya Aoki</a>
|
<a href=/people/r/ryohei-sasano/>Ryohei Sasano</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1246><div class="card-body p-3 small">We focus on non-standard usages of common words on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In the context of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, words sometimes have other usages that are totally different from their original. In this study, we attempt to distinguish non-standard usages on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> from standard ones in an unsupervised manner. Our basic idea is that non-standardness can be measured by the inconsistency between the expected meaning of the target word and the given context. For this purpose, we use context embeddings derived from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Our experimental results show that the model leveraging the context embedding outperforms other methods and provide us with findings, for example, on how to construct context embeddings and which corpus to use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1247 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1247.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1247/>Connotation Frames of Power and Agency in Modern Films</a></strong><br><a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/m/marcella-cindy-prasettio/>Marcella Cindy Prasettio</a>
|
<a href=/people/a/ari-holtzman/>Ari Holtzman</a>
|
<a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1247><div class="card-body p-3 small">The framing of an action influences how we perceive its actor. We introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of <a href=https://en.wikipedia.org/wiki/Power_(social_and_political)>power</a> and <a href=https://en.wikipedia.org/wiki/Agency_(sociology)>agency</a> are implicitly projected on actors through their actions. We use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known <a href=https://en.wikipedia.org/wiki/Bechdel_test>Bechdel test</a>. Our contributions include an extended lexicon of connotation frames along with a <a href=https://en.wikipedia.org/wiki/User_interface>web interface</a> that provides a comprehensive analysis through the lens of connotation frames.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1248 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1248.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1248/>Controlling Human Perception of Basic User Traits</a></strong><br><a href=/people/d/daniel-preotiuc-pietro/>Daniel Preoţiuc-Pietro</a>
|
<a href=/people/s/sharath-chandra-guntuku/>Sharath Chandra Guntuku</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1248><div class="card-body p-3 small">Much of our <a href=https://en.wikipedia.org/wiki/Online_communication>online communication</a> is text-mediated and, lately, more common with <a href=https://en.wikipedia.org/wiki/Intelligent_agent>automated agents</a>. Unlike interacting with humans, these <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> currently do not tailor their language to the type of person they are communicating to. In this pilot study, we measure the extent to which human perception of basic user trait information gender and age is controllable through text. Using automatic models of gender and age prediction, we estimate which tweets posted by a user are more likely to mis-characterize his traits. We perform multiple controlled crowdsourcing experiments in which we show that we can reduce the human prediction accuracy of gender to almost random an over 20 % drop in accuracy. Our experiments show that it is practically feasible for multiple applications such as text generation, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a> or <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to be tailored to specific traits and perceived as such.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1250.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1250 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1250 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1250/>Assessing Objective Recommendation Quality through Political Forecasting</a></strong><br><a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a>
|
<a href=/people/m/masoud-rouhizadeh/>Masoud Rouhizadeh</a>
|
<a href=/people/m/michael-bishop/>Michael Bishop</a>
|
<a href=/people/p/philip-tetlock/>Philip Tetlock</a>
|
<a href=/people/b/barbara-mellers/>Barbara Mellers</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1250><div class="card-body p-3 small">Recommendations are often rated for their subjective quality, but few researchers have studied comment quality in terms of objective utility. We explore recommendation quality assessment with respect to both subjective (i.e. users&#8217; ratings) and objective (i.e., did it influence? did <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> improve decisions?) metrics in a massive online geopolitical forecasting system, ultimately comparing linguistic characteristics of each quality metric. Using a variety of <a href=https://en.wikipedia.org/wiki/Software_feature>features</a>, we predict all types of quality with better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than the simple yet strong baseline of comment length. Looking at the most predictive content illustrates rater biases ; for example, forecasters are subjectively biased in favor of comments mentioning business transactions or dealings as well as material things, even though such comments do not indeed prove any more useful objectively. Additionally, more complex <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence constructions</a>, as evidenced by subordinate conjunctions, are characteristic of comments leading to objective improvements in <a href=https://en.wikipedia.org/wiki/Forecasting>forecasting</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1251 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1251/>Never Abandon Minorities : Exhaustive Extraction of Bursty Phrases on <a href=https://en.wikipedia.org/wiki/Microblogging>Microblogs</a> Using Set Cover Problem</a></strong><br><a href=/people/m/masumi-shirakawa/>Masumi Shirakawa</a>
|
<a href=/people/t/takahiro-hara/>Takahiro Hara</a>
|
<a href=/people/t/takuya-maekawa/>Takuya Maekawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1251><div class="card-body p-3 small">We propose a language-independent data-driven method to exhaustively extract bursty phrases of arbitrary forms (e.g., phrases other than simple noun phrases) from <a href=https://en.wikipedia.org/wiki/Microblogging>microblogs</a>. The burst (i.e., the rapid increase of the occurrence) of a phrase causes the burst of overlapping N-grams including incomplete ones. In other words, bursty incomplete N-grams inevitably overlap bursty phrases. Thus, the proposed method performs the extraction of bursty phrases as the <a href=https://en.wikipedia.org/wiki/Set_cover_problem>set cover problem</a> in which all bursty N-grams are covered by a minimum set of bursty phrases. Experimental results using Japanese Twitter data showed that the proposed method outperformed word-based, noun phrase-based, and segmentation-based methods both in terms of accuracy and coverage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1252 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234174 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1252/>Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision</a></strong><br><a href=/people/h/haoruo-peng/>Haoruo Peng</a>
|
<a href=/people/m/ming-wei-chang/>Ming-Wei Chang</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1252><div class="card-body p-3 small">Neural networks have achieved state-of-the-art performance on several structured-output prediction tasks, trained in a fully supervised fashion. However, annotated examples in structured domains are often costly to obtain, which thus limits the applications of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. In this work, we propose Maximum Margin Reward Networks, a neural network-based framework that aims to learn from both explicit (full structures) and implicit supervision signals (delayed feedback on the correctness of the predicted structure). On <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, our model outperforms previous systems on the benchmark datasets, CoNLL-2003 and WebQuestionsSP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1253.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1253 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1253 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236158 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1253/>The Impact of Modeling Overall Argumentation with Tree Kernels</a></strong><br><a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a>
|
<a href=/people/d/dora-kiesel/>Dora Kiesel</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1253><div class="card-body p-3 small">Several approaches have been proposed to model either the explicit sequential structure of an argumentative text or its implicit hierarchical structure. So far, the adequacy of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> of overall argumentation remains unclear. This paper asks what type of <a href=https://en.wikipedia.org/wiki/Structure_(mathematical_logic)>structure</a> is actually important to tackle downstream tasks in computational argumentation. We analyze patterns in the overall argumentation of texts from three corpora. Then, we adapt the idea of positional tree kernels in order to capture sequential and hierarchical argumentative structure together for the first time. In systematic experiments for three text classification tasks, we find strong evidence for the impact of both types of <a href=https://en.wikipedia.org/wiki/Structure>structure</a>. Our results suggest that either of <a href=https://en.wikipedia.org/wiki/Cofactor_(biochemistry)>them</a> is necessary while their combination may be beneficial.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1254 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233944 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1254/>Learning Generic Sentence Representations Using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/z/zhe-gan/>Zhe Gan</a>
|
<a href=/people/y/yunchen-pu/>Yunchen Pu</a>
|
<a href=/people/r/ricardo-henao/>Ricardo Henao</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1254><div class="card-body p-3 small">We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes. The model is learned by using a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> as an <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to map an input sentence into a <a href=https://en.wikipedia.org/wiki/Continuous_or_discrete_variable>continuous vector</a>, and using a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>long short-term memory recurrent neural network</a> as a decoder. Several <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are considered, including sentence reconstruction and future sentence prediction. Further, a hierarchical encoder-decoder model is proposed to encode a sentence to predict multiple future sentences. By training our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on a large collection of <a href=https://en.wikipedia.org/wiki/Novel>novels</a>, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> over competing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1255 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235456 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1255/>Repeat before Forgetting : Spaced Repetition for Efficient and Effective Training of <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a></a></strong><br><a href=/people/h/hadi-amiri/>Hadi Amiri</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1255><div class="card-body p-3 small">We present a novel approach for training <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>artificial neural networks</a>. Our approach is inspired by broad evidence in psychology that shows human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (spaced repetition). We investigate the analogy between training neural models and findings in psychology about human memory model and develop an efficient and effective <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to train neural models. The core part of our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is a cognitively-motivated scheduler according to which training instances and their reviews are spaced over time. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> uses only 34-50 % of data per epoch, is 2.9-4.8 times faster than standard <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, and outperforms competing state-of-the-art baselines. Our <a href=https://en.wikipedia.org/wiki/Code>code</a> is available at.<url>scholar.harvard.edu/hadi/RbF/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1256 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235246 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1256/>Part-of-Speech Tagging for <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> with Adversarial Neural Networks<span class=acl-fixed-case>T</span>witter with Adversarial Neural Networks</a></strong><br><a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/h/haoran-huang/>Haoran Huang</a>
|
<a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1256><div class="card-body p-3 small">In this work, we study the problem of <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> for <a href=https://en.wikipedia.org/wiki/Twitter>Tweets</a>. In contrast to newswire articles, <a href=https://en.wikipedia.org/wiki/Twitter>Tweets</a> are usually informal and contain numerous out-of-vocabulary words. Moreover, there is a lack of large scale labeled datasets for this <a href=https://en.wikipedia.org/wiki/Domain_(mathematical_analysis)>domain</a>. To tackle these challenges, we propose a novel <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to make use of out-of-domain labeled data, unlabeled in-domain data, and labeled in-domain data. Inspired by adversarial neural networks, the proposed method tries to learn common features through adversarial discriminator. In addition, we hypothesize that domain-specific features of target domain should be preserved in some degree. Hence, the proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> adopts a sequence-to-sequence autoencoder to perform this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. Experimental results on three different datasets show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves better performance than state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1257.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1257 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1257 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1257/>Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings</a></strong><br><a href=/people/b/bofang-li/>Bofang Li</a>
|
<a href=/people/t/tao-liu/>Tao Liu</a>
|
<a href=/people/z/zhe-zhao/>Zhe Zhao</a>
|
<a href=/people/b/buzhou-tang/>Buzhou Tang</a>
|
<a href=/people/a/aleksandr-drozd/>Aleksandr Drozd</a>
|
<a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/x/xiaoyong-du/>Xiaoyong Du</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1257><div class="card-body p-3 small">The number of word embedding models is growing every year. Most of them are based on the co-occurrence information of words and their contexts. However, it is still an open question what is the best definition of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>. We provide a systematical investigation of 4 different <a href=https://en.wikipedia.org/wiki/Context_(language_use)>syntactic context types</a> and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context representations</a> for learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Comprehensive experiments are conducted to evaluate their effectiveness on 6 extrinsic and intrinsic tasks. We hope that this paper, along with the published code, would be helpful for choosing the best context type and representation for a given task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1258 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238232586 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1258" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1258/>Does syntax help discourse segmentation? Not so much</a></strong><br><a href=/people/c/chloe-braud/>Chloé Braud</a>
|
<a href=/people/o/ophelie-lacroix/>Ophélie Lacroix</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1258><div class="card-body p-3 small">Discourse segmentation is the first step in building <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse parsers</a>. Most work on discourse segmentation does not scale to real-world discourse parsing across languages, for two reasons : (i) models rely on constituent trees, and (ii) experiments have relied on gold standard identification of sentence and token boundaries. We therefore investigate to what extent <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituents</a> can be replaced with universal dependencies, or left out completely, as well as how state-of-the-art <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>segmenters</a> fare in the absence of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence boundaries</a>. Our results show that dependency information is less useful than expected, but we provide a fully scalable, robust model that only relies on part-of-speech information, and show that it performs well across languages in the absence of any gold-standard annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1259 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238232142 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1259/>Deal or No Deal? End-to-End Learning of Negotiation Dialogues</a></strong><br><a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/d/denis-yarats/>Denis Yarats</a>
|
<a href=/people/y/yann-dauphin/>Yann Dauphin</a>
|
<a href=/people/d/devi-parikh/>Devi Parikh</a>
|
<a href=/people/d/dhruv-batra/>Dhruv Batra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1259><div class="card-body p-3 small">Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a>. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who can not observe each other&#8217;s reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for <a href=https://en.wikipedia.org/wiki/Negotiation>negotiation</a>, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1260.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1260 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1260 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1260.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231562 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1260/>Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning<span class=acl-fixed-case>DQN</span> for Safe and Efficient On-line Dialogue Policy Learning</a></strong><br><a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/x/xiang-zhou/>Xiang Zhou</a>
|
<a href=/people/c/cheng-chang/>Cheng Chang</a>
|
<a href=/people/r/runzhe-yang/>Runzhe Yang</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1260><div class="card-body p-3 small">Hand-crafted rules and reinforcement learning (RL) are two popular choices to obtain dialogue policy. The rule-based policy is often reliable within predefined scope but not self-adaptable, whereas RL is evolvable with data but often suffers from a bad initial performance. We employ a companion learning framework to integrate the two approaches for on-line dialogue policy learning, in which a pre-defined rule-based policy acts as a teacher and guides a data-driven RL system by giving example actions as well as additional rewards. A novel agent-aware dropout Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teacher&#8217;s experiences. AAD-DQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by <a href=https://en.wikipedia.org/wiki/Dropping_out>dropout</a> to control the timing of consultation and learning. Simulation experiments showed that the proposed approach can significantly improve both safetyand efficiency of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus.<i>companion learning</i> framework to integrate the two approaches for <i>on-line</i> dialogue policy learning, in which a pre-defined rule-based policy acts as a &#8220;teacher&#8221; and guides a data-driven RL system by giving example actions as well as additional rewards. A novel <i>agent-aware dropout</i> Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teacher&#8217;s experiences. AAD-DQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by dropout to control the timing of consultation and learning. Simulation experiments showed that the proposed approach can significantly improve both <i>safety</i>\n\nand <i>efficiency</i> of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1261.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1261 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1261 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236302 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1261/>Towards Debate Automation : a Recurrent Model for Predicting Debate Winners</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1261><div class="card-body p-3 small">In this paper we introduce a practical first step towards the creation of an automated debate agent : a state-of-the-art recurrent predictive model for predicting debate winners. By having an accurate <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a>, we are able to objectively rate the quality of a statement made at a specific turn in a debate. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is based on a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network architecture</a> with <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, which allows the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to effectively account for the entire debate when making its prediction. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on a dataset of debate transcripts annotated with <a href=https://en.wikipedia.org/wiki/Audience_measurement>audience favorability</a> of the debate teams. Finally, we discuss how future work can leverage our proposed model for the creation of an automated debate agent. We accomplish this by determining the model input that will maximize audience favorability toward a given side of a debate at an arbitrary turn.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1262 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1262" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1262/>Further Investigation into Reference Bias in Monolingual Evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1262><div class="card-body p-3 small">Monolingual evaluation of Machine Translation (MT) aims to simplify human assessment by requiring assessors to compare the meaning of the MT output with a reference translation, opening up the task to a much larger pool of genuinely qualified evaluators. Monolingual evaluation runs the risk, however, of bias in favour of MT systems that happen to produce translations superficially similar to the reference and, consistent with this intuition, previous investigations have concluded monolingual assessment to be strongly biased in this respect. On re-examination of past analyses, we identify a series of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results for show no significant evidence of reference bias in monolingual evaluation of MT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1263 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1263.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1263/>A Challenge Set Approach to Evaluating <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/p/pierre-isabelle/>Pierre Isabelle</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/g/george-foster/>George Foster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1263><div class="card-body p-3 small">Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and <a href=https://en.wikipedia.org/wiki/Error_analysis_(linguistics)>error analysis</a>. A challenge set consists of a small set of sentences, each hand-designed to probe a system&#8217;s capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> provides not only a more fine-grained picture of the strengths of <a href=https://en.wikipedia.org/wiki/Nervous_system>neural systems</a>, but also insight into which linguistic phenomena remain out of reach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1264 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1264/>Knowledge Distillation for Bilingual Dictionary Induction</a></strong><br><a href=/people/n/ndapandula-nakashole/>Ndapandula Nakashole</a>
|
<a href=/people/r/raphael-flauger/>Raphael Flauger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1264><div class="card-body p-3 small">Leveraging zero-shot learning to learn <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>mapping functions</a> between <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a> of different languages is a promising approach to bilingual dictionary induction. However, <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> using this <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> have not yet achieved high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In this paper, we propose a bridging approach, where our main contribution is a knowledge distillation training objective. As teachers, rich resource translation paths are exploited in this <a href=https://en.wikipedia.org/wiki/Role>role</a>. And as learners, translation paths involving low resource languages learn from the teachers. Our training objective allows seamless addition of teacher translation paths for any given low resource pair. Since our approach relies on the quality of monolingual word embeddings, we also propose to enhance <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representations</a> of both the source and target language with <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a>. Our experiments on various languages show large performance gains from our distillation training objective, obtaining as high as 17 % accuracy improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1265.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1265 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1265 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1265/>Machine Translation, it’s a question of style, innit? The case of English tag questions<span class=acl-fixed-case>E</span>nglish tag questions</a></strong><br><a href=/people/r/rachel-bawden/>Rachel Bawden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1265><div class="card-body p-3 small">In this paper, we address the problem of generating English tag questions (TQs) (e.g. it is, is n&#8217;t it?) in Machine Translation (MT). We propose a post-edition solution, formulating the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a multi-class classification task. We present (i) the automatic annotation of English TQs in a parallel corpus of subtitles and (ii) an approach using a series of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> to predict TQ forms, which we use to post-edit state-of-the-art MT outputs. Our method provides significant improvements in English TQ translation when translating from <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, in turn improving the fluidity, naturalness, grammatical correctness and pragmatic coherence of MT output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1266 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1266/>Deciphering Related Languages</a></strong><br><a href=/people/n/nima-pourdamghani/>Nima Pourdamghani</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1266><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for <a href=https://en.wikipedia.org/wiki/Translation>translating texts</a> between close language pairs. The <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> does not require parallel data, and it does not require the languages to be written in the same script. We show results for six language pairs : Afrikaans / Dutch, Bosnian / Serbian, Danish / Swedish, Macedonian / Bulgarian, Malaysian / Indonesian, and Polish / Belorussian. We report BLEU scores showing our method to outperform others that do not use parallel data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1267 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1267" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1267/>Identifying Cognate Sets Across Dictionaries of Related Languages</a></strong><br><a href=/people/a/adam-st-arnaud/>Adam St Arnaud</a>
|
<a href=/people/d/david-beck/>David Beck</a>
|
<a href=/people/g/grzegorz-kondrak/>Grzegorz Kondrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1267><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/System>system</a> for identifying <a href=https://en.wikipedia.org/wiki/Cognate>cognate sets</a> across dictionaries of related languages. The likelihood of a <a href=https://en.wikipedia.org/wiki/Cognate>cognate relationship</a> is calculated on the basis of a rich set of <a href=https://en.wikipedia.org/wiki/Distinctive_feature>features</a> that capture both <a href=https://en.wikipedia.org/wiki/Semantic_similarity>phonetic and semantic similarity</a>, as well as the presence of regular sound correspondences. The similarity scores are used to cluster words from different languages that may originate from a common proto-word. When tested on the <a href=https://en.wikipedia.org/wiki/Algonquian_languages>Algonquian language family</a>, our system detects 63 % of cognate sets while maintaining cluster purity of 70 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1268.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1268 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1268 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1268" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1268/>Learning Language Representations for Typology Prediction</a></strong><br><a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/p/patrick-littell/>Patrick Littell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1268><div class="card-body p-3 small">One central mystery of neural NLP is what neural models know about their subject matter. When a neural machine translation system learns to translate from one language to another, does it learn the <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> or semantics of the languages? Can this knowledge be extracted from the <a href=https://en.wikipedia.org/wiki/System>system</a> to fill holes in human scientific knowledge? Existing typological databases contain relatively full feature specifications for only a few hundred languages. Exploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one NMT system from 1017 languages into <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and use this to predict information missing from typological databases. Experiments show that the proposed method is able to infer not only syntactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages geographic and phylogenetic neighbors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1269.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1269 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1269 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1269/>Cheap Translation for Cross-Lingual Named Entity Recognition</a></strong><br><a href=/people/s/stephen-mayhew/>Stephen Mayhew</a>
|
<a href=/people/c/chen-tse-tsai/>Chen-Tse Tsai</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1269><div class="card-body p-3 small">Recent work in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has attempted to deal with low-resource languages but still assumed a resource level that is not present for most <a href=https://en.wikipedia.org/wiki/Language>languages</a>, e.g., the availability of <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> in the target language. We propose a simple method for cross-lingual named entity recognition (NER) that works well in settings with very minimal resources. Our approach makes use of a <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> to translate annotated data available in one or several high resource language(s) into the target language, and learns a standard monolingual NER model there. Further, when <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> is available in the target language, our method can enhance <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia based methods</a> to yield state-of-the-art NER results ; we evaluate on 7 diverse languages, improving the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by an average of 5.5 % F1 points. With the minimal resources required, this is an extremely portable cross-lingual NER approach, as illustrated using a truly low-resource language, <a href=https://en.wikipedia.org/wiki/Uyghur_language>Uyghur</a>.<i>very</i> minimal resources. Our approach makes use of a lexicon to &#8220;translate&#8221; annotated data available in one or several high resource language(s) into the target language, and learns a standard monolingual NER model there. Further, when Wikipedia is available in the target language, our method can enhance Wikipedia based methods to yield state-of-the-art NER results; we evaluate on 7 diverse languages, improving the state-of-the-art by an average of 5.5% F1 points. With the minimal resources required, this is an extremely portable cross-lingual NER approach, as illustrated using a truly low-resource language, Uyghur.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1270 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1270.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1270/>Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1270><div class="card-body p-3 small">Existing approaches to automatic VerbNet-style verb classification are heavily dependent on <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> and therefore limited to languages with mature <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP pipelines</a>. In this work, we propose a novel cross-lingual transfer method for inducing VerbNets for multiple languages. To the best of our knowledge, this is the first study which demonstrates how the architectures for learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> can be applied to this challenging syntactic-semantic task. Our method uses cross-lingual translation pairs to tie each of the six target languages into a bilingual vector space with English, jointly specialising the representations to encode the relational information from English VerbNet. A standard clustering algorithm is then run on top of the VerbNet-specialised representations, using <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>vector dimensions</a> as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for learning verb classes. Our results show that the proposed cross-lingual transfer approach sets new state-of-the-art verb classification performance across all six target languages explored in this work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1271 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1271" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1271/>Classification of telicity using cross-linguistic annotation projection</a></strong><br><a href=/people/a/annemarie-friedrich/>Annemarie Friedrich</a>
|
<a href=/people/d/damyana-gateva/>Damyana Gateva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1271><div class="card-body p-3 small">This paper addresses the automatic recognition of telicity, an aspectual notion. A telic event includes a natural endpoint (she walked home), while an atelic event does not (she walked around). Recognizing this difference is a prerequisite for temporal natural language understanding. In <a href=https://en.wikipedia.org/wiki/English_language>English</a>, this classification task is difficult, as <a href=https://en.wikipedia.org/wiki/Telicity>telicity</a> is a covert linguistic category. In contrast, in <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic languages</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a> is part of a verb&#8217;s meaning and even available in <a href=https://en.wikipedia.org/wiki/Machine-readable_dictionary>machine-readable dictionaries</a>. Our contributions are as follows. We successfully leverage additional silver standard training data in the form of projected annotations from parallel English-Czech data as well as context information, improving automatic telicity classification for English significantly compared to previous work. We also create a new data set of <a href=https://en.wikipedia.org/wiki/English_literature>English texts</a> manually annotated with <a href=https://en.wikipedia.org/wiki/Telicity>telicity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1274.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1274 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1274 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1274/>Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures</a></strong><br><a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/a/avirup-sil/>Avirup Sil</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/r/radu-florian/>Radu Florian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1274><div class="card-body p-3 small">Slot Filling (SF) aims to extract the values of certain types of attributes (or slots, such as person : cities_of_residence) for a given entity from a large collection of source documents. In this paper we propose an effective DNN architecture for SF with the following new strategies : (1). Take a regularized dependency graph instead of a raw sentence as input to DNN, to compress the wide contexts between query and candidate filler ; (2). Incorporate two attention mechanisms : local attention learned from query and candidate filler, and global attention learned from external knowledge bases, to guide the model to better select indicative contexts to determine slot type. Experiments show that this framework outperforms <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on both <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> (16 % absolute F-score gain) and slot filling validation for each individual <a href=https://en.wikipedia.org/wiki/System>system</a> (up to 8.5 % absolute F-score gain).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1275 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1275.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1275" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1275/>Identifying Products in Online Cybercrime Marketplaces : A Dataset for Fine-grained Domain Adaptation</a></strong><br><a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a>
|
<a href=/people/r/rebecca-portnoff/>Rebecca Portnoff</a>
|
<a href=/people/s/sadia-afroz/>Sadia Afroz</a>
|
<a href=/people/d/damon-mccoy/>Damon McCoy</a>
|
<a href=/people/k/kirill-levchenko/>Kirill Levchenko</a>
|
<a href=/people/v/vern-paxson/>Vern Paxson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1275><div class="card-body p-3 small">One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and annotate <a href=https://en.wikipedia.org/wiki/Data>data</a> from four different forums. Each of these <a href=https://en.wikipedia.org/wiki/Internet_forum>forums</a> constitutes its own fine-grained domain in that the <a href=https://en.wikipedia.org/wiki/Internet_forum>forums</a> cover different market sectors with different properties, even though all forums are in the broad domain of <a href=https://en.wikipedia.org/wiki/Cybercrime>cybercrime</a>. We characterize these domain differences in the context of a <a href=https://en.wikipedia.org/wiki/Machine_learning>learning-based system</a> : <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a> see decreased accuracy when applied to new forums, and standard techniques for <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a> and <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> have limited effectiveness on this data, which suggests the need to improve these techniques. We release a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 1,938 annotated posts from across the four forums.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1276.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1276 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1276 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1276.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1276/>Labeling Gaps Between Words : Recognizing Overlapping Mentions with Mention Separators</a></strong><br><a href=/people/a/aldrian-obaja-muis/>Aldrian Obaja Muis</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1276><div class="card-body p-3 small">In this paper, we propose a new <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> that is capable of recognizing overlapping mentions. We introduce a novel notion of mention separators that can be effectively used to capture how mentions overlap with one another. On top of a novel multigraph representation that we introduce, we show that efficient and exact <a href=https://en.wikipedia.org/wiki/Inference>inference</a> can still be performed. We present some theoretical analysis on the differences between our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and a recently proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for recognizing overlapping mentions, and discuss the possible implications of the differences. Through extensive empirical analysis on standard datasets, we demonstrate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1277.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1277 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1277 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1277.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1277" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1277/>Deep Joint Entity Disambiguation with Local Neural Attention</a></strong><br><a href=/people/o/octavian-eugen-ganea/>Octavian-Eugen Ganea</a>
|
<a href=/people/t/thomas-hofmann/>Thomas Hofmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1277><div class="card-body p-3 small">We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> with more traditional approaches such as <a href=https://en.wikipedia.org/wiki/Graphical_model>graphical models</a> and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> at moderate computational costs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1278.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1278 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1278 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1278/>MinIE : Minimizing Facts in Open Information Extraction<span class=acl-fixed-case>M</span>in<span class=acl-fixed-case>IE</span>: Minimizing Facts in Open Information Extraction</a></strong><br><a href=/people/k/kiril-gashteovski/>Kiril Gashteovski</a>
|
<a href=/people/r/rainer-gemulla/>Rainer Gemulla</a>
|
<a href=/people/l/luciano-del-corro/>Luciano del Corro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1278><div class="card-body p-3 small">The goal of Open Information Extraction (OIE) is to extract surface relations and their arguments from natural-language text in an unsupervised, domain-independent manner. In this paper, we propose MinIE, an OIE system that aims to provide useful, compact extractions with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and <a href=https://en.wikipedia.org/wiki/Precision_and_recall>recall</a>. MinIE approaches these goals by (1) representing information about polarity, <a href=https://en.wikipedia.org/wiki/Linguistic_modality>modality</a>, <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>attribution</a>, and quantities with semantic annotations instead of in the actual extraction, and (2) identifying and removing parts that are considered overly specific. We conducted an experimental study with several real-world datasets and found that MinIE achieves competitive or higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and <a href=https://en.wikipedia.org/wiki/Precision_and_recall>recall</a> than most prior systems, while at the same time producing shorter, semantically enriched extractions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1279.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1279 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1279 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1279/>Scientific Information Extraction with Semi-supervised Neural Tagging</a></strong><br><a href=/people/y/yi-luan/>Yi Luan</a>
|
<a href=/people/m/mari-ostendorf/>Mari Ostendorf</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1279><div class="card-body p-3 small">This paper addresses the problem of extracting keyphrases from <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific articles</a> and categorizing them as corresponding to a task, process, or material. We cast the problem as sequence tagging and introduce <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised methods</a> to a neural tagging model, which builds on recent advances in <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> performance on the 2017 SemEval Task 10 ScienceIE task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1280 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1280/>NITE : A Neural Inductive Teaching Framework for Domain Specific NER<span class=acl-fixed-case>NITE</span>: A Neural Inductive Teaching Framework for Domain Specific <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/s/siliang-tang/>Siliang Tang</a>
|
<a href=/people/n/ning-zhang/>Ning Zhang</a>
|
<a href=/people/j/jinjiang-zhang/>Jinjiang Zhang</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1280><div class="card-body p-3 small">In domain-specific NER, due to insufficient labeled training data, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep models</a> usually fail to behave normally. In this paper, we proposed a novel Neural Inductive TEaching framework (NITE) to transfer knowledge from existing domain-specific NER models into an arbitrary <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> in a teacher-student training manner. NITE is a general framework that builds upon <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and <a href=https://en.wikipedia.org/wiki/Multiple_instance_learning>multiple instance learning</a>, which collaboratively not only transfers knowledge to a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep student network</a> but also reduces the noise from teachers. NITE can help <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> to effectively utilize existing <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resources</a> (i.e., <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, labeled and unlabeled data) in a small domain. The experiment resulted on Disease NER proved that without using any labeled data, NITE can significantly boost the performance of a CNN-bidirectional LSTM-CRF NER neural network nearly over 30 % in terms of F1-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1281 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1281.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1281" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1281/>Speeding up Reinforcement Learning-based Information Extraction Training using Asynchronous Methods</a></strong><br><a href=/people/a/aditya-sharma/>Aditya Sharma</a>
|
<a href=/people/z/zarana-parekh/>Zarana Parekh</a>
|
<a href=/people/p/partha-talukdar/>Partha Talukdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1281><div class="card-body p-3 small">RLIE-DQN is a recently proposed Reinforcement Learning-based Information Extraction (IE) technique which is able to incorporate external evidence during the extraction process. RLIE-DQN trains a single agent sequentially, training on one instance at a time. This results in significant training slowdown which is undesirable. We leverage recent advances in parallel RL training using <a href=https://en.wikipedia.org/wiki/Asynchronous_I/O>asynchronous methods</a> and propose RLIE-A3C. RLIE-A3C trains multiple agents in parallel and is able to achieve upto 6x training speedup over RLIE-DQN, while suffering no loss in average accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1282.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1282 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1282 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1282.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1282" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1282/>Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks</a></strong><br><a href=/people/p/peng-hsuan-li/>Peng-Hsuan Li</a>
|
<a href=/people/r/ruo-ping-dong/>Ruo-Ping Dong</a>
|
<a href=/people/y/yu-siang-wang/>Yu-Siang Wang</a>
|
<a href=/people/j/ju-chieh-chou/>Ju-Chieh Chou</a>
|
<a href=/people/w/wei-yun-ma/>Wei-Yun Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1282><div class="card-body p-3 small">In this paper, we utilize the linguistic structures of texts to improve <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> by BRNN-CNN, a special bidirectional recursive network attached with a convolutional network. Motivated by the observation that <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> are highly related to <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>linguistic constituents</a>, we propose a constituent-based BRNN-CNN for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. In contrast to classical sequential labeling methods, the system first identifies which text chunks are possible named entities by whether they are <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>linguistic constituents</a>. Then it classifies these chunks with a constituency tree structure by recursively propagating syntactic and semantic information to each constituent node. This <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> surpasses current state-of-the-art on OntoNotes 5.0 with automatically generated parses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1283 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1283" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1283/>Fast and Accurate Entity Recognition with Iterated Dilated Convolutions</a></strong><br><a href=/people/e/emma-strubell/>Emma Strubell</a>
|
<a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/d/david-belanger/>David Belanger</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1283><div class="card-body p-3 small">Today when many practitioners run basic NLP on the entire <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> and large-volume traffic, faster methods are paramount to saving time and energy costs. Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining per-token vector representations serving as input to labeling tasks such as NER (often followed by prediction in a linear-chain CRF). Though expressive and accurate, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> fail to fully exploit <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU parallelism</a>, limiting their computational efficiency. This paper proposes a faster alternative to Bi-LSTMs for NER : Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallelism</a>, ID-CNNs permit fixed-depth convolutions to run in parallel across entire documents. We describe a distinct combination of network structure, parameter sharing and training procedures that enable dramatic 14-20x test-time speedups while retaining <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire document are more accurate than Bi-LSTM-CRFs while attaining 8x faster test time speeds.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1284.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1284 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1284 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1284/>Entity Linking via Joint Encoding of Types, Descriptions, and Context</a></strong><br><a href=/people/n/nitish-gupta/>Nitish Gupta</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1284><div class="card-body p-3 small">For accurate <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a>, we need to capture various information aspects of an entity, such as its description in a <a href=https://en.wikipedia.org/wiki/Knowledge_base>KB</a>, contexts in which it is mentioned, and structured knowledge. Additionally, a linking system should work on texts from different domains without requiring domain-specific training data or hand-engineered features. In this work we present a neural, modular entity linking system that learns a unified dense representation for each entity using multiple sources of information, such as its description, contexts around its mentions, and its fine-grained types. We show that the resulting entity linking system is effective at combining these sources, and performs competitively, sometimes out-performing current state-of-the-art systems across datasets, without requiring any domain-specific training data or hand-engineered features. We also show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can effectively embed entities that are new to the KB, and is able to link its mentions accurately.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1285 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1285/>An Insight Extraction System on BioMedical Literature with Deep Neural Networks<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>M</span>edical Literature with Deep Neural Networks</a></strong><br><a href=/people/h/hua-he/>Hua He</a>
|
<a href=/people/k/kris-ganjam/>Kris Ganjam</a>
|
<a href=/people/n/navendu-jain/>Navendu Jain</a>
|
<a href=/people/j/jessica-lundin/>Jessica Lundin</a>
|
<a href=/people/r/ryen-white/>Ryen White</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1285><div class="card-body p-3 small">Mining biomedical text offers an opportunity to automatically discover important facts and infer associations among them. As new scientific findings appear across a large collection of biomedical publications, our aim is to tap into this literature to automate biomedical knowledge extraction and identify important insights from them. Towards that goal, we develop a <a href=https://en.wikipedia.org/wiki/System>system</a> with novel <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> to extract insights on biomedical literature. Evaluation shows our <a href=https://en.wikipedia.org/wiki/System>system</a> is able to provide insights with competitive accuracy of human acceptance and its relation extraction component outperforms previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1286.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1286 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1286 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1286/>Word Etymology as Native Language Interference</a></strong><br><a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1286><div class="card-body p-3 small">We present experiments that show the influence of <a href=https://en.wikipedia.org/wiki/First_language>native language</a> on <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a> when producing text in another language in this particular case <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We start from the premise that non-native English speakers will choose lexical items that are close to words in their native language. This leads us to an etymology-based representation of documents written by people whose mother tongue is an <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European language</a>. Based on this <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a> we grow a <a href=https://en.wikipedia.org/wiki/Language_family>language family tree</a>, that matches closely the <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European language tree</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1287 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1287/>A Simpler and More Generalizable Story Detector using Verb and Character Features</a></strong><br><a href=/people/j/joshua-eisenberg/>Joshua Eisenberg</a>
|
<a href=/people/m/mark-finlayson/>Mark Finlayson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1287><div class="card-body p-3 small">Story detection is the task of determining whether or not a unit of text contains a story. Prior approaches achieved a maximum performance of 0.66 F1, and did not generalize well across different corpora. We present a new state-of-the-art <a href=https://en.wikipedia.org/wiki/Sensor>detector</a> that achieves a maximum performance of 0.75 <a href=https://en.wikipedia.org/wiki/F-number>F1</a> (a 14 % improvement), with significantly greater generalizability than previous work. In particular, our detector achieves performance above 0.70 <a href=https://en.wikipedia.org/wiki/F-number>F1</a> across a variety of combinations of lexically different corpora for training and testing, as well as dramatic improvements (up to 4,000 %) in performance when trained on a small, disfluent data set. The new <a href=https://en.wikipedia.org/wiki/Sensor>detector</a> uses two basic types of featuresones related to <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a>, and ones related to characterstotaling 283 specific features overall ; previous detectors used tens of thousands of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, and so this <a href=https://en.wikipedia.org/wiki/Sensor>detector</a> represents a significant simplification along with increased performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1288 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1288/>Multi-modular domain-tailored OCR post-correction<span class=acl-fixed-case>OCR</span> post-correction</a></strong><br><a href=/people/s/sarah-schulz/>Sarah Schulz</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1288><div class="card-body p-3 small">One of the main obstacles for many Digital Humanities projects is the low data availability. Texts have to be digitized in an expensive and time consuming process whereas Optical Character Recognition (OCR) post-correction is one of the time-critical factors. At the example of OCR post-correction, we show the adaptation of a generic system to solve a specific problem with little data. The system accounts for a diversity of errors encountered in OCRed texts coming from different time periods in the domain of literature. We show that the combination of different approaches, such as e.g. Statistical Machine Translation and <a href=https://en.wikipedia.org/wiki/Spell_checker>spell checking</a>, with the help of a ranking mechanism tremendously improves over single-handed approaches. Since we consider the accessibility of the resulting tool as a crucial part of Digital Humanities collaborations, we describe the <a href=https://en.wikipedia.org/wiki/Workflow>workflow</a> we suggest for efficient text recognition and subsequent automatic and manual post-correction</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1289 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1289/>Learning to Predict Charges for Criminal Cases with Legal Basis</a></strong><br><a href=/people/b/bingfeng-luo/>Bingfeng Luo</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/j/jianbo-xu/>Jianbo Xu</a>
|
<a href=/people/x/xiang-zhang/>Xiang Zhang</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1289><div class="card-body p-3 small">The charge prediction task is to determine appropriate charges for a given case, which is helpful for legal assistant systems where the user input is fact description. We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1290 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1290/>Quantifying the Effects of Text Duplication on Semantic Models</a></strong><br><a href=/people/a/alexandra-schofield/>Alexandra Schofield</a>
|
<a href=/people/l/laure-thompson/>Laure Thompson</a>
|
<a href=/people/d/david-mimno/>David Mimno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1290><div class="card-body p-3 small">Duplicate documents are a pervasive problem in text datasets and can have a strong effect on <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised models</a>. Methods to remove duplicate texts are typically heuristic or very expensive, so it is vital to know when and why they are needed. We measure the sensitivity of two latent semantic methods to the presence of different levels of <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>document repetition</a>. By artificially creating different forms of duplicate text we confirm several hypotheses about how repeated text impacts <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. While a small amount of duplication is tolerable, substantial over-representation of subsets of the text may overwhelm meaningful topical patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1291.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1291 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1291 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1291/>Identifying Semantically Deviating Outlier Documents</a></strong><br><a href=/people/h/honglei-zhuang/>Honglei Zhuang</a>
|
<a href=/people/c/chi-wang/>Chi Wang</a>
|
<a href=/people/f/fangbo-tao/>Fangbo Tao</a>
|
<a href=/people/l/lance-kaplan/>Lance Kaplan</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1291><div class="card-body p-3 small">A document outlier is a document that substantially deviates in semantics from the majority ones in a corpus. Automatic identification of document outliers can be valuable in many <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>, such as <a href=https://en.wikipedia.org/wiki/Screening_(medicine)>screening health records</a> for <a href=https://en.wikipedia.org/wiki/Medical_error>medical mistakes</a>. In this paper, we study the problem of mining semantically deviating document outliers in a given corpus. We develop a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> to identify frequent and characteristic semantic regions in the word embedding space to represent the given corpus, and a robust outlierness measure which is resistant to noisy content in documents. Experiments conducted on two real-world textual data sets show that our method can achieve an up to 135 % improvement over baselines in terms of <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> at top-1 % of the outlier ranking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1292.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1292 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1292 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1292/>Detecting and Explaining Causes From Text For a Time Series Event</a></strong><br><a href=/people/d/dongyeop-kang/>Dongyeop Kang</a>
|
<a href=/people/v/varun-gangal/>Varun Gangal</a>
|
<a href=/people/a/ang-lu/>Ang Lu</a>
|
<a href=/people/z/zheng-chen/>Zheng Chen</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1292><div class="card-body p-3 small">Explaining underlying causes or effects about events is a challenging but valuable task. We define a novel problem of generating explanations of a time series event by (1) searching cause and effect relationships of the time series with textual data and (2) constructing a connecting chain between them to generate an explanation. To detect causal features from text, we propose a novel method based on the Granger causality of time series between <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted from text such as <a href=https://en.wikipedia.org/wiki/N-gram>N-grams</a>, topics, sentiments, and their composition. The generation of the sequence of causal entities requires a commonsense causative knowledge base with efficient <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1293.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1293 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1293 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1293/>A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential Data of MOOC<span class=acl-fixed-case>MOOC</span></a></strong><br><a href=/people/z/zhuoxuan-jiang/>Zhuoxuan Jiang</a>
|
<a href=/people/s/shanshan-feng/>Shanshan Feng</a>
|
<a href=/people/g/gao-cong/>Gao Cong</a>
|
<a href=/people/c/chunyan-miao/>Chunyan Miao</a>
|
<a href=/people/x/xiaoming-li/>Xiaoming Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1293><div class="card-body p-3 small">Recent years have witnessed the proliferation of Massive Open Online Courses (MOOCs). With massive learners being offered <a href=https://en.wikipedia.org/wiki/Massive_open_online_course>MOOCs</a>, there is a demand that the forum contents within <a href=https://en.wikipedia.org/wiki/Massive_open_online_course>MOOCs</a> need to be classified in order to facilitate both learners and instructors. Therefore we investigate a significant <a href=https://en.wikipedia.org/wiki/Application_software>application</a>, which is to associate <a href=https://en.wikipedia.org/wiki/Internet_forum>forum threads</a> to subtitles of video clips. This task can be regarded as a document ranking problem, and the key is how to learn a distinguishable text representation from word sequences and learners&#8217; behavior sequences. In this paper, we propose a novel cascade model, which can capture both the latent semantics and latent similarity by modeling <a href=https://en.wikipedia.org/wiki/Massive_open_online_course>MOOC data</a>. Experimental results on two real-world datasets demonstrate that our textual representation outperforms state-of-the-art unsupervised counterparts for the application.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1294.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1294 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1294 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1294.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1294/>Identifying the Provision of Choices in Privacy Policy Text</a></strong><br><a href=/people/k/kanthashree-mysore-sathyendra/>Kanthashree Mysore Sathyendra</a>
|
<a href=/people/s/shomir-wilson/>Shomir Wilson</a>
|
<a href=/people/f/florian-schaub/>Florian Schaub</a>
|
<a href=/people/s/sebastian-zimmeck/>Sebastian Zimmeck</a>
|
<a href=/people/n/norman-sadeh/>Norman Sadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1294><div class="card-body p-3 small">Websites&#8217; and mobile apps&#8217; privacy policies, written in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, tend to be long and difficult to understand. Information privacy revolves around the fundamental principle of Notice and choice, namely the idea that users should be able to make informed decisions about what information about them can be collected and how it can be used. Internet users want control over their privacy, but their choices are often hidden in long and convoluted privacy policy texts. Moreover, little (if any) prior work has been done to detect the provision of choices in text. We address this challenge of enabling user choice by automatically identifying and extracting pertinent <a href=https://en.wikipedia.org/wiki/Choice_language>choice language</a> in <a href=https://en.wikipedia.org/wiki/Privacy_policy>privacy policies</a>. In particular, we present a two-stage architecture of classification models to identify opt-out choices in privacy policy text, labelling common varieties of choices with a mean F1 score of 0.735. Our techniques enable the creation of systems to help Internet users to learn about their choices, thereby effectuating notice and choice and improving <a href=https://en.wikipedia.org/wiki/Internet_privacy>Internet privacy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1295 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1295/>An Empirical Analysis of Edit Importance between Document Versions</a></strong><br><a href=/people/t/tanya-goyal/>Tanya Goyal</a>
|
<a href=/people/s/sachin-kelkar/>Sachin Kelkar</a>
|
<a href=/people/m/manas-agarwal/>Manas Agarwal</a>
|
<a href=/people/j/jeenu-grover/>Jeenu Grover</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1295><div class="card-body p-3 small">In this paper, we present a novel approach to infer significance of various textual edits to documents. An author may make several edits to a document ; each edit varies in its impact to the content of the document. While some edits are surface changes and introduce negligible change, other edits may change the content / tone of the document significantly. In this paper, we perform an analysis on the human perceptions of edit importance while reviewing documents from one version to the next. We identify <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> that influence edit importance and model it in a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression based setting</a>. We show that the predicted importance by our approach is highly correlated with the human perceived importance, established by a Mechanical Turk study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1296.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1296 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1296 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1296" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1296/>Transition-Based Disfluency Detection using <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a><span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/s/shaolei-wang/>Shaolei Wang</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/m/meishan-zhang/>Meishan Zhang</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1296><div class="card-body p-3 small">In this paper, we model the problem of disfluency detection using a transition-based framework, which incrementally constructs and labels the disfluency chunk of input sentences using a new transition system without syntax information. Compared with sequence labeling methods, it can capture non-local chunk-level features ; compared with joint parsing and disfluency detection methods, it is free for noise in syntax. Experiments show that our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves state-of-the-art <a href=https://en.wikipedia.org/wiki/F-score>f-score</a> of 87.5 % on the commonly used English Switchboard test set, and a set of in-house annotated Chinese data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1299 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231330 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1299/>A Study of Style in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> : Controlling the Formality of Machine Translation Output</a></strong><br><a href=/people/x/xing-niu/>Xing Niu</a>
|
<a href=/people/m/marianna-martindale/>Marianna Martindale</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1299><div class="card-body p-3 small">Stylistic variations of language, such as <a href=https://en.wikipedia.org/wiki/Formality>formality</a>, carry speakers&#8217; intention beyond literal meaning and should be conveyed adequately in <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We propose to use lexical formality models to control the formality level of machine translation output. We demonstrate the effectiveness of our approach in empirical evaluations, as measured by <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a> and <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human assessments</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1300 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1300 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1300.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235696 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1300/>Sharp Models on Dull Hardware : Fast and Accurate Neural Machine Translation Decoding on the <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPU</a><span class=acl-fixed-case>CPU</span></a></strong><br><a href=/people/j/jacob-devlin/>Jacob Devlin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1300><div class="card-body p-3 small">Attentional sequence-to-sequence models have become the new standard for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, but one challenge of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> is a significant increase in training and decoding cost compared to phrase-based systems. In this work we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation (NMT), while achieving CPU decoding speed / throughput close to that of a phrasal decoder. We approach this problem from two angles : First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a> which uses an RNN (GRU / LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This architecture achieves similar <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> to a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep recurrent model</a>, at a small fraction of the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and decoding cost</a>. By combining these techniques, our best <a href=https://en.wikipedia.org/wiki/System>system</a> achieves a very competitive <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 38.3 BLEU on WMT English-French NewsTest2014, while decoding at 100 words / sec on <a href=https://en.wikipedia.org/wiki/Thread_(computing)>single-threaded CPU</a>. We believe this is the best published accuracy / speed trade-off of an NMT system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1302 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231953 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1302/>Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources<span class=acl-fixed-case>POS</span> Tagging without Cross-Lingual Resources</a></strong><br><a href=/people/j/joo-kyung-kim/>Joo-Kyung Kim</a>
|
<a href=/people/y/young-bum-kim/>Young-Bum Kim</a>
|
<a href=/people/r/ruhi-sarikaya/>Ruhi Sarikaya</a>
|
<a href=/people/e/eric-fosler-lussier/>Eric Fosler-Lussier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1302><div class="card-body p-3 small">Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language. In this paper, we introduce a cross-lingual transfer learning model for <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a> without ancillary resources such as <a href=https://en.wikipedia.org/wiki/Parallel_corpora>parallel corpora</a>. The proposed cross-lingual model utilizes a common BLSTM that enables <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> from other languages, and private BLSTMs for language-specific representations. The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language. Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238233708 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1303/>Image Pivoting for Learning Multilingual Multimodal Representations</a></strong><br><a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/f/frank-keller/>Frank Keller</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1303><div class="card-body p-3 small">In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of <a href=https://en.wikipedia.org/wiki/Image_retrieval>image search</a> and <a href=https://en.wikipedia.org/wiki/Computer_vision>image understanding</a>. Our model learns a common <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> for images and their descriptions in two different languages (which need not be parallel) by considering the <a href=https://en.wikipedia.org/wiki/Image>image</a> as a pivot between two languages. We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and on semantic textual similarity of image descriptions in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In both cases we achieve state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1304 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234744 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1304/>Neural Machine Translation with Source Dependency Representation</a></strong><br><a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a>
|
<a href=/people/a/akihiro-tamura/>Akihiro Tamura</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1304><div class="card-body p-3 small">Source dependency information has been successfully introduced into <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel NMT with source dependency representation to improve translation performance of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>, especially long sentences. Empirical results on NIST Chinese-to-English translation task show that our <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>method</a> achieves 1.6 BLEU improvements on average over a strong <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>NMT system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1306 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238235659 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1306/>Sequence Effects in Crowdsourced Annotations</a></strong><br><a href=/people/n/nitika-mathur/>Nitika Mathur</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1306><div class="card-body p-3 small">Manual data annotation is a vital component of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP research</a>. When designing <a href=https://en.wikipedia.org/wiki/Annotation>annotation tasks</a>, properties of the annotation interface can unintentionally lead to artefacts in the resulting dataset, biasing the evaluation. In this paper, we explore sequence effects where <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> of an item are affected by the preceding items. Having assigned one label to an instance, the annotator may be less (or more) likely to assign the same label to the next. During rating tasks, seeing a low quality item may affect the score given to the next item either positively or negatively. We see clear evidence of both types of effects using auto-correlation studies over three different crowdsourced datasets. We then recommend a simple way to minimise sequence effects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1308 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236230 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1308/>The strange geometry of <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a> with negative sampling</a></strong><br><a href=/people/d/david-mimno/>David Mimno</a>
|
<a href=/people/l/laure-thompson/>Laure Thompson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1308><div class="card-body p-3 small">Despite their ubiquity, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> trained with skip-gram negative sampling (SGNS) remain poorly understood. We find that vector positions are not simply determined by <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>, but rather occupy a narrow cone, diametrically opposed to the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context vectors</a>. We show that this geometric concentration depends on the ratio of positive to negative examples, and that it is neither theoretically nor empirically inherent in related embedding algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1309 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1309.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238234701 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1309/>Natural Language Processing with Small Feed-Forward Networks</a></strong><br><a href=/people/j/jan-a-botha/>Jan A. Botha</a>
|
<a href=/people/e/emily-pitler/>Emily Pitler</a>
|
<a href=/people/j/ji-ma/>Ji Ma</a>
|
<a href=/people/a/anton-bakalov/>Anton Bakalov</a>
|
<a href=/people/a/alex-salcianu/>Alex Salcianu</a>
|
<a href=/people/d/david-weiss/>David Weiss</a>
|
<a href=/people/r/ryan-mcdonald/>Ryan McDonald</a>
|
<a href=/people/s/slav-petrov/>Slav Petrov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1309><div class="card-body p-3 small">We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like <a href=https://en.wikipedia.org/wiki/Mobile_phone>mobile phones</a>, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1311 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231647 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1311" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1311/>Analogs of Linguistic Structure in Deep Representations</a></strong><br><a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1311><div class="card-body p-3 small">We investigate the compositional structure of message vectors computed by a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep network</a> trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to <a href=https://en.wikipedia.org/wiki/Negation>negation</a>, <a href=https://en.wikipedia.org/wiki/Logical_conjunction>conjunction</a>, and <a href=https://en.wikipedia.org/wiki/Logical_disjunction>disjunction</a>. Our results suggest that neural representations are capable of spontaneously developing a <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> with functional analogues to qualitative properties of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1312 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231213 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1312/>A Simple <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>Regularization-based Algorithm</a> for Learning Cross-Domain Word Embeddings</a></strong><br><a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/v/vincent-zheng/>Vincent Zheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1312><div class="card-body p-3 small">Learning word embeddings has received a significant amount of attention recently. Often, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> are learned in an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised manner</a> from a large collection of text. The genre of the text typically plays an important role in the effectiveness of the resulting <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. How to effectively train word embedding models using data from different domains remains a problem that is less explored. In this paper, we present a simple yet effective method for learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> based on text from different domains. We demonstrate the effectiveness of our approach through extensive experiments on various down-stream NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1313 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1313/>Learning what to read : Focused machine reading</a></strong><br><a href=/people/e/enrique-noriega-atala/>Enrique Noriega-Atala</a>
|
<a href=/people/m/marco-a-valenzuela-escarcega/>Marco A. Valenzuela-Escárcega</a>
|
<a href=/people/c/clayton-morrison/>Clayton Morrison</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1313><div class="card-body p-3 small">Recent efforts in <a href=https://en.wikipedia.org/wiki/Bioinformatics>bioinformatics</a> have achieved tremendous progress in the machine reading of biomedical literature, and the assembly of the extracted biochemical interactions into large-scale models such as protein signaling pathways. However, batch machine reading of literature at today&#8217;s scale (PubMed alone indexes over 1 million papers per year) is unfeasible due to both cost and <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>processing overhead</a>. In this work, we introduce a focused reading approach to guide the machine reading of biomedical literature towards what literature should be read to answer a biomedical query as efficiently as possible. We introduce a family of <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for focused reading, including an intuitive, strong baseline, and a second approach which uses a reinforcement learning (RL) framework that learns when to explore (widen the search) or exploit (narrow it). We demonstrate that the RL approach is capable of answering more queries than the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, while being more efficient, i.e., reading fewer documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1314 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238232487 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1314/>DOC : Deep Open Classification of Text Documents<span class=acl-fixed-case>DOC</span>: Deep Open Classification of Text Documents</a></strong><br><a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1314><div class="card-body p-3 small">Traditional <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> makes the <a href=https://en.wikipedia.org/wiki/Closed-world_assumption>closed-world assumption</a> that the classes appeared in the test data must have appeared in training. This also applies to text learning or <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. As learning is used increasingly in dynamic open environments where some new / test documents may not belong to any of the training classes, identifying these novel documents during <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> presents an important problem. This <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1315 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1315.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238231770 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1315/>Charmanteau : Character Embedding Models For Portmanteau Creation<span class=acl-fixed-case>C</span>harmanteau: Character Embedding Models For Portmanteau Creation</a></strong><br><a href=/people/v/varun-gangal/>Varun Gangal</a>
|
<a href=/people/h/harsh-jhamtani/>Harsh Jhamtani</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1315><div class="card-body p-3 small">Portmanteaus are a word formation phenomenon where two words combine into a new word. We propose character-level neural sequence-to-sequence (S2S) methods for the task of portmanteau generation that are end-to-end-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channel-style model, which allows for the incorporation of unsupervised word lists, improving performance over a standard source-to-target model. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1316 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236651 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1316/>Using Automated Metaphor Identification to Aid in Detection and Prediction of First-Episode Schizophrenia</a></strong><br><a href=/people/e/e-dario-gutierrez/>E. Darío Gutiérrez</a>
|
<a href=/people/g/guillermo-a-cecchi/>Guillermo Cecchi</a>
|
<a href=/people/c/cheryl-corcoran/>Cheryl Corcoran</a>
|
<a href=/people/p/philip-corlett/>Philip Corlett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1316><div class="card-body p-3 small">The diagnosis of serious mental health conditions such as <a href=https://en.wikipedia.org/wiki/Schizophrenia>schizophrenia</a> is based on the judgment of clinicians whose training takes several years, and can not be easily formalized into objective measures. However, previous research suggests there are disturbances in aspects of the language use of patients with schizophrenia. Using metaphor-identification and sentiment-analysis algorithms to automatically generate <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, we create a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a>, that, with high accuracy, can predict which patients will develop (or currently suffer from) <a href=https://en.wikipedia.org/wiki/Schizophrenia>schizophrenia</a>. To our knowledge, this study is the first to demonstrate the utility of automated metaphor identification algorithms for detection or prediction of disease.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1317 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236521 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1317/>Truth of Varying Shades : Analyzing Language in Fake News and Political Fact-Checking</a></strong><br><a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/j/jin-yea-jang/>Jin Yea Jang</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1317><div class="card-body p-3 small">We present an analytic study on the language of news media in the context of <a href=https://en.wikipedia.org/wiki/Fact-checking>political fact-checking</a> and fake news detection. We compare the language of real news with that of <a href=https://en.wikipedia.org/wiki/Satire>satire</a>, <a href=https://en.wikipedia.org/wiki/Hoax>hoaxes</a>, and <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a> to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a <a href=https://en.wikipedia.org/wiki/Case_study>case study</a> based on <a href=https://en.wikipedia.org/wiki/PolitiFact>PolitiFact.com</a> using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1318 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236263 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1318/>Topic-Based Agreement and Disagreement in <a href=https://en.wikipedia.org/wiki/United_States_Electoral_College>US Electoral Manifestos</a><span class=acl-fixed-case>US</span> Electoral Manifestos</a></strong><br><a href=/people/s/stefano-menini/>Stefano Menini</a>
|
<a href=/people/f/federico-nanni/>Federico Nanni</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1318><div class="card-body p-3 small">We present a topic-based analysis of agreement and disagreement in <a href=https://en.wikipedia.org/wiki/Manifesto>political manifestos</a>, which relies on a new method for topic detection based on key concept clustering. Our approach outperforms both standard techniques like LDA and a state-of-the-art graph-based method, and provides promising initial results for this new task in <a href=https://en.wikipedia.org/wiki/Computational_social_science>computational social science</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1319 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/238236824 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D17-1319/>Zipporah : a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora<span class=acl-fixed-case>Z</span>ipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora</a></strong><br><a href=/people/h/hainan-xu/>Hainan Xu</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1319><div class="card-body p-3 small">We introduce <a href=https://en.wikipedia.org/wiki/Zipporah>Zipporah</a>, a fast and scalable data cleaning system. We propose a novel type of bag-of-words translation feature, and train <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression models</a> to classify good data and synthetic noisy data in the proposed <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature space</a>. The trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is used to score parallel sentences in the data pool for <a href=https://en.wikipedia.org/wiki/Selection_bias>selection</a>. As shown in experiments, <a href=https://en.wikipedia.org/wiki/Zipporah>Zipporah</a> selects a high-quality parallel corpus from a large, mixed quality data pool. In particular, for one <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy dataset</a>, <a href=https://en.wikipedia.org/wiki/Zipporah>Zipporah</a> achieves a 2.1 BLEU score improvement with using 1/5 of the data over using the entire corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1320 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1320" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1320/>Bringing Structure into Summaries : Crowdsourcing a Benchmark Corpus of Concept Maps</a></strong><br><a href=/people/t/tobias-falke/>Tobias Falke</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1320><div class="card-body p-3 small">Concept maps can be used to concisely represent important information and bring structure into <a href=https://en.wikipedia.org/wiki/Collection_(artwork)>large document collections</a>. Therefore, we study a variant of <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a> that produces summaries in the form of concept maps. However, suitable evaluation datasets for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> along with a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a> and proposed evaluation protocol to enable further research on this variant of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1321 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D17-1321.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1321" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1321/>Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog</a></strong><br><a href=/people/s/satwik-kottur/>Satwik Kottur</a>
|
<a href=/people/j/jose-moura/>José Moura</a>
|
<a href=/people/s/stefan-lee/>Stefan Lee</a>
|
<a href=/people/d/dhruv-batra/>Dhruv Batra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1321><div class="card-body p-3 small">A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocols</a> developed by the agents, learned without any human supervision ! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of &#8216;negative&#8217; results culminating in a &#8216;positive&#8217; one showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> does not emerge &#8216;naturally&#8217;,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1322 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-1322/>Depression and Self-Harm Risk Assessment in Online Forums</a></strong><br><a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/n/nazli-goharian/>Nazli Goharian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1322><div class="card-body p-3 small">Users suffering from <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health conditions</a> often turn to online resources for support, including specialized online support communities or general communities such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. In this work, we present a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for supporting and studying users in both types of <a href=https://en.wikipedia.org/wiki/Community>communities</a>. We propose <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for identifying posts in <a href=https://en.wikipedia.org/wiki/Peer_support>support communities</a> that may indicate a risk of <a href=https://en.wikipedia.org/wiki/Self-harm>self-harm</a>, and demonstrate that our approach outperforms strong previously proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for identifying such posts. Self-harm is closely related to <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a>, which makes identifying depressed users on <a href=https://en.wikipedia.org/wiki/Internet_forum>general forums</a> a crucial related task. We introduce a large-scale general forum dataset consisting of users with self-reported depression diagnoses matched with control users. We show how our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can be applied to effectively identify <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depressed users</a> from their use of language alone. We demonstrate that our method outperforms strong <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a> on this general forum dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-1323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-1323 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-1323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-1323" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-1323/>Men Also Like Shopping : Reducing Gender Bias Amplification using Corpus-level Constraints</a></strong><br><a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/t/tianlu-wang/>Tianlu Wang</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-1323><div class="card-body p-3 small">Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study <a href=https://en.wikipedia.org/wiki/Data>data</a> and <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> associated with multilabel object classification and visual semantic role labeling. We find that (a) <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for these tasks contain significant <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> and (b) <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> further amplify existing bias. For example, the activity cooking is over 33 % more likely to involve females than males in a training set, and a trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> further amplifies the disparity to 68 % at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> based on <a href=https://en.wikipedia.org/wiki/Lagrangian_and_Eulerian_specification_of_the_flow_field>Lagrangian relaxation</a> for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5 % and 40.5 % for multilabel classification and visual semantic role labeling, respectively</div></div></div><hr><div id=d17-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D17-2/>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2000/>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></strong><br><a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/michael-paul/>Michael Paul</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2001/>The NLTK FrameNet API : Designing for Discoverability with a Rich Linguistic Resource<span class=acl-fixed-case>NLTK</span> <span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et <span class=acl-fixed-case>API</span>: Designing for Discoverability with a Rich Linguistic Resource</a></strong><br><a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/c/chuck-wooters/>Chuck Wooters</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2001><div class="card-body p-3 small">A new <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python API</a>, integrated within the <a href=https://en.wikipedia.org/wiki/NLTK>NLTK suite</a>, offers access to the <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet 1.7 lexical database</a>. The lexicon (structured in terms of frames) as well as annotated sentences can be processed programatically, or browsed with human-readable displays via the interactive Python prompt.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-2002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-2002/>Argotario : Computational Argumentation Meets Serious Games<span class=acl-fixed-case>A</span>rgotario: Computational Argumentation Meets Serious Games</a></strong><br><a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/r/raffael-hannemann/>Raffael Hannemann</a>
|
<a href=/people/c/christian-pollak/>Christian Pollak</a>
|
<a href=/people/c/christopher-klamm/>Christopher Klamm</a>
|
<a href=/people/p/patrick-pauli/>Patrick Pauli</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2002><div class="card-body p-3 small">An important skill in <a href=https://en.wikipedia.org/wiki/Critical_thinking>critical thinking</a> and <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a> is the ability to spot and recognize <a href=https://en.wikipedia.org/wiki/Fallacy>fallacies</a>. Fallacious arguments, omnipresent in argumentative discourse, can be deceptive, manipulative, or simply leading to &#8216;wrong moves&#8217; in a discussion. Despite their importance, argumentation scholars and NLP researchers with focus on argumentation quality have not yet investigated <a href=https://en.wikipedia.org/wiki/Fallacy>fallacies</a> empirically. The nonexistence of resources dealing with <a href=https://en.wikipedia.org/wiki/Fallacy>fallacious argumentation</a> calls for scalable approaches to data acquisition and annotation, for which the serious games methodology offers an appealing, yet unexplored, alternative. We present Argotario, a <a href=https://en.wikipedia.org/wiki/Serious_game>serious game</a> that deals with fallacies in everyday argumentation. Argotario is a multilingual, open-source, platform-independent application with strong educational aspects, accessible at.<url>www.argotario.net</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2003/>An Analysis and Visualization Tool for Case Study Learning of Linguistic Concepts</a></strong><br><a href=/people/c/cecilia-ovesdotter-alm/>Cecilia Ovesdotter Alm</a>
|
<a href=/people/b/benjamin-s-meyers/>Benjamin Meyers</a>
|
<a href=/people/e/emily-prudhommeaux/>Emily Prud’hommeaux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2003><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Educational_technology>educational tool</a> that integrates computational linguistics resources for use in non-technical undergraduate language science courses. By using the tool in conjunction with evidence-driven pedagogical case studies, we strive to provide opportunities for students to gain an understanding of linguistic concepts and analysis through the lens of realistic problems in feasible ways. Case studies tend to be used in legal, business, and health education contexts, but less in the teaching and learning of linguistics. The approach introduced also has potential to encourage students across training backgrounds to continue on to computational language analysis coursework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2004/>GraphDocExplore : A Framework for the Experimental Comparison of Graph-based Document Exploration Techniques<span class=acl-fixed-case>G</span>raph<span class=acl-fixed-case>D</span>oc<span class=acl-fixed-case>E</span>xplore: A Framework for the Experimental Comparison of Graph-based Document Exploration Techniques</a></strong><br><a href=/people/t/tobias-falke/>Tobias Falke</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2004><div class="card-body p-3 small">Graphs have long been proposed as a tool to browse and navigate in a collection of documents in order to support <a href=https://en.wikipedia.org/wiki/Exploratory_search>exploratory search</a>. Many techniques to automatically extract different types of <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>, showing for example <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> or <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> and different relationships between them, have been suggested. While experimental evidence that they are indeed helpful exists for some of them, it is largely unknown which type of <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> is most helpful for a specific exploratory task. However, carrying out experimental comparisons with human subjects is challenging and time-consuming. Towards this end, we present the GraphDocExplore framework. It provides an intuitive web interface for graph-based document exploration that is optimized for experimental user studies. Through a generic <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph interface</a>, different methods to extract <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> from text can be plugged into the <a href=https://en.wikipedia.org/wiki/System>system</a>. Hence, they can be compared at minimal implementation effort in an environment that ensures controlled comparisons. The <a href=https://en.wikipedia.org/wiki/System>system</a> is publicly available under an open-source license.<i>GraphDocExplore</i>\n framework. It provides an intuitive web interface for graph-based document\n exploration that is optimized for experimental user studies. Through a\n generic graph interface, different methods to extract graphs from text can\n be plugged into the system. Hence, they can be compared at minimal\n implementation effort in an environment that ensures controlled\n comparisons. The system is publicly available under an open-source\n license.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2005/>SGNMT A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies<span class=acl-fixed-case>SGNMT</span> – A Flexible <span class=acl-fixed-case>NMT</span> Decoding Platform for Quick Prototyping of New Models and Search Strategies</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/e/eva-hasler/>Eva Hasler</a>
|
<a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2005><div class="card-body p-3 small">This paper introduces SGNMT, our experimental platform for machine translation research. SGNMT provides a generic interface to neural and symbolic scoring modules (predictors) with left-to-right semantic such as translation models like NMT, language models, translation lattices, n-best lists or other kinds of scores and constraints. Predictors can be combined with other <a href=https://en.wikipedia.org/wiki/Prediction>predictors</a> to form complex decoding tasks. SGNMT implements a number of <a href=https://en.wikipedia.org/wiki/Search_algorithm>search strategies</a> for traversing the space spanned by the predictors which are appropriate for different predictor constellations. Adding new predictors or decoding strategies is particularly easy, making it a very efficient tool for prototyping new research ideas. SGNMT is actively being used by students in the MPhil program in <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a>, Speech and Language Technology at the University of Cambridge for course work and these s, as well as for most of the research work in our group.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2006/>StruAP : A Tool for Bundling Linguistic Trees through Structure-based Abstract Pattern<span class=acl-fixed-case>S</span>tru<span class=acl-fixed-case>AP</span>: A Tool for Bundling Linguistic Trees through Structure-based Abstract Pattern</a></strong><br><a href=/people/k/kohsuke-yanai/>Kohsuke Yanai</a>
|
<a href=/people/m/misa-sato/>Misa Sato</a>
|
<a href=/people/t/toshihiko-yanase/>Toshihiko Yanase</a>
|
<a href=/people/k/kenzo-kurotsuchi/>Kenzo Kurotsuchi</a>
|
<a href=/people/y/yuta-koreeda/>Yuta Koreeda</a>
|
<a href=/people/y/yoshiki-niwa/>Yoshiki Niwa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2006><div class="card-body p-3 small">We present a tool for developing tree structure patterns that makes it easy to define the relations among textual phrases and create a <a href=https://en.wikipedia.org/wiki/Search_engine_indexing>search index</a> for these newly defined <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. By using the proposed <a href=https://en.wikipedia.org/wiki/Tool>tool</a>, users develop tree structure patterns through abstracting <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>syntax trees</a>. The tool features (1) intuitive pattern syntax, (2) unique functions such as recursive call of patterns and use of lexicon dictionaries, and (3) whole workflow support for relation development and validation. We report the current implementation of the <a href=https://en.wikipedia.org/wiki/Tool>tool</a> and its effectiveness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2007/>KnowYourNyms? A Game of Semantic Relationships<span class=acl-fixed-case>K</span>now<span class=acl-fixed-case>Y</span>our<span class=acl-fixed-case>N</span>yms? A Game of Semantic Relationships</a></strong><br><a href=/people/r/ross-mechanic/>Ross Mechanic</a>
|
<a href=/people/d/dean-fulgoni/>Dean Fulgoni</a>
|
<a href=/people/h/hannah-cutler/>Hannah Cutler</a>
|
<a href=/people/s/sneha-rajana/>Sneha Rajana</a>
|
<a href=/people/z/zheyuan-liu/>Zheyuan Liu</a>
|
<a href=/people/b/bradley-jackson/>Bradley Jackson</a>
|
<a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2007><div class="card-body p-3 small">Semantic relation knowledge is crucial for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. We introduce KnowYourNyms?, a <a href=https://en.wikipedia.org/wiki/Browser_game>web-based game</a> for learning <a href=https://en.wikipedia.org/wiki/Semantic_Web>semantic relations</a>. While providing users with an engaging experience, the <a href=https://en.wikipedia.org/wiki/Application_software>application</a> collects large amounts of data that can be used to improve semantic relation classifiers. The data also broadly informs us of how people perceive the relationships between words, providing useful insights for research in <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a> and <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2008/>The Projector : An Interactive Annotation Projection Visualization Tool</a></strong><br><a href=/people/a/alan-akbik/>Alan Akbik</a>
|
<a href=/people/r/roland-vollgraf/>Roland Vollgraf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2008><div class="card-body p-3 small">Previous works proposed annotation projection in parallel corpora to inexpensively generate <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> or propbanks for new languages. In this approach, <a href=https://en.wikipedia.org/wiki/Annotation>linguistic annotation</a> is automatically transferred from a resource-rich source language (SL) to translations in a target language (TL). However, annotation projection may be adversely affected by translational divergences between specific language pairs. For this reason, previous work often required careful qualitative analysis of projectability of specific annotation in order to define strategies to address quality and coverage issues. In this demonstration, we present THE PROJECTOR, an interactive GUI designed to assist researchers in such analysis : it allows users to execute and visually inspect annotation projection in a range of different settings. We give an overview of the <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>GUI</a>, discuss use cases and illustrate how the tool can facilitate discussions with the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2009/>Interactive Visualization for Linguistic Structure</a></strong><br><a href=/people/a/aaron-sarnat/>Aaron Sarnat</a>
|
<a href=/people/v/vidur-joshi/>Vidur Joshi</a>
|
<a href=/people/c/cristian-petrescu-prahova/>Cristian Petrescu-Prahova</a>
|
<a href=/people/a/alvaro-herrasti/>Alvaro Herrasti</a>
|
<a href=/people/b/brandon-stilson/>Brandon Stilson</a>
|
<a href=/people/m/mark-hopkins/>Mark Hopkins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2009><div class="card-body p-3 small">We provide a visualization library and <a href=https://en.wikipedia.org/wiki/User_interface>web interface</a> for interactively exploring a parse tree or a <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>forest of parses</a>. The <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a> is not tied to any particular linguistic representation, but provides a <a href=https://en.wikipedia.org/wiki/Application_programming_interface>general-purpose API</a> for the interactive exploration of hierarchical linguistic structure. To facilitate rapid understanding of a complex structure, the <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> offers several important features, including expand / collapse functionality, positional and color cues, explicit visual support for sequential structure, and dynamic highlighting to convey node-to-text correspondence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2010/>DLATK : Differential Language Analysis ToolKit<span class=acl-fixed-case>DLATK</span>: Differential Language Analysis <span class=acl-fixed-case>T</span>ool<span class=acl-fixed-case>K</span>it</a></strong><br><a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a>
|
<a href=/people/s/salvatore-giorgi/>Salvatore Giorgi</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/p/patrick-crutchley/>Patrick Crutchley</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a>
|
<a href=/people/j/johannes-eichstaedt/>Johannes Eichstaedt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2010><div class="card-body p-3 small">We present Differential Language Analysis Toolkit (DLATK), an open-source python package and command-line tool developed for conducting social-scientific language analyses. While DLATK provides standard NLP pipeline steps such as <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a> or SVM-classification, its novel strengths lie in analyses useful for psychological, health, and social science : (1) incorporation of extra-linguistic structured information, (2) specified levels and units of analysis (e.g. document, user, community), (3) statistical metrics for continuous outcomes, and (4) robust, proven, and accurate pipelines for social-scientific prediction problems. DLATK integrates multiple popular packages (SKLearn, Mallet), enables interactive usage (Jupyter Notebooks), and generally follows object oriented principles to make it easy to tie in additional libraries or storage technologies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2011/>QUINT : Interpretable Question Answering over Knowledge Bases<span class=acl-fixed-case>QUINT</span>: Interpretable Question Answering over Knowledge Bases</a></strong><br><a href=/people/a/abdalghani-abujabal/>Abdalghani Abujabal</a>
|
<a href=/people/r/rishiraj-saha-roy/>Rishiraj Saha Roy</a>
|
<a href=/people/m/mohamed-yahya/>Mohamed Yahya</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2011><div class="card-body p-3 small">We present QUINT, a live system for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> over <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. QUINT automatically learns role-aligned utterance-query templates from user questions paired with their answers. When QUINT answers a question, it visualizes the complete derivation sequence from the natural language utterance to the final answer. The derivation provides an explanation of how the <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>syntactic structure</a> of the question was used to derive the structure of a SPARQL query, and how the phrases in the question were used to instantiate different parts of the query. When an answer seems unsatisfactory, the <a href=https://en.wikipedia.org/wiki/Derivation_(logic)>derivation</a> provides valuable insights towards reformulating the question.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-2012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-2012/>Function Assistant : A Tool for NL Querying of APIs<span class=acl-fixed-case>NL</span> Querying of <span class=acl-fixed-case>API</span>s</a></strong><br><a href=/people/k/kyle-richardson/>Kyle Richardson</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2012><div class="card-body p-3 small">In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is designed to help end-users of a target API quickly find information about <a href=https://en.wikipedia.org/wiki/Subroutine>functions</a> through high-level natural language queries, or descriptions. For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> using the semantic parsing approach of (Richardson and Kuhn, 2017). Translations are automatically learned from example text-code pairs in example <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APIs</a>. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> includes features for building translation pipelines and <a href=https://en.wikipedia.org/wiki/Query_language>query engines</a> for arbitrary source code projects. To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2013 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2013/>MoodSwipe : A Soft Keyboard that Suggests MessageBased on User-Specified Emotions<span class=acl-fixed-case>M</span>ood<span class=acl-fixed-case>S</span>wipe: A Soft Keyboard that Suggests <span class=acl-fixed-case>M</span>essage<span class=acl-fixed-case>B</span>ased on User-Specified Emotions</a></strong><br><a href=/people/c/chieh-yang-huang/>Chieh-Yang Huang</a>
|
<a href=/people/t/tristan-labetoulle/>Tristan Labetoulle</a>
|
<a href=/people/t/ting-hao-huang/>Ting-Hao Huang</a>
|
<a href=/people/y/yi-pei-chen/>Yi-Pei Chen</a>
|
<a href=/people/h/hung-chen-chen/>Hung-Chen Chen</a>
|
<a href=/people/v/vallari-srivastava/>Vallari Srivastava</a>
|
<a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2013><div class="card-body p-3 small">We present MoodSwipe, a soft keyboard that suggests text messages given the user-specified emotions utilizing the real dialog data. The aim of MoodSwipe is to create a convenient user interface to enjoy the technology of <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a> and text suggestion, and at the same time to collect labeled data automatically for developing more advanced technologies. While users select the MoodSwipe keyboard, they can type as usual but sense the emotion conveyed by their text and receive suggestions for their message as a benefit. In MoodSwipe, the detected emotions serve as the medium for suggested texts, where viewing the latter is the incentive to correcting the former. We conduct several experiments to show the superiority of the emotion classification models trained on the dialog data, and further to verify good emotion cues are important context for text suggestion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2015/>HeidelPlace : An Extensible Framework for <a href=https://en.wikipedia.org/wiki/Geoparsing>Geoparsing</a><span class=acl-fixed-case>H</span>eidel<span class=acl-fixed-case>P</span>lace: An Extensible Framework for Geoparsing</a></strong><br><a href=/people/l/ludwig-richter/>Ludwig Richter</a>
|
<a href=/people/j/johanna-geiss/>Johanna Geiß</a>
|
<a href=/people/a/andreas-spitz/>Andreas Spitz</a>
|
<a href=/people/m/michael-gertz/>Michael Gertz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2015><div class="card-body p-3 small">Geographic information extraction from textual data sources, called <a href=https://en.wikipedia.org/wiki/Geoparsing>geoparsing</a>, is a key task in <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a> and central to subsequent spatial analysis approaches. Several geoparsers are available that support this task, each with its own (often limited or specialized) <a href=https://en.wikipedia.org/wiki/Gazetteer>gazetteer</a> and its own approaches to toponym detection and resolution. In this demonstration paper, we present HeidelPlace, an extensible framework in support of <a href=https://en.wikipedia.org/wiki/Geoparsing>geoparsing</a>. Key features of HeidelPlace include a generic gazetteer model that supports the integration of place information from different knowledge bases, and a pipeline approach that enables an effective combination of diverse modules tailored to specific geoparsing tasks. This makes HeidelPlace a valuable tool for testing and evaluating different gazetteer sources and geoparsing methods. In the demonstration, we show how to set up a geoparsing workflow with HeidelPlace and how it can be used to compare and consolidate the output of different geoparsing approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-2016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-2016/>Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/f/fide-marten/>Fide Marten</a>
|
<a href=/people/e/eugen-ruppert/>Eugen Ruppert</a>
|
<a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2016><div class="card-body p-3 small">Interpretability of a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a> is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our <a href=https://en.wikipedia.org/wiki/System>system</a>, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D17-2018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D17-2018/>SupWSD : A Flexible Toolkit for Supervised Word Sense Disambiguation<span class=acl-fixed-case>S</span>up<span class=acl-fixed-case>WSD</span>: A Flexible Toolkit for Supervised Word Sense Disambiguation</a></strong><br><a href=/people/s/simone-papandrea/>Simone Papandrea</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/c/claudio-delli-bovi/>Claudio Delli Bovi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2018><div class="card-body p-3 small">In this demonstration we present SupWSD, a <a href=https://en.wikipedia.org/wiki/Java_API>Java API</a> for supervised Word Sense Disambiguation (WSD). This toolkit includes the implementation of a state-of-the-art supervised WSD system, together with a Natural Language Processing pipeline for <a href=https://en.wikipedia.org/wiki/Data_preprocessing>preprocessing</a> and <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a>. Our aim is to provide an easy-to-use tool for the research community, designed to be modular, fast and scalable for training and testing on large datasets. The source code of SupWSD is available at.<url>http://github.com/SI3P/SupWSD</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-2021/>Interactive Visualization and Manipulation of Attention-based Neural Machine Translation</a></strong><br><a href=/people/j/jaesong-lee/>Jaesong Lee</a>
|
<a href=/people/j/joong-hwi-shin/>Joong-Hwi Shin</a>
|
<a href=/people/j/jun-seok-kim/>Jun-Seok Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-2021><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> provides high-quality translation, it is still hard to interpret and analyze its behavior. We present an interactive interface for visualizing and intervening behavior of NMT, specifically concentrating on the behavior of beam search mechanism and attention component. The tool (1) visualizes <a href=https://en.wikipedia.org/wiki/Search_tree>search tree</a> and <a href=https://en.wikipedia.org/wiki/Attention>attention</a> and (2) provides interface to adjust <a href=https://en.wikipedia.org/wiki/Search_tree>search tree</a> and attention weight (manually or automatically) at real-time. We show the <a href=https://en.wikipedia.org/wiki/Tool>tool</a> gives various methods to understand <a href=https://en.wikipedia.org/wiki/Nonlinear_functional_analysis>NMT</a>.</div></div></div><hr><div id=d17-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/D17-3/>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3001/>Acquisition, Representation and Usage of Conceptual Hierarchies</a></strong><br><a href=/people/m/marius-pasca/>Marius Pasca</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3001><div class="card-body p-3 small">Through subsumption and <a href=https://en.wikipedia.org/wiki/Instantiation_principle>instantiation</a>, individual instances (artificial intelligence, the spotted pig) otherwise spanning a wide range of domains can be brought together and organized under conceptual hierarchies. The hierarchies connect more specific concepts (computer science subfields, gastropubs) to more general concepts (academic disciplines, restaurants) through IsA relations. Explicit or implicit properties applicable to, and defining, more general concepts are inherited by their more specific concepts, down to the instances connected to the lower parts of the <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchies</a>. Subsumption represents a crisp, universally-applicable principle towards consistently representing IsA relations in any knowledge resource. Yet knowledge resources often exhibit significant differences in their scope, representation choices and intended usage, to cause significant differences in their expected usage and impact on various <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. This tutorial examines the theoretical foundations of subsumption, and its practical embodiment through IsA relations compiled manually or extracted automatically. It addresses IsA relations from their formal definition ; through practical choices made in their representation within the larger and more widely-used of the available knowledge resources ; to their automatic acquisition from document repositories, as opposed to their manual compilation by human contributors ; to their impact in <a href=https://en.wikipedia.org/wiki/Textual_analysis>text analysis</a> and <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>. As <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> move away from returning a set of links and closer to returning results that more directly answer queries, IsA relations play an increasingly important role towards a better understanding of documents and queries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3002/>Computational Sarcasm</a></strong><br><a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/a/aditya-joshi/>Aditya Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3002><div class="card-body p-3 small">Sarcasm is a form of <a href=https://en.wikipedia.org/wiki/Irony>verbal irony</a> that is intended to express contempt or ridicule. Motivated by challenges posed by sarcastic text to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, computational approaches to <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> have witnessed a growing interest at NLP forums in the past decade. Computational sarcasm refers to automatic approaches pertaining to <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>. The tutorial will provide a bird&#8217;s-eye view of the research in computational sarcasm for text, while focusing on significant milestones. The tutorial begins with linguistic theories of sarcasm, with a focus on incongruity : a useful notion that underlies <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> and other forms of <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a>. Since the most significant work in computational sarcasm is sarcasm detection : predicting whether a given piece of text is sarcastic or not, sarcasm detection forms the focus hereafter. We begin our discussion on sarcasm detection with <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, touching on strategies, challenges and nature of datasets. Then, we describe algorithms for sarcasm detection : rule-based (where a specific evidence of sarcasm is utilised as a rule), statistical classifier-based (where features are designed for a statistical classifier), a topic model-based technique, and deep learning-based algorithms for sarcasm detection. In case of each of these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>, we refer to our work on sarcasm detection and share our learnings. Since information beyond the text to be classified, contextual information is useful for sarcasm detection, we then describe approaches that use such information through conversational context or author-specific context. We then follow it by novel areas in computational sarcasm such as sarcasm generation, sarcasm v / s irony classification, etc. We then summarise the tutorial and describe future directions based on errors reported in past work. The tutorial will end with a demonstration of our work on sarcasm detection. This tutorial will be of interest to researchers investigating computational sarcasm and related areas such as <a href=https://en.wikipedia.org/wiki/Computational_humour>computational humour</a>, figurative language understanding, emotion and sentiment sentiment analysis, etc. The tutorial is motivated by our continually evolving survey paper of sarcasm detection, that is available on arXiv at : Joshi, Aditya, Pushpak Bhattacharyya, and Mark James Carman. Automatic Sarcasm Detection : A Survey. arXiv preprint arXiv:1602.03426 (2016).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3003/>Graph-based Text Representations: Boosting Text Mining, <span class=acl-fixed-case>NLP</span> and Information Retrieval with Graphs</a></strong><br><a href=/people/f/fragkiskos-d-malliaros/>Fragkiskos D. Malliaros</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3003><div class="card-body p-3 small">Graphs or networks have been widely used as modeling tools in Natural Language Processing (NLP), Text Mining (TM) and Information Retrieval (IR). Traditionally, the unigram bag-of-words representation is applied; that way, a document is represented as a multiset of its terms, disregarding dependencies between the terms. Although several variants and extensions of this modeling approach have been proposed (e.g., the n-gram model), the main weakness comes from the underlying term independence assumption. The order of the terms within a document is completely disregarded and any relationship between terms is not taken into account in the final task (e.g., text categorization). Nevertheless, as the heterogeneity of text collections is increasing (especially with respect to document length and vocabulary), the research community has started exploring different document representations aiming to capture more fine-grained contexts of co-occurrence between different terms, challenging the well-established unigram bag-of-words model. To this direction, graphs constitute a well-developed model that has been adopted for text representation. The goal of this tutorial is to offer a comprehensive presentation of recent methods that rely on graph-based text representations to deal with various tasks in NLP and IR. We will describe basic as well as novel graph theoretic concepts and we will examine how they can be applied in a wide range of text-related application domains.\n\nAll the material associated to the tutorial will be available at: http://fragkiskosm.github.io/projects/graph_text_tutorial</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-3004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3004/>Semantic Role Labeling</a></strong><br><a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3004><div class="card-body p-3 small">This tutorial describes semantic role labelling (SRL), the task of mapping text to shallow semantic representations of eventualities and their participants. The tutorial introduces the SRL task and discusses recent research directions related to the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The audience of this tutorial will learn about the linguistic background and motivation for semantic roles, and also about a range of <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> for this task, from early approaches to the current state-of-the-art. We will further discuss recently proposed variations to the traditional SRL task, including topics such as semantic proto-role labeling. We also cover techniques for reducing required annotation effort, such as methods exploiting unlabeled corpora (semi-supervised and unsupervised techniques), model adaptation across languages and domains, and methods for crowdsourcing semantic role annotation (e.g., question-answer driven SRL). Methods based on different machine learning paradigms, including <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, generative Bayesian models, graph-based algorithms and bootstrapping style techniques. Beyond sentence-level SRL, we discuss work that involves semantic roles in discourse. In particular, we cover <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a> and <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> related to the task of identifying implicit roles and linking them to discourse antecedents. We introduce different approaches to this task from the literature, including models based on <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, centering, and selectional preferences. We also review how new insights gained through them can be useful for the traditional SRL task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3005/>Memory Augmented Neural Networks for Natural Language Processing</a></strong><br><a href=/people/c/caglar-gulcehre/>Caglar Gulcehre</a>
|
<a href=/people/s/sarath-chandar/>Sarath Chandar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3005><div class="card-body p-3 small">Designing of general-purpose learning algorithms is a long-standing goal of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a>. A general purpose AI agent should be able to have a memory that it can store and retrieve information from. Despite the success of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> in particular with the introduction of LSTMs and GRUs to this area, there are still a set of complex tasks that can be challenging for conventional <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Those tasks often require a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> to be equipped with an explicit, <a href=https://en.wikipedia.org/wiki/External_memory>external memory</a> in which a larger, potentially unbounded, set of facts need to be stored. They include but are not limited to, <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>, <a href=https://en.wikipedia.org/wiki/Planning>planning</a>, episodic question-answering and learning compact algorithms. Recently two promising approaches based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to this type of tasks have been proposed : Memory Networks and Neural Turing Machines. In this tutorial, we will give an overview of this new paradigm of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> with <a href=https://en.wikipedia.org/wiki/Memory>memory</a>. We will present a unified architecture for Memory Augmented Neural Networks (MANN) and discuss the ways in which one can address the external memory and hence read / write from it. Then we will introduce <a href=https://en.wikipedia.org/wiki/Neural_Turing_machine>Neural Turing Machines</a> and Memory Networks as specific instantiations of this general architecture. In the second half of the tutorial, we will focus on recent advances in MANN which focus on the following questions : How can we read / write from an extremely large memory in a scalable way? How can we design efficient non-linear addressing schemes? How can we do efficient <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a> using large scale memory and an <a href=https://en.wikipedia.org/wiki/Episodic_memory>episodic memory</a>? The answer to any one of these questions introduces a variant of MANN. We will conclude the tutorial with several open challenges in MANN and its applications to NLP.We will introduce several <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> of MANN in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> throughout the tutorial. Few examples include <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, visual question answering, and <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>. For updated information and material, please refer to our tutorial website : https://sites.google.com/view/mann-emnlp2017/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D17-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D17-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D17-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D17-3007/>Cross-Lingual Word Representations : Induction and Evaluation</a></strong><br><a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D17-3007><div class="card-body p-3 small">In recent past, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> as a field has seen tremendous utility of distributional word vector representations as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> in downstream tasks. The fact that these <a href=https://en.wikipedia.org/wiki/Word_vector>word vectors</a> can be trained on unlabeled monolingual corpora of a language makes them an inexpensive resource in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. With the increasing use of monolingual word vectors, there is a need for word vectors that can be used as efficiently across multiple languages as monolingually. Therefore, learning bilingual and multilingual word embeddings / vectors is currently an important research topic. These vectors offer an elegant and language-pair independent way to represent content across different languages. This tutorial aims to bring NLP researchers up to speed with the current techniques in cross-lingual word representation learning. We will first discuss how to induce cross-lingual word representations (covering both bilingual and multilingual ones) from various data types and resources (e.g., parallel data, comparable data, non-aligned monolingual data in different languages, dictionaries and theasuri, or, even, images, eye-tracking data). We will then discuss how to evaluate such <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a>, intrinsically and extrinsically. We will introduce researchers to state-of-the-art methods for constructing cross-lingual word representations and discuss their applicability in a broad range of downstream NLP applications. We will deliver a detailed survey of the current methods, discuss best training and evaluation practices and use-cases, and provide links to publicly available implementations, datasets, and pre-trained models.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>