<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Balto-Slavic Natural Language Processing (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Balto-Slavic Natural Language Processing (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021bsnlp-1>Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li></ul></div></div><div id=2021bsnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.bsnlp-1/>Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.0/>Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing</a></strong><br><a href=/people/b/bogdan-babych/>Bogdan Babych</a>
|
<a href=/people/o/olga-kanishcheva/>Olga Kanishcheva</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/v/vasyl-starko/>Vasyl Starko</a>
|
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a>
|
<a href=/people/m/michal-marcinczuk/>Michał Marcińczuk</a>
|
<a href=/people/s/senja-pollak/>Senja Pollak</a>
|
<a href=/people/p/pavel-priban/>Pavel Přibáň</a>
|
<a href=/people/m/marko-robnik-sikonja/>Marko Robnik-Šikonja</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.1/>HerBERT : Efficiently Pretrained Transformer-based Language Model for Polish<span class=acl-fixed-case>H</span>er<span class=acl-fixed-case>BERT</span>: Efficiently Pretrained Transformer-based Language Model for <span class=acl-fixed-case>P</span>olish</a></strong><br><a href=/people/r/robert-mroczkowski/>Robert Mroczkowski</a>
|
<a href=/people/p/piotr-rybak/>Piotr Rybak</a>
|
<a href=/people/a/alina-wroblewska/>Alina Wróblewska</a>
|
<a href=/people/i/ireneusz-gawlik/>Ireneusz Gawlik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--1><div class="card-body p-3 small">BERT-based models are currently used for solving nearly all Natural Language Processing (NLP) tasks and most often achieve state-of-the-art results. Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures. Several ablation studies investigating how to train BERT-like models have been carried out, but the vast majority of them concerned only the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. A <a href=https://en.wikipedia.org/wiki/Procedure_(term)>training procedure</a> designed for <a href=https://en.wikipedia.org/wiki/English_language>English</a> does not have to be universal and applicable to other especially typologically different languages. Therefore, this paper presents the first ablation study focused on <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, which, unlike the isolating English language, is a <a href=https://en.wikipedia.org/wiki/Fusional_language>fusional language</a>. We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models. In addition to multilingual model initialization, other <a href=https://en.wikipedia.org/wiki/Factor_analysis>factors</a> that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length. Based on the proposed procedure, a Polish BERT-based language model HerBERT is trained. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art results on multiple downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.4/>Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company’s Reputation</a></strong><br><a href=/people/n/nikolay-babakov/>Nikolay Babakov</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a>
|
<a href=/people/o/olga-kozlova/>Olga Kozlova</a>
|
<a href=/people/n/nikita-semenov/>Nikita Semenov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--4><div class="card-body p-3 small">Not all topics are equally flammable in terms of <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> : a calm discussion of turtles or fishing less often fuels inappropriate toxic dialogues than a discussion of politics or <a href=https://en.wikipedia.org/wiki/Sexual_minority>sexual minorities</a>. We define a set of sensitive topics that can yield inappropriate and toxic messages and describe the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> of collecting and labelling a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for appropriateness. While <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> in user-generated data is well-studied, we aim at defining a more fine-grained notion of <a href=https://en.wikipedia.org/wiki/Inappropriateness>inappropriateness</a>. The core of <a href=https://en.wikipedia.org/wiki/Inappropriateness>inappropriateness</a> is that it can harm the reputation of a speaker. This is different from <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> in two respects : (i) <a href=https://en.wikipedia.org/wiki/Inappropriateness>inappropriateness</a> is topic-related, and (ii) inappropriate message is not toxic but still unacceptable. We collect and release two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> : a topic-labelled dataset and an appropriateness-labelled dataset. We also release pre-trained classification models trained on this <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bsnlp-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.6/>RuSentEval : Linguistic Source, Encoder Force !<span class=acl-fixed-case>R</span>u<span class=acl-fixed-case>S</span>ent<span class=acl-fixed-case>E</span>val: Linguistic Source, Encoder Force!</a></strong><br><a href=/people/v/vladislav-mikhailov/>Vladislav Mikhailov</a>
|
<a href=/people/e/ekaterina-taktasheva/>Ekaterina Taktasheva</a>
|
<a href=/people/e/elina-sigdel/>Elina Sigdel</a>
|
<a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--6><div class="card-body p-3 small">The success of pre-trained transformer language models has brought a great deal of interest on how these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> work, and what they learn about language. However, prior research in the field is mainly devoted to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and little is known regarding other languages. To this end, we introduce RuSentEval, an enhanced set of 14 probing tasks for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, including ones that have not been explored yet. We apply a combination of complementary probing methods to explore the distribution of various linguistic properties in five multilingual transformers for two typologically contrasting languages Russian and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Our results provide intriguing findings that contradict the common understanding of how linguistic knowledge is represented, and demonstrate that some properties are learned in a similar manner despite the language differences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.7/>Exploratory Analysis of News Sentiment Using Subgroup Discovery</a></strong><br><a href=/people/a/anita-valmarska/>Anita Valmarska</a>
|
<a href=/people/l/luis-adrian-cabrera-diego/>Luis Adrián Cabrera-Diego</a>
|
<a href=/people/e/elvys-linhares-pontes/>Elvys Linhares Pontes</a>
|
<a href=/people/s/senja-pollak/>Senja Pollak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--7><div class="card-body p-3 small">In this study, we present an exploratory analysis of a Slovenian news corpus, in which we investigate the association between named entities and <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> in the <a href=https://en.wikipedia.org/wiki/News>news</a>. We propose a methodology that combines Named Entity Recognition and Subgroup Discovery-a descriptive rule learning technique for identifying groups of examples that share the same class label (sentiment) and pattern (features-Named Entities). The approach is used to induce the positive and negative sentiment class rules that reveal interesting patterns related to different Slovenian and international politicians, organizations, and locations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.8/>Creating an Aligned Russian Text Simplification Dataset from Language Learner Data<span class=acl-fixed-case>R</span>ussian Text Simplification Dataset from Language Learner Data</a></strong><br><a href=/people/a/anna-dmitrieva/>Anna Dmitrieva</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--8><div class="card-body p-3 small">Parallel language corpora where regular texts are aligned with their simplified versions can be used in both <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Theoretical_linguistics>theoretical linguistic studies</a>. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts. Today, there exist a few parallel datasets for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Simple English, but many other languages lack such <a href=https://en.wikipedia.org/wiki/Data>data</a>. In this paper we describe our work on creating an aligned Russian-Simple Russian dataset composed of <a href=https://en.wikipedia.org/wiki/Russian_language>Russian literature texts</a> adapted for learners of <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> as a foreign language. This will be the first parallel dataset in this domain, and one of the first Simple Russian datasets in general.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.9/>Multilingual Named Entity Recognition and Matching Using <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and Dedupe for <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic Languages</a><span class=acl-fixed-case>BERT</span> and Dedupe for <span class=acl-fixed-case>S</span>lavic Languages</a></strong><br><a href=/people/m/marko-prelevikj/>Marko Prelevikj</a>
|
<a href=/people/s/slavko-zitnik/>Slavko Zitnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--9><div class="card-body p-3 small">This paper describes the University of Ljubljana (UL FRI) Group&#8217;s submissions to the shared task at the Balto-Slavic Natural Language Processing (BSNLP) 2021 Workshop. We experiment with multiple BERT-based models, pre-trained in multi-lingual, Croatian-Slovene-English and Slovene-only data. We perform training iteratively and on the concatenated data of previously available NER datasets. For the normalization task we use Stanza lemmatizer, while for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity matching</a> we implemented a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> using the Dedupe library. The performance of evaluations suggests that multi-source settings outperform less-resourced approaches. The best NER models achieve 0.91 <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> on Slovene training data splits while the best official submission achieved F-scores of 0.84 and 0.78 for relaxed partial matching and strict settings, respectively. In multi-lingual NER setting we achieve <a href=https://en.wikipedia.org/wiki/F-number>F-scores</a> of 0.82 and 0.74.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bsnlp-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.13/>Benchmarking Pre-trained Language Models for Multilingual NER : TraSpaS at the BSNLP2021 Shared Task<span class=acl-fixed-case>NER</span>: <span class=acl-fixed-case>T</span>ra<span class=acl-fixed-case>S</span>pa<span class=acl-fixed-case>S</span> at the <span class=acl-fixed-case>BSNLP</span>2021 Shared Task</a></strong><br><a href=/people/m/marek-suppa/>Marek Suppa</a>
|
<a href=/people/o/ondrej-jariabka/>Ondrej Jariabka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--13><div class="card-body p-3 small">In this paper we describe TraSpaS, a submission to the third shared task on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> hosted as part of the Balto-Slavic Natural Language Processing (BSNLP) Workshop. In it we evaluate various pre-trained language models on the NER task using three open-source NLP toolkits : character level language model with Stanza, language-specific BERT-style models with SpaCy and Adapter-enabled XLM-R with Trankit. Our results show that the Trankit-based models outperformed those based on the other two toolkits, even when trained on smaller amounts of data. Our code is available at.<url>https://github.com/NaiveNeuron/slavner-2021</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.14/>Named Entity Recognition and Linking Augmented with Large-Scale Structured Data</a></strong><br><a href=/people/p/pawel-rychlikowski/>Paweł Rychlikowski</a>
|
<a href=/people/b/bartlomiej-najdecki/>Bartłomiej Najdecki</a>
|
<a href=/people/a/adrian-lancucki/>Adrian Lancucki</a>
|
<a href=/people/a/adam-kaczmarek/>Adam Kaczmarek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--14><div class="card-body p-3 small">In this paper we describe our submissions to the 2nd and 3rd SlavNER Shared Tasks held at BSNLP 2019 and BSNLP 2021, respectively. The tasks focused on the analysis of Named Entities in multilingual Web documents in <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic languages</a> with rich inflection. Our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> takes advantage of large collections of both unstructured and structured documents. The former serve as data for unsupervised training of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and embeddings of lexical units. The latter refers to <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and its structured counterpart-Wikidata, our source of <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization rules</a>, and real-world entities. With the aid of those resources, our system could recognize, normalize and link entities, while being trained with only small amounts of labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bsnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bsnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bsnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bsnlp-1.15/>Slav-NER : the 3rd Cross-lingual Challenge on Recognition, Normalization, <a href=https://en.wikipedia.org/wiki/Language_classification>Classification</a>, and Linking of Named Entities across <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic Languages</a><span class=acl-fixed-case>NER</span>: the 3rd Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across <span class=acl-fixed-case>S</span>lavic Languages</a></strong><br><a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/b/bogdan-babych/>Bogdan Babych</a>
|
<a href=/people/z/zara-kancheva/>Zara Kancheva</a>
|
<a href=/people/o/olga-kanishcheva/>Olga Kanishcheva</a>
|
<a href=/people/m/maria-lebedeva/>Maria Lebedeva</a>
|
<a href=/people/m/michal-marcinczuk/>Michał Marcińczuk</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/p/petya-osenova/>Petya Osenova</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/s/senja-pollak/>Senja Pollak</a>
|
<a href=/people/p/pavel-priban/>Pavel Přibáň</a>
|
<a href=/people/i/ivaylo-radev/>Ivaylo Radev</a>
|
<a href=/people/m/marko-robnik-sikonja/>Marko Robnik-Sikonja</a>
|
<a href=/people/v/vasyl-starko/>Vasyl Starko</a>
|
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bsnlp-1--15><div class="card-body p-3 small">This paper describes Slav-NER : the 3rd Multilingual Named Entity Challenge in <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic languages</a>. The tasks involve recognizing mentions of named entities in <a href=https://en.wikipedia.org/wiki/Web_page>Web documents</a>, normalization of the names, and cross-lingual linking. The Challenge covers six languages and five entity types, and is organized as part of the 8th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference. Ten teams participated in the competition. Performance for the named entity recognition task reached 90 % <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>, much higher than reported in the first edition of the Challenge. Seven teams covered all six languages, and five teams participated in the cross-lingual entity linking task. Detailed valuation information is available on the shared task web page.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>