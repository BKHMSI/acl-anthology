<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Annual Meeting of the Association for Computational Linguistics (2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Annual Meeting of the Association for Computational Linguistics (2017)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#p17-1>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a>
<span class="badge badge-info align-middle ml-1">156&nbsp;papers</span></li><li><a class=align-middle href=#p17-2>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a>
<span class="badge badge-info align-middle ml-1">86&nbsp;papers</span></li><li><a class=align-middle href=#p17-3>Proceedings of ACL 2017, Student Research Workshop</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#p17-4>Proceedings of ACL 2017, System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#p17-5>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li></ul></div></div><div id=p17-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P17-1/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1000/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></strong><br><a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952967 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1001/>Adversarial Multi-task Learning for Text Classification</a></strong><br><a href=/people/p/pengfei-liu/>Pengfei Liu</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1001><div class="card-body p-3 small">Neural network models have shown their promising opportunities for <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can be regarded as off-the-shelf knowledge and easily transferred to new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. The datasets of all 16 <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are publicly available at.<url>http://nlp.fudan.edu.cn/data/</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1002.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953034 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1002/>Neural End-to-End Learning for Computational Argumentation Mining</a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1002><div class="card-body p-3 small">We investigate <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural techniques</a> for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning &#8216;natural&#8217; subtasks, in a multi-task learning setup, improves performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1003.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953110 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1003/>Neural Symbolic Machines : Learning Semantic Parsers on <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> with Weak Supervision<span class=acl-fixed-case>F</span>reebase with Weak Supervision</a></strong><br><a href=/people/c/chen-liang/>Chen Liang</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a>
|
<a href=/people/q/quoc-le/>Quoc Le</a>
|
<a href=/people/k/kenneth-forbus/>Kenneth D. Forbus</a>
|
<a href=/people/n/ni-lao/>Ni Lao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1003><div class="card-body p-3 small">Harnessing the statistical power of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to perform <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> and <a href=https://en.wikipedia.org/wiki/Computer_algebra>symbolic reasoning</a> is difficult, when it requires executing efficient <a href=https://en.wikipedia.org/wiki/Discrete_mathematics>discrete operations</a> against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural programmer, i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a <a href=https://en.wikipedia.org/wiki/Computer_algebra>symbolic computer</a>, i.e., a <a href=https://en.wikipedia.org/wiki/Lisp_(programming_language)>Lisp interpreter</a> that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the <a href=https://en.wikipedia.org/wiki/Stability_theory>stability</a> of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> or domain-specific knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953158 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1004/>Neural Relation Extraction with Multi-lingual Attention</a></strong><br><a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1004><div class="card-body p-3 small">Relation extraction has been widely used for finding unknown relational facts from <a href=https://en.wikipedia.org/wiki/Plain_text>plain text</a>. Most existing methods focus on exploiting mono-lingual data for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, ignoring massive information from the texts in various languages. To address this issue, we introduce a multi-lingual neural relation extraction framework, which employs mono-lingual attention to utilize the information within mono-lingual texts and further proposes cross-lingual attention to consider the information consistency and complementarity among cross-lingual texts. Experimental results on real-world datasets show that, our model can take advantage of multi-lingual texts and consistently achieve significant improvements on relation extraction as compared with baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954083 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1005" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1005/>Learning Structured Natural Language Representations for Semantic Parsing</a></strong><br><a href=/people/j/jianpeng-cheng/>Jianpeng Cheng</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/v/vijay-saraswat/>Vijay Saraswat</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1005><div class="card-body p-3 small">We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEOQUERY and WEBQUESTIONS. The induced predicate-argument structures shed light on the types of <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> useful for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> and how these are different from linguistically motivated ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1006.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954143 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1006/>Morph-fitting : Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/d/diarmuid-o-seaghdha/>Diarmuid Ó Séaghdha</a>
|
<a href=/people/s/steve-young/>Steve Young</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1006><div class="card-body p-3 small">Morphologically rich languages accentuate two properties of distributional vector space models : 1) the difficulty of inducing accurate representations for low-frequency word forms ; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding systems</a>, which may infer that &#8216;inexpensive&#8217; is a rephrasing for &#8216;expensive&#8217; or may not associate &#8216;acquire&#8217; with &#8216;acquires&#8217;. In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces. Instead, our method injects <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological constraints</a> generated using simple language-specific rules, pulling <a href=https://en.wikipedia.org/wiki/Inflection>inflectional forms</a> of the same word close together and pushing <a href=https://en.wikipedia.org/wiki/Morphological_derivation>derivational antonyms</a> far apart. In intrinsic evaluation over four languages, we show that our approach : 1) improves low-frequency word estimates ; and 2) boosts the semantic quality of the entire word vector collection. Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> for tackling long-tail phenomena in language understanding tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954195 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1007/>Skip-Gram Zipf + Uniform = <a href=https://en.wikipedia.org/wiki/Vector-valued_function>Vector Additivity</a><span class=acl-fixed-case>Z</span>ipf + Uniform = Vector Additivity</a></strong><br><a href=/people/a/alex-gittens/>Alex Gittens</a>
|
<a href=/people/d/dimitris-achlioptas/>Dimitris Achlioptas</a>
|
<a href=/people/m/michael-w-mahoney/>Michael W. Mahoney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1007><div class="card-body p-3 small">In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected side-effect of such models is that their vectors often exhibit compositionality, i.e., addingtwo word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., man + royal = king. This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of <a href=https://en.wikipedia.org/wiki/Vector_calculus>vector calculus</a> in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby : the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.<i>adding</i>\n\ntwo word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., &#8220;man&#8221; + &#8220;royal&#8221; = &#8220;king&#8221;. This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby: the parameters of SDR models can be obtained from those of Skip-Gram models simply by adding information on symbol frequencies. This shows that Skip-Gram embeddings are optimal in the sense of Globerson and Tishby and, further, implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954248 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1008/>The State of the Art in Semantic Representation</a></strong><br><a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/a/ari-rappoport/>Ari Rappoport</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1008><div class="card-body p-3 small">Semantic representation is receiving growing attention in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> in the past few years, and many proposals for semantic schemes (e.g., AMR, <a href=https://en.wikipedia.org/wiki/UCCA>UCCA</a>, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953264 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1009/>Joint Learning for Event Coreference Resolution</a></strong><br><a href=/people/j/jing-lu/>Jing Lu</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1009><div class="card-body p-3 small">While joint models have been developed for many NLP tasks, the vast majority of event coreference resolvers, including the top-performing resolvers competing in the recent TAC KBP 2016 Event Nugget Detection and Coreference task, are pipeline-based, where the propagation of errors from the trigger detection component to the event coreference component is a major performance limiting factor. To address this problem, we propose a model for jointly learning event coreference, trigger detection, and event anaphoricity. Our joint model is novel in its choice of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and its features for capturing cross-task interactions. To our knowledge, this is the first attempt to train a mention-ranking model and employ event anaphoricity for event coreference. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the best results to date on the KBP 2016 English and Chinese datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953310 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1010/>Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution</a></strong><br><a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/q/qingyu-yin/>Qingyu Yin</a>
|
<a href=/people/w/weinan-zhang/>Wei-Nan Zhang</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1010><div class="card-body p-3 small">Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Design_of_experiments>approach</a> significantly outperforms the state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> with an absolute improvements of 3.1 % <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> on OntoNotes 5.0 data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953346 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1011/>Discourse Mode Identification in Essays</a></strong><br><a href=/people/w/wei-song/>Wei Song</a>
|
<a href=/people/d/dong-wang/>Dong Wang</a>
|
<a href=/people/r/ruiji-fu/>Ruiji Fu</a>
|
<a href=/people/l/lizhen-liu/>Lizhen Liu</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1011><div class="card-body p-3 small">Discourse modes play an important role in <a href=https://en.wikipedia.org/wiki/Composition_(language)>writing composition</a> and <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>. This paper presents a study on the manual and automatic identification of narration, exposition, description, argument and emotion expressing sentences in narrative essays. We annotate a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to study the characteristics of <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse modes</a> and describe a neural sequence labeling model for identification. Evaluation results show that discourse modes can be identified automatically with an average F1-score of 0.7. We further demonstrate that discourse modes can be used as <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> that improve automatic essay scoring (AES). The impacts of discourse modes for <a href=https://en.wikipedia.org/wiki/Advanced_Encryption_Standard>AES</a> are also discussed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234951392 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1013/>Deep Neural Machine Translation with <a href=https://en.wikipedia.org/wiki/Linear_algebra>Linear Associative Unit</a></a></strong><br><a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/z/zhengdong-lu/>Zhengdong Lu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1013><div class="card-body p-3 small">Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with its capability in modeling complex functions and capturing complex linguistic structures. However NMT with deep architecture in its encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often makes the optimization much more difficult. To address this problem we propose a novel linear associative units (LAU) to reduce the gradient propagation path inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs uses linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported on results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1014.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952236 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1014" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1014/>Neural AMR : Sequence-to-Sequence Models for Parsing and Generation<span class=acl-fixed-case>AMR</span>: Sequence-to-Sequence Models for Parsing and Generation</a></strong><br><a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/s/srinivasan-iyer/>Srinivasan Iyer</a>
|
<a href=/people/m/mark-yatskar/>Mark Yatskar</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1014><div class="card-body p-3 small">Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1016.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952320 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1016/>Automatically Generating Rhythmic Verse with <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a></a></strong><br><a href=/people/j/jack-hopkins/>Jack Hopkins</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1016><div class="card-body p-3 small">We propose two novel <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a> for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic encoding</a> to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as <a href=https://en.wikipedia.org/wiki/Rhyme>rhyme</a>, <a href=https://en.wikipedia.org/wiki/Rhythm>rhythm</a> and <a href=https://en.wikipedia.org/wiki/Alliteration>alliteration</a>. The second approach considers poetry generation as a <a href=https://en.wikipedia.org/wiki/Constraint_satisfaction_problem>constraint satisfaction problem</a> where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54 % of the time. In addition, participants rated a machine-generated poem to be the best amongst all evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952361 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1017/>Creating Training Corpora for NLG Micro-Planners<span class=acl-fixed-case>NLG</span> Micro-Planners</a></strong><br><a href=/people/c/claire-gardent/>Claire Gardent</a>
|
<a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a>
|
<a href=/people/s/shashi-narayan/>Shashi Narayan</a>
|
<a href=/people/l/laura-perez-beltrachini/>Laura Perez-Beltrachini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1017><div class="card-body p-3 small">In this paper, we present a novel framework for semi-automatically creating linguistically challenging micro-planning data-to-text corpora from existing Knowledge Bases. Because our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> pairs data of varying size and shape with texts ranging from simple clauses to short texts, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> created using this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> provides a challenging benchmark for microplanning. Another feature of this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is that it can be applied to any large scale knowledge base and can therefore be used to train and learn KB verbalisers. We apply our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> to <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia data</a> and compare the resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with Wen et al. 2016&#8217;s. We show that while Wen et al.&#8217;s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of handling the complex interactions occurring during in micro-planning between lexicalisation, aggregation, surface realisation, referring expression generation and sentence segmentation. To encourage researchers to take up this challenge, we made available a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 21,855 data / text pairs created using this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> in the context of the WebNLG shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1018/>Gated Self-Matching Networks for Reading Comprehension and Question Answering</a></strong><br><a href=/people/w/wenhui-wang/>Wenhui Wang</a>
|
<a href=/people/n/nan-yang/>Nan Yang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1018><div class="card-body p-3 small">In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The <a href=https://en.wikipedia.org/wiki/Statistical_model>single model</a> achieves 71.3 % on the evaluation metrics of exact match on the hidden test set, while the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble model</a> further boosts the results to 75.9 %. At the time of submission of the paper, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> holds the first place on the SQuAD leaderboard for both single and ensemble model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1019/>Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning</a></strong><br><a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/c/cao-liu/>Cao Liu</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1019><div class="card-body p-3 small">Generating answer with natural language sentence is very important in real-world question answering systems, which needs to obtain a right answer as well as a coherent natural response. In this paper, we propose an end-to-end question answering system called COREQA in sequence-to-sequence learning, which incorporates copying and retrieving mechanisms to generate natural answers within an encoder-decoder framework. Specifically, in COREQA, the semantic units (words, phrases and entities) in a natural answer are dynamically predicted from the vocabulary, copied from the given question and/or retrieved from the corresponding knowledge base jointly. Our empirical study on both synthetic and real-world datasets demonstrates the efficiency of COREQA, which is able to generate correct, coherent and natural answers for knowledge inquired questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1021/>An End-to-End Model for Question Answering over <a href=https://en.wikipedia.org/wiki/Knowledge_base>Knowledge Base</a> with Cross-Attention Combining Global Knowledge</a></strong><br><a href=/people/y/yanchao-hao/>Yanchao Hao</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/z/zhanyi-liu/>Zhanyi Liu</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1021><div class="card-body p-3 small">With the rapid growth of knowledge bases (KBs) on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, how to take full advantage of them becomes increasingly important. Question answering over knowledge base (KB-QA) is one of the promising approaches to access the substantial knowledge. Meanwhile, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put more emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is not easy to express the proper information in the question. Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. As a result, it could alleviates the out-of-vocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely. The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1022.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954361 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1022/>Translating Neuralese</a></strong><br><a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/a/anca-dragan/>Anca Dragan</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1022><div class="card-body p-3 small">Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these <a href=https://en.wikipedia.org/wiki/Policy>policies</a> are effective for many <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents&#8217; messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that <a href=https://en.wikipedia.org/wiki/Agent-based_model>agent messages</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language strings</a> mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954406 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1023/>Obtaining referential word meanings from visual and distributional information : Experiments on object naming</a></strong><br><a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1023><div class="card-body p-3 small">We investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in <a href=https://en.wikipedia.org/wiki/Outline_of_object_recognition>object recognition</a>, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings. We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954462 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1024/>FOIL it ! Find One mismatch between Image and Language caption<span class=acl-fixed-case>FOIL</span> it! Find One mismatch between Image and Language caption</a></strong><br><a href=/people/r/ravi-shekhar/>Ravi Shekhar</a>
|
<a href=/people/s/sandro-pezzelle/>Sandro Pezzelle</a>
|
<a href=/people/y/yauhen-klimovich/>Yauhen Klimovich</a>
|
<a href=/people/a/aurelie-herbelot/>Aurélie Herbelot</a>
|
<a href=/people/m/moin-nabi/>Moin Nabi</a>
|
<a href=/people/e/enver-sangineto/>Enver Sangineto</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1024><div class="card-body p-3 small">In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and &#8216;foil&#8217; captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake (&#8216;foil word&#8217;). We show that current LaVi models fall into the traps of this data and perform badly on three tasks : a) <a href=https://en.wikipedia.org/wiki/Closed_captioning>caption classification</a> (correct vs. foil) ; b) foil word detection ; c) foil word correction. Humans, in contrast, have near-perfect performance on those <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We demonstrate that merely utilising language cues is not enough to model FOIL-COCO and that it challenges the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by requiring a fine-grained understanding of the relation between text and image.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954495 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1025/>Verb Physics : Relative Physical Knowledge of Actions and Objects</a></strong><br><a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1025><div class="card-body p-3 small">Learning commonsense knowledge from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language text</a> is nontrivial due to reporting bias : people rarely state the obvious, e.g., My house is bigger than me. However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, Tyler entered his house implies that his house is bigger than Tyler. In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame <a href=https://en.wikipedia.org/wiki/Knowledge_acquisition>knowledge acquisition</a> as joint inference over two closely related problems : learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from <a href=https://en.wikipedia.org/wiki/Language>language</a> and that joint inference over different types of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> improves performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953499 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1026/>A * CCG Parsing with a Supertag and Dependency Factored Model<span class=acl-fixed-case>A</span>* <span class=acl-fixed-case>CCG</span> Parsing with a Supertag and Dependency Factored Model</a></strong><br><a href=/people/m/masashi-yoshikawa/>Masashi Yoshikawa</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1026><div class="card-body p-3 small">We propose a new A * CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs. Our <a href=https://en.wikipedia.org/wiki/Factorization>factored model</a> allows the precomputation of all probabilities and runs very efficiently, while modeling <a href=https://en.wikipedia.org/wiki/Sentence_(mathematical_logic)>sentence structures</a> explicitly via <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>dependencies</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the state-of-the-art results on English and Japanese CCG parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953567 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1027/>A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing</a></strong><br><a href=/people/d/daniel-fernandez-gonzalez/>Daniel Fernández-González</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1027><div class="card-body p-3 small">Restricted non-monotonicity has been shown beneficial for the projective arc-eager dependency parser in previous research, as posterior decisions can repair mistakes made in previous states due to the lack of information. In this paper, we propose a novel, fully non-monotonic transition system based on the non-projective Covington algorithm. As a <a href=https://en.wikipedia.org/wiki/Non-monotonic_logic>non-monotonic system</a> requires exploration of erroneous actions during the training process, we develop several non-monotonic variants of the recently defined dynamic oracle for the Covington parser, based on tight approximations of the loss. Experiments on datasets from the CoNLL-X and CoNLL-XI shared tasks show that a non-monotonic dynamic oracle outperforms the monotonic version in the majority of languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234951631 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1029/>Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction</a></strong><br><a href=/people/c/chunting-zhou/>Chunting Zhou</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1029><div class="card-body p-3 small">Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a>. The <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> can use <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to handle both discrete and continuous latent variables to exploit various <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features of data</a>. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1033.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234951882 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1033" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1033/>Topically Driven Neural Language Model</a></strong><br><a href=/people/j/jey-han-lau/>Jey Han Lau</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1033><div class="card-body p-3 small">Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> also has the ability to generate related sentences for a topic, providing another way to interpret topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952512 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1035/>Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Network</a></a></strong><br><a href=/people/a/abhijit-mishra/>Abhijit Mishra</a>
|
<a href=/people/k/kuntal-dey/>Kuntal Dey</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1035><div class="card-body p-3 small">Cognitive NLP systems- i.e., NLP systems that make use of behavioral data-augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc. Such extraction of features is typically manual. We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> and Sarcasm Detection, and that even the extraction and choice of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement / gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is based on Convolutional Neural Network (CNN). The <a href=https://en.wikipedia.org/wiki/CNN>CNN</a> learns features from both gaze and <a href=https://en.wikipedia.org/wiki/Written_language>text</a> and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952560 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1036" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1036/>An Unsupervised Neural Attention Model for Aspect Extraction</a></strong><br><a href=/people/r/ruidan-he/>Ruidan He</a>
|
<a href=/people/w/wee-sun-lee/>Wee Sun Lee</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a>
|
<a href=/people/d/daniel-dahlmeier/>Daniel Dahlmeier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1036><div class="card-body p-3 small">Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. While fairly successful, these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherent aspects</a>. The model improves <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the <a href=https://en.wikipedia.org/wiki/Embedding>embedding space</a>. In addition, we use an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to de-emphasize irrelevant words during training, further improving the <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence of aspects</a>. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1037/>Other Topics You May Also Agree or Disagree : Modeling Inter-Topic Preferences using Tweets and Matrix Factorization</a></strong><br><a href=/people/a/akira-sasaki/>Akira Sasaki</a>
|
<a href=/people/k/kazuaki-hanawa/>Kazuaki Hanawa</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1037><div class="card-body p-3 small">We presents in this paper our approach for modeling inter-topic preferences of Twitter users : for example, those who agree with the <a href=https://en.wikipedia.org/wiki/Trans-Pacific_Partnership>Trans-Pacific Partnership (TPP)</a> also agree with <a href=https://en.wikipedia.org/wiki/Free_trade>free trade</a>. This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including <a href=https://en.wikipedia.org/wiki/Opinion_poll>public opinion survey</a>, <a href=https://en.wikipedia.org/wiki/Prediction>electoral prediction</a>, <a href=https://en.wikipedia.org/wiki/Political_campaign>electoral campaigns</a>, and online debates. In order to extract users&#8217; preferences on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, we design linguistic patterns in which people agree and disagree about specific topics (e.g., A is completely wrong). By applying these linguistic patterns to a collection of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization : representing users&#8217; preference as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our presented approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1038/>Automatically Labeled Data Generation for Large Scale Event Extraction</a></strong><br><a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/s/shulin-liu/>Shulin Liu</a>
|
<a href=/people/x/xiang-zhang/>Xiang Zhang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1038><div class="card-body p-3 small">Modern models of <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a> hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> learned from these <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1039.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1039" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1039/>Time Expression Analysis and Recognition Using Syntactic Token Types and <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>General Heuristic Rules</a></a></strong><br><a href=/people/x/xiaoshi-zhong/>Xiaoshi Zhong</a>
|
<a href=/people/a/aixin-sun/>Aixin Sun</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1039><div class="card-body p-3 small">Extracting time expressions from free text is a fundamental task for many applications. We analyze the time expressions from four datasets and find that only a small group of words are used to express time information, and the words in time expressions demonstrate similar syntactic behaviour. Based on the findings, we propose a type-based approach, named SynTime, to recognize time expressions. Specifically, we define three main syntactic token types, namely time token, modifier, and numeral, to group time-related regular expressions over tokens. On the types we design general <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic rules</a> to recognize <a href=https://en.wikipedia.org/wiki/Time_complexity>time expressions</a>. In recognition, SynTime first identifies the time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions. As a light-weight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text of different types and of different domains. Experiment on benchmark datasets and tweets data shows that SynTime outperforms state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1041.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954608 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1041" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1041/>A Syntactic Neural Model for General-Purpose Code Generation</a></strong><br><a href=/people/p/pengcheng-yin/>Pengcheng Yin</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1041><div class="card-body p-3 small">We consider the problem of parsing <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language descriptions</a> into source code written in a <a href=https://en.wikipedia.org/wiki/General-purpose_programming_language>general-purpose programming language</a> like <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a>. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language descriptions</a>, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1043 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954715 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1043" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1043/>Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation Parsing using <span class=acl-fixed-case>LSTM</span> Recurrent Neural Networks</a></strong><br><a href=/people/w/william-foland/>William Foland</a>
|
<a href=/people/j/james-h-martin/>James H. Martin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1043><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/System>system</a> which parses sentences into Abstract Meaning Representations, improving state-of-the-art results for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> by more than 5 %. AMR graphs represent semantic content using linguistic properties such as semantic roles, <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>, <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a>, and more. The AMR parser does not rely on a syntactic pre-parse, or heavily engineered features, and uses five <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> as the key architectural components for inferring AMR graphs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954784 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1044" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1044/>Deep Semantic Role Labeling : What Works and What’s Next</a></strong><br><a href=/people/l/luheng-he/>Luheng He</a>
|
<a href=/people/k/kenton-lee/>Kenton Lee</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1044><div class="card-body p-3 small">We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a>. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10 % relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953832 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1045" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1045/>Towards <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>End-to-End Reinforcement Learning</a> of Dialogue Agents for Information Access</a></strong><br><a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/l/lihong-li/>Lihong Li</a>
|
<a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/f/faisal-ahmad/>Faisal Ahmed</a>
|
<a href=/people/l/li-deng/>Li Deng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1045><div class="card-body p-3 small">This paper proposes KB-InfoBot-a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the <a href=https://en.wikipedia.org/wiki/System>system</a> and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced soft posterior distribution over the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> that indicates which entities the user is interested in. Integrating the soft retrieval process with a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learner</a> leads to higher task success rate and <a href=https://en.wikipedia.org/wiki/Reward_system>reward</a> in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953894 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1046" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1046/>Sequential Matching Network : A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots</a></strong><br><a href=/people/y/yu-wu/>Yu Wu</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/c/chen-xing/>Chen Xing</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1046><div class="card-body p-3 small">We study response selection for multi-turn conversation in retrieval based chatbots. Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among the utterances or important information in the context. We propose a sequential matching network (SMN) to address both problems. SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN)</a> which models relationships among the utterances. The final matching score is calculated with the hidden states of the <a href=https://en.wikipedia.org/wiki/Random-access_memory>RNN</a>. Empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1049.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1049/>Found in Translation : Reconstructing Phylogenetic Language Trees from Translations</a></strong><br><a href=/people/e/ella-rabinovich/>Ella Rabinovich</a>
|
<a href=/people/n/noam-ordan/>Noam Ordan</a>
|
<a href=/people/s/shuly-wintner/>Shuly Wintner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1049><div class="card-body p-3 small">Translation has played an important role in <a href=https://en.wikipedia.org/wiki/Trade>trade</a>, <a href=https://en.wikipedia.org/wiki/Law>law</a>, <a href=https://en.wikipedia.org/wiki/Commerce>commerce</a>, <a href=https://en.wikipedia.org/wiki/Politics>politics</a>, and <a href=https://en.wikipedia.org/wiki/Literature>literature</a> for thousands of years. Translators have always tried to be invisible ; ideal translations should look as if they were written originally in the target language. We show that traces of the source language remain in the translation product to the extent that it is possible to uncover the history of the source language by looking only at the translation. Specifically, we automatically reconstruct phylogenetic language trees from monolingual texts (translated from several source languages). The signal of the source language is so powerful that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is retained even after two phases of <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. This strongly indicates that source language interference is the most dominant characteristic of translated texts, overshadowing the more subtle signals of universal properties of <a href=https://en.wikipedia.org/wiki/Translation>translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234951959 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1050/>Predicting Native Language from Gaze</a></strong><br><a href=/people/y/yevgeni-berzak/>Yevgeni Berzak</a>
|
<a href=/people/c/chie-nakamura/>Chie Nakamura</a>
|
<a href=/people/s/suzanne-flynn/>Suzanne Flynn</a>
|
<a href=/people/b/boris-katz/>Boris Katz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1050><div class="card-body p-3 small">A fundamental question in <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a> concerns the role of a speaker&#8217;s first language in second language acquisition. We present a novel <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for studying this question : analysis of eye-movement patterns in second language reading of free-form text. Using this <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a>, we demonstrate for the first time that the native language of English learners can be predicted from their gaze fixations when reading <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We provide analysis of classifier uncertainty and learned features, which indicates that differences in English reading are likely to be rooted in linguistic divergences across native languages. The presented <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> complements production studies and offers new ground for advancing research on <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingualism</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1051.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1051.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1051.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952666 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1051/>MORSE : Semantic-ally Drive-n MORpheme SEgment-er<span class=acl-fixed-case>MORSE</span>: Semantic-ally Drive-n <span class=acl-fixed-case>MOR</span>pheme <span class=acl-fixed-case>SE</span>gment-er</a></strong><br><a href=/people/t/tarek-sakakini/>Tarek Sakakini</a>
|
<a href=/people/s/suma-bhat/>Suma Bhat</a>
|
<a href=/people/p/pramod-viswanath/>Pramod Viswanath</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1051><div class="card-body p-3 small">We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> is the first to consider vocabulary-wide syntactico-semantic information for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a>. We validate our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> across datasets and present state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956392 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1052/>Deep Pyramid Convolutional Neural Networks for Text Categorization</a></strong><br><a href=/people/r/rie-johnson/>Rie Johnson</a>
|
<a href=/people/t/tong-zhang/>Tong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1052><div class="card-body p-3 small">This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text categorization that can efficiently represent long-range associations in text. In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data. However, the associated <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> increases as the <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a> go deeper, which poses serious challenges in practical applications. Moreover, it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data. Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much. We call it deep pyramid CNN. The proposed model with 15 <a href=https://en.wikipedia.org/wiki/Weighting>weight layers</a> outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1053.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1053.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956455 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1053/>Improved Neural Relation Detection for Knowledge Base Question Answering</a></strong><br><a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/k/kazi-saidul-hasan/>Kazi Saidul Hasan</a>
|
<a href=/people/c/cicero-dos-santos/>Cicero dos Santos</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a>
|
<a href=/people/b/bowen-zhou/>Bowen Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1053><div class="card-body p-3 small">Relation detection is a core component of many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a> including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning which detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different levels of abstraction. Additionally, we propose a simple KBQA system that integrates <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1054.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956524 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1054" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1054/>Deep Keyphrase Generation</a></strong><br><a href=/people/r/rui-meng/>Rui Meng</a>
|
<a href=/people/s/sanqiang-zhao/>Sanqiang Zhao</a>
|
<a href=/people/s/shuguang-han/>Shuguang Han</a>
|
<a href=/people/d/daqing-he/>Daqing He</a>
|
<a href=/people/p/peter-brusilovsky/>Peter Brusilovsky</a>
|
<a href=/people/y/yu-chi/>Yu Chi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1054><div class="card-body p-3 small">Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at.<i>deep keyphrase generation</i> since it\n attempts to capture the deep semantic meaning of the content with a deep\n learning method. Empirical analysis on six datasets demonstrates that our\n proposed model not only achieves a significant performance boost on\n extracting keyphrases that appear in the source text, but also can\n generate absent keyphrases based on the semantic meaning of the text. Code\n and dataset are available at <url>https://github.com/memray/seq2seq-keyphrase</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956590 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1055" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1055/>Attention-over-Attention Neural Networks for Reading Comprehension</a></strong><br><a href=/people/y/yiming-cui/>Yiming Cui</a>
|
<a href=/people/z/zhipeng-chen/>Zhipeng Chen</a>
|
<a href=/people/s/si-wei/>Si Wei</a>
|
<a href=/people/s/shijin-wang/>Shijin Wang</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1055><div class="card-body p-3 small">Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. The proposed model aims to place another <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> over the document-level attention and induces attended attention for final answer predictions. One advantage of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is simpler than related works while giving excellent performance. In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as <a href=https://en.wikipedia.org/wiki/CNN>CNN</a> and Children&#8217;s Book Test.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957958 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1056/>Alignment at Work : Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations</a></strong><br><a href=/people/g/gabriel-doyle/>Gabriel Doyle</a>
|
<a href=/people/a/amir-goldberg/>Amir Goldberg</a>
|
<a href=/people/s/sameer-srivastava/>Sameer Srivastava</a>
|
<a href=/people/m/michael-c-frank/>Michael Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1056><div class="card-body p-3 small">Cultural fit is widely believed to affect the success of individuals and the groups to which they belong. Yet <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> remains an elusive, poorly measured construct. Recent research draws on <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> to measure cultural fit but overlooks asymmetries in <a href=https://en.wikipedia.org/wiki/Cultural_adaptation>cultural adaptation</a>. By contrast, we develop a directed, dynamic measure of cultural fit based on linguistic alignment, which estimates the influence of one person&#8217;s word use on another&#8217;s and distinguishes between two enculturation mechanisms : <a href=https://en.wikipedia.org/wiki/Internalization>internalization</a> and <a href=https://en.wikipedia.org/wiki/Emotional_self-regulation>self-regulation</a>. We use this measure to trace employees&#8217; enculturation trajectories over a large, multi-year corpus of corporate emails and find that patterns of alignment in the first six months of employment are predictive of individuals&#8217; downstream outcomes, especially involuntary exit. Further predictive analyses suggest referential alignment plays an overlooked role in linguistic alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1057.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958014 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1057" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1057/>Representations of language in a model of visually grounded speech signal</a></strong><br><a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a>
|
<a href=/people/l/lieke-gelderloos/>Lieke Gelderloos</a>
|
<a href=/people/a/afra-alishahi/>Afra Alishahi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1057><div class="card-body p-3 small">We present a visually grounded model of <a href=https://en.wikipedia.org/wiki/Speech_perception>speech perception</a> which projects spoken utterances and <a href=https://en.wikipedia.org/wiki/Image>images</a> to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958072 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1058/>Spectral Analysis of <a href=https://en.wikipedia.org/wiki/Information_density>Information Density</a> in <a href=https://en.wikipedia.org/wiki/Dialogue>Dialogue</a> Predicts Collaborative Task Performance</a></strong><br><a href=/people/y/yang-xu/>Yang Xu</a>
|
<a href=/people/d/david-reitter/>David Reitter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1058><div class="card-body p-3 small">We propose a <a href=https://en.wikipedia.org/wiki/Point_of_view_(philosophy)>perspective</a> on <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> that focuses on relative information contributions of conversation partners as a key to successful communication. We predict the success of collaborative task in English and Danish corpora of task-oriented dialogue. Two features are extracted from the frequency domain representations of the lexical entropy series of each interlocutor, power spectrum overlap (PSO) and relative phase (RP). We find that PSO is a negative predictor of task success, while RP is a positive one. An <a href=https://en.wikipedia.org/wiki/Symmetric_multiprocessing>SVM</a> with these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> significantly improved on previous task success prediction models. Our findings suggest that the strategic distribution of information density between interlocutors is relevant to task success.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957188 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1059/>Affect-LM : A Neural Language Model for Customizable Affective Text Generation<span class=acl-fixed-case>LM</span>: A Neural Language Model for Customizable Affective Text Generation</a></strong><br><a href=/people/s/sayan-ghosh/>Sayan Ghosh</a>
|
<a href=/people/m/mathieu-chollet/>Mathieu Chollet</a>
|
<a href=/people/e/eugene-laksana/>Eugene Laksana</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/s/stefan-scherer/>Stefan Scherer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1059><div class="card-body p-3 small">Human verbal communication includes <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective messages</a> which are conveyed through use of <a href=https://en.wikipedia.org/wiki/Emotion>emotionally colored words</a>. There has been a lot of research effort in this direction but the problem of integrating state-of-the-art neural language models with <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective information</a> remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generation of conversational text, conditioned on <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect categories</a>. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using <a href=https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk>Amazon Mechanical Turk</a> show that Affect-LM can generate naturally looking emotional sentences without sacrificing <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical correctness</a>. Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective information</a> in conversational text can improve language model prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957265 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1060/>Domain Attention with an Ensemble of Experts</a></strong><br><a href=/people/y/young-bum-kim/>Young-Bum Kim</a>
|
<a href=/people/k/karl-stratos/>Karl Stratos</a>
|
<a href=/people/d/dongchan-kim/>Dongchan Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1060><div class="card-body p-3 small">An important problem in <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> is to quickly generalize to a new domain with limited supervision given K existing domains. One approach is to retrain a global model across all K + 1 domains using standard techniques, for instance Daum III (2009). However, it is desirable to adapt without having to re-estimate a global model from scratch each time a new domain with potentially new intents and slots is added. We describe a <a href=https://en.wikipedia.org/wiki/Solution>solution</a> based on attending an ensemble of domain experts. We assume K domain specific intent and slot models trained on respective domains. When given domain K + 1, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> uses a weighted combination of the K domain experts&#8217; feedback along with its own opinion to make predictions on the new domain. In experiments, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms baselines that do not use domain adaptation and also performs better than the full retraining approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1061.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1061.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957321 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1061" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1061/>Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders</a></strong><br><a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/r/ran-zhao/>Ran Zhao</a>
|
<a href=/people/m/maxine-eskenazi/>Maxine Eskenazi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1061><div class="card-body p-3 small">While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> from word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that capture the discourse-level diversity in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> uses <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> to learn a distribution over potential conversational intents and generates diverse responses using only <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy decoders</a>. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved through introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence of discourse-level decision-making.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954880 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1064/>Modeling Source Syntax for Neural Machine Translation</a></strong><br><a href=/people/j/junhui-li/>Junhui Li</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/m/muhua-zhu/>Muhua Zhu</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1064><div class="card-body p-3 small">Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements. Specifically, we linearize <a href=https://en.wikipedia.org/wiki/Parse_tree>parse trees</a> of source sentences to obtain structural label sequences. On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT : 1) Parallel RNN encoder that learns word and label annotation vectors parallelly ; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy ; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed. Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy. It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points. Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1066 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1066.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955713 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1066/>Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning</a></strong><br><a href=/people/j/jing-ma/>Jing Ma</a>
|
<a href=/people/w/wei-gao/>Wei Gao</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1066><div class="card-body p-3 small">How fake news goes viral via <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., <a href=https://en.wikipedia.org/wiki/Fake_news>fake information</a>, out of <a href=https://en.wikipedia.org/wiki/Microblogging>microblog posts</a> based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect <a href=https://en.wikipedia.org/wiki/Rumor>rumors</a> more quickly and accurately than state-of-the-art rumor detection models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955738 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1067/>EmoNet : Fine-Grained Emotion Detection with Gated Recurrent Neural Networks<span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>N</span>et: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks</a></strong><br><a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1067><div class="card-body p-3 small">Accurate <a href=https://en.wikipedia.org/wiki/Emotion_detection>detection of emotion</a> from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a> has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> on it. We achieve a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on 24 fine-grained types of emotions (with an average <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 87.58 %). We also extend the task beyond <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion types</a> to model Robert Plutick&#8217;s 8 primary emotion dimensions, acquiring a superior <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 95.68 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1068.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955801 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1068/>Beyond Binary Labels : Political Ideology Prediction of Twitter Users<span class=acl-fixed-case>T</span>witter Users</a></strong><br><a href=/people/d/daniel-preotiuc-pietro/>Daniel Preoţiuc-Pietro</a>
|
<a href=/people/y/ye-liu/>Ye Liu</a>
|
<a href=/people/d/daniel-hopkins/>Daniel Hopkins</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1068><div class="card-body p-3 small">Automatic political orientation prediction from social media posts has to date proven successful only in distinguishing between publicly declared liberals and <a href=https://en.wikipedia.org/wiki/Conservatism_in_the_United_States>conservatives</a> in the US. This study examines users&#8217; <a href=https://en.wikipedia.org/wiki/Political_ideology>political ideology</a> using a seven-point scale which enables us to identify politically moderate and neutral users groups which are of particular interest to political scientists and pollsters. Using a novel <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> with political ideology labels self-reported through surveys, our goal is two-fold : a) to characterize the groups of politically engaged users through language use on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> ; b) to build a fine-grained model that predicts <a href=https://en.wikipedia.org/wiki/Ideology>political ideology</a> of unseen users. Our results identify differences in both political leaning and engagement and the extent to which each group tweets using political keywords. Finally, we demonstrate how to improve ideology prediction accuracy by exploiting the relationships between the user groups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1069 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955861 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1069/>Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/k/kristen-johnson/>Kristen Johnson</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1069><div class="card-body p-3 small">Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring <a href=https://en.wikipedia.org/wiki/Political_framing>political framing</a> typically analyze frame usage in longer texts, such as <a href=https://en.wikipedia.org/wiki/Public_speaking>congressional speeches</a>. We present a collection of weakly supervised models which harness collective classification to predict the frames used in <a href=https://en.wikipedia.org/wiki/Discourse_analysis>political discourse</a> on the <a href=https://en.wikipedia.org/wiki/Microblogging>microblogging platform</a>, <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Our global probabilistic models show that by combining both lexical features of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> and network-based behavioral features of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, we are able to increase the average, <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised F1 score</a> by 21.52 points over a lexical baseline alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1070 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956745 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1070/>A Nested Attention Neural Hybrid Model for Grammatical Error Correction</a></strong><br><a href=/people/j/jianshu-ji/>Jianshu Ji</a>
|
<a href=/people/q/qinlong-wang/>Qinlong Wang</a>
|
<a href=/people/k/kristina-toutanova/>Kristina Toutanova</a>
|
<a href=/people/y/yongen-gong/>Yongen Gong</a>
|
<a href=/people/s/steven-truong/>Steven Truong</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1070><div class="card-body p-3 small">Grammatical error correction (GEC) systems strive to correct both global errors inword order and usage, and local errors inspelling and <a href=https://en.wikipedia.org/wiki/Inflection>inflection</a>. Further developing upon recent work on <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, we propose a new hybrid neural model with nested attention layers for GEC.Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information, and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective incorrecting local errors that involve small edits in orthography.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956802 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1071/>TextFlow : A Text Similarity Measure based on Continuous Sequences<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>F</span>low: A Text Similarity Measure based on Continuous Sequences</a></strong><br><a href=/people/y/yassine-mrabet/>Yassine Mrabet</a>
|
<a href=/people/h/halil-kilicoglu/>Halil Kilicoglu</a>
|
<a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1071><div class="card-body p-3 small">Text similarity measures are used in multiple tasks such as <a href=https://en.wikipedia.org/wiki/Plagiarism_detection>plagiarism detection</a>, information ranking and <a href=https://en.wikipedia.org/wiki/Paraphrase_recognition>recognition of paraphrases</a> and <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a>. While recent advances in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> highlighted the relevance of sequential models in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>, existing <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measures</a> do not fully exploit the sequential nature of language. Examples of such <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measures</a> include <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> and skip-grams overlap which rely on distinct slices of the input texts. In this paper we present a novel text similarity measure inspired from a common representation in <a href=https://en.wikipedia.org/wiki/Sequence_alignment>DNA sequence alignment algorithms</a>. The new <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a>, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity value</a>. Our experiments on 8 different datasets show very encouraging results in <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a>, textual entailment recognition and ranking relevance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1072.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1072.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956834 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1072" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1072/>Friendships, Rivalries, and Trysts : Characterizing Relations between Ideas in Texts</a></strong><br><a href=/people/c/chenhao-tan/>Chenhao Tan</a>
|
<a href=/people/d/dallas-card/>Dallas Card</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1072><div class="card-body p-3 small">Understanding how ideas relate to each other is a fundamental question in many domains, ranging from <a href=https://en.wikipedia.org/wiki/Intellectual_history>intellectual history</a> to public communication. Because ideas are naturally embedded in texts, we propose the first framework to systematically characterize the relations between ideas based on their occurrence in a corpus of documents, independent of how these ideas are represented. Combining two statisticscooccurrence within documents and prevalence correlation over timeour approach reveals a number of different ways in which ideas can cooperate and compete. For instance, two ideas can closely track each other&#8217;s prevalence over time, and yet rarely cooccur, almost like a cold war scenario. We observe that pairwise cooccurrence and prevalence correlation exhibit different distributions. We further demonstrate that our approach is able to uncover intriguing relations between ideas through in-depth case studies on <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> and <a href=https://en.wikipedia.org/wiki/Academic_publishing>research papers</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958212 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1073/>Polish evaluation dataset for compositional distributional semantics models<span class=acl-fixed-case>P</span>olish evaluation dataset for compositional distributional semantics models</a></strong><br><a href=/people/a/alina-wroblewska/>Alina Wróblewska</a>
|
<a href=/people/k/katarzyna-krasnowska-kieras/>Katarzyna Krasnowska-Kieraś</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1073><div class="card-body p-3 small">The paper presents a procedure of building an evaluation dataset. for the validation of compositional distributional semantics models estimated for <a href=https://en.wikipedia.org/wiki/Language>languages</a> other than <a href=https://en.wikipedia.org/wiki/English_language>English</a>. The procedure generally builds on steps designed to assemble the SICK corpus, which contains pairs of English sentences annotated for <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a> and <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a>, because we aim at building a comparable dataset. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the need for language-specific transformation rules. The designed procedure is verified on <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, a <a href=https://en.wikipedia.org/wiki/Fusional_language>fusional language</a> with a relatively free word order, and contributes to building a <a href=https://en.wikipedia.org/wiki/Polish_language>Polish evaluation dataset</a>. The resource consists of 10 K sentence pairs which are human-annotated for <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a> and <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> may be used for the evaluation of compositional distributional semantics models of <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958258 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1074" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1074/>Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction</a></strong><br><a href=/people/c/christopher-bryant/>Christopher Bryant</a>
|
<a href=/people/m/mariano-felice/>Mariano Felice</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1074><div class="card-body p-3 small">Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rule-based framework. This not only facilitates error type evaluation at different levels of <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a>, but can also be used to reduce annotator workload and standardise existing GEC datasets. Human experts rated the automatic edits as Good or Acceptable in at least 95 % of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1075.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1075 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1075 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958313 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1075/>Evaluation Metrics for Machine Reading Comprehension : Prerequisite Skills and Readability</a></strong><br><a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/y/yusuke-kido/>Yusuke Kido</a>
|
<a href=/people/h/hikaru-yokono/>Hikaru Yokono</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1075><div class="card-body p-3 small">Knowing the quality of reading comprehension (RC) datasets is important for the development of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural-language understanding systems</a>. In this study, two classes of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> were adopted for evaluating RC datasets : prerequisite skills and <a href=https://en.wikipedia.org/wiki/Readability>readability</a>. We applied these classes to six existing <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, including MCTest and SQuAD, and highlighted the characteristics of the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> according to each metric and the correlation between the two classes. Our dataset analysis suggests that the <a href=https://en.wikipedia.org/wiki/Readability>readability</a> of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy to read but difficult to answer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957467 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1076/>A Minimal Span-Based Neural Constituency Parser</a></strong><br><a href=/people/m/mitchell-stern/>Mitchell Stern</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1076><div class="card-body p-3 small">In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming techniques</a>, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1077.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1077.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957523 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1077/>Semantic Dependency Parsing via Book Embedding</a></strong><br><a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/j/junjie-cao/>Junjie Cao</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1077><div class="card-body p-3 small">We model a <a href=https://en.wikipedia.org/wiki/Dependency_graph>dependency graph</a> as a book, a particular kind of <a href=https://en.wikipedia.org/wiki/Topological_space>topological space</a>, for semantic dependency parsing. The spine of the book is made up of a sequence of words, and each page contains a subset of noncrossing arcs. To build a semantic graph for a given sentence, we design new Maximum Subgraph algorithms to generate noncrossing graphs on each page, and a Lagrangian Relaxation-based algorithm tocombine pages into a book. Experiments demonstrate the effectiveness of the bookembedding framework across a wide range of conditions. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> obtains comparable results with a state-of-the-art transition-based parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1078 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957587 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1078" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1078/>Neural Word Segmentation with Rich Pretraining</a></strong><br><a href=/people/j/jie-yang/>Jie Yang</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/f/fei-dong/>Fei Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1078><div class="card-body p-3 small">Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>, automatic segmentation and <a href=https://en.wikipedia.org/wiki/Point_of_interest>POS</a>. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, leading to accuracies competitive to the best methods on six benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955136 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1079/>Neural Machine Translation via Binary Code Prediction</a></strong><br><a href=/people/y/yusuke-oda/>Yusuke Oda</a>
|
<a href=/people/p/philip-arthur/>Philip Arthur</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1079><div class="card-body p-3 small">In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a <a href=https://en.wikipedia.org/wiki/Binary_code>binary code</a> for each word and can reduce <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time</a> / <a href=https://en.wikipedia.org/wiki/Memory_complexity>memory requirements</a> of the <a href=https://en.wikipedia.org/wiki/Input/output>output layer</a> to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> of the proposed model : using <a href=https://en.wikipedia.org/wiki/Error_correction_code>error-correcting codes</a> and combining <a href=https://en.wikipedia.org/wiki/Softmax>softmax</a> and <a href=https://en.wikipedia.org/wiki/Binary_code>binary codes</a>. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the <a href=https://en.wikipedia.org/wiki/Softmax>softmax</a>, while reducing memory usage to the order of less than 1/10 and improving decoding speed on <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPUs</a> by x5 to x10.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955981 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1081" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1081/>Context-Dependent Sentiment Analysis in <a href=https://en.wikipedia.org/wiki/User-generated_content>User-Generated Videos</a></a></strong><br><a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/n/navonil-majumder/>Navonil Majumder</a>
|
<a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1081><div class="card-body p-3 small">Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in <a href=https://en.wikipedia.org/wiki/Video>videos</a>. Current research considers utterances as independent entities, i.e., ignores the interdependencies and relations among the utterances of a video. In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process. Our method shows 5-10 % performance improvement over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> and high robustness to <a href=https://en.wikipedia.org/wiki/Generalizability>generalizability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956951 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1083/>Tandem Anchoring : a Multiword Anchor Approach for Interactive Topic Modeling</a></strong><br><a href=/people/j/jeffrey-lund/>Jeffrey Lund</a>
|
<a href=/people/c/connor-cook/>Connor Cook</a>
|
<a href=/people/k/kevin-seppi/>Kevin Seppi</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1083><div class="card-body p-3 small">Interactive topic models are powerful tools for those seeking to understand large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as <a href=https://en.wikipedia.org/wiki/News_anchor>anchors</a>, going beyond existing single word anchor algorithmsan approach we call Tandem Anchors. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and non-interactive approaches. Tandem anchors are faster and more intuitive than existing interactive approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956988 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1084/>Apples to Apples : Learning Semantics of Common Entities Through a Novel Comprehension Task</a></strong><br><a href=/people/o/omid-bakhshandeh/>Omid Bakhshandeh</a>
|
<a href=/people/j/james-allen/>James Allen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1084><div class="card-body p-3 small">Understanding common entities and their attributes is a primary requirement for any <a href=https://en.wikipedia.org/wiki/System>system</a> that comprehends <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. In order to enable learning about common entities, we introduce a novel machine comprehension task, GuessTwo : given a short paragraph comparing different aspects of two real-world semantically-similar entities, a system should guess what those entities are. Accomplishing this task requires <a href=https://en.wikipedia.org/wiki/Deep_learning>deep language understanding</a> which enables <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, connecting each comparison paragraph to different levels of knowledge about world entities and their attributes. So far we have crowdsourced a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of more than 14 K comparison paragraphs comparing entities from a variety of categories such as fruits and animals. We have designed two <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>schemes</a> for evaluation : open-ended, and binary-choice prediction. For benchmarking further progress in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we have collected a set of paragraphs as the test set on which human can accomplish the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 94.2 % on open-ended prediction. We have implemented various <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for tackling the task, ranging from semantic-driven to neural models. The semantic-driven approach outperforms the neural models, however, the results indicate that the task is very challenging across the models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957033 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1085/>Going out on a limb : Joint Extraction of Entity Mentions and Relations without Dependency Trees</a></strong><br><a href=/people/a/arzoo-katiyar/>Arzoo Katiyar</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1085><div class="card-body p-3 small">We present a novel attention-based recurrent neural network for joint extraction of entity mentions and relations. We show that <a href=https://en.wikipedia.org/wiki/Attention>attention</a> along with long short term memory (LSTM) network can extract semantic relations between entity mentions without having access to dependency trees. Experiments on Automatic Content Extraction (ACE) corpora show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms feature-based joint model by Li and Ji (2014). We also compare our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with an end-to-end tree-based LSTM model (SPTree) by Miwa and Bansal (2016) and show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs within 1 % on entity mentions and 2 % on relations. Our fine-grained analysis also shows that our model performs significantly better on Agent-Artifact relations, while SPTree performs better on Physical and Part-Whole relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958455 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1086" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1086/>Naturalizing a <a href=https://en.wikipedia.org/wiki/Programming_language>Programming Language</a> via Interactive Learning</a></strong><br><a href=/people/s/sida-i-wang/>Sida I. Wang</a>
|
<a href=/people/s/samuel-ginn/>Samuel Ginn</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1086><div class="card-body p-3 small">Our goal is to create a convenient <a href=https://en.wikipedia.org/wiki/Natural-language_user_interface>natural language interface</a> for performing well-specified but complex actions such as <a href=https://en.wikipedia.org/wiki/Data_analysis>analyzing data</a>, manipulating text, and <a href=https://en.wikipedia.org/wiki/Database_query>querying databases</a>. However, existing <a href=https://en.wikipedia.org/wiki/Interface_(computing)>natural language interfaces</a> for such <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are quite primitive compared to the power one wields with a <a href=https://en.wikipedia.org/wiki/Programming_language>programming language</a>. To bridge this gap, we start with a core programming language and allow users to naturalize the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the <a href=https://en.wikipedia.org/wiki/Natural_language>naturalized language</a> in 85.9 % of the last 10 K utterances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1087.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958514 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1087/>Semantic Word Clusters Using Signed Spectral Clustering</a></strong><br><a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/j/jean-gallier/>Jean Gallier</a>
|
<a href=/people/d/dean-foster/>Dean Foster</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1087><div class="card-body p-3 small">Vector space representations of words capture many aspects of word similarity, but such methods tend to produce <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a> in which <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> (as well as synonyms) are close to each other. For <a href=https://en.wikipedia.org/wiki/Spectral_clustering>spectral clustering</a> using such word embeddings, words are points in a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> where <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> are linked with positive weights, while <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> are linked with negative weights. We present a new signed spectral normalized graph cut algorithm, signed clustering, that overlays existing thesauri upon distributionally derived vector representations of words, so that antonym relationships between word pairs are represented by negative weights. Our signed clustering algorithm produces clusters of words that simultaneously capture distributional and synonym relations. By using randomized spectral decomposition (Halko et al., 2011) and <a href=https://en.wikipedia.org/wiki/Sparse_matrix>sparse matrices</a>, our method is both fast and scalable. We validate our <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a> using datasets containing human judgments of word pair similarities and show the benefit of using our word clusters for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment prediction</a>.<i>signed clustering</i>, that overlays existing thesauri upon\n distributionally derived vector representations of words, so that antonym\n relationships between word pairs are represented by negative weights. Our\n signed clustering algorithm produces clusters of words that simultaneously\n capture distributional and synonym relations. By using randomized spectral\n decomposition (Halko et al., 2011) and sparse matrices, our method is both\n fast and scalable. We validate our clusters using datasets containing\n human judgments of word pair similarities and show the benefit of using\n our word clusters for sentiment prediction.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958563 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1088/>An Interpretable Knowledge Transfer Model for Knowledge Base Completion</a></strong><br><a href=/people/q/qizhe-xie/>Qizhe Xie</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/z/zihang-dai/>Zihang Dai</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1088><div class="card-body p-3 small">Knowledge bases are important resources for a variety of natural language processing tasks but suffer from <a href=https://en.wikipedia.org/wiki/Completeness_(logic)>incompleteness</a>. We propose a novel <a href=https://en.wikipedia.org/wiki/Embedding>embedding model</a>, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and <a href=https://en.wikipedia.org/wiki/Concept>concepts</a>, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasetsWN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1089.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958614 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1089/>Learning a Neural Semantic Parser from User Feedback</a></strong><br><a href=/people/s/srinivasan-iyer/>Srinivasan Iyer</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/a/alvin-cheung/>Alvin Cheung</a>
|
<a href=/people/j/jayant-krishnamurthy/>Jayant Krishnamurthy</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1089><div class="card-body p-3 small">We present an approach to rapidly and easily build <a href=https://en.wikipedia.org/wiki/Natural-language_user_interface>natural language interfaces</a> to <a href=https://en.wikipedia.org/wiki/Database>databases</a> for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to <a href=https://en.wikipedia.org/wiki/SQL>SQL</a> with its full expressivity, bypassing any intermediate meaning representations. These <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of <a href=https://en.wikipedia.org/wiki/SQL>SQL</a> facilitates gathering annotations for incorrect predictions using the <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd</a>, which is directly used to improve our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. This complete <a href=https://en.wikipedia.org/wiki/Feedback>feedback loop</a>, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> for an online academic database from scratch.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1090 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957724 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1090/>Joint Modeling of Content and Discourse Relations in Dialogues</a></strong><br><a href=/people/k/kechen-qin/>Kechen Qin</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/j/joseph-kim/>Joseph Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1090><div class="card-body p-3 small">We present a joint modeling approach to identify salient discussion points in spoken meetings as well as to label the <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> between speaker turns. A variation of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is also discussed when <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> are treated as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a>. Experimental results on two popular meeting corpora show that our joint model can outperform state-of-the-art approaches for both phrase-based content selection and discourse relation prediction tasks. We also evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on predicting the consistency among team members&#8217; understanding of their group decisions. Classifiers trained with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> constructed from our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieve significant better predictive performance than the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1091 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957758 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1091" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1091/>Argument Mining with Structured SVMs and <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNNs</a><span class=acl-fixed-case>SVM</span>s and <span class=acl-fixed-case>RNN</span>s</a></strong><br><a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/j/joonsuk-park/>Joonsuk Park</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1091><div class="card-body p-3 small">We propose a novel factor graph model for <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>, designed for settings in which the argumentative relations in a document do not necessarily form a <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a>. (This is the case in over 20 % of the web comments dataset we release.) Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> supports <a href=https://en.wikipedia.org/wiki/Symmetric_multiprocessing>SVM and RNN parametrizations</a>, can enforce <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>structure constraints</a> (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured baselines</a> in both web comments and argumentative essay datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1095 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1095.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955407 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1095/>Bayesian Modeling of Lexical Resources for Low-Resource Settings<span class=acl-fixed-case>B</span>ayesian Modeling of Lexical Resources for Low-Resource Settings</a></strong><br><a href=/people/n/nicholas-andrews/>Nicholas Andrews</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1095><div class="card-body p-3 small">Lexical resources such as <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a> and <a href=https://en.wikipedia.org/wiki/Gazetteer>gazetteers</a> are often used as auxiliary data for tasks such as part-of-speech induction and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named-entity recognition</a>. However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> which generalize better. In this paper, we investigate a more robust approach : we stipulate that the <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> is the result of an assumed generative process. Practically, this means that we may treat the lexical resources as observations under the proposed <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a>. The lexical resources provide training data for the <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> without requiring separate data to estimate lexical feature weights. We evaluate the proposed approach in two settings : part-of-speech induction and low-resource named-entity recognition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955469 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1096/>Semi-Supervised QA with Generative Domain-Adaptive Nets<span class=acl-fixed-case>QA</span> with Generative Domain-Adaptive Nets</a></strong><br><a href=/people/z/zhilin-yang/>Zhilin Yang</a>
|
<a href=/people/j/junjie-hu/>Junjie Hu</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/w/william-cohen/>William Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1096><div class="card-body p-3 small">We study the problem of semi-supervised question answeringutilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> obtains substantial improvement from unlabeled text.<i>Generative Domain-Adaptive Nets</i>. In this framework, we train a generative model to generate\n questions based on the unlabeled text, and combine model-generated\n questions with human-generated questions for training question answering\n models. We develop novel domain adaptation algorithms, based on\n reinforcement learning, to alleviate the discrepancy between the\n model-generated data distribution and the human-generated data\n distribution. Experiments show that our proposed framework obtains\n substantial improvement from unlabeled text.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955545 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1097" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1097/>From Language to Programs : Bridging <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> and Maximum Marginal Likelihood</a></strong><br><a href=/people/k/kelvin-guu/>Kelvin Guu</a>
|
<a href=/people/p/panupong-pasupat/>Panupong Pasupat</a>
|
<a href=/people/e/evan-liu/>Evan Liu</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1097><div class="card-body p-3 small">Our goal is to learn a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> that maps natural language utterances into executable programs when only indirect supervision is available : examples are labeled with the correct execution result, but not the program itself. Consequently, we must search the space of programs for those that output the correct result, while not being misled by spurious programs : incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning (RL)</a> and maximum marginal likelihood (MML), and then present a new <a href=https://en.wikipedia.org/wiki/Machine_learning>learning algorithm</a> that combines the strengths of both. The new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized exploration of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent context-dependent semantic parsing task.<i>spurious programs</i>: incorrect programs that coincidentally output the correct\n result. We connect two common learning paradigms, reinforcement learning\n (RL) and maximum marginal likelihood (MML), and then present a new\n learning algorithm that combines the strengths of both. The new algorithm\n guards against spurious programs by combining the systematic search\n traditionally employed in MML with the randomized exploration of RL, and\n by updating parameters such that probability is spread more evenly across\n consistent programs. We apply our learning algorithm to a new neural\n semantic parser and show significant gains over existing state-of-the-art\n results on a recent context-dependent semantic parsing task.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956203 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1098" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1098/>Diversity driven attention model for query-based abstractive summarization</a></strong><br><a href=/people/p/preksha-nema/>Preksha Nema</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a>
|
<a href=/people/a/anirban-laha/>Anirban Laha</a>
|
<a href=/people/b/balaraman-ravindran/>Balaraman Ravindran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1098><div class="card-body p-3 small">Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, extractive summarization, dialog systems, etc. But <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> clearly outperforms vanilla encode-attend-decode models with a gain of 28 % (absolute) in ROUGE-L scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1099.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1099.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956256 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1099" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1099/>Get To The Point : <a href=https://en.wikipedia.org/wiki/Summarization>Summarization</a> with Pointer-Generator Networks</a></strong><br><a href=/people/a/abigail-see/>Abigail See</a>
|
<a href=/people/p/peter-j-liu/>Peter J. Liu</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1099><div class="card-body p-3 small">Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have two shortcomings : they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via <a href=https://en.wikipedia.org/wiki/Pointing_device>pointing</a>, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use <a href=https://en.wikipedia.org/wiki/Coverage_(telecommunication)>coverage</a> to keep track of what has been summarized, which discourages <a href=https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)>repetition</a>. We apply our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1100 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956306 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1100/>Supervised Learning of Automatic Pyramid for Optimization-Based Multi-Document Summarization</a></strong><br><a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/j/judith-eckle-kohler/>Judith Eckle-Kohler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1100><div class="card-body p-3 small">We present a new supervised framework that learns to estimate automatic Pyramid scores and uses them for optimization-based extractive multi-document summarization. For learning automatic Pyramid scores, we developed a method for automatic training data generation which is based on a <a href=https://en.wikipedia.org/wiki/Genetic_algorithm>genetic algorithm</a> using automatic Pyramid as the <a href=https://en.wikipedia.org/wiki/Fitness_function>fitness function</a>. Our experimental evaluation shows that our new framework significantly outperforms strong baselines regarding automatic Pyramid, and that there is much room for improvement in comparison with the upper-bound for automatic Pyramid.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956352 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1101/>Selective Encoding for Abstractive Sentence Summarization</a></strong><br><a href=/people/q/qingyu-zhou/>Qingyu Zhou</a>
|
<a href=/people/n/nan-yang/>Nan Yang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1101><div class="card-body p-3 small">We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. The selective gate network constructs a second level sentence representation by controlling the <a href=https://en.wikipedia.org/wiki/Information_flow>information flow</a> from <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1103.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958888 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1103" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1103/>Towards an Automatic Turing Test : Learning to Evaluate Dialogue Responses<span class=acl-fixed-case>T</span>uring Test: Learning to Evaluate Dialogue Responses</a></strong><br><a href=/people/r/ryan-lowe/>Ryan Lowe</a>
|
<a href=/people/m/michael-noseworthy/>Michael Noseworthy</a>
|
<a href=/people/i/iulian-vlad-serban/>Iulian Vlad Serban</a>
|
<a href=/people/n/nicolas-angelard-gontier/>Nicolas Angelard-Gontier</a>
|
<a href=/people/y/yoshua-bengio/>Yoshua Bengio</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1103><div class="card-body p-3 small">Automatically evaluating the quality of dialogue responses for <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured domains</a> is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM)that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model&#8217;s predictions correlate significantly, and at a level much higher than word-overlap metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue mod-els unseen during training, an important step for automatic dialogue evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234946242 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1106/>Visualizing and Understanding Neural Machine Translation</a></strong><br><a href=/people/y/yanzhuo-ding/>Yanzhuo Ding</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a>
|
<a href=/people/h/huanbo-luan/>Huanbo Luan</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1106><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> has made remarkable progress in recent years, it is hard to interpret its internal workings due to the continuous representations and <a href=https://en.wikipedia.org/wiki/Nonlinear_system>non-linearity</a> of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. In this work, we propose to use layer-wise relevance propagation (LRP) to compute the contribution of each contextual word to arbitrary hidden states in the attention-based encoder-decoder framework. We show that <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualization</a> with LRP helps to interpret the internal workings of NMT and analyze translation errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1107/>Detecting annotation noise in automatically labelled data</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1107><div class="card-body p-3 small">We introduce a method for <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection</a> in automatically annotated text, aimed at supporting the creation of high-quality language resources at affordable cost. Our method combines an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised generative model</a> with <a href=https://en.wikipedia.org/wiki/Supervisor>human supervision</a> from <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a>. We test our approach on in-domain and out-of-domain data in two languages, in AL simulations and in a real world setting. For all settings, the results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is able to detect annotation errors with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and high <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234959124 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1108/>Abstractive Document Summarization with a Graph-Based Attentional Neural Model</a></strong><br><a href=/people/j/jiwei-tan/>Jiwei Tan</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a>
|
<a href=/people/j/jianguo-xiao/>Jianguo Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1108><div class="card-body p-3 small">Abstractive summarization is the ultimate goal of <a href=https://en.wikipedia.org/wiki/Document_summarization>document summarization</a> research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>saliency factor</a> of summarization, which has been overlooked by prior works. Experimental results demonstrate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234959176 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1109/>Probabilistic Typology : Deep Generative Models of Vowel Inventories</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1109><div class="card-body p-3 small">Linguistic typology studies the range of structures present in <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while mostbut not alllanguages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology : What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234944621 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1110/>Adversarial Multi-Criteria Learning for Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/x/xinchi-chen/>Xinchi Chen</a>
|
<a href=/people/z/zhan-shi/>Zhan Shi</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1110><div class="card-body p-3 small">Different <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic perspectives</a> causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1112/>Robust Incremental Neural Semantic Graph Parsing</a></strong><br><a href=/people/j/jan-buys/>Jan Buys</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1112><div class="card-body p-3 small">Parsing sentences to linguistically-expressive semantic representations is a key goal of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. Yet <a href=https://en.wikipedia.org/wiki/Statistical_parsing>statistical parsing</a> has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with <a href=https://en.wikipedia.org/wiki/Lexical_analysis>unlexicalized predicates</a> and their <a href=https://en.wikipedia.org/wiki/Lexical_analysis>token alignments</a>. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69 % Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234945423 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1113" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1113/>Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme</a></strong><br><a href=/people/s/suncong-zheng/>Suncong Zheng</a>
|
<a href=/people/f/feng-wang/>Feng Wang</a>
|
<a href=/people/h/hongyun-bao/>Hongyun Bao</a>
|
<a href=/people/y/yuexing-hao/>Yuexing Hao</a>
|
<a href=/people/p/peng-zhou/>Peng Zhou</a>
|
<a href=/people/b/bo-xu/>Bo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1113><div class="card-body p-3 small">Joint extraction of entities and relations is an important task in <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. To tackle this problem, we firstly propose a novel <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging scheme</a> that can convert the joint extraction task to a <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging problem</a>.. Then, based on our <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging scheme</a>, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What&#8217;s more, the <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end model</a> proposed in this paper, achieves the best results on the <a href=https://en.wikipedia.org/wiki/Data_set>public dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1116.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234945928 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1116/>Unifying Text, <a href=https://en.wikipedia.org/wiki/Metadata>Metadata</a>, and User Network Representations with a <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Network</a> for Geolocation Prediction</a></strong><br><a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1116><div class="card-body p-3 small">We propose a novel geolocation prediction model using a complex neural network. Geolocation prediction in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has attracted many researchers to use information of various types. Our model unifies <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, <a href=https://en.wikipedia.org/wiki/Metadata>metadata</a>, and user network representations with an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to overcome previous ensemble approaches. In an evaluation using two open datasets, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exhibited a maximum 3.8 % increase in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and a maximum of 6.6 % increase in accuracy@161 against previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We further analyzed several intermediate layers of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, which revealed that their states capture some statistical characteristics of the datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1117 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1117.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234946056 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1117/>Multi-Task Video Captioning with Video and Entailment Generation</a></strong><br><a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1117><div class="card-body p-3 small">Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks : a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailing caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1118/>Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts</a></strong><br><a href=/people/l/leandro-santos/>Leandro Santos</a>
|
<a href=/people/e/edilson-anselmo-correa-junior/>Edilson Anselmo Corrêa Júnior</a>
|
<a href=/people/o/osvaldo-novais-oliveira-jr/>Osvaldo Oliveira Jr</a>
|
<a href=/people/d/diego-raphael-amancio/>Diego Amancio</a>
|
<a href=/people/l/leticia-mansur/>Letícia Mansur</a>
|
<a href=/people/s/sandra-aluisio/>Sandra Aluísio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1118><div class="card-body p-3 small">Mild Cognitive Impairment (MCI) is a <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental disorder</a> difficult to diagnose. Linguistic features, mainly from <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>, have been used to detect MCI, but this is not suitable for large-scale assessments. MCI disfluencies produce non-grammatical speech that requires manual or high precision automatic correction of transcripts. In this paper, we modeled transcripts into <a href=https://en.wikipedia.org/wiki/Complex_network>complex networks</a> and enriched them with word embedding (CNE) to better represent short texts produced in <a href=https://en.wikipedia.org/wiki/Neuropsychological_assessment>neuropsychological assessments</a>. The network measurements were applied with well-known <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> to automatically identify MCI in transcripts, in a binary classification task. A comparison was made with the performance of traditional approaches using Bag of Words (BoW) and linguistic features for three datasets : DementiaBank in English, and Cinderella and Arizona-Battery in Portuguese. Overall, CNE provided higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than using only <a href=https://en.wikipedia.org/wiki/Complex_network>complex networks</a>, while <a href=https://en.wikipedia.org/wiki/Support_vector_machine>Support Vector Machine</a> was superior to other <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. CNE provided the highest accuracies for DementiaBank and Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably owing to its short narratives. The approach using <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> yielded higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> if the transcriptions of the Cinderella dataset were manually revised. Taken together, the results indicate that <a href=https://en.wikipedia.org/wiki/Complex_network>complex networks</a> enriched with <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> is promising for detecting MCI in large-scale assessments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1120 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1120/>Chat Detection in an Intelligent Assistant : Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems</a></strong><br><a href=/people/s/satoshi-akasaki/>Satoshi Akasaki</a>
|
<a href=/people/n/nobuhiro-kaji/>Nobuhiro Kaji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1120><div class="card-body p-3 small">Recently emerged intelligent assistants on <a href=https://en.wikipedia.org/wiki/Smartphone>smartphones</a> and <a href=https://en.wikipedia.org/wiki/Home_appliance>home electronics</a> (e.g., <a href=https://en.wikipedia.org/wiki/Siri>Siri</a> and <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a>) can be seen as novel hybrids of domain-specific task-oriented spoken dialogue systems and open-domain non-task-oriented ones. To realize such hybrid dialogue systems, this paper investigates determining whether or not a user is going to have a chat with the <a href=https://en.wikipedia.org/wiki/System>system</a>. To address the lack of benchmark datasets for this task, we construct a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of 15,160 utterances collected from the real log data of a commercial intelligent assistant (and will release the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to facilitate future research activity). In addition, we investigate using tweets and Web search queries for handling open-domain user utterances, which characterize the task of chat detection. Experimental experiments demonstrated that, while simple <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a> are effective, the use of the <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> and search queries further improves the F_1-score from 86.21 to 87.53.<tex-math>_1</tex-math>-score from 86.21 to 87.53.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306164201 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1121" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1121/>A Neural Local Coherence Model</a></strong><br><a href=/people/d/dat-tien-nguyen/>Dat Tien Nguyen</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1121><div class="card-body p-3 small">We propose a local coherence model based on a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>, thanks to the power of <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representation</a>. We present a pairwise ranking method to train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state of the art results outperforming existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> by a good margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1122 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1122.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1122/>Data-Driven Broad-Coverage Grammars for Opinionated Natural Language Generation (ONLG)<span class=acl-fixed-case>ONLG</span>)</a></strong><br><a href=/people/t/tomer-cagan/>Tomer Cagan</a>
|
<a href=/people/s/stefan-l-frank/>Stefan L. Frank</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1122><div class="card-body p-3 small">Opinionated Natural Language Generation (ONLG) is a new, challenging, task that aims to automatically generate human-like, subjective, responses to opinionated articles online. We present a data-driven architecture for ONLG that generates subjective responses triggered by users&#8217; agendas, consisting of topics and sentiments, and based on wide-coverage automatically-acquired generative grammars. We compare three types of grammatical representations that we design for ONLG, which interleave different layers of linguistic information and are induced from a new, enriched dataset we developed. Our evaluation shows that generation with Relational-Realizational (Tsarfaty and Sima&#8217;an, 2008) inspired grammar gets better language model scores than lexicalized grammars &#8216;a la Collins (2003), and that the latter gets better human-evaluation scores. We also show that conditioning the generation on topic models makes generated responses more relevant to the document content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1123 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1123" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1123/>Learning to Ask : Neural Question Generation for Reading Comprehension</a></strong><br><a href=/people/x/xinya-du/>Xinya Du</a>
|
<a href=/people/j/junru-shao/>Junru Shao</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1123><div class="card-body p-3 small">We study automatic question generation for sentences from text passages in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline ; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e.,, <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).<i>i.e.,</i>,\n grammaticality, fluency) and as more difficult to answer (in terms of\n syntactic and lexical divergence from the original text and reasoning\n needed to answer).\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1126/>Learning to Generate Market Comments from Stock Prices</a></strong><br><a href=/people/s/soichiro-murakami/>Soichiro Murakami</a>
|
<a href=/people/a/akihiko-watanabe/>Akihiko Watanabe</a>
|
<a href=/people/a/akira-miyazawa/>Akira Miyazawa</a>
|
<a href=/people/k/keiichi-goshima/>Keiichi Goshima</a>
|
<a href=/people/t/toshihiko-yanase/>Toshihiko Yanase</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1126><div class="card-body p-3 small">This paper presents a novel encoder-decoder model for automatically generating market comments from <a href=https://en.wikipedia.org/wiki/Share_price>stock prices</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> first encodes both short- and long-term series of stock prices so that it can mention short- and long-term changes in stock prices. In the decoding phase, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can also generate a numerical value by selecting an appropriate <a href=https://en.wikipedia.org/wiki/Arithmetic>arithmetic operation</a> such as subtraction or rounding, and applying it to the input stock prices. Empirical experiments show that our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates market comments at the <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> and the informativeness approaching human-generated reference texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1127 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1127/>Can Syntax Help? Improving an LSTM-based Sentence Compression Model for New Domains<span class=acl-fixed-case>LSTM</span>-based Sentence Compression Model for New Domains</a></strong><br><a href=/people/l/liangguo-wang/>Liangguo Wang</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a>
|
<a href=/people/h/hai-leong-chieu/>Hai Leong Chieu</a>
|
<a href=/people/c/chen-hui-ong/>Chen Hui Ong</a>
|
<a href=/people/d/dandan-song/>Dandan Song</a>
|
<a href=/people/l/lejian-liao/>Lejian Liao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1127><div class="card-body p-3 small">In this paper, we study how to improve the domain adaptability of a deletion-based Long Short-Term Memory (LSTM) neural network model for sentence compression. We hypothesize that syntactic information helps in making such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> more robust across domains. We propose two major changes to the model : using explicit syntactic features and introducing syntactic constraints through Integer Linear Programming (ILP). Our evaluation shows that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> works better than the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> as well as a traditional non-neural-network-based model in a cross-domain setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1128 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1128/>Transductive Non-linear Learning for Chinese Hypernym Prediction<span class=acl-fixed-case>C</span>hinese Hypernym Prediction</a></strong><br><a href=/people/c/chengyu-wang/>Chengyu Wang</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a>
|
<a href=/people/a/aoying-zhou/>Aoying Zhou</a>
|
<a href=/people/x/xiaofeng-he/>Xiaofeng He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1128><div class="card-body p-3 small">Finding the correct hypernyms for entities is essential for <a href=https://en.wikipedia.org/wiki/Taxonomy_learning>taxonomy learning</a>, fine-grained entity categorization, <a href=https://en.wikipedia.org/wiki/Query_understanding>query understanding</a>, etc. Due to the flexibility of the <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese language</a>, it is challenging to identify <a href=https://en.wikipedia.org/wiki/Hypernym>hypernyms</a> in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> accurately. Rather than extracting <a href=https://en.wikipedia.org/wiki/Hypernym>hypernyms</a> from texts, in this paper, we present a transductive learning approach to establish <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> from entities to <a href=https://en.wikipedia.org/wiki/Hypernym>hypernyms</a> in the embedding space directly. It combines linear and non-linear embedding projection models, with the capacity of encoding arbitrary language-specific rules. Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1129 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1129/>A Constituent-Centric Neural Architecture for Reading Comprehension</a></strong><br><a href=/people/p/pengtao-xie/>Pengtao Xie</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1129><div class="card-body p-3 small">Reading comprehension (RC), aiming to understand natural texts and answer questions therein, is a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In this paper, we study the RC problem on the Stanford Question Answering Dataset (SQuAD). Observing from the training set that most correct answers are centered around constituents in the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a>, we design a constituent-centric neural architecture where the generation of candidate answers and their representation learning are both based on constituents and guided by the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse tree</a>. Under this architecture, the search space of candidate answers can be greatly reduced without sacrificing the coverage of correct answers and the syntactic, hierarchical and compositional structure among constituents can be well captured, which contributes to better representation learning of the candidate answers. On SQuAD, our method achieves the state of the art performance and the ablation study corroborates the effectiveness of individual modules.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1130 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1130" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1130/>Cross-lingual Distillation for Text Classification</a></strong><br><a href=/people/r/ruochen-xu/>Ruochen Xu</a>
|
<a href=/people/y/yiming-yang/>Yiming Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1130><div class="card-body p-3 small">Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy of categories</a>. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the <a href=https://en.wikipedia.org/wiki/Model_building>model training</a> to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating <a href=https://en.wikipedia.org/wiki/English_language>English</a> as the source language and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Japan>Japan</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-art methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1131 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1131/>Understanding and Predicting Empathic Behavior in Counseling Therapy</a></strong><br><a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/k/kenneth-resnicow/>Kenneth Resnicow</a>
|
<a href=/people/s/satinder-singh/>Satinder Singh</a>
|
<a href=/people/l/lawrence-an/>Lawrence An</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1131><div class="card-body p-3 small">Counselor empathy is associated with better outcomes in <a href=https://en.wikipedia.org/wiki/List_of_counseling_topics>psychology and behavioral counseling</a>. In this paper, we explore several aspects pertaining to counseling interaction dynamics and their relation to <a href=https://en.wikipedia.org/wiki/Empathy>counselor empathy</a> during motivational interviewing encounters. Particularly, we analyze aspects such as participants&#8217; engagement, participants&#8217; verbal and nonverbal accommodation, as well as topics being discussed during the conversation, with the final goal of identifying linguistic and acoustic markers of counselor empathy. We also show how we can use these findings alongside other raw linguistic and acoustic features to build accurate counselor empathy classifiers with accuracies of up to 80 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1132 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1132/>Leveraging Knowledge Bases in <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> for Improving Machine Reading<span class=acl-fixed-case>LSTM</span>s for Improving Machine Reading</a></strong><br><a href=/people/b/bishan-yang/>Bishan Yang</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1132><div class="card-body p-3 small">This paper focuses on how to take advantage of external knowledge bases (KBs) to improve <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> for <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a>. Traditional methods that exploit knowledge from KBs encode knowledge as discrete indicator features. Not only do these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> generalize poorly, but they require task-specific feature engineering to achieve good performance. We propose KBLSTM, a novel neural model that leverages continuous representations of KBs to enhance the learning of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> for <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a>. To effectively integrate background knowledge with information from the currently processed text, our model employs an attention mechanism with a sentinel to adaptively decide whether to attend to background knowledge and which information from KBs is useful. Experimental results show that our model achieves accuracies that surpass the previous state-of-the-art results for both <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity extraction</a> and event extraction on the widely used ACE2005 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1133 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1133/>Prerequisite Relation Learning for Concepts in MOOCs<span class=acl-fixed-case>MOOC</span>s</a></strong><br><a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/c/chengjiang-li/>Chengjiang Li</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/j/jie-tang/>Jie Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1133><div class="card-body p-3 small">What prerequisite knowledge should students achieve a level of mastery before moving forward to learn subsequent coursewares? We study the extent to which the prerequisite relation between knowledge concepts in Massive Open Online Courses (MOOCs) can be inferred automatically. In particular, what kinds of information can be leverage to uncover the potential prerequisite relation between knowledge concepts. We first propose a representation learning-based method for learning latent representations of course concepts, and then investigate how different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> capture the prerequisite relations between concepts. Our experiments on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> form <a href=https://en.wikipedia.org/wiki/Coursera>Coursera</a> show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves significant improvements (+5.9-48.0 % by F1-score) comparing with existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1136 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1136.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1136.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1136/>Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks</a></strong><br><a href=/people/a/abhisek-chakrabarty/>Abhisek Chakrabarty</a>
|
<a href=/people/o/onkar-arun-pandit/>Onkar Arun Pandit</a>
|
<a href=/people/u/utpal-garain/>Utpal Garain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1136><div class="card-body p-3 small">We introduce a composite deep neural network architecture for supervised and language independent context sensitive lemmatization. The proposed method considers the task as to identify the correct edit tree representing the transformation between a word-lemma pair. To find the <a href=https://en.wikipedia.org/wiki/Lemma_(mathematics)>lemma</a> of a surface word, we exploit two successive bidirectional gated recurrent structures-the first one is used to extract the character level dependencies and the next one captures the contextual information of the given word. The key advantages of our model compared to the state-of-the-art lemmatizers such as Lemming and Morfette are-(i) it is independent of human decided features (ii) except the gold lemma, no other expensive morphological attribute is required for joint learning. We evaluate the <a href=https://en.wikipedia.org/wiki/Lemmatizer>lemmatizer</a> on nine languages-Bengali, <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a>, <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Latin>Latin</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. It is found that except <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>, the proposed method outperforms Lemming and Morfette on the other languages. To train the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>, we develop a gold lemma annotated dataset (having 1,702 sentences with a total of 20,257 word tokens), which is an additional contribution of this work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1137.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1137/>Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling</a></strong><br><a href=/people/k/kazuya-kawakami/>Kazuya Kawakami</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1137><div class="card-body p-3 small">Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> : the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the bursty distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus ; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> across this range of languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1138 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1138.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1138/>Bandit Structured Prediction for Neural Sequence-to-Sequence Learning</a></strong><br><a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/a/artem-sokolov/>Artem Sokolov</a>
|
<a href=/people/s/stefan-riezler/>Stefan Riezler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1138><div class="card-body p-3 small">Bandit structured prediction describes a stochastic optimization framework where <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> is performed from partial feedback. This <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate <a href=https://en.wikipedia.org/wiki/Control_variates>control variates</a> into our <a href=https://en.wikipedia.org/wiki/Machine_learning>learning algorithms</a> for <a href=https://en.wikipedia.org/wiki/Variance_reduction>variance reduction</a> and improved <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 <a href=https://en.wikipedia.org/wiki/Bitwise_operation>BLEU points</a> for domain adaptation from simulated bandit feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1139" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1139/>Prior Knowledge Integration for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> using Posterior Regularization</a></strong><br><a href=/people/j/jiacheng-zhang/>Jiacheng Zhang</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a>
|
<a href=/people/h/huanbo-luan/>Huanbo Luan</a>
|
<a href=/people/j/jingfang-xu/>Jingfang Xu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1139><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge. In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. We represent prior knowledge sources as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> in a <a href=https://en.wikipedia.org/wiki/Log-linear_model>log-linear model</a>, which guides the learning processing of the neural translation model. Experiments on Chinese-English dataset show that our approach leads to significant improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1140/>Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation</a></strong><br><a href=/people/j/jinchao-zhang/>Jinchao Zhang</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1140><div class="card-body p-3 small">This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance. Our proposed models enable <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves the state-of-the-art performance on translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1142 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1142/>Combating Human Trafficking with Multimodal Deep Models</a></strong><br><a href=/people/e/edmund-tong/>Edmund Tong</a>
|
<a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/c/cara-jones/>Cara Jones</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1142><div class="card-body p-3 small">Human trafficking is a global epidemic affecting millions of people across the planet. Sex trafficking, the dominant form of <a href=https://en.wikipedia.org/wiki/Human_trafficking>human trafficking</a>, has seen a significant rise mostly due to the abundance of escort websites, where human traffickers can openly advertise among at-will escort advertisements. In this paper, we take a major step in the automatic detection of advertisements suspected to pertain to <a href=https://en.wikipedia.org/wiki/Human_trafficking>human trafficking</a>. We present a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> called Trafficking-10k, with more than 10,000 advertisements annotated for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains two sources of information per advertisement : <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> and <a href=https://en.wikipedia.org/wiki/Image>images</a>. For the accurate detection of trafficking advertisements, we designed and trained a deep multimodal model called the Human Trafficking Deep Network (HTDN).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1143 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1143.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1143.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1143/>MalwareTextDB : A Database for Annotated Malware Articles<span class=acl-fixed-case>M</span>alware<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>DB</span>: A Database for Annotated Malware Articles</a></strong><br><a href=/people/s/swee-kiat-lim/>Swee Kiat Lim</a>
|
<a href=/people/a/aldrian-obaja-muis/>Aldrian Obaja Muis</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/c/chen-hui-ong/>Chen Hui Ong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1143><div class="card-body p-3 small">Cybersecurity risks and malware threats are becoming increasingly dangerous and common. Despite the severity of the problem, there has been few NLP efforts focused on tackling <a href=https://en.wikipedia.org/wiki/Computer_security>cybersecurity</a>. In this paper, we discuss the construction of a new <a href=https://en.wikipedia.org/wiki/Database>database</a> for annotated malware texts. An annotation framework is introduced based on the MAEC vocabulary for defining malware characteristics, along with a database consisting of 39 annotated APT reports with a total of 6,819 sentences. We also use the <a href=https://en.wikipedia.org/wiki/Database>database</a> to construct <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that can potentially help <a href=https://en.wikipedia.org/wiki/Computer_security>cybersecurity researchers</a> in their data collection and analytics efforts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1144 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1144/>A Corpus of Annotated Revisions for Studying Argumentative Writing</a></strong><br><a href=/people/f/fan-zhang/>Fan Zhang</a>
|
<a href=/people/h/homa-b-hashemi/>Homa B. Hashemi</a>
|
<a href=/people/r/rebecca-hwa/>Rebecca Hwa</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1144><div class="card-body p-3 small">This paper presents ArgRewrite, a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of between-draft revisions of argumentative essays. Drafts are manually aligned at the sentence level, and the writer&#8217;s purpose for each revision is annotated with categories analogous to those used in <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> and <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a>. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1145 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1145" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1145/>Watset : Automatic Induction of Synsets from a Graph of Synonyms<span class=acl-fixed-case>W</span>atset: Automatic Induction of Synsets from a Graph of Synonyms</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1145><div class="card-body p-3 small">This paper presents a new graph-based approach that induces synsets using <a href=https://en.wikipedia.org/wiki/Synonym>synonymy dictionaries</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. First, we build a weighted graph of synonyms extracted from commonly available resources, such as <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a>. Second, we apply <a href=https://en.wikipedia.org/wiki/Word-sense_induction>word sense induction</a> to deal with ambiguous words. Finally, we cluster the disambiguated version of the ambiguous input graph into <a href=https://en.wikipedia.org/wiki/Synset>synsets</a>. Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform a fuzzy clustering of the graph. Despite its simplicity, our approach shows excellent results, outperforming five competitive state-of-the-art methods in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> on three gold standard datasets for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> derived from large-scale manually constructed lexical resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1147 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1147" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1147/>TriviaQA : A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension<span class=acl-fixed-case>T</span>rivia<span class=acl-fixed-case>QA</span>: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</a></strong><br><a href=/people/m/mandar-joshi/>Mandar Joshi</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/d/daniel-s-weld/>Daniel Weld</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1147><div class="card-body p-3 small">We present TriviaQA, a challenging reading comprehension dataset containing over 650 K question-answer-evidence triples. TriviaQA includes 95 K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms : a feature-based classifier and a state-of-the-art <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23 % and 40 % vs. 80 %), suggesting that TriviaQA is a challenging testbed that is worth significant future study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1149 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1149/>Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding</a></strong><br><a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/x/xu-chen/>Xu Chen</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1149><div class="card-body p-3 small">Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense. In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1150 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1150/>Interactive Learning of Grounded Verb Semantics towards Human-Robot Communication</a></strong><br><a href=/people/l/lanbo-she/>Lanbo She</a>
|
<a href=/people/j/joyce-chai/>Joyce Chai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1150><div class="card-body p-3 small">To enable human-robot communication and collaboration, previous works represent grounded verb semantics as the potential change of state to the physical world caused by these verbs. Grounded verb semantics are acquired mainly based on the parallel data of the use of a <a href=https://en.wikipedia.org/wiki/Verb_phrase>verb phrase</a> and its corresponding sequences of primitive actions demonstrated by humans. The rich interaction between teachers and students that is considered important in learning new skills has not yet been explored. To address this limitation, this paper presents a new interactive learning approach that allows robots to proactively engage in interaction with human partners by asking good questions to learn <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for grounded verb semantics. The proposed approach uses <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to allow the robot to acquire an optimal policy for its question-asking behaviors by maximizing the <a href=https://en.wikipedia.org/wiki/Reward_system>long-term reward</a>. Our empirical results have shown that the interactive learning approach leads to more reliable models for grounded verb semantics, especially in the noisy environment which is full of uncertainties. Compared to previous work, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> acquired from <a href=https://en.wikipedia.org/wiki/Interactive_learning>interactive learning</a> result in a 48 % to 145 % performance gain when applied in new situations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1153 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1153/>Linguistic analysis of differences in portrayal of movie characters</a></strong><br><a href=/people/a/anil-ramakrishna/>Anil Ramakrishna</a>
|
<a href=/people/v/victor-r-martinez/>Victor R. Martínez</a>
|
<a href=/people/n/nikolaos-malandrakis/>Nikolaos Malandrakis</a>
|
<a href=/people/k/karan-singla/>Karan Singla</a>
|
<a href=/people/s/shrikanth-narayanan/>Shrikanth Narayanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1153><div class="card-body p-3 small">We examine differences in portrayal of characters in <a href=https://en.wikipedia.org/wiki/Film>movies</a> using psycholinguistic and graph theoretic measures computed directly from <a href=https://en.wikipedia.org/wiki/Screenplay>screenplays</a>. Differences are examined with respect to characters&#8217; gender, race, age and other metadata. Psycholinguistic metrics are extrapolated to dialogues in movies using a <a href=https://en.wikipedia.org/wiki/Linear_regression>linear regression model</a> built on a set of manually annotated seed words. Interesting patterns are revealed about relationships between genders of production team and the gender ratio of characters. Several correlations are noted between <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>race</a>, age of characters and the <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1154 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1154.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1154/>Linguistically Regularized LSTM for Sentiment Classification<span class=acl-fixed-case>LSTM</span> for Sentiment Classification</a></strong><br><a href=/people/q/qiao-qian/>Qiao Qian</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a>
|
<a href=/people/j/jinhao-lei/>Jinhao Lei</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1154><div class="card-body p-3 small">This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation ; or do not fully employ linguistic resources (e.g., sentiment lexicons, <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation words</a>, intensity words). In this paper, we propose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation words</a>, and <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>intensity words</a>. Results show that our models are able to capture the linguistic role of sentiment words, <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation words</a>, and <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>intensity words</a> in sentiment expression.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1155 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1155" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1155/>Sarcasm SIGN : Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation<span class=acl-fixed-case>SIGN</span>: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation</a></strong><br><a href=/people/l/lotem-peled/>Lotem Peled</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1155><div class="card-body p-3 small">Sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment. In other words, <a href=https://en.wikipedia.org/wiki/Sarcasm>Sarcasm</a> is the giant chasm between what I say, and the person who does n&#8217;t get it.. In this paper we present the novel task of sarcasm interpretation, defined as the generation of a non-sarcastic utterance conveying the same message as the original sarcastic one. We introduce a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 3000 sarcastic tweets, each interpreted by five human judges. Addressing the task as monolingual machine translation (MT), we experiment with MT algorithms and evaluation measures. We then present SIGN : an MT based sarcasm interpretation algorithm that targets <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment words</a>, a defining element of textual sarcasm. We show that while the scores of n-gram based automatic measures are similar for all interpretation models, SIGN&#8217;s interpretations are scored higher by humans for adequacy and sentiment polarity. We conclude with a discussion on future research directions for our new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1156 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1156/>Active Sentiment Domain Adaptation</a></strong><br><a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/j/jun-yan/>Jun Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1156><div class="card-body p-3 small">Domain adaptation is an important technology to handle domain dependence problem in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis field</a>. Existing methods usually rely on sentiment classifiers trained in source domains. However, their performance may heavily decline if the distributions of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment features</a> in source and target domains have significant difference. In this paper, we propose an active sentiment domain adaptation approach to handle this problem. Instead of the source domain sentiment classifiers, our approach adapts the general-purpose sentiment lexicons to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode, as well as the domain-specific sentiment similarities among words mined from unlabeled samples of target domain. A unified model is proposed to fuse different types of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment information</a> and train sentiment classifier for target domain. Extensive experiments on benchmark datasets show that our approach can train accurate sentiment classifier with less labeled samples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1157 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1157/>Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models<span class=acl-fixed-case>IR</span> Models</a></strong><br><a href=/people/n/navid-rekabsaz/>Navid Rekabsaz</a>
|
<a href=/people/m/mihai-lupu/>Mihai Lupu</a>
|
<a href=/people/a/artem-baklanov/>Artem Baklanov</a>
|
<a href=/people/a/alexander-dur/>Alexander Dür</a>
|
<a href=/people/l/linda-andersson/>Linda Andersson</a>
|
<a href=/people/a/allan-hanbury/>Allan Hanbury</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1157><div class="card-body p-3 small">Volatility predictionan essential concept in financial marketshas recently been addressed using <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis methods</a>. We investigate the sentiment of annual disclosures of companies in <a href=https://en.wikipedia.org/wiki/Stock_market>stock markets</a> to forecast volatility. We specifically explore the use of recent Information Retrieval (IR) term weighting models that are effectively extended by related terms using <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. In parallel to <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual information</a>, <a href=https://en.wikipedia.org/wiki/Market_data>factual market data</a> have been widely used as the mainstream approach to forecast market risk. We therefore study different fusion methods to combine text and market data resources. Our word embedding-based approach significantly outperforms <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art methods</a>. In addition, we investigate the characteristics of the reports of the companies in different financial sectors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1158 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1158.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1158.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1158" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1158/>CANE : Context-Aware Network Embedding for Relation Modeling<span class=acl-fixed-case>CANE</span>: Context-Aware Network Embedding for Relation Modeling</a></strong><br><a href=/people/c/cunchao-tu/>Cunchao Tu</a>
|
<a href=/people/h/han-liu/>Han Liu</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1158><div class="card-body p-3 small">Network embedding (NE) is playing a critical role in <a href=https://en.wikipedia.org/wiki/Network_analysis_(electrical_circuits)>network analysis</a>, due to its ability to represent <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>vertices</a> with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> respectively. Therefore, we present Context-Aware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>vertices</a> with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with existing NE models on three real-world datasets. Experimental results show that CANE achieves significant improvement than state-of-the-art methods on link prediction and comparable performance on vertex classification. The source code and datasets can be obtained from.<url>https://github.com/thunlp/CANE</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1160 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1160.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1160.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1160" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1160/>Generic Axiomatization of Families of Noncrossing Graphs in Dependency Parsing</a></strong><br><a href=/people/a/anssi-yli-jyra/>Anssi Yli-Jyrä</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1160><div class="card-body p-3 small">We present a simple encoding for unlabeled noncrossing graphs and show how its latent counterpart helps us to represent several families of directed and undirected graphs used in syntactic and semantic parsing of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> as context-free languages. The families are separated purely on the basis of forbidden patterns in latent encoding, eliminating the need to differentiate the families of non-crossing graphs in inference algorithms : one <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> works for all when the search space can be controlled in parser input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1161 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1161" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1161/>Semi-supervised sequence tagging with bidirectional language models</a></strong><br><a href=/people/m/matthew-e-peters/>Matthew E. Peters</a>
|
<a href=/people/w/waleed-ammar/>Waleed Ammar</a>
|
<a href=/people/c/chandra-bhagavatula/>Chandra Bhagavatula</a>
|
<a href=/people/r/russell-power/>Russell Power</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1161><div class="card-body p-3 small">Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> that operates on word-level representations to produce context sensitive representations is trained on relatively little <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a>. In this paper, we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a> and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer</a> or joint learning with additional labeled data and task specific gazetteers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1162 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1162.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1162" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1162/>Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings</a></strong><br><a href=/people/h/he-he/>He He</a>
|
<a href=/people/a/anusha-balakrishnan/>Anusha Balakrishnan</a>
|
<a href=/people/m/mihail-eric/>Mihail Eric</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1162><div class="card-body p-3 small">We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this <a href=https://en.wikipedia.org/wiki/Setting_(narrative)>setting</a> poses new challenges for existing <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>. We collected a dataset of 11 K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.<i>symmetric collaborative dialogue</i> setting in which two agents,\n each with private knowledge, must strategically communicate to achieve a\n common goal. The open-ended dialogue state in this setting poses new\n challenges for existing dialogue systems. We collected a dataset of 11K\n human-human dialogues, which exhibits interesting lexical, semantic, and\n strategic elements. To model both structured knowledge and unstructured\n language, we propose a neural model with dynamic knowledge graph\n embeddings that evolve as the dialogue progresses. Automatic and human\n evaluations show that our model is both more effective at achieving the\n goal and more human-like than baseline neural and rule-based models.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1163.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1163 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1163 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1163/>Neural Belief Tracker : Data-Driven Dialogue State Tracking</a></strong><br><a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/d/diarmuid-o-seaghdha/>Diarmuid Ó Séaghdha</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/b/blaise-thomson/>Blaise Thomson</a>
|
<a href=/people/s/steve-young/>Steve Young</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1163><div class="card-body p-3 small">One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user&#8217;s goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either : a) Spoken Language Understanding models that require large amounts of annotated training data ; or b) hand-crafted lexicons for capturing some of the linguistic variation in users&#8217; language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1164 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1164/>Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms</a></strong><br><a href=/people/s/shulin-liu/>Shulin Liu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1164><div class="card-body p-3 small">This paper tackles the task of event detection (ED), which involves identifying and categorizing events. We argue that <a href=https://en.wikipedia.org/wiki/Argument>arguments</a> provide significant clues to this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, but they are either completely ignored or exploited in an indirect manner in existing detection approaches. In this work, we propose to exploit <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>argument information</a> explicitly for <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>ED</a> via supervised attention mechanisms. In specific, we systematically investigate the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> under the supervision of different <a href=https://en.wikipedia.org/wiki/Attentional_control>attention strategies</a>. Experimental results show that our approach advances <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-arts</a> and achieves the best F1 score on ACE 2005 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1165 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1165" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1165/>Topical Coherence in LDA-based Models through Induced Segmentation<span class=acl-fixed-case>LDA</span>-based Models through Induced Segmentation</a></strong><br><a href=/people/h/hesam-amoualian/>Hesam Amoualian</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a>
|
<a href=/people/e/eric-gaussier/>Eric Gaussier</a>
|
<a href=/people/g/georgios-balikas/>Georgios Balikas</a>
|
<a href=/people/m/massih-r-amini/>Massih R. Amini</a>
|
<a href=/people/m/marianne-clausel/>Marianne Clausel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1165><div class="card-body p-3 small">This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a <a href=https://en.wikipedia.org/wiki/Copula_(linguistics)>copula</a>, binding the topics associated to the words of a segment. In addition, this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1167.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1167 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1167 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1167.Presentation.pptx data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1167/>Search-based Neural Structured Learning for Sequential Question Answering</a></strong><br><a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a>
|
<a href=/people/m/ming-wei-chang/>Ming-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1167><div class="card-body p-3 small">Recent work in <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task : answering sequences of simple but inter-related questions. We collect a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 6,066 question sequences that inquire about <a href=https://en.wikipedia.org/wiki/Semi-structured_model>semi-structured tables</a> from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, with 17,553 question-answer pairs in total. To solve this sequential question answering task, we propose a novel dynamic neural semantic parsing framework trained using a weakly supervised reward-guided search. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> effectively leverages the sequential context to outperform state-of-the-art QA systems that are designed to answer highly complex questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1168 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1168" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1168/>Gated-Attention Readers for Text Comprehension</a></strong><br><a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/h/hanxiao-liu/>Hanxiao Liu</a>
|
<a href=/people/z/zhilin-yang/>Zhilin Yang</a>
|
<a href=/people/w/william-cohen/>William Cohen</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1168><div class="card-body p-3 small">In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this taskthe CNN & Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1169 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1169.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1169/>Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering</a></strong><br><a href=/people/j/jianbo-ye/>Jianbo Ye</a>
|
<a href=/people/y/yanran-li/>Yanran Li</a>
|
<a href=/people/z/zhaohui-wu/>Zhaohui Wu</a>
|
<a href=/people/j/james-z-wang/>James Z. Wang</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/j/jia-li/>Jia Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1169><div class="card-body p-3 small">Word embeddings have become widely-used in <a href=https://en.wikipedia.org/wiki/Document_analysis>document analysis</a>. While a large number of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for mapping words to vector spaces have been developed, it remains undetermined how much net gain can be achieved over traditional approaches based on <a href=https://en.wikipedia.org/wiki/Bag-of-words_model>bag-of-words</a>. In this paper, we propose a new document clustering approach by combining any <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> with a state-of-the-art <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for clustering empirical distributions. By using the Wasserstein distance between distributions, the word-to-word semantic relationship is taken into account in a principled way. The new <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering method</a> is easy to use and consistently outperforms other <a href=https://en.wikipedia.org/wiki/Cluster_analysis>methods</a> on a variety of <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a>. More importantly, the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> provides an effective <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for determining when and how much <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> contribute to <a href=https://en.wikipedia.org/wiki/Document_analysis>document analysis</a>. Experimental results with multiple embedding models are reported.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1170 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1170" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1170/>Towards a Seamless Integration of Word Senses into Downstream NLP Applications<span class=acl-fixed-case>NLP</span> Applications</a></strong><br><a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1170><div class="card-body p-3 small">Lexical ambiguity can impede NLP systems from accurate understanding of semantics. Despite its potential benefits, the integration of sense-level information into <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP systems</a> has remained understudied. By incorporating a novel disambiguation algorithm into a state-of-the-art classification model, we create a pipeline to integrate sense-level information into downstream NLP applications. We show that a simple disambiguation of the input text can lead to consistent performance improvement on multiple topic categorization and polarity detection datasets, particularly when the fine granularity of the underlying sense inventory is reduced and the document is sufficiently large. Our results also point to the need for sense representation research to focus more on in vivo evaluations which target the performance in downstream NLP applications rather than artificial benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1171 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1171/>Reading Wikipedia to Answer Open-Domain Questions<span class=acl-fixed-case>W</span>ikipedia to Answer Open-Domain Questions</a></strong><br><a href=/people/d/danqi-chen/>Danqi Chen</a>
|
<a href=/people/a/adam-fisch/>Adam Fisch</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a>
|
<a href=/people/a/antoine-bordes/>Antoine Bordes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1171><div class="card-body p-3 small">This paper proposes to tackle open-domain question answering using <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> as the unique knowledge source : the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a> (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1172 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1172" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1172/>Learning to Skim Text</a></strong><br><a href=/people/a/adams-wei-yu/>Adams Wei Yu</a>
|
<a href=/people/h/hongrae-lee/>Hongrae Lee</a>
|
<a href=/people/q/quoc-le/>Quoc Le</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1172><div class="card-body p-3 small">Recurrent Neural Networks are showing much promise in many sub-areas of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, ranging from <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a> to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> that learns how far to jump after reading a few words of the input text. We employ a standard <a href=https://en.wikipedia.org/wiki/Policy_gradient_method>policy gradient method</a> to train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, news article classification and automatic Q&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1173 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1173/>An Algebra for Feature Extraction</a></strong><br><a href=/people/v/vivek-srikumar/>Vivek Srikumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1173><div class="card-body p-3 small">Though <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> is a necessary first step in statistical NLP, it is often seen as a mere preprocessing step. Yet, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can dominate <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time</a>, both during training, and especially at deployment. In this paper, we formalize <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> from an algebraic perspective. Our formalization allows us to define a message passing algorithm that can restructure <a href=https://en.wikipedia.org/wiki/Software_feature>feature templates</a> to be more computationally efficient. We show via experiments on text chunking and relation extraction that this restructuring does indeed speed up <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> in practice by reducing redundant computation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1174/>Chunk-based Decoder for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/s/shonosuke-ishiwatari/>Shonosuke Ishiwatari</a>
|
<a href=/people/j/jingtao-yao/>Jingtao Yao</a>
|
<a href=/people/s/shujie-liu/>Shujie Liu</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/n/naoki-yoshinaga/>Naoki Yoshinaga</a>
|
<a href=/people/m/masaru-kitsuregawa/>Masaru Kitsuregawa</a>
|
<a href=/people/w/weijia-jia/>Weijia Jia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1174><div class="card-body p-3 small">Chunks (or phrases) once played a pivotal role in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. By using a chunk rather than a word as the basic translation unit, local (intra-chunk) and global (inter-chunk) word orders and dependencies can be easily modeled. The chunk structure, despite its importance, has not been considered in the <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>decoders</a> used for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a>. In this paper, we propose chunk-based decoders for (NMT), each of which consists of a chunk-level decoder and a word-level decoder. The chunk-level decoder models global dependencies while the word-level decoder decides the local word order in a chunk. To output a target sentence, the chunk-level decoder generates a chunk representation containing global information, which the word-level decoder then uses as a basis to predict the words inside the chunk. Experimental results show that our proposed decoders can significantly improve <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance in a WAT &#8216;16 English-to-Japanese translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1175 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1175/>Doubly-Attentive Decoder for Multi-modal Neural Machine Translation</a></strong><br><a href=/people/i/iacer-calixto/>Iacer Calixto</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/n/nick-campbell/>Nick Campbell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1175><div class="card-body p-3 small">We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1176 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1176/>A Teacher-Student Framework for Zero-Resource Neural Machine Translation</a></strong><br><a href=/people/y/yun-chen/>Yun Chen</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a>
|
<a href=/people/y/yong-cheng/>Yong Cheng</a>
|
<a href=/people/v/victor-o-k-li/>Victor O.K. Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1176><div class="card-body p-3 small">While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a <a href=https://en.wikipedia.org/wiki/Third_language>third language</a>. Based on the assumption, our method is able to train a source-to-target NMT model (student) without parallel corpora available guided by an existing pivot-to-target NMT model (teacher) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1177 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1177" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1177/>Improved <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with a Syntax-Aware Encoder and Decoder</a></strong><br><a href=/people/h/huadong-chen/>Huadong Chen</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1177><div class="card-body p-3 small">Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>. In this paper, we improve this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidirectional tree encoder which learns both sequential and tree structured representations ; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential attentional model as well as a stronger baseline with a bottom-up tree encoder and word coverage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1178/>Cross-lingual Name Tagging and Linking for 282 Languages</a></strong><br><a href=/people/x/xiaoman-pan/>Xiaoman Pan</a>
|
<a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/j/joel-nothman/>Joel Nothman</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1178><div class="card-body p-3 small">The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. Given a document in any of these languages, our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods : generating silver-standard annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1179 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1179.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1179.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1179/>Adversarial Training for Unsupervised Bilingual Lexicon Induction</a></strong><br><a href=/people/m/meng-zhang/>Meng Zhang</a>
|
<a href=/people/y/yang-liu-ict/>Yang Liu</a>
|
<a href=/people/h/huanbo-luan/>Huanbo Luan</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1179><div class="card-body p-3 small">Word embeddings are well known to capture <a href=https://en.wikipedia.org/wiki/Regularization_(linguistics)>linguistic regularities</a> of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previous endeavors to connect separate monolingual word embeddings typically require cross-lingual signals as supervision, either in the form of <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> or seed lexicon. In this work, we show that such cross-lingual connection can actually be established without any form of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>. We achieve this end by formulating the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a natural adversarial game, and investigating techniques that are crucial to successful <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1180 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1180.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1180/>Estimating Code-Switching on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> with a Novel Generalized Word-Level Language Detection Technique<span class=acl-fixed-case>T</span>witter with a Novel Generalized Word-Level Language Detection Technique</a></strong><br><a href=/people/s/shruti-rijhwani/>Shruti Rijhwani</a>
|
<a href=/people/r/royal-sequiera/>Royal Sequiera</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a>
|
<a href=/people/c/chandra-shekhar-maddila/>Chandra Shekhar Maddila</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1180><div class="card-body p-3 small">Word-level language detection is necessary for analyzing code-switched text, where multiple languages could be mixed within a sentence. Existing models are restricted to code-switching between two specific languages and fail in real-world scenarios as text input rarely has a priori information on the languages used. We present a novel unsupervised word-level language detection technique for code-switched text for an arbitrarily large number of languages, which does not require any manually annotated training data. Our experiments with <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> in seven languages show a 74 % relative error reduction in word-level labeling with respect to competitive baselines. We then use this system to conduct a large-scale quantitative analysis of code-switching patterns on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, both global as well as region-specific, with 58 M tweets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1182 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1182/>One-Shot Neural Cross-Lingual Transfer for Paradigm Completion</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1182><div class="card-body p-3 small">We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58 % higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of <a href=https://en.wikipedia.org/wiki/Language_family>language relatedness</a> strongly influences the ability to transfer <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1183 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1183.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1183" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1183/>Morphological Inflection Generation with Hard Monotonic Attention</a></strong><br><a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1183><div class="card-body p-3 small">We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft (Bahdanau, 2014) attention models for the task, shedding some light on the features such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> extract.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1184 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1184.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1184/>From Characters to Words to in Between : Do We Capture <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>Morphology</a>?</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1184><div class="card-body p-3 small">Words can be represented by composing the representations of subword units such as word segments, <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters</a>, and/or character n-grams. While such <a href=https://en.wikipedia.org/wiki/Depiction>representations</a> are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological typologies</a>. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the <a href=https://en.wikipedia.org/wiki/Morphological_typology>morphological typology</a> of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement : none of the character-level models match the predictive accuracy of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with access to true morphological analyses, even when learned from an order of magnitude more data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1186 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1186" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1186/>Deep Multitask Learning for Semantic Dependency Parsing</a></strong><br><a href=/people/h/hao-peng/>Hao Peng</a>
|
<a href=/people/s/sam-thomson/>Sam Thomson</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1186><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural architecture</a> that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approachesone that shares parameters across formalisms, and one that uses higher-order structures to predict the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at.<url>https://github.com/Noahs-ARK/NeurboParser</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1187 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1187.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1187.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1187" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1187/>Improved Word Representation Learning with <a href=https://en.wikipedia.org/wiki/Sememe>Sememes</a></a></strong><br><a href=/people/y/yilin-niu/>Yilin Niu</a>
|
<a href=/people/r/ruobing-xie/>Ruobing Xie</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1187><div class="card-body p-3 small">Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes. Since <a href=https://en.wikipedia.org/wiki/Sememe>sememes</a> are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases. In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks. The key idea is to utilize word sememes to capture exact meanings of a word within specific contexts accurately. More specifically, we follow the framework of <a href=https://en.wikipedia.org/wiki/Skip-gram>Skip-gram</a> and present three sememe-encoded models to learn representations of sememes, <a href=https://en.wikipedia.org/wiki/Word_sense>senses</a> and words, where we apply the attention scheme to detect word senses in various contexts. We conduct experiments on two tasks including word similarity and word analogy, and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> significantly outperform baselines. The results indicate that WRL can benefit from <a href=https://en.wikipedia.org/wiki/Sememe>sememes</a> via the attention scheme, and also confirm our models being capable of correctly modeling sememe information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1188 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1188" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1188/>Learning Character-level Compositionality with Visual Features</a></strong><br><a href=/people/f/frederick-liu/>Frederick Liu</a>
|
<a href=/people/h/han-lu/>Han Lu</a>
|
<a href=/people/c/chieh-lo/>Chieh Lo</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1188><div class="card-body p-3 small">Previous work has modeled the compositionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a> has an effect even on the character-level : the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the <a href=https://en.wikipedia.org/wiki/Character_(arts)>character</a> and running it through a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, and <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry topical content which resulting in embeddings that are coherent in visual space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1189 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1189/>A Progressive Learning Approach to Chinese SRL Using <a href=https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity>Heterogeneous Data</a><span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>SRL</span> Using Heterogeneous Data</a></strong><br><a href=/people/q/qiaolin-xia/>Qiaolin Xia</a>
|
<a href=/people/l/lei-sha/>Lei Sha</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1189><div class="card-body p-3 small">Previous studies on Chinese semantic role labeling (SRL) have concentrated on a single semantically annotated corpus. But the training data of single corpus is often limited. Whereas the other existing semantically annotated corpora for Chinese SRL are scattered across different annotation frameworks. But still, Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly <a href=https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity>heterogeneous data</a>. In this paper, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1190 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1190/>Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings</a></strong><br><a href=/people/j/john-wieting/>John Wieting</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1190><div class="card-body p-3 small">We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizing</a> aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the Gated Recurrent Averaging Network, that is inspired by <a href=https://en.wikipedia.org/wiki/Average>averaging</a> and LSTMs while outperforming them both. We analyze our learned <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, finding evidence of preferences for particular parts of speech and dependency relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1191 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1191/>Ontology-Aware Token Embeddings for Prepositional Phrase Attachment</a></strong><br><a href=/people/p/pradeep-dasigi/>Pradeep Dasigi</a>
|
<a href=/people/w/waleed-ammar/>Waleed Ammar</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1191><div class="card-body p-3 small">Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed <a href=https://en.wikipedia.org/wiki/Semantics>semantic concepts</a> (or synsets) as defined in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and <a href=https://en.wikipedia.org/wiki/Conceptual_model>model parameters</a>. We show that using context-sensitive embeddings improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the PP attachment model by 5.4 % absolute points, which amounts to a 34.4 % relative reduction in errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1192.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1192.Datasets.tgz data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1192/>Identifying 1950s American Jazz Musicians : Fine-Grained IsA Extraction via Modifier Composition<span class=acl-fixed-case>A</span>merican Jazz Musicians: Fine-Grained <span class=acl-fixed-case>I</span>s<span class=acl-fixed-case>A</span> Extraction via Modifier Composition</a></strong><br><a href=/people/e/ellie-pavlick/>Ellie Pavlick</a>
|
<a href=/people/m/marius-pasca/>Marius Paşca</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1192><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for populating <a href=https://en.wikipedia.org/wiki/Class_(set_theory)>fine-grained classes</a> (e.g., 1950s American jazz musicians) with <a href=https://en.wikipedia.org/wiki/Class_(set_theory)>instances</a> (e.g., Charles Mingus). While state-of-the-art methods tend to treat class labels as single lexical units, the proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a 10 point increase in <a href=https://en.wikipedia.org/wiki/Analysis_of_covariance>AUC</a>, over a strong baseline relying on widely-used Hearst patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1193.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1193.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-1193/>Parsing to 1-Endpoint-Crossing, Pagenumber-2 Graphs</a></strong><br><a href=/people/j/junjie-cao/>Junjie Cao</a>
|
<a href=/people/s/sheng-huang/>Sheng Huang</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1193><div class="card-body p-3 small">We study the Maximum Subgraph problem in deep dependency parsing. We consider two restrictions to deep dependency graphs : (a) 1-endpoint-crossing and (b) pagenumber-2. Our main contribution is an exact <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that obtains maximum subgraphs satisfying both restrictions simultaneously in time O(n5). Moreover, ignoring one linguistically-rare structure descreases the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> to O(n4). We also extend our quartic-time algorithm into a practical <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> with a discriminative disambiguation model and evaluate its performance on four linguistic data sets used in semantic dependency parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1194.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1194 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1194 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-1194.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-1194" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-1194/>Semi-supervised Multitask Learning for Sequence Labeling</a></strong><br><a href=/people/m/marek-rei/>Marek Rei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1194><div class="card-body p-3 small">We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection</a> in learner texts, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>, without requiring any additional annotated or unannotated data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-1195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-1195 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-1195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-1195/>Semantic Parsing of Pre-university Math Problems</a></strong><br><a href=/people/t/takuya-matsuzaki/>Takuya Matsuzaki</a>
|
<a href=/people/t/takumi-ito/>Takumi Ito</a>
|
<a href=/people/h/hidenao-iwane/>Hidenao Iwane</a>
|
<a href=/people/h/hirokazu-anai/>Hirokazu Anai</a>
|
<a href=/people/n/noriko-h-arai/>Noriko H. Arai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-1195><div class="card-body p-3 small">We have been developing an end-to-end math problem solving system that accepts <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language input</a>. The current paper focuses on how we analyze the problem sentences to produce <a href=https://en.wikipedia.org/wiki/Logical_form>logical forms</a>. We chose a hybrid approach combining a shallow syntactic analyzer and a manually-developed lexicalized grammar. A feature of the <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammar</a> is that it is extensively typed on the basis of a <a href=https://en.wikipedia.org/wiki/Formal_ontology>formal ontology</a> for pre-university math. These types are helpful in <a href=https://en.wikipedia.org/wiki/Semantic_disambiguation>semantic disambiguation</a> inside and across sentences. Experimental results show that the hybrid system produces a well-formed logical form with 88 % precision and 56 % recall.</div></div></div><hr><div id=p17-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P17-2/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2000/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</a></strong><br><a href=/people/r/regina-barzilay/>Regina Barzilay</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953227 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2001/>Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths<span class=acl-fixed-case>LSTM</span> over Dependency Paths</a></strong><br><a href=/people/f/fei-cheng/>Fei Cheng</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2001><div class="card-body p-3 small">Temporal relation classification is becoming an active research field. Lots of methods have been proposed, while most of them focus on extracting <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> from <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>external resources</a>. Less attention has been paid to a significant advance in a closely related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> : <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. In this work, we borrow a state-of-the-art method in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> by adopting bidirectional long short-term memory (Bi-LSTM) along dependency paths (DP). We make a common root assumption to extend DP representations of cross-sentence links. In the final comparison to two state-of-the-art systems on TimeBank-Dense, our model achieves comparable performance, without using external knowledge, as well as manually annotated attributes of entities (class, tense, polarity, etc.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954327 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2002/>AMR-to-text Generation with Synchronous Node Replacement Grammar<span class=acl-fixed-case>AMR</span>-to-text Generation with Synchronous Node Replacement Grammar</a></strong><br><a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/x/xiaochang-peng/>Xiaochang Peng</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/z/zhiguo-wang/>Zhiguo Wang</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2002><div class="card-body p-3 small">This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, graph-to-string rules are learned using a <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic extraction algorithm</a>. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a>, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> gives the state-of-the-art result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234953454 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2003/>Lexical Features in <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a> : To be Used With Caution</a></strong><br><a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2003><div class="card-body p-3 small">Lexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the <a href=https://en.wikipedia.org/wiki/Phenomenon>linguistic phenomena</a> at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical features</a>, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234951559 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2004/>Alternative Objective Functions for Training MT Evaluation Metrics<span class=acl-fixed-case>MT</span> Evaluation Metrics</a></strong><br><a href=/people/m/milos-stanojevic/>Miloš Stanojević</a>
|
<a href=/people/k/khalil-simaan/>Khalil Sima’an</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2004><div class="card-body p-3 small">MT evaluation metrics are tested for correlation with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> either at the sentence- or the corpus-level. Trained <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> ignore <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus-level judgments</a> and are trained for high sentence-level correlation only. We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized. To this end we present a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> trained for <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus-level</a> and show empirical comparison against a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> trained for <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence-level</a> exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> trained to optimize both objectives simultaneously and show that it is far more stable thanand on average outperformsboth <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on both objectives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2006/>Vector space models for evaluating semantic fluency in autism</a></strong><br><a href=/people/e/emily-prudhommeaux/>Emily Prud’hommeaux</a>
|
<a href=/people/j/jan-van-santen/>Jan van Santen</a>
|
<a href=/people/d/douglas-gliner/>Douglas Gliner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2006><div class="card-body p-3 small">A common <a href=https://en.wikipedia.org/wiki/Test_(assessment)>test</a> administered during <a href=https://en.wikipedia.org/wiki/Neurological_examination>neurological examination</a> is the semantic fluency test, in which the patient must list as many examples of a given semantic category as possible under timed conditions. Poor performance is associated with neurological conditions characterized by impairments in <a href=https://en.wikipedia.org/wiki/Executive_functions>executive function</a>, such as <a href=https://en.wikipedia.org/wiki/Dementia>dementia</a>, <a href=https://en.wikipedia.org/wiki/Schizophrenia>schizophrenia</a>, and <a href=https://en.wikipedia.org/wiki/Autism_spectrum>autism spectrum disorder (ASD)</a>. Methods for analyzing semantic fluency responses at the level of detail necessary to uncover these differences have typically relied on subjective manual annotation. In this paper, we explore automated approaches for scoring semantic fluency responses that leverage ontological resources and distributional semantic models to characterize the semantic fluency responses produced by young children with and without ASD. Using these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>, we find significant differences in the semantic fluency responses of children with ASD, demonstrating the utility of using objective methods for clinical language analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234954045 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2008/>Incorporating <a href=https://en.wikipedia.org/wiki/Uncertainty>Uncertainty</a> into <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> for Spoken Language Assessment</a></strong><br><a href=/people/a/andrey-malinin/>Andrey Malinin</a>
|
<a href=/people/a/anton-ragni/>Anton Ragni</a>
|
<a href=/people/k/kate-knill/>Kate Knill</a>
|
<a href=/people/m/mark-gales/>Mark Gales</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2008><div class="card-body p-3 small">There is a growing demand for automatic assessment of spoken English proficiency. These systems need to handle large variations in input data owing to the wide range of candidate skill levels and L1s, and errors from ASR. Some candidates will be a poor match to the training data set, undermining the validity of the predicted grade. For high stakes tests it is essential for such systems not only to grade well, but also to provide a measure of their uncertainty in their predictions, enabling rejection to human graders. Previous work examined Gaussian Process (GP) graders which, though successful, do not scale well with large data sets. Deep Neural Network (DNN) may also be used to provide <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a> using Monte-Carlo Dropout (MCD). This paper proposes a novel method to yield uncertainty and compares it to GPs and DNNs with MCD. The proposed approach explicitly teaches a DNN to have low uncertainty on training data and high uncertainty on generated artificial data. On experiments conducted on data from the Business Language Testing Service (BULATS), the proposed approach is found to outperform GPs and DNNs with MCD in uncertainty-based rejection whilst achieving comparable grading performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2009.Notes.zip data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952193 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2009/>Incorporating Dialectal Variability for Socially Equitable Language Identification</a></strong><br><a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2009><div class="card-body p-3 small">Language identification (LID) is a critical first step for processing <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual text</a>. Yet most LID systems are not designed to handle the linguistic diversity of global platforms like <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, where local dialects and rampant code-switching lead language classifiers to systematically miss minority dialect speakers and multilingual speakers. We propose a new dataset and a character-based sequence-to-sequence model for LID designed to support dialectal and multilingual language varieties. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on multiple LID benchmarks. Furthermore, in a case study using <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> for health tracking, our method substantially increases the availability of texts written by underrepresented populations, enabling the development of socially inclusive NLP tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234952906 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2010/>Evaluating Compound Splitters Extrinsically with Textual Entailment</a></strong><br><a href=/people/g/glorianna-jagfeld/>Glorianna Jagfeld</a>
|
<a href=/people/p/patrick-ziering/>Patrick Ziering</a>
|
<a href=/people/l/lonneke-van-der-plas/>Lonneke van der Plas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2010><div class="card-body p-3 small">Traditionally, compound splitters are evaluated intrinsically on <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold-standard data</a> or extrinsically on the task of <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>. We explore a novel way for the extrinsic evaluation of compound splitters, namely recognizing textual entailment. Compound splitting has great potential for this novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that is both transparent and well-defined. Moreover, we show that it addresses certain aspects that are either ignored in intrinsic evaluations or compensated for by taskinternal mechanisms in <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>. We show significant improvements using different compound splitting methods on a German textual entailment dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955092 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2012" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2012/>Learning to Parse and Translate Improves <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/a/akiko-eriguchi/>Akiko Eriguchi</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2012><div class="card-body p-3 small">There has been relatively little attention to incorporating linguistic prior to <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2013 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955925 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2013/>On the Distribution of Lexical Features at Multiple Levels of Analysis</a></strong><br><a href=/people/f/fatemeh-almodaresi/>Fatemeh Almodaresi</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a>
|
<a href=/people/v/vivek-kulkarni/>Vivek Kulkarni</a>
|
<a href=/people/m/mohsen-zakeri/>Mohsen Zakeri</a>
|
<a href=/people/s/salvatore-giorgi/>Salvatore Giorgi</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2013><div class="card-body p-3 small">Natural language processing has increasingly moved from modeling documents and words toward studying the people behind the language. This move to working with data at the user or community level has presented the field with different characteristics of linguistic data. In this paper, we empirically characterize various lexical distributions at different levels of analysis, showing that, while most features are decidedly sparse and non-normal at the message-level (as with traditional NLP), they follow the <a href=https://en.wikipedia.org/wiki/Central_limit_theorem>central limit theorem</a> to become much more <a href=https://en.wikipedia.org/wiki/Log-normal_distribution>Log-normal</a> or even Normal at the user- and county-levels. Finally, we demonstrate that modeling lexical features for the correct level of analysis leads to marked improvements in common social scientific prediction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956698 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2014/>Exploring Neural Text Simplification Models</a></strong><br><a href=/people/s/sergiu-nisioi/>Sergiu Nisioi</a>
|
<a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2014><div class="card-body p-3 small">We present the first attempt at using sequence to sequence neural networks to model text simplification (TS). Unlike the previously proposed automated TS systems, our neural text simplification (NTS) systems are able to simultaneously perform lexical simplification and content reduction. An extensive human evaluation of the output has shown that NTS systems achieve almost perfect <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a> and meaning preservation of output sentences and higher level of simplification than the state-of-the-art automated TS systems</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2017.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2017.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234958413 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2017/>Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection</a></strong><br><a href=/people/y/youxuan-jiang/>Youxuan Jiang</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/w/walter-lasecki/>Walter S. Lasecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2017><div class="card-body p-3 small">Linguistically diverse datasets are critical for training and evaluating robust <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning systems</a>, but <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> is a costly process that often requires experts. Crowdsourcing the process of <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> is an effective means of expanding <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language datasets</a>, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>instructions</a>, <a href=https://en.wikipedia.org/wiki/Incentive>incentives</a>, <a href=https://en.wikipedia.org/wiki/Data_domain>data domains</a>, and <a href=https://en.wikipedia.org/wiki/Workflow>workflows</a>. We manually analyzed paraphrases for correctness, <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, and <a href=https://en.wikipedia.org/wiki/Linguistic_diversity>linguistic diversity</a>. Our observations provide new insight into the trade-offs between <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and diversity in crowd responses that arise as a result of <a href=https://en.wikipedia.org/wiki/Design_of_experiments>task design</a>, providing guidance for future paraphrase generation procedures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957642 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2018/>Arc-swift : A Novel Transition System for Dependency Parsing</a></strong><br><a href=/people/p/peng-qi/>Peng Qi</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2018><div class="card-body p-3 small">Transition-based dependency parsers often need sequences of local shift and reduce operations to produce certain attachments. Correct individual decisions hence require global information about the sentence context and mistakes cause <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. This paper proposes a novel <a href=https://en.wikipedia.org/wiki/Transition_(computer_science)>transition system</a>, arc-swift, that enables direct attachments between tokens farther apart with a single <a href=https://en.wikipedia.org/wiki/Transition_(computer_science)>transition</a>. This allows the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to leverage <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical information</a> more directly in transition decisions. Hence, arc-swift can achieve significantly better performance with a very small <a href=https://en.wikipedia.org/wiki/Beam_diameter>beam size</a>. Our <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> reduce error by 3.77.6 % relative to those using existing transition systems on the Penn Treebank dependency parsing task and English Universal Dependencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957682 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2019/>A Generative Parser with a Discriminative Recognition Algorithm</a></strong><br><a href=/people/j/jianpeng-cheng/>Jianpeng Cheng</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2019><div class="card-body p-3 small">Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and are often outperformed by discriminative models. We propose a framework for parsing and language modeling which marries a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on <a href=https://en.wikipedia.org/wiki/Expectation_maximization>expectation maximization</a> and <a href=https://en.wikipedia.org/wiki/Variational_inference>variational inference</a>, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treen-bank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art single-model language modeling score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2021.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955359 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2021/>Towards String-To-Tree Neural Machine Translation</a></strong><br><a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2021><div class="card-body p-3 small">We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. An experiment on the WMT16 German-English news translation task resulted in an improved BLEU score when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during <a href=https://en.wikipedia.org/wiki/Translation>translation</a> in comparison to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. A small-scale human evaluation also showed an advantage to the syntax-aware system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2022 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956090 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2022/>Learning Lexico-Functional Patterns for First-Person Affect</a></strong><br><a href=/people/l/lena-reed/>Lena Reed</a>
|
<a href=/people/j/jiaqi-wu/>Jiaqi Wu</a>
|
<a href=/people/s/shereen-oraby/>Shereen Oraby</a>
|
<a href=/people/p/pranav-anand/>Pranav Anand</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2022><div class="card-body p-3 small">Informal first-person narratives are a unique resource for computational models of everyday events and people&#8217;s affective reactions to them. People blogging about their day tend not to explicitly say I am happy. Instead they describe situations from which other humans can readily infer their <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective reactions</a>. However current sentiment dictionaries are missing much of the information needed to make similar inferences. We build on recent work that models affect in terms of lexical predicate functions and affect on the predicate&#8217;s arguments. We present a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> to learn proxies for these <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>functions</a> from <a href=https://en.wikipedia.org/wiki/First-person_narrative>first-person narratives</a>. We construct a novel fine-grained test set, and show that the patterns we learn improve our ability to predict first-person affective reactions to everyday events, from a Stanford sentiment baseline of.67F to.75F.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956125 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2023/>Lifelong Learning CRF for Supervised Aspect Extraction<span class=acl-fixed-case>CRF</span> for Supervised Aspect Extraction</a></strong><br><a href=/people/l/lei-shu/>Lei Shu</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2023><div class="card-body p-3 small">This paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can still improve its extraction with experiences in its applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234956168 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2024/>Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization</a></strong><br><a href=/people/y/ye-zhang/>Ye Zhang</a>
|
<a href=/people/m/matthew-lease/>Matthew Lease</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2024><div class="card-body p-3 small">A fundamental advantage of neural models for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is their ability to learn <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> or <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>domain specific ontologies</a> such as the <a href=https://en.wikipedia.org/wiki/Unified_Medical_Language_System>Unified Medical Language System (UMLS)</a>. We propose a general, novel method for exploiting such <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resources</a> via weight sharing. Prior work on weight sharing in <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> has considered it largely as a means of <a href=https://en.wikipedia.org/wiki/Data_compression>model compression</a>. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234957130 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2025/>Improving Neural Parsing by Disentangling Model Combination and Reranking Effects</a></strong><br><a href=/people/d/daniel-fried/>Daniel Fried</a>
|
<a href=/people/m/mitchell-stern/>Mitchell Stern</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2025><div class="card-body p-3 small">Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for direct search in these <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a>. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234955606 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2026/>Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function</a></strong><br><a href=/people/o/oren-melamud/>Oren Melamud</a>
|
<a href=/people/j/jacob-goldberger/>Jacob Goldberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2026><div class="card-body p-3 small">In this paper we define a measure of dependency between two <a href=https://en.wikipedia.org/wiki/Random_variable>random variables</a>, based on the Jensen-Shannon (JS) divergence between their <a href=https://en.wikipedia.org/wiki/Joint_probability_distribution>joint distribution</a> and the product of their <a href=https://en.wikipedia.org/wiki/Marginal_distribution>marginal distributions</a>. Then, we show that word2vec&#8217;s skip-gram with negative sampling embedding algorithm finds the optimal low-dimensional approximation of this JS dependency measure between the words and their contexts. The gap between the optimal score and the low-dimensional approximation is demonstrated on a standard <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234959028 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2028/>The Role of <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>Prosody</a> and <a href=https://en.wikipedia.org/wiki/Speech_register>Speech Register</a> in <a href=https://en.wikipedia.org/wiki/Word_segmentation>Word Segmentation</a> : A Computational Modelling Perspective</a></strong><br><a href=/people/b/bogdan-ludusan/>Bogdan Ludusan</a>
|
<a href=/people/r/reiko-mazuka/>Reiko Mazuka</a>
|
<a href=/people/m/mathieu-bernard/>Mathieu Bernard</a>
|
<a href=/people/a/alejandrina-cristia/>Alejandrina Cristia</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2028><div class="card-body p-3 small">This study explores the role of <a href=https://en.wikipedia.org/wiki/Speech_register>speech register</a> and <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosody</a> for the task of <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>. Since these two factors are thought to play an important role in early language acquisition, we aim to quantify their contribution for this task. We study a Japanese corpus containing both infant- and adult-directed speech and we apply four different word segmentation models, with and without knowledge of <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosodic boundaries</a>. The results showed that the difference between registers is smaller than previously reported and that prosodic boundary information helps more adult- than infant-directed speech.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234959057 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2029/>A Two-Stage Parsing Method for Text-Level Discourse Analysis</a></strong><br><a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2029><div class="card-body p-3 small">Previous work introduced transition-based algorithms to form a unified architecture of parsing rhetorical structures (including span, nuclearity and relation), but did not achieve satisfactory performance. In this paper, we propose that transition-based model is more appropriate for parsing the naked discourse tree (i.e., identifying span and nuclearity) due to data sparsity. At the same time, we argue that relation labeling can benefit from naked tree structure and should be treated elaborately with consideration of three kinds of <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> including within-sentence, across-sentence and across-paragraph relations. Thus, we design a pipelined two-stage parsing method for generating an RST tree from text. Experimental results show that our method achieves state-of-the-art performance, especially on span and nuclearity identification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2030 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234959088 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2030/>Error-repair Dependency Parsing for Ungrammatical Texts</a></strong><br><a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2030><div class="card-body p-3 small">We propose a new dependency parsing scheme which jointly parses a sentence and repairs grammatical errors by extending the non-directional transition-based formalism of Goldberg and Elhadad (2010) with three additional actions : SUBSTITUTE, DELETE, INSERT. Because these actions may cause an infinite loop in derivation, we also introduce simple constraints that ensure the <a href=https://en.wikipedia.org/wiki/Parsing>parser termination</a>. We evaluate our model with respect to dependency accuracy and grammaticality improvements for ungrammatical sentences, demonstrating the robustness and applicability of our scheme.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2031 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2031.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234946385 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2031/>Attention Strategies for Multi-Source Sequence-to-Sequence Learning</a></strong><br><a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/j/jindrich-helcl/>Jindřich Helcl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2031><div class="card-body p-3 small">Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities. We propose two novel approaches to combine the outputs of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> over each source sequence, flat and hierarchical. We compare the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> with existing techniques and present results of systematic evaluation of those <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on the WMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> achieve competitive results on both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234946757 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2033/>A Neural Model for User Geolocation and Lexical Dialectology</a></strong><br><a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2033><div class="card-body p-3 small">We propose a simple yet effective text-based user geolocation model based on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> with one hidden layer, which achieves state of the art performance over three Twitter benchmark geolocation datasets, in addition to producing word and phrase embeddings in the hidden layer that we show to be useful for detecting dialectal terms. As part of our analysis of dialectal terms, we release DAREDS, a dataset for evaluating dialect term detection methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2035/>Neural Architecture for Temporal Relation Extraction : A Bi-LSTM Approach for Detecting Narrative Containers<span class=acl-fixed-case>B</span>i-<span class=acl-fixed-case>LSTM</span> Approach for Detecting Narrative Containers</a></strong><br><a href=/people/j/julien-tourille/>Julien Tourille</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2035><div class="card-body p-3 small">We present a neural architecture for containment relation identification between medical events and/or temporal expressions. We experiment on a corpus of de-identified clinical notes in English from the Mayo Clinic, namely the THYME corpus. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> of 0.613 and outperforms the best result reported on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to date.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2036 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2036/>How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models</a></strong><br><a href=/people/z/zhiliang-tian/>Zhiliang Tian</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a>
|
<a href=/people/l/lili-mou/>Lili Mou</a>
|
<a href=/people/y/yiping-song/>Yiping Song</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2036><div class="card-body p-3 small">Generative conversational systems are attracting increasing attention in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. Recently, researchers have noticed the importance of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> in dialog processing, and built various <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>. However, there is no systematic comparison to analyze how to use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> effectively. In this paper, we conduct an empirical study to compare various <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and investigate the effect of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> in <a href=https://en.wikipedia.org/wiki/Dialogue>dialog systems</a>. We also propose a variant that explicitly weights context vectors by context-query relevance, outperforming the other baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2037 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2037/>Cross-lingual and cross-domain discourse segmentation of entire documents</a></strong><br><a href=/people/c/chloe-braud/>Chloé Braud</a>
|
<a href=/people/o/ophelie-lacroix/>Ophélie Lacroix</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2037><div class="card-body p-3 small">Discourse segmentation is a crucial step in building end-to-end discourse parsers. However, discourse segmenters only exist for a few languages and domains. Typically they only detect intra-sentential segment boundaries, assuming gold standard sentence and token segmentation, and relying on high-quality syntactic parses and rich heuristics that are not generally available across languages and domains. In this paper, we propose statistical discourse segmenters for five languages and three domains that do not rely on gold pre-annotations. We also consider the problem of learning discourse segmenters when no labeled data is available for a language. Our <a href=https://en.wikipedia.org/wiki/Supervised_learning>fully supervised system</a> obtains 89.5 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> for <a href=https://en.wikipedia.org/wiki/News_agency>English newswire</a>, with slight drops in performance on other domains, and we report supervised and unsupervised (cross-lingual) results for five languages in total.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2039 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2039/>Argumentation Quality Assessment : Theory vs. Practice</a></strong><br><a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/n/nona-naderi/>Nona Naderi</a>
|
<a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2039><div class="card-body p-3 small">Argumentation quality is viewed differently in <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation theory</a> and in practical assessment approaches. This paper studies to what extent the views match empirically. We find that most observations on quality phrased spontaneously are in fact adequately represented by <a href=https://en.wikipedia.org/wiki/Theory>theory</a>. Even more, relative comparisons of arguments in practice correlate with absolute quality ratings based on theory. Our results clarify how the two views can learn from each other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2040 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2040/>A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations<span class=acl-fixed-case>C</span>hinese Implicit Discourse Relations</a></strong><br><a href=/people/s/samuel-ronnqvist/>Samuel Rönnqvist</a>
|
<a href=/people/n/niko-schenk/>Niko Schenk</a>
|
<a href=/people/c/christian-chiarcos/>Christian Chiarcos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2040><div class="card-body p-3 small">We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to selectively focus on the relevant parts of an input sequence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2042 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2042/>Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings</a></strong><br><a href=/people/c/changxing-wu/>Changxing Wu</a>
|
<a href=/people/x/xiaodong-shi/>Xiaodong Shi</a>
|
<a href=/people/y/yidong-chen/>Yidong Chen</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a>
|
<a href=/people/b/boli-wang/>Boli Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2042><div class="card-body p-3 small">We introduce a simple and effective method to learn discourse-specific word embeddings (DSWE) for implicit discourse relation recognition. Specifically, DSWE is learned by performing connective classification on massive explicit discourse data, and capable of capturing discourse relationships between words. On the PDTB data set, using DSWE as <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> achieves significant improvements over <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2043/>Oracle Summaries of Compressive Summarization</a></strong><br><a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nishino/>Masaaki Nishino</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2043><div class="card-body p-3 small">This paper derives an Integer Linear Programming (ILP) formulation to obtain an oracle summary of the compressive summarization paradigm in terms of ROUGE. The oracle summary is essential to reveal the upper bound performance of the <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a>. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2044 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2044/>Japanese Sentence Compression with a Large Training Dataset<span class=acl-fixed-case>J</span>apanese Sentence Compression with a Large Training Dataset</a></strong><br><a href=/people/s/shun-hasegawa/>Shun Hasegawa</a>
|
<a href=/people/y/yuta-kikuchi/>Yuta Kikuchi</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2044><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/English_language>English</a>, high-quality sentence compression models by deleting words have been trained on automatically created large training datasets. We work on Japanese sentence compression by a similar approach. To create a large Japanese training dataset, a method of creating English training dataset is modified based on the characteristics of the <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese language</a>. The created <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is used to train Japanese sentence compression models based on the <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2046 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2046/>English Event Detection With Translated Language Features<span class=acl-fixed-case>E</span>nglish Event Detection With Translated Language Features</a></strong><br><a href=/people/s/sam-wei/>Sam Wei</a>
|
<a href=/people/i/igor-korostil/>Igor Korostil</a>
|
<a href=/people/j/joel-nothman/>Joel Nothman</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2046><div class="card-body p-3 small">We propose novel radical features from <a href=https://en.wikipedia.org/wiki/Automatic_translation>automatic translation</a> for <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a>. Event detection is a complex language processing task for which it is expensive to collect training data, making <a href=https://en.wikipedia.org/wiki/Generalization>generalisation</a> challenging. We derive meaningful subword features from <a href=https://en.wikipedia.org/wiki/Automatic_translation>automatic translations</a> into target language. Results suggest this <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is particularly useful when using languages with writing systems that facilitate easy decomposition into subword features, e.g., logograms and <a href=https://en.wikipedia.org/wiki/Cangjie>Cangjie</a>. The best result combines logogram features from <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> with syllable features from <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>, providing an additional 3.0 points <a href=https://en.wikipedia.org/wiki/F-score>f-score</a> when added to state-of-the-art generalisation features on the TAC KBP 2015 Event Nugget task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2047 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2047/>EviNets : <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> for Combining Evidence Signals for Factoid Question Answering<span class=acl-fixed-case>E</span>vi<span class=acl-fixed-case>N</span>ets: Neural Networks for Combining Evidence Signals for Factoid Question Answering</a></strong><br><a href=/people/d/denis-savenkov/>Denis Savenkov</a>
|
<a href=/people/e/eugene-agichtein/>Eugene Agichtein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2047><div class="card-body p-3 small">A critical task for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> is the final answer selection stage, which has to combine multiple signals available about each answer candidate. This paper proposes EviNets : a novel <a href=https://en.wikipedia.org/wiki/Neural_network>neural network architecture</a> for factoid question answering. EviNets scores candidate answer entities by combining the available supporting evidence, e.g., <a href=https://en.wikipedia.org/wiki/Knowledge_base>structured knowledge bases</a> and <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text documents</a>. EviNets represents each piece of evidence with a dense embeddings vector, scores their relevance to the question, and aggregates the support for each candidate to predict their final scores. Each of the <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> is generic and allows plugging in a variety of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for semantic similarity scoring and <a href=https://en.wikipedia.org/wiki/Information_aggregation>information aggregation</a>. We demonstrate the effectiveness of EviNets in experiments on the existing TREC QA and WikiMovies benchmarks, and on the new Yahoo ! Answers dataset introduced in this paper. EviNets can be extended to other information types and could facilitate future work on combining evidence signals for joint reasoning in question answering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2048 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2048/>Pocket Knowledge Base Population</a></strong><br><a href=/people/t/travis-wolfe/>Travis Wolfe</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2048><div class="card-body p-3 small">Existing Knowledge Base Population methods extract relations from a closed relational schema with limited coverage leading to sparse KBs. We propose Pocket Knowledge Base Population (PKBP), the task of dynamically constructing a KB of entities related to a query and finding the best characterization of relationships between entities. We describe novel Open Information Extraction methods which leverage the PKB to find informative trigger words. We evaluate using existing KBP shared-task data as well anew annotations collected for this work. Our methods produce high quality KB from just text with many more entities and relationships than existing KBP systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2049 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2049.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2049/>Answering Complex Questions Using Open Information Extraction</a></strong><br><a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2049><div class="card-body p-3 small">While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>, but to date such <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> has only been used to answer simple questions with retrieval-based methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for <a href=https://en.wikipedia.org/wiki/Open_IE>Open IE</a>, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2050 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2050/>Bootstrapping for Numerical Open IE<span class=acl-fixed-case>IE</span></a></strong><br><a href=/people/s/swarnadeep-saha/>Swarnadeep Saha</a>
|
<a href=/people/h/harinder-pal/>Harinder Pal</a>
|
<a href=/people/m/mausam/>Mausam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2050><div class="card-body p-3 small">We design and release BONIE, the first open numerical relation extractor, for extracting Open IE tuples where one of the arguments is a number or a quantity-unit phrase. BONIE uses <a href=https://en.wikipedia.org/wiki/Bootstrapping_(compilers)>bootstrapping</a> to learn the specific dependency patterns that express numerical relations in a sentence. BONIE&#8217;s novelty lies in task-specific customizations, such as inferring implicit relations, which are clear due to context such as units (for e.g., &#8216;square kilometers&#8217; suggests area, even if the word &#8216;area&#8217; is missing in the sentence). BONIE obtains 1.5x <a href=https://en.wikipedia.org/wiki/Yield_(engineering)>yield</a> and 15 point precision gain on numerical facts over a state-of-the-art Open IE system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2051 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2051/>Feature-Rich Networks for Knowledge Base Completion</a></strong><br><a href=/people/a/alexandros-komninos/>Alexandros Komninos</a>
|
<a href=/people/s/suresh-manandhar/>Suresh Manandhar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2051><div class="card-body p-3 small">We propose jointly modelling <a href=https://en.wikipedia.org/wiki/Knowledge_base>Knowledge Bases</a> and aligned text with Feature-Rich Networks. Our models perform Knowledge Base Completion by learning to represent and compose diverse feature types from partially aligned and noisy resources. We perform experiments on <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> utilizing additional entity type information and syntactic textual relations. Our evaluation suggests that the proposed models can better incorporate side information than previously proposed combinations of bilinear models with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>, showing large improvements when scoring the plausibility of unobserved facts with associated textual mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2052/>Fine-Grained Entity Typing with High-Multiplicity Assignments</a></strong><br><a href=/people/m/maxim-rabinovich/>Maxim Rabinovich</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2052><div class="card-body p-3 small">As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2053 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2053/>Group Sparse CNNs for Question Classification with Answer Sets<span class=acl-fixed-case>CNN</span>s for Question Classification with Answer Sets</a></strong><br><a href=/people/m/mingbo-ma/>Mingbo Ma</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a>
|
<a href=/people/b/bowen-zhou/>Bowen Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2053><div class="card-body p-3 small">Question classification is an important task with wide applications. However, traditional techniques treat questions as general sentences, ignoring the corresponding answer data. In order to consider answer information into question modeling, we first introduce novel group sparse autoencoders which refine question representation by utilizing group information in the answer set. We then propose novel group sparse CNNs which naturally learn question representation with respect to their answers by implanting group sparse autoencoders into traditional CNNs. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperform strong <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a> on four datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2055.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2055.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2055/>Cardinal Virtues : Extracting Relation Cardinalities from Text</a></strong><br><a href=/people/p/paramita-mirza/>Paramita Mirza</a>
|
<a href=/people/s/simon-razniewski/>Simon Razniewski</a>
|
<a href=/people/f/fariz-darari/>Fariz Darari</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2055><div class="card-body p-3 small">Information extraction (IE) from text has largely focused on relations between individual entities, such as who has won which award. However, some facts are never fully mentioned, and no IE method has perfect <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>. Thus, it is beneficial to also tap contents about the cardinalities of these relations, for example, how many awards someone has won. We introduce this novel <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> of extracting cardinalities and discusses the specific challenges that set it apart from standard IE. We present a distant supervision method using <a href=https://en.wikipedia.org/wiki/Conditional_random_field>conditional random fields</a>. A preliminary evaluation results in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> between 3 % and 55 %, depending on the difficulty of relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2056 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2056/>Integrating <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Linguistic Features</a> in Factuality Prediction over Unified Datasets</a></strong><br><a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/j/judith-eckle-kohler/>Judith Eckle-Kohler</a>
|
<a href=/people/y/yevgeniy-puzikov/>Yevgeniy Puzikov</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2056><div class="card-body p-3 small">Previous models for the assessment of commitment towards a predicate in a sentence (also known as factuality prediction) were trained and tested against a specific annotated dataset, subsequently limiting the generality of their results. In this work we propose an intuitive method for mapping three previously annotated corpora onto a single factuality scale, thereby enabling <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to be tested across these <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>. In addition, we design a novel model for factuality prediction by first extending a previous rule-based factuality prediction system and applying it over an abstraction of dependency trees, and then using the output of this system in a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifier</a>. We show that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on all three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We make both the unified factuality corpus and our new model publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2057/>Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks</a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/m/manzil-zaheer/>Manzil Zaheer</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2057><div class="card-body p-3 small">Existing <a href=https://en.wikipedia.org/wiki/Question_answering>question answering methods</a> infer answers either from a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> or from <a href=https://en.wikipedia.org/wiki/Text_corpus>raw text</a>. While knowledge base (KB) methods are good at answering compositional questions, their performance is often affected by the incompleteness of the KB. Au contraire, <a href=https://en.wikipedia.org/wiki/Web_page>web text</a> contains millions of facts that are absent in the KB, however in an <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured form</a>. Universal schema can support reasoning on the union of both structured KBs and <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a> by aligning them in a common embedded space. In this paper we extend universal schema to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language question answering</a>, employing Memory networks to attend to the large body of facts in the combination of text and KB. Our <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> can be trained in an end-to-end fashion on question-answer pairs. Evaluation results on Spades fill-in-the-blank question answering dataset show that exploiting universal schema for <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> is better than using either a KB or text alone. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> also outperforms the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by 8.5 F1 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2059/>A <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Network</a> with Visual Text Composition Behavior</a></strong><br><a href=/people/h/hongyu-guo/>Hongyu Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2059><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Natural_language>natural languages</a> are compositional, how state-of-the-art neural models achieve <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a> is still unclear. We propose a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep network</a>, which not only achieves competitive <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the <a href=https://en.wikipedia.org/wiki/Social_network>network</a> distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2060.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2060/>Neural System Combination for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/w/wenpeng-hu/>Wenpeng Hu</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2060><div class="card-body p-3 small">Neural machine translation (NMT) becomes a new approach to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and generates much more fluent results compared to statistical machine translation (SMT). However, SMT is usually better than NMT in translation adequacy. It is therefore a promising direction to combine the advantages of both <a href=https://en.wikipedia.org/wiki/Network_topology>NMT</a> and <a href=https://en.wikipedia.org/wiki/Network_topology>SMT</a>. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chinese-to-English translation task show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2061 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2061.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2061/>An Empirical Comparison of Domain Adaptation Methods for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/c/chenhui-chu/>Chenhui Chu</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2061><div class="card-body p-3 small">In this paper, we propose a novel domain adaptation method named mixed fine tuning for neural machine translation (NMT). We combine two existing approaches namely <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine tuning</a> and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> which is a mix of the in-domain and out-of-domain corpora. All <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> are augmented with <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>artificial tags</a> to indicate specific domains. We empirically compare our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> against <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine tuning</a> and multi domain methods and discuss its benefits and shortcomings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2062 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2062/>Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings</a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2062><div class="card-body p-3 small">We propose a new method for extracting pseudo-parallel sentences from a pair of large monolingual corpora, without relying on any document-level information. Our method first exploits <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in order to efficiently evaluate trillions of candidate sentence pairs and then a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to find the most reliable ones. We report significant improvements in <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> when using a <a href=https://en.wikipedia.org/wiki/Machine_translation>translation model</a> trained on the sentence pairs extracted from in-domain monolingual corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2063 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2063/>Feature Hashing for Language and Dialect Identification</a></strong><br><a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/m/mark-dras/>Mark Dras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2063><div class="card-body p-3 small">We evaluate <a href=https://en.wikipedia.org/wiki/Feature_hashing>feature hashing</a> for language identification (LID), a <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> not previously used for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Using a standard dataset, we first show that while <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> performance is high, LID data is highly dimensional and mostly sparse (99.5 %) as it includes large vocabularies for many languages ; memory requirements grow as languages are added. Next we apply <a href=https://en.wikipedia.org/wiki/Hash_function>hashing</a> using various hash sizes, demonstrating that there is no performance loss with <a href=https://en.wikipedia.org/wiki/Dimensionality_reduction>dimensionality reductions</a> of up to 86 %. We also show that using an ensemble of low-dimension hash-based classifiers further boosts performance. Feature hashing is highly useful for LID and holds great promise for future work in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2064 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2064.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2064/>Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM<span class=acl-fixed-case>C</span>hinese Word Usage Errors for Non-Native <span class=acl-fixed-case>C</span>hinese Learners with Bidirectional <span class=acl-fixed-case>LSTM</span></a></strong><br><a href=/people/y/yow-ting-shiue/>Yow-Ting Shiue</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2064><div class="card-body p-3 small">Selecting appropriate words to compose a sentence is one common problem faced by <a href=https://en.wikipedia.org/wiki/Foreign_language>non-native Chinese learners</a>. In this paper, we propose (bidirectional) LSTM sequence labeling models and explore various <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to detect word usage errors in Chinese sentences. By combining CWINDOW word embedding features and POS information, the best bidirectional LSTM model achieves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> 0.5138 and MRR 0.6789 on the HSK dataset. For 80.79 % of the test data, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> ranks the ground-truth within the top two at position level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2065 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2065.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2065/>Automatic Compositor Attribution in the First Folio of Shakespeare</a></strong><br><a href=/people/m/maria-ryskina/>Maria Ryskina</a>
|
<a href=/people/h/hannah-alpert-abrams/>Hannah Alpert-Abrams</a>
|
<a href=/people/d/dan-garrette/>Dan Garrette</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2065><div class="card-body p-3 small">Compositor attribution, the clustering of pages in a historical printed document by the individual who set the type, is a bibliographic task that relies on analysis of orthographic variation and inspection of visual details of the printed page. In this paper, we introduce a novel <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised model</a> that jointly describes the textual and visual features needed to distinguish compositors. Applied to images of Shakespeare&#8217;s First Folio, our model predicts attributions that agree with the manual judgements of bibliographers with an accuracy of 87 %, even on text that is the output of <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2066 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2066/>STAIR Captions : Constructing a Large-Scale Japanese Image Caption Dataset<span class=acl-fixed-case>STAIR</span> Captions: Constructing a Large-Scale <span class=acl-fixed-case>J</span>apanese Image Caption Dataset</a></strong><br><a href=/people/y/yuya-yoshikawa/>Yuya Yoshikawa</a>
|
<a href=/people/y/yutaro-shigeto/>Yutaro Shigeto</a>
|
<a href=/people/a/akikazu-takeuchi/>Akikazu Takeuchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2066><div class="card-body p-3 small">In recent years, automatic generation of image descriptions (captions), that is, image captioning, has attracted a great deal of attention. In this paper, we particularly consider generating <a href=https://en.wikipedia.org/wiki/Japanese_writing_system>Japanese captions</a> for <a href=https://en.wikipedia.org/wiki/Image>images</a>. Since most available caption datasets have been constructed for <a href=https://en.wikipedia.org/wiki/English_language>English language</a>, there are few datasets for <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. To tackle this problem, we construct a large-scale Japanese image caption dataset based on images from MS-COCO, which is called STAIR Captions. STAIR Captions consists of 820,310 <a href=https://en.wikipedia.org/wiki/Japanese_writing_system>Japanese captions</a> for 164,062 images. In the experiment, we show that a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> trained using STAIR Captions can generate more natural and better Japanese captions, compared to those generated using English-Japanese machine translation after generating English captions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2067 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2067" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2067/>Liar, Liar Pants on Fire : A New Benchmark Dataset for Fake News Detection</a></strong><br><a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2067><div class="card-body p-3 small">Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR : a new, publicly available <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for fake news detection. We collected a decade-long, 12.8 K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> can be used for <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking research</a> as well. Notably, this new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate <a href=https://en.wikipedia.org/wiki/Meta_data>meta-data</a> with text. We show that this hybrid approach can improve a text-only deep learning model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2068 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2068/>English Multiword Expression-aware Dependency Parsing Including Named Entities<span class=acl-fixed-case>E</span>nglish Multiword Expression-aware Dependency Parsing Including Named Entities</a></strong><br><a href=/people/a/akihiko-kato/>Akihiko Kato</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2068><div class="card-body p-3 small">Because syntactic structures and spans of multiword expressions (MWEs) are independently annotated in many English syntactic corpora, they are generally inconsistent with respect to one another, which is harmful to the implementation of an aggregate system. In this work, we construct a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> that ensures consistency between dependency structures and MWEs, including <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>. Further, we explore <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that predict both MWE-spans and an MWE-aware dependency structure. Experimental results show that our joint model using additional MWE-span features achieves an MWE recognition improvement of 1.35 points over a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2069 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2069.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2069.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2069" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2069/>Improving Semantic Composition with Offset Inference</a></strong><br><a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/j/julie-weeds/>Julie Weeds</a>
|
<a href=/people/j/jeremy-reffin/>Jeremy Reffin</a>
|
<a href=/people/d/david-weir/>David Weir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2069><div class="card-body p-3 small">Count-based distributional semantic models suffer from sparsity due to unobserved but plausible co-occurrences in any text collection. This problem is amplified for models like Anchored Packed Trees (APTs), that take the grammatical type of a co-occurrence into account. We therefore introduce a novel form of distributional inference that exploits the rich type structure in APTs and infers <a href=https://en.wikipedia.org/wiki/Missing_data>missing data</a> by the same mechanism that is used for semantic composition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2071 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2071" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2071/>Temporal Word Analogies : Identifying Lexical Replacement with Diachronic Word Embeddings</a></strong><br><a href=/people/t/terrence-szymanski/>Terrence Szymanski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2071><div class="card-body p-3 small">This paper introduces the concept of temporal word analogies : pairs of words which occupy the same <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> at different points in time. One well-known property of word embeddings is that they are able to effectively model traditional word analogies (word w_1 is to word w_2 as word w_3 is to word w_4) through vector addition. Here, I show that temporal word analogies (word w_1 at time t _ is like word w_2 at time t _) can effectively be modeled with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space. When applied to a diachronic corpus of news articles, this method is able to identify temporal word analogies such as <a href=https://en.wikipedia.org/wiki/Ronald_Reagan>Ronald Reagan</a> in 1987 is like Bill Clinton in 1997, or <a href=https://en.wikipedia.org/wiki/Walkman>Walkman</a> in 1987 is like <a href=https://en.wikipedia.org/wiki/IPod>iPod</a> in 2007.<tex-math>w_1</tex-math> is to word <tex-math>w_2</tex-math> as word <tex-math>w_3</tex-math> is to word <tex-math>w_4</tex-math>&#8221;) through vector addition. Here, I show that temporal word analogies (&#8220;word <tex-math>w_1</tex-math> at time <tex-math>t_\\alpha</tex-math> is like word <tex-math>w_2</tex-math> at time <tex-math>t_\\beta</tex-math>&#8221;) can effectively be modeled with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space. When applied to a diachronic corpus of news articles, this method is able to identify temporal word analogies such as &#8220;Ronald Reagan in 1987 is like Bill Clinton in 1997&#8221;, or &#8220;Walkman in 1987 is like iPod in 2007&#8221;.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2072 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2072/>Methodical Evaluation of Arabic Word Embeddings<span class=acl-fixed-case>A</span>rabic Word Embeddings</a></strong><br><a href=/people/m/mohammed-elrazzaz/>Mohammed Elrazzaz</a>
|
<a href=/people/s/shady-elbassuoni/>Shady Elbassuoni</a>
|
<a href=/people/k/khaled-shaban/>Khaled Shaban</a>
|
<a href=/people/c/chadi-helwe/>Chadi Helwe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2072><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning techniques</a> have been proposed to obtain meaningful representations of words from text. In this study, we evaluate these various techniques when used to generate Arabic word embeddings. We first build a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> for the <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a> that can be utilized to perform intrinsic evaluation of different <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We then perform additional extrinsic evaluations of the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> based on two NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2073 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2073/>Multilingual Connotation Frames : A Case Study on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> for Targeted Sentiment Analysis and Forecast</a></strong><br><a href=/people/h/hannah-rashkin/>Hannah Rashkin</a>
|
<a href=/people/e/eric-bell/>Eric Bell</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2073><div class="card-body p-3 small">People around the globe respond to major real world events through <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. To study targeted public sentiments across many languages and geographic locations, we introduce multilingual connotation frames : an extension from English connotation frames of Rashkin et al. (2016) with 10 additional <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European languages</a>, focusing on the implied sentiments among event participants engaged in a frame. As a case study, we present large scale analysis on targeted public sentiments toward salient events and <a href=https://en.wikipedia.org/wiki/Non-physical_entity>entities</a> using 1.2 million multilingual connotation frames extracted from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2074 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2074/>Best-Worst Scaling More Reliable than Rating Scales : A Case Study on Sentiment Intensity Annotation</a></strong><br><a href=/people/s/svetlana-kiritchenko/>Svetlana Kiritchenko</a>
|
<a href=/people/s/saif-mohammad/>Saif Mohammad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2074><div class="card-body p-3 small">Rating scales are a widely used method for <a href=https://en.wikipedia.org/wiki/Annotation>data annotation</a> ; however, they present several challenges, such as difficulty in maintaining inter- and intra-annotator consistency. Bestworst scaling (BWS) is an alternative method of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> that is claimed to produce high-quality annotations while keeping the required number of annotations similar to that of <a href=https://en.wikipedia.org/wiki/Scale_(social_sciences)>rating scales</a>. However, the veracity of this claim has never been systematically established. Here for the first time, we set up an experiment that directly compares the rating scale method with BWS. We show that with the same total number of annotations, BWS produces significantly more reliable results than the <a href=https://en.wikipedia.org/wiki/Rating_scale>rating scale</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2078 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2078.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2078.Software.tgz data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2078/>Parser Adaptation for <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> by Integrating Normalization</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2078><div class="card-body p-3 small">This work explores different approaches of using normalization for parser adaptation. Traditionally, <a href=https://en.wikipedia.org/wiki/Normalization_(image_processing)>normalization</a> is used as separate pre-processing step. We show that integrating the <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization model</a> into the <a href=https://en.wikipedia.org/wiki/Parsing>parsing algorithm</a> is more beneficial. This way, multiple normalization candidates can be leveraged, which improves <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We test this hypothesis by modifying the Berkeley parser ; out-of-the-box it achieves an <a href=https://en.wikipedia.org/wiki/Feasible_region>F1 score</a> of 66.52. Our integrated approach reaches a significant improvement with an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of 67.36, while using the best <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization sequence</a> results in an <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> of only 66.94.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2079 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2079/>AliMe Chat : A Sequence to Sequence and Rerank based Chatbot Engine<span class=acl-fixed-case>A</span>li<span class=acl-fixed-case>M</span>e Chat: A Sequence to Sequence and Rerank based Chatbot Engine</a></strong><br><a href=/people/m/minghui-qiu/>Minghui Qiu</a>
|
<a href=/people/f/feng-lin-li/>Feng-Lin Li</a>
|
<a href=/people/s/siyu-wang/>Siyu Wang</a>
|
<a href=/people/x/xing-gao/>Xing Gao</a>
|
<a href=/people/y/yan-chen/>Yan Chen</a>
|
<a href=/people/w/weipeng-zhao/>Weipeng Zhao</a>
|
<a href=/people/h/haiqing-chen/>Haiqing Chen</a>
|
<a href=/people/j/jun-huang/>Jun Huang</a>
|
<a href=/people/w/wei-chu/>Wei Chu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2079><div class="card-body p-3 small">We propose AliMe Chat, an open-domain chatbot engine that integrates the joint results of Information Retrieval (IR) and Sequence to Sequence (Seq2Seq) based generation models. AliMe Chat uses an attentive Seq2Seq based rerank model to optimize the joint results. Extensive experiments show our <a href=https://en.wikipedia.org/wiki/Engine>engine</a> outperforms both <a href=https://en.wikipedia.org/wiki/Infrared>IR</a> and generation based models. We launch AliMe Chat for a real-world industrial application and observe better results than another public chatbot.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2080 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2080/>A Conditional Variational Framework for Dialog Generation</a></strong><br><a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/y/yanran-li/>Yanran Li</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/s/shuzi-niu/>Shuzi Niu</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a>
|
<a href=/people/g/guoping-long/>Guoping Long</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2080><div class="card-body p-3 small">Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> allowing conditional response generation based on specific attributes. These <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a> can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> on two different scenarios, where the attribute refers to <a href=https://en.wikipedia.org/wiki/Generic_property>genericness</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment states</a> respectively. The experiment result testified the potential of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, where meaningful responses can be generated in accordance with the specified attributes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2081 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2081/>Question Answering through <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a> from Large Fine-grained Supervision Data</a></strong><br><a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2081><div class="card-body p-3 small">We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the previous best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by more than 8 %. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2082 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2082/>Self-Crowdsourcing Training for <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/a/azad-abad/>Azad Abad</a>
|
<a href=/people/m/moin-nabi/>Moin Nabi</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2082><div class="card-body p-3 small">In this paper we introduce a self-training strategy for <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. The training examples are automatically selected to train the <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd workers</a>. Our experimental results show an impact of 5 % Improvement in terms of <a href=https://en.wikipedia.org/wiki/F-number>F1</a> for relation extraction task, compared to the method based on distant supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2083 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2083/>A Generative Attentional Neural Network Model for Dialogue Act Classification</a></strong><br><a href=/people/q/quan-hung-tran/>Quan Hung Tran</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/i/ingrid-zukerman/>Ingrid Zukerman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2083><div class="card-body p-3 small">We propose a novel generative neural network architecture for Dialogue Act classification. Building upon the Recurrent Neural Network framework, our model incorporates a novel attentional technique and a label to label connection for <a href=https://en.wikipedia.org/wiki/Sequence_learning>sequence learning</a>, akin to Hidden Markov Models. The experiments show that both of these innovations lead our model to outperform strong baselines for dialogue act classification on MapTask and Switchboard corpora. We further empirically analyse the effectiveness of each of the new <a href=https://en.wikipedia.org/wiki/Innovation>innovations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2084 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2084/>Salience Rank : Efficient <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>Keyphrase Extraction</a> with Topic Modeling</a></strong><br><a href=/people/n/nedelina-teneva/>Nedelina Teneva</a>
|
<a href=/people/w/weiwei-cheng/>Weiwei Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2084><div class="card-body p-3 small">Topical PageRank (TPR) uses latent topic distribution inferred by Latent Dirichlet Allocation (LDA) to perform ranking of noun phrases extracted from documents. The ranking procedure consists of running <a href=https://en.wikipedia.org/wiki/PageRank>PageRank</a> K times, where K is the number of topics used in the LDA model. In this paper, we propose a modification of <a href=https://en.wikipedia.org/wiki/Time_complexity>TPR</a>, called Salience Rank. Salience Rank only needs to run <a href=https://en.wikipedia.org/wiki/PageRank>PageRank</a> once and extracts comparable or better <a href=https://en.wikipedia.org/wiki/String_(computer_science)>keyphrases</a> on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. In addition to quality and efficiency benefit, our method has the flexibility to extract keyphrases with varying tradeoffs between topic specificity and corpus specificity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2085 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2085/>List-only Entity Linking</a></strong><br><a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2085><div class="card-body p-3 small">Traditional Entity Linking (EL) technologies rely on rich structures and properties in the target knowledge base (KB). However, in many applications, the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> as List-only Entity Linking problem. Fortunately, some mentions may have more cues for linking, which can be used as seed mentions to bridge other mentions and the uninformative entities. In this work, we select most linkable mentions as <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(S)>seed mentions</a> and disambiguate other mentions by comparing them with the <a href=https://en.wikipedia.org/wiki/List_of_Latin_phrases_(S)>seed mentions</a> rather than directly with the entities. Our experiments on linking mentions to seven automatically mined lists show promising results and demonstrate the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2087 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2087/>Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model</a></strong><br><a href=/people/p/paria-jamshid-lou/>Paria Jamshid Lou</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2087><div class="card-body p-3 small">This paper presents a model for disfluency detection in spontaneous speech transcripts called LSTM Noisy Channel Model. The model uses a Noisy Channel Model (NCM) to generate n-best candidate disfluency analyses and a Long Short-Term Memory (LSTM) language model to score the underlying fluent sentences of each analysis. The LSTM language model scores, along with other <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, are used in a MaxEnt reranker to identify the most plausible analysis. We show that using an LSTM language model in the reranking process of noisy channel disfluency model improves the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in disfluency detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2088 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2088.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2088/>On the Equivalence of Holographic and Complex Embeddings for Link Prediction</a></strong><br><a href=/people/k/katsuhiko-hayashi/>Katsuhiko Hayashi</a>
|
<a href=/people/m/masashi-shimbo/>Masashi Shimbo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2088><div class="card-body p-3 small">We show the equivalence of two state-of-the-art models for link prediction / knowledge graph completion : Nickel et al&#8217;s holographic embeddings and Trouillon et al.&#8217;s complex embeddings. We first consider a spectral version of the holographic embeddings, exploiting the <a href=https://en.wikipedia.org/wiki/Frequency_domain>frequency domain</a> in the <a href=https://en.wikipedia.org/wiki/Fourier_transform>Fourier transform</a> for efficient computation. The analysis of the resulting <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> reveals that it can be viewed as an instance of the complex embeddings with a certain constraint imposed on the initial vectors upon training. Conversely, any set of complex embeddings can be converted to a set of equivalent holographic embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2089 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2089/>Sentence Embedding for Neural Machine Translation Domain Adaptation</a></strong><br><a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2089><div class="card-body p-3 small">Although new corpora are becoming increasingly available for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, only those that belong to the same or similar domains are typically able to improve <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. Recently Neural Machine Translation (NMT) has become prominent in the field. However, most of the existing domain adaptation methods only focus on phrase-based machine translation. In this paper, we exploit the NMT&#8217;s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU points</a>, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2090 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2090.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2090" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2090/>Data Augmentation for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/m/marzieh-fadaee/>Marzieh Fadaee</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2090><div class="card-body p-3 small">The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2092 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2092" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2092/>Chunk-Based Bi-Scale Decoder for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/x/xiaohua-liu/>Xiaohua Liu</a>
|
<a href=/people/h/hang-li/>Hang Li</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2092><div class="card-body p-3 small">In typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all <a href=https://en.wikipedia.org/wiki/Granularity>linguistic granularities</a> in the same time-scale of <a href=https://en.wikipedia.org/wiki/Neural_network>RNN</a>. In this paper, we propose a new type of <a href=https://en.wikipedia.org/wiki/Code>decoder</a> for NMT, which splits the decode state into two parts and updates them in two different <a href=https://en.wikipedia.org/wiki/Time_complexity>time-scales</a>. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improves the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance over the state-of-the-art NMT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2093 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2093.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2093" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2093/>Model Transfer for Tagging Low-resource Languages using a <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>Bilingual Dictionary</a></a></strong><br><a href=/people/m/meng-fang/>Meng Fang</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2093><div class="card-body p-3 small">Cross-lingual model transfer is a compelling and popular method for predicting annotations in a low-resource language, whereby parallel corpora provide a bridge to a high-resource language, and its associated annotated corpora. However, parallel data is not readily available for many languages, limiting the applicability of these approaches. We address these drawbacks in our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> which takes advantage of cross-lingual word embeddings trained solely on a high coverage dictionary. We propose a novel neural network model for joint training from both sources of data based on cross-lingual word embeddings, and show substantial empirical improvements over baseline techniques. We also propose several active learning heuristics, which result in improvements over competitive benchmark methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2094/>EuroSense : Automatic Harvesting of Multilingual Sense Annotations from Parallel Text<span class=acl-fixed-case>E</span>uro<span class=acl-fixed-case>S</span>ense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text</a></strong><br><a href=/people/c/claudio-delli-bovi/>Claudio Delli Bovi</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2094><div class="card-body p-3 small">Parallel corpora are widely used in a variety of Natural Language Processing tasks, from <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> to cross-lingual Word Sense Disambiguation, where parallel sentences can be exploited to automatically generate high-quality sense annotations on a large scale. In this paper we present EuroSense, a multilingual sense-annotated resource based on the joint disambiguation of the Europarl parallel corpus, with almost 123 million sense annotations for over 155 thousand distinct concepts and entities from a language-independent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2095 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2095/>Challenging Language-Dependent Segmentation for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> : An Application to Machine Translation and Part-of-Speech Tagging<span class=acl-fixed-case>A</span>rabic: An Application to Machine Translation and Part-of-Speech Tagging</a></strong><br><a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/s/stephan-vogel/>Stephan Vogel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2095><div class="card-body p-3 small">Word segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Off-the-shelf tools, however, are : i) complicated to use and ii) domain / dialect dependent. We explore three language-independent alternatives to morphological segmentation using : i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> and POS tagging, we found these <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a <a href=https://en.wikipedia.org/wiki/Ratio>ratio</a> close to 1 or greater, gives optimal performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2096 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=P17-2096" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/P17-2096/>Fast and Accurate Neural Word Segmentation for Chinese<span class=acl-fixed-case>C</span>hinese</a></strong><br><a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/y/yuan-xin/>Yuan Xin</a>
|
<a href=/people/y/yongjian-wu/>Yongjian Wu</a>
|
<a href=/people/f/feiyue-huang/>Feiyue Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2096><div class="card-body p-3 small">Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of <a href=https://en.wikipedia.org/wiki/Chinese_word_segmentation>Chinese word segmentation</a>. However, both training and working procedures of the current neural models are computationally inefficient. In this paper, we propose a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2097 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2097/>Pay Attention to the Ending : Strong Neural Baselines for the ROC Story Cloze Task<span class=acl-fixed-case>ROC</span> Story Cloze Task</a></strong><br><a href=/people/z/zheng-cai/>Zheng Cai</a>
|
<a href=/people/l/lifu-tu/>Lifu Tu</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2097><div class="card-body p-3 small">We consider the ROC story cloze task (Mostafazadeh et al., 2016) and present several findings. We develop a model that uses hierarchical recurrent networks with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> to encode the sentences in the story and score candidate endings. By discarding the large training set and only training on the validation set, we achieve an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 74.7 %. Even when we discard the <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>story plots</a> (sentences before the ending) and only train to choose the better of two endings, we can still reach 72.5 %. We then analyze this ending-only task setting. We estimate human accuracy to be 78 % and find several types of clues that lead to this high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, including those related to <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, <a href=https://en.wikipedia.org/wiki/Negation>negation</a>, and general ending likelihood regardless of the story context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2099 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2099/>Representing Sentences as Low-Rank Subspaces</a></strong><br><a href=/people/j/jiaqi-mu/>Jiaqi Mu</a>
|
<a href=/people/s/suma-bhat/>Suma Bhat</a>
|
<a href=/people/p/pramod-viswanath/>Pramod Viswanath</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2099><div class="card-body p-3 small">Sentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications. We observe a simple geometry of sentences the word representations of a given sentence (on average 10.23 words in all SemEval datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation, we represent a sentence by the low-rank subspace spanned by its <a href=https://en.wikipedia.org/wiki/Word_(group_theory)>word vectors</a>. Such an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised representation</a> is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models, including skip-thought vectors, by 15 % on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2100 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2100/>Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization<span class=acl-fixed-case>C</span>hinese Social Media Text Summarization</a></strong><br><a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/q/qi-su/>Qi Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2100><div class="card-body p-3 small">Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve <a href=https://en.wikipedia.org/wiki/Relevance_(information_retrieval)>semantic relevance</a> between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a <a href=https://en.wikipedia.org/wiki/Encoder>decoder</a>. Besides, the <a href=https://en.wikipedia.org/wiki/Similarity_score>similarity score</a> between the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> is maximized during training. Our experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a> on a <a href=https://en.wikipedia.org/wiki/Social_media>social media corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2102 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2102.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2102/>Separating Facts from Fiction : Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/s/svitlana-volkova/>Svitlana Volkova</a>
|
<a href=/people/k/kyle-shaffer/>Kyle Shaffer</a>
|
<a href=/people/j/jin-yea-jang/>Jin Yea Jang</a>
|
<a href=/people/n/nathan-hodas/>Nathan Hodas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2102><div class="card-body p-3 small">Pew research polls report 62 percent of U.S. adults get news on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> (Gottfried and Shearer, 2016). In a December poll, 64 percent of U.S. adults said that made-up news has caused a great deal of confusion about the facts of current events (Barthel et al., 2016). Fabricated stories in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, ranging from <a href=https://en.wikipedia.org/wiki/Propaganda>deliberate propaganda</a> to <a href=https://en.wikipedia.org/wiki/Hoax>hoaxes</a> and <a href=https://en.wikipedia.org/wiki/Satire>satire</a>, contributes to this confusion in addition to having serious effects on global stability. In this work we build predictive models to classify 130 thousand news posts as suspicious or verified, and predict four sub-types of suspicious news satire, <a href=https://en.wikipedia.org/wiki/Hoax>hoaxes</a>, <a href=https://en.wikipedia.org/wiki/Clickbait>clickbait</a> and <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>. We show that <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a> trained on tweet content and <a href=https://en.wikipedia.org/wiki/Social_network>social network interactions</a> outperform <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical models</a>. Unlike previous work on deception detection, we find that adding syntax and grammar features to our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> does not improve performance. Incorporating linguistic features improves <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results, however, social interaction features are most informative for finer-grained separation between four types of suspicious news posts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2103 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/P17-2103.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-2103/>Recognizing Counterfactual Thinking in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media Texts</a></a></strong><br><a href=/people/y/youngseo-son/>Youngseo Son</a>
|
<a href=/people/a/anneke-buffone/>Anneke Buffone</a>
|
<a href=/people/j/joe-raso/>Joe Raso</a>
|
<a href=/people/a/allegra-larche/>Allegra Larche</a>
|
<a href=/people/a/anthony-janocko/>Anthony Janocko</a>
|
<a href=/people/k/kevin-zembroski/>Kevin Zembroski</a>
|
<a href=/people/h/h-andrew-schwartz/>H Andrew Schwartz</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2103><div class="card-body p-3 small">Counterfactual statements, describing events that did not occur and their consequents, have been studied in areas including <a href=https://en.wikipedia.org/wiki/Problem_solving>problem-solving</a>, affect management, and behavior regulation. People with more <a href=https://en.wikipedia.org/wiki/Counterfactual_thinking>counterfactual thinking</a> tend to perceive life events as more personally meaningful. Nevertheless, <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a> have not been studied in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>. We create a counterfactual tweet dataset and explore approaches for detecting counterfactuals using rule-based and supervised statistical approaches. A combined rule-based and statistical approach yielded the best results (F1 = 0.77) outperforming either <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>approach</a> used alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2104 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2104/>Temporal Orientation of Tweets for Predicting Income of Users</a></strong><br><a href=/people/m/mohammed-hasanuzzaman/>Mohammed Hasanuzzaman</a>
|
<a href=/people/s/sabyasachi-kamila/>Sabyasachi Kamila</a>
|
<a href=/people/m/mandeep-kaur/>Mandeep Kaur</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2104><div class="card-body p-3 small">Automatically estimating a user&#8217;s socio-economic profile from their language use in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> can significantly help <a href=https://en.wikipedia.org/wiki/Social_science>social science research</a> and various downstream applications ranging from <a href=https://en.wikipedia.org/wiki/Business>business</a> to <a href=https://en.wikipedia.org/wiki/Politics>politics</a>. The current paper presents the first study where user cognitive structure is used to build a predictive model of income. In particular, we first develop a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> using a weakly supervised learning framework to automatically time-tag tweets as past, present, or future. We quantify a user&#8217;s overall temporal orientation based on their distribution of tweets, and use it to build a predictive model of income. Our analysis uncovers a correlation between future temporal orientation and <a href=https://en.wikipedia.org/wiki/Income>income</a>. Finally, we measure the predictive power of future temporal orientation on <a href=https://en.wikipedia.org/wiki/Income>income</a> by performing <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2105 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2105/>Character-Aware Neural Morphological Disambiguation</a></strong><br><a href=/people/a/alymzhan-toleu/>Alymzhan Toleu</a>
|
<a href=/people/g/gulmira-tolegen/>Gulmira Tolegen</a>
|
<a href=/people/a/aibek-makazhanov/>Aibek Makazhanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2105><div class="card-body p-3 small">We develop a language-independent, deep learning-based approach to the task of morphological disambiguation. Guided by the intuition that the correct analysis should be most similar to the context, we propose dense representations for morphological analyses and surface context and a simple yet effective way of combining the two to perform disambiguation. Our approach improves on the language-dependent state of the art for two <a href=https://en.wikipedia.org/wiki/Agglutinative_language>agglutinative languages</a> (Turkish and Kazakh) and can be potentially applied to other morphologically complex languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2106 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2106/>Character Composition Model with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for Dependency Parsing on Morphologically Rich Languages</a></strong><br><a href=/people/x/xiang-yu/>Xiang Yu</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2106><div class="card-body p-3 small">We present a transition-based dependency parser that uses a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> to compose word representations from characters. The character composition model shows great improvement over the word-lookup model, especially for parsing <a href=https://en.wikipedia.org/wiki/Agglutinative_language>agglutinative languages</a>. These improvements are even better than using pre-trained word embeddings from extra data. On the SPMRL data sets, our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms the previous best greedy parser (Ballesteros et. al, 2015) by a margin of 3 % on average.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-2107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-2107 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-2107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-2107/>How (not) to train a dependency parser : The curious case of jackknifing part-of-speech taggers</a></strong><br><a href=/people/z/zeljko-agic/>Željko Agić</a>
|
<a href=/people/n/natalie-schluter/>Natalie Schluter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-2107><div class="card-body p-3 small">In dependency parsing, jackknifing taggers is indiscriminately used as a simple adaptation strategy. Here, we empirically evaluate when and how (not) to use <a href=https://en.wikipedia.org/wiki/Jackknifing>jackknifing</a> in <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. On 26 languages, we reveal a preference that conflicts with, and surpasses the ubiquitous ten-folding. We show no clear benefits of tagging the training data in cross-lingual parsing.</div></div></div><hr><div id=p17-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-3.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P17-3/>Proceedings of ACL 2017, Student Research Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-3000/>Proceedings of <span class=acl-fixed-case>ACL</span> 2017, Student Research Workshop</a></strong><br><a href=/people/a/allyson-ettinger/>Allyson Ettinger</a>
|
<a href=/people/s/spandana-gella/>Spandana Gella</a>
|
<a href=/people/m/matthieu-labeau/>Matthieu Labeau</a>
|
<a href=/people/c/cecilia-ovesdotter-alm/>Cecilia Ovesdotter Alm</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p></div><hr><div id=p17-4><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-4.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P17-4/>Proceedings of ACL 2017, System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-4000/>Proceedings of <span class=acl-fixed-case>ACL</span> 2017, System Demonstrations</a></strong><br><a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p></div><hr><div id=p17-5><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/P17-5/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-5000/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></strong><br><a href=/people/m/maja-popovic/>Maja Popović</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-5001 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-5001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-5001/>NLP for Precision Medicine<span class=acl-fixed-case>NLP</span> for Precision Medicine</a></strong><br><a href=/people/h/hoifung-poon/>Hoifung Poon</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a>
|
<a href=/people/k/kristina-toutanova/>Kristina Toutanova</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-5001><div class="card-body p-3 small">We will introduce <a href=https://en.wikipedia.org/wiki/Precision_medicine>precision medicine</a> and showcase the vast opportunities for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> in this burgeoning field with great societal impact. We will review pressing NLP problems, state-of-the art methods, and important applications, as well as <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, medical resources, and practical issues. The tutorial will provide an accessible overview of <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedicine</a>, and does not presume knowledge in <a href=https://en.wikipedia.org/wiki/Biology>biology</a> or <a href=https://en.wikipedia.org/wiki/Health_care>healthcare</a>. The ultimate goal is to reduce the entry barrier for NLP researchers to contribute to this exciting domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-5002/>Multimodal Machine Learning : Integrating Language, <a href=https://en.wikipedia.org/wiki/Visual_perception>Vision</a> and Speech</a></strong><br><a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/t/tadas-baltrusaitis/>Tadas Baltrušaitis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-5002><div class="card-body p-3 small">Multimodal machine learning is a vibrant multi-disciplinary research field which addresses some of the original goals of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a> by integrating and modeling multiple communicative modalities, including linguistic, acoustic and visual messages. With the initial research on audio-visual speech recognition and more recently with image and video captioning projects, this research field brings some unique challenges for multimodal researchers given the heterogeneity of the data and the contingency often found between modalities. This tutorial builds upon a recent course taught at Carnegie Mellon University during the Spring 2016 semester (CMU course 11-777) and two tutorials presented at CVPR 2016 and ICMI 2016. The present tutorial will review fundamental concepts of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and deep neural networks before describing the five main challenges in multimodal machine learning : (1) multimodal representation learning, (2) translation & mapping, (3) modality alignment, (4) multimodal fusion and (5) co-learning. The tutorial will also present state-of-the-art <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that were recently proposed to solve multimodal applications such as image captioning, video descriptions and visual question-answer. We will also discuss the current and upcoming challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-5004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-5004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234950627 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-5004/>Deep Learning for Dialogue Systems</a></strong><br><a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tür</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-5004><div class="card-body p-3 small">In the past decade, goal-oriented spoken dialogue systems have been the most prominent component in today&#8217;s virtual personal assistants. The classic <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> have rather complex and/or modular pipelines. The advance of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning technologies</a> has recently risen the applications of neural models to dialogue modeling. However, how to successfully apply deep learning based approaches to a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> is still challenging. Hence, this tutorial is designed to focus on an overview of the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> development while describing most recent research for building <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> and summarizing the challenges, in order to allow researchers to study the potential improvements of the state-of-the-art <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>. The tutorial material is available at http://deepdialogue.miulab.tw.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-5005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-5005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-5005/>Beyond Words : <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> for Multiword Expressions and Collocations</a></strong><br><a href=/people/v/valia-kordoni/>Valia Kordoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-5005><div class="card-body p-3 small">Deep learning has recently shown much promise for NLP applications. Traditionally, in most NLP approaches, documents or sentences are represented by a sparse bag-of-words representation. There is now a lot of work which goes beyond this by adopting a distributed representation of words, by constructing a so-called neural embedding or vector space representation of each word or document. The aim of this tutorial is to go beyond the learning of word vectors and present methods for learning vector representations for <a href=https://en.wikipedia.org/wiki/Multiword_expression>Multiword Expressions</a> and bilingual phrase pairs, all of which are useful for various NLP applications. This tutorial aims to provide attendees with a clear notion of the linguistic and distributional characteristics of <a href=https://en.wikipedia.org/wiki/Multiword_expression>Multiword Expressions (MWEs)</a>, their relevance for the intersection of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, what methods and resources are available to support their use, and what more could be done in the future. Our target audience are researchers and practitioners in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, parsing (syntactic and semantic) and language technology, not necessarily experts in MWEs, who are interested in tasks that involve or could benefit from considering MWEs as a pervasive phenomenon in human language and communication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-5006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-5006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234948956 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-5006/>Tutorial : Making Better Use of the Crowd<span class=acl-fixed-case>T</span>utorial: Making Better Use of the Crowd</a></strong><br><a href=/people/j/jennifer-wortman-vaughan/>Jennifer Wortman Vaughan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-5006><div class="card-body p-3 small">Over the last decade, <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> has been used to harness the power of human computation to solve tasks that are notoriously difficult to solve with computers alone, such as determining whether or not an image contains a <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a>, rating the relevance of a website, or verifying the phone number of a business. The natural language processing community was early to embrace <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> as a tool for quickly and inexpensively obtaining annotated data to train <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>. Once this <a href=https://en.wikipedia.org/wiki/Data>data</a> is collected, it can be handed off to <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that learn to perform basic NLP tasks such as <a href=https://en.wikipedia.org/wiki/Translation>translation</a> or <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Usually this handoff is where interaction with the crowd ends. The crowd provides the data, but the ultimate goal is to eventually take humans out of the loop. Are there better ways to make use of the crowd?In this tutorial, I will begin with a showcase of innovative uses of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> that go beyond <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> and <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. I will discuss applications to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, hybrid intelligence or human in the loop AI systems that leverage the complementary strengths of humans and machines in order to achieve more than either could achieve alone, and large scale studies of human behavior online. I will then spend the majority of the tutorial diving into recent research aimed at understanding who crowdworkers are, how they behave, and what this should teach us about best practices for interacting with the crowd.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>