<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Annual Meeting of the Association for Computational Linguistics (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Annual Meeting of the Association for Computational Linguistics (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021acl-long>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a>
<span class="badge badge-info align-middle ml-1">230&nbsp;papers</span></li><li><a class=align-middle href=#2021acl-short>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a>
<span class="badge badge-info align-middle ml-1">53&nbsp;papers</span></li><li><a class=align-middle href=#2021acl-srw>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li><li><a class=align-middle href=#2021acl-demo>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">19&nbsp;papers</span></li><li><a class=align-middle href=#2021acl-tutorials>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2021bppf-1>Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2021case-1>Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021)</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#2021dialdoc-1>Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2021ecnlp-1>Proceedings of The 4th Workshop on e-Commerce and NLP</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2021gebnlp-1>Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2021gem-1>Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2021internlp-1>Proceedings of the First Workshop on Interactive Learning for Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2021iwpt-1>Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021)</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#2021iwslt-1>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#2021lchange-1>Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#2021metanlp-1>Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2021mwe-1>Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2021nlp4posimpact-1>Proceedings of the 1st Workshop on NLP for Positive Impact</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2021nlp4prog-1>Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#2021repl4nlp-1>Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#2021semeval-1>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a>
<span class="badge badge-info align-middle ml-1">71&nbsp;papers</span></li><li><a class=align-middle href=#2021sigmorphon-1>Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2021splurobonlp-1>Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2021spnlp-1>Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2021starsem-1>Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#2021unimplicit-1>Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2021wat-1>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li><li><a class=align-middle href=#2021woah-1>Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li></ul></div></div><div id=2021acl-long><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.acl-long/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.0/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></strong><br><a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.2/>How Did This Get Funded? ! Automatically Identifying Quirky Scientific Achievements<span class=acl-fixed-case>H</span>ow Did This Get Funded?! <span class=acl-fixed-case>A</span>utomatically Identifying Quirky Scientific Achievements</a></strong><br><a href=/people/c/chen-shani/>Chen Shani</a>
|
<a href=/people/n/nadav-borenstein/>Nadav Borenstein</a>
|
<a href=/people/d/dafna-shahaf/>Dafna Shahaf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--2><div class="card-body p-3 small">Humor is an important <a href=https://en.wikipedia.org/wiki/Social_phenomenon>social phenomenon</a>, serving complex social and psychological functions. However, despite being studied for millennia <a href=https://en.wikipedia.org/wiki/Humour>humor</a> is computationally not well understood, often considered an AI-complete problem. In this work, we introduce a novel <a href=https://en.wikipedia.org/wiki/Setting_(narrative)>setting</a> in humor mining : automatically detecting funny and unusual scientific papers. We are inspired by the <a href=https://en.wikipedia.org/wiki/Ig_Nobel_Prize>Ig Nobel prize</a>, a satirical prize awarded annually to celebrate funny scientific achievements (example past winner : Are cows more likely to lie down the longer they stand?). This challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> has unique characteristics that make <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> particularly suitable for <a href=https://en.wikipedia.org/wiki/Machine_learning>automatic learning</a>. We construct a dataset containing thousands of funny papers and use it to learn <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>, combining findings from <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a> and <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> with recent advances in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. We use our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to identify potentially funny papers in a large <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of over 630,000 articles. The results demonstrate the potential of our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>, and more broadly the utility of integrating state-of-the-art NLP methods with insights from more traditional disciplines</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.3/>Engage the Public : Poll Question Generation for Social Media Posts</a></strong><br><a href=/people/z/zexin-lu/>Zexin Lu</a>
|
<a href=/people/k/keyang-ding/>Keyang Ding</a>
|
<a href=/people/y/yuji-zhang/>Yuji Zhang</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--3><div class="card-body p-3 small">This paper presents a novel <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to generate <a href=https://en.wikipedia.org/wiki/Opinion_poll>poll questions</a> for <a href=https://en.wikipedia.org/wiki/Social_media>social media posts</a>. It offers an easy way to hear the voice from the public and learn from their feelings to important <a href=https://en.wikipedia.org/wiki/Social_issue>social topics</a>. While most related work tackles <a href=https://en.wikipedia.org/wiki/Formal_language>formal languages</a> (e.g., exam papers), we generate <a href=https://en.wikipedia.org/wiki/Opinion_poll>poll questions</a> for short and colloquial social media messages exhibiting severe data sparsity. To deal with that, we propose to encode user comments and discover latent topics therein as contexts. They are then incorporated into a sequence-to-sequence (S2S) architecture for question generation and its extension with dual decoders to additionally yield poll choices (answers). For experiments, we collect a large-scale Chinese dataset from <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Sina Weibo</a> containing over 20 K polls. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the popular S2S models without exploiting topics from comments and the dual decoder design can further benefit the prediction of both questions and answers. Human evaluations further exhibit our superiority in yielding high-quality polls helpful to draw user engagements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.4/>HateCheck : Functional Tests for Hate Speech Detection Models<span class=acl-fixed-case>H</span>ate<span class=acl-fixed-case>C</span>heck: Functional Tests for Hate Speech Detection Models</a></strong><br><a href=/people/p/paul-rottger/>Paul Röttger</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/d/dong-nguyen/>Dong Nguyen</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/h/helen-margetts/>Helen Margetts</a>
|
<a href=/people/j/janet-pierrehumbert/>Janet Pierrehumbert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--4><div class="card-body p-3 small">Detecting online hate is a difficult task that even state-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> struggle with. Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> such as <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a>. However, this <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> makes it difficult to identify specific model weak points. It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, we introduce HateCheck, a suite of <a href=https://en.wikipedia.org/wiki/Functional_testing>functional tests</a> for hate speech detection models. We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders. We craft test cases for each functionality and validate their quality through a structured annotation process. To illustrate HateCheck&#8217;s utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.10/>Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation<span class=acl-fixed-case>NLG</span> with Language Agnostic Delexicalisation</a></strong><br><a href=/people/g/giulio-zhou/>Giulio Zhou</a>
|
<a href=/people/g/gerasimos-lampouras/>Gerasimos Lampouras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--10><div class="card-body p-3 small">Concept-to-text Natural Language Generation is the task of expressing an input meaning representation in natural language. Previous approaches in this task have been able to generalise to rare or unseen instances by relying on a delexicalisation of the input. However, this often requires that the input appears verbatim in the output text. This poses challenges in multilingual settings, where the task expands to generate the output text in multiple languages given the same input. In this paper, we explore the application of multilingual models in concept-to-text and propose Language Agnostic Delexicalisation, a novel delexicalisation method that uses multilingual pretrained embeddings, and employs a character-level post-editing model to inflect words in their correct form during relexicalisation. Our experiments across five datasets and five languages show that multilingual models outperform monolingual models in concept-to-text and that our framework outperforms previous approaches, especially in low resource conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.12/>Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking</a></strong><br><a href=/people/j/jinyu-guo/>Jinyu Guo</a>
|
<a href=/people/k/kai-shuang/>Kai Shuang</a>
|
<a href=/people/j/jijie-li/>Jijie Li</a>
|
<a href=/people/z/zihan-wang/>Zihan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--12><div class="card-body p-3 small">The goal of dialogue state tracking (DST) is to predict the current dialogue state given all previous dialogue contexts. Existing approaches generally predict the dialogue state at every turn from scratch. However, the overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn. Therefore, the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value generation. To address this problem, we devise the two-stage DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue, and the Slot Value Generator based on the dialogue history. The Dual Slot Selector determines each slot whether to update slot value or to inherit the slot value from the previous turn from two aspects : (1) if there is a strong relationship between it and the current turn dialogue utterances ; (2) if a slot value with high reliability can be obtained for it through the current turn dialogue. The slots selected to be updated are permitted to enter the Slot Value Generator to update values by a hybrid method, while the other slots directly inherit the values from the previous turn. Empirical results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves 56.93 %, 60.73 %, and 58.04 % joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets respectively and achieves a new state-of-the-art performance with significant improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.14/>BoB : BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data<span class=acl-fixed-case>B</span>o<span class=acl-fixed-case>B</span>: <span class=acl-fixed-case>BERT</span> Over <span class=acl-fixed-case>BERT</span> for Training Persona-based Dialogue Models from Limited Personalized Data</a></strong><br><a href=/people/h/haoyu-song/>Haoyu Song</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/k/kaiyan-zhang/>Kaiyan Zhang</a>
|
<a href=/people/w/weinan-zhang/>Wei-Nan Zhang</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--14><div class="card-body p-3 small">Maintaining a consistent persona is essential for dialogue agents. Although tremendous advancements have been brought, the limited-scale of annotated personalized dialogue datasets is still a barrier towards training robust and consistent persona-based dialogue models. This work shows how this challenge can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a BERT-based encoder and two BERT-based decoders, where one decoder is for response generation, and another is for consistency understanding. In particular, to learn the ability of consistency understanding from large-scale non-dialogue inference data, we train the second decoder in an unlikelihood manner. Under different limited data settings, both automatic and human evaluations demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms strong baselines in response quality and persona consistency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.15/>GL-GIN : Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling<span class=acl-fixed-case>GL</span>-<span class=acl-fixed-case>GIN</span>: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling</a></strong><br><a href=/people/l/libo-qin/>Libo Qin</a>
|
<a href=/people/f/fuxuan-wei/>Fuxuan Wei</a>
|
<a href=/people/t/tianbao-xie/>Tianbao Xie</a>
|
<a href=/people/x/xiao-xu/>Xiao Xu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--15><div class="card-body p-3 small">Multi-intent SLU can handle multiple intents in an utterance, which has attracted increasing attention. However, the state-of-the-art joint models heavily rely on <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive approaches</a>, resulting in two issues : slow inference speed and <a href=https://en.wikipedia.org/wiki/Information_leakage>information leakage</a>. In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate. Specifically, we propose a Global-Locally Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intent-slot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance. Experimental results on two public datasets show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves state-of-the-art performance while being 11.5 times faster.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.17/>Modularized Interaction Network for Named Entity Recognition</a></strong><br><a href=/people/f/fei-li/>Fei Li</a>
|
<a href=/people/z/zheng-wang/>Zheng Wang</a>
|
<a href=/people/s/siu-cheung-hui/>Siu Cheung Hui</a>
|
<a href=/people/l/lejian-liao/>Lejian Liao</a>
|
<a href=/people/d/dandan-song/>Dandan Song</a>
|
<a href=/people/j/jing-xu/>Jing Xu</a>
|
<a href=/people/g/guoxiu-he/>Guoxiu He</a>
|
<a href=/people/m/meihuizi-jia/>Meihuizi Jia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--17><div class="card-body p-3 small">Although the existing Named Entity Recognition (NER) models have achieved promising performance, they suffer from certain drawbacks. The sequence labeling-based NER models do not perform well in recognizing long entities as they focus only on word-level information, while the segment-based NER models which focus on processing segment instead of single word are unable to capture the word-level dependencies within the segment. Moreover, as boundary detection and type prediction may cooperate with each other for the NER task, it is also important for the two sub-tasks to mutually reinforce each other by sharing their information. In this paper, we propose a novel Modularized Interaction Network (MIN) model which utilizes both segment-level information and word-level dependencies, and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the NER task. We have conducted extensive experiments based on three NER benchmark datasets. The performance results have shown that the proposed MIN model has outperformed the current state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.19/>UniRE : A Unified Label Space for Entity Relation Extraction<span class=acl-fixed-case>U</span>ni<span class=acl-fixed-case>RE</span>: A Unified Label Space for Entity Relation Extraction</a></strong><br><a href=/people/y/yijun-wang/>Yijun Wang</a>
|
<a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/j/junchi-yan/>Junchi Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--19><div class="card-body p-3 small">Many joint entity relation extraction models setup two separated label spaces for the two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-tasks</a> (i.e., <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity detection</a> and relation classification). We argue that this setting may hinder the <a href=https://en.wikipedia.org/wiki/Information_exchange>information interaction</a> between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks&#8217; label spaces. The input of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell&#8217;s label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.22/>Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation<span class=acl-fixed-case>B</span>ayes Risk Decoding in Neural Machine Translation</a></strong><br><a href=/people/m/mathias-muller/>Mathias Müller</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--22><div class="card-body p-3 small">Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to copy noise in training data or domain shift. Recent work has tied these shortcomings to <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> the de facto standard inference algorithm in NMT and Eikema & Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead. In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>. We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.23/>Multi-Head Highly Parallelized LSTM Decoder for Neural Machine Translation<span class=acl-fixed-case>LSTM</span> Decoder for Neural Machine Translation</a></strong><br><a href=/people/h/hongfei-xu/>Hongfei Xu</a>
|
<a href=/people/q/qiuhui-liu/>Qiuhui Liu</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/m/meng-zhang/>Meng Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--23><div class="card-body p-3 small">One of the reasons Transformer translation models are popular is that self-attention networks for <a href=https://en.wikipedia.org/wiki/Context_model>context modelling</a> can be easily parallelized at sequence level. However, the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> of a self-attention network is O(n^2), increasing quadratically with sequence length. By contrast, the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> of LSTM-based approaches is only O(n). In practice, however, LSTMs are much slower to train than self-attention networks as they can not be parallelized at sequence level : to model context, the current LSTM state relies on the full LSTM computation of the preceding state. This has to be computed n times for a sequence of length n. The <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformations</a> involved in the LSTM gate and state computations are the major cost factors in this. To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context. This allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations. We then connect the outputs of each parallel step with computationally cheap element-wise computations. We call this the Highly Parallelized LSTM. To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head attention in the Transformer. The experiments show that our MHPLSTM decoder achieves significant BLEU improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard LSTM.<tex-math>O(n^2)</tex-math>, increasing quadratically with sequence length. By contrast, the complexity of LSTM-based approaches is only O(n). In practice, however, LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model context, the current LSTM state relies on the full LSTM computation of the preceding state. This has to be computed n times for a sequence of length n. The linear transformations involved in the LSTM gate and state computations are the major cost factors in this. To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context. This allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations. We then connect the outputs of each parallel step with computationally cheap element-wise computations. We call this the Highly Parallelized LSTM. To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head attention in the Transformer. The experiments show that our MHPLSTM decoder achieves significant BLEU improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard LSTM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.24/>A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment</a></strong><br><a href=/people/j/jingyi-zhang/>Jingyi Zhang</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--24><div class="card-body p-3 small">Word alignment and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> are two closely related <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. Neural translation models, such as RNN-based and Transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy. High-quality <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a> can help <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> in many different ways, such as missing word detection, annotation transfer and lexicon injection. Existing methods for learning <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a> include statistical word aligners (e.g. GIZA++) and recently neural word alignment models. This paper presents a bidirectional Transformer based alignment (BTBA) model for <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning</a> of the word alignment task. Our BTBA model predicts the current target word by attending the source context and both left-side and right-side target context to produce accurate target-to-source attention (alignment). We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and self-supervised training. We test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner GIZA++.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.25/>Learning Language Specific Sub-network for Multilingual Machine Translation</a></strong><br><a href=/people/z/zehui-lin/>Zehui Lin</a>
|
<a href=/people/l/liwei-wu/>Liwei Wu</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--25><div class="card-body p-3 small">Multilingual neural machine translation aims at learning a single <a href=https://en.wikipedia.org/wiki/Machine_translation>translation model</a> for multiple languages. These jointly trained models often suffer from performance degradationon rich-resource language pairs. We attribute this degeneration to parameter interference. In this paper, we propose LaSS to jointly train a single unified multilingual MT model. LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference. Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation. LaSS boosts zero-shot translation with an average of 8.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on 30 language pairs. Codes and trained models are available at https://github.com/NLP-Playground/LaSS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.27/>Bridge-Based Active Domain Adaptation for Aspect Term Extraction</a></strong><br><a href=/people/z/zhuang-chen/>Zhuang Chen</a>
|
<a href=/people/t/tieyun-qian/>Tieyun Qian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--27><div class="card-body p-3 small">As a fine-grained task, the annotation cost of aspect term extraction is extremely high. Recent attempts alleviate this issue using <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> that transfers common knowledge across domains. Since most <a href=https://en.wikipedia.org/wiki/Aspect_(computer_programming)>aspect terms</a> are domain-specific, they can not be transferred directly. Existing methods solve this problem by associating <a href=https://en.wikipedia.org/wiki/Aspect_(linguistics)>aspect terms</a> with pivot words (we call this passive domain adaptation because the transfer of aspect terms relies on the links to pivots). However, all these <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> need either manually labeled pivot words or expensive <a href=https://en.wikipedia.org/wiki/System_resource>computing resources</a> to build associations. In this paper, we propose a novel active domain adaptation method. Our goal is to transfer aspect terms by actively supplementing transferable knowledge. To this end, we construct syntactic bridges by recognizing syntactic roles as pivots instead of as links to pivots. We also build semantic bridges by retrieving transferable semantic prototypes. Extensive experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> significantly outperforms previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.28/>Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks</a></strong><br><a href=/people/x/xiaocui-yang/>Xiaocui Yang</a>
|
<a href=/people/s/shi-feng/>Shi Feng</a>
|
<a href=/people/y/yifei-zhang/>Yifei Zhang</a>
|
<a href=/people/d/daling-wang/>Daling Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--28><div class="card-body p-3 small">With the popularity of <a href=https://en.wikipedia.org/wiki/Smartphone>smartphones</a>, we have witnessed the rapid proliferation of multimodal posts on various <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different <a href=https://en.wikipedia.org/wiki/Modal_logic>modalities</a> to capture <a href=https://en.wikipedia.org/wiki/Hidden-surface_determination>hidden representations</a>. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.30/>PASS : Perturb-and-Select Summarizer for Product Reviews<span class=acl-fixed-case>PASS</span>: Perturb-and-Select Summarizer for Product Reviews</a></strong><br><a href=/people/n/nadav-oved/>Nadav Oved</a>
|
<a href=/people/r/ran-levy/>Ran Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--30><div class="card-body p-3 small">The product reviews summarization task aims to automatically produce a short summary for a set of reviews of a given product. Such summaries are expected to aggregate a range of different opinions in a concise, coherent and informative manner. This challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> gives rise to two shortcomings in existing work. First, summarizers tend to favor generic content that appears in reviews for many different products, resulting in template-like, less informative summaries. Second, as reviewers often disagree on the pros and cons of a given product, summarizers sometimes yield inconsistent, self-contradicting summaries. We propose the PASS system (Perturb-and-Select Summarizer) that employs a large pre-trained Transformer-based model (T5 in our case), which follows a few-shot fine-tuning scheme. A key component of the PASS system relies on applying systematic perturbations to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s input during <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, which allows it to generate multiple different summaries per product. We develop a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for ranking these summaries according to desired criteria, coherence in our case, enabling our <a href=https://en.wikipedia.org/wiki/System>system</a> to almost entirely avoid the problem of <a href=https://en.wikipedia.org/wiki/Self-contradiction>self-contradiction</a>. We compare our system against strong baselines on publicly available datasets, and show that it produces summaries which are more informative, diverse and coherent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.31/>Deep Differential Amplifier for Extractive Summarization</a></strong><br><a href=/people/r/ruipeng-jia/>Ruipeng Jia</a>
|
<a href=/people/y/yanan-cao/>Yanan Cao</a>
|
<a href=/people/f/fang-fang/>Fang Fang</a>
|
<a href=/people/y/yuchen-zhou/>Yuchen Zhou</a>
|
<a href=/people/z/zheng-fang/>Zheng Fang</a>
|
<a href=/people/y/yanbing-liu/>Yanbing Liu</a>
|
<a href=/people/s/shi-wang/>Shi Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--31><div class="card-body p-3 small">For sentence-level extractive summarization, there is a disproportionate ratio of selected and unselected sentences, leading to flatting the summary features when maximizing the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. The imbalanced classification of summarization is inherent, which ca n&#8217;t be addressed by common algorithms easily. In this paper, we conceptualize the single-document extractive summarization as a rebalance problem and present a deep differential amplifier framework. Specifically, we first calculate and amplify the semantic difference between each sentence and all other sentences, and then apply the residual unit as the second item of the <a href=https://en.wikipedia.org/wiki/Differential_amplifier>differential amplifier</a> to deepen the <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>. Finally, to compensate for the imbalance, the corresponding objective loss of minority class is boosted by a weighted cross-entropy. In contrast to previous approaches, this model pays more attention to the pivotal information of one sentence, instead of all the informative context modeling by recurrent or Transformer architecture. We demonstrate experimentally on two benchmark datasets that our <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarizer</a> performs competitively against state-of-the-art methods. Our source code will be available on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.33/>Self-Supervised Multimodal Opinion Summarization</a></strong><br><a href=/people/j/jinbae-im/>Jinbae Im</a>
|
<a href=/people/m/moonki-kim/>Moonki Kim</a>
|
<a href=/people/h/hoyeop-lee/>Hoyeop Lee</a>
|
<a href=/people/h/hyunsouk-cho/>Hyunsouk Cho</a>
|
<a href=/people/s/sehee-chung/>Sehee Chung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--33><div class="card-body p-3 small">Recently, opinion summarization, which is the generation of a summary from multiple reviews, has been conducted in a self-supervised manner by considering a sampled review as a pseudo summary. However, non-text data such as <a href=https://en.wikipedia.org/wiki/Image>image</a> and metadata related to reviews have been considered less often. To use the abundant information contained in non-text data, we propose a self-supervised multimodal opinion summarization framework called MultimodalSum. Our framework obtains a representation of each modality using a separate <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> for each modality, and the text decoder generates a summary. To resolve the inherent heterogeneity of multimodal data, we propose a multimodal training pipeline. We first pretrain the text encoderdecoder based solely on text modality data. Subsequently, we pretrain the non-text modality encoders by considering the pretrained text decoder as a pivot for the homogeneous representation of multimodal data. Finally, to fuse multimodal representations, we train the entire <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> in an end-to-end manner. We demonstrate the superiority of MultimodalSum by conducting experiments on Yelp and Amazon datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.40.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.40/>Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification</a></strong><br><a href=/people/g/george-chrysostomou/>George Chrysostomou</a>
|
<a href=/people/n/nikolaos-aletras/>Nikolaos Aletras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--40><div class="card-body p-3 small">Neural network architectures in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> often use <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> to produce <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distributions</a> over input token representations. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace, 2019 ; Serrano and Smith, 2019 ; Wiegreffe and Pinter, 2019) have showed that it can not generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks. In this paper, we seek to improve the faithfulness of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention-based explanations</a> for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attention-based explanations across two attention mechanisms, five encoders and five text classification datasets without sacrificing predictive performance. Finally, we demonstrate that TaSc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.43" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.43/>Learning Relation Alignment for Calibrated Cross-modal Retrieval</a></strong><br><a href=/people/s/shuhuai-ren/>Shuhuai Ren</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/g/guangxiang-zhao/>Guangxiang Zhao</a>
|
<a href=/people/r/rui-men/>Rui Men</a>
|
<a href=/people/a/an-yang/>An Yang</a>
|
<a href=/people/j/jingren-zhou/>Jingren Zhou</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/h/hongxia-yang/>Hongxia Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--43><div class="card-body p-3 small">Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task. To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the <a href=https://en.wikipedia.org/wiki/Binary_relation>linguistic relation</a> among the words and the <a href=https://en.wikipedia.org/wiki/Binary_relation>visual relation</a> among the regions. The neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> performance and the <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. In this paper, we first propose a novel <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, Intra-modal Self-attention Distance (ISD), to quantify the relation consistency by measuring the <a href=https://en.wikipedia.org/wiki/Semantic_distance>semantic distance</a> between linguistic and visual relations. In response, we present Inter-modal Alignment on Intra-modal Self-attentions (IAIS), a regularized training method to optimize the ISD and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment. The IAIS regularizer boosts the performance of prevailing models on Flickr30k and MS COCO datasets by a considerable margin, which demonstrates the superiority of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.44.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.44" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.44/>KM-BART : Knowledge Enhanced Multimodal BART for Visual Commonsense Generation<span class=acl-fixed-case>KM</span>-<span class=acl-fixed-case>BART</span>: Knowledge Enhanced Multimodal <span class=acl-fixed-case>BART</span> for Visual Commonsense Generation</a></strong><br><a href=/people/y/yiran-xing/>Yiran Xing</a>
|
<a href=/people/z/zai-shi/>Zai Shi</a>
|
<a href=/people/z/zhao-meng/>Zhao Meng</a>
|
<a href=/people/g/gerhard-lakemeyer/>Gerhard Lakemeyer</a>
|
<a href=/people/y/yunpu-ma/>Yunpu Ma</a>
|
<a href=/people/r/roger-wattenhofer/>Roger Wattenhofer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--44><div class="card-body p-3 small">We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We adapt the generative BART architecture (Lewis et al., 2020) to a multimodal model with visual and textual inputs. We further develop novel pretraining tasks to improve the <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> performance on the Visual Commonsense Generation (VCG) task. In particular, our pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs. To the best of our knowledge, we are the first to propose a dedicated <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> for improving <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance on the VCG task. Experimental results show that our model reaches state-of-the-art performance on the VCG task (Park et al., 2020) by applying these novel pretraining tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.45" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.45/>Cascaded Head-colliding Attention</a></strong><br><a href=/people/l/lin-zheng/>Lin Zheng</a>
|
<a href=/people/z/zhiyong-wu/>Zhiyong Wu</a>
|
<a href=/people/l/lingpeng-kong/>Lingpeng Kong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--45><div class="card-body p-3 small">Transformers have advanced the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> on a variety of important <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. At the cornerstone of the Transformer architecture is the multi-head attention (MHA) mechanism which models pairwise interactions between the elements of the sequence. Despite its massive success, the current <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> ignores interactions among different heads, leading to the problem that many of the heads are redundant in practice, which greatly wastes the capacity of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. To improve parameter efficiency, we re-formulate the MHA as a <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable model</a> from a probabilistic perspective. We present cascaded head-colliding attention (CODA) which explicitly models the interactions between attention heads through a hierarchical variational distribution. We conduct extensive experiments and demonstrate that <a href=https://en.wikipedia.org/wiki/CODA>CODA</a> outperforms the transformer baseline, by 0.6 perplexity on Wikitext-103 in <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, and by 0.6 BLEU on WMT14 EN-DE in machine translation, due to its improvements on the parameter efficiency.<b>c</b>ascaded head-c<b>o</b>lli<b>d</b>ing <b>a</b>ttention (CODA) which explicitly models the interactions between attention heads through a hierarchical variational distribution. We conduct extensive experiments and demonstrate that CODA outperforms the transformer baseline, by 0.6 perplexity on Wikitext-103 in language modeling, and by 0.6 BLEU on WMT14 EN-DE in machine translation, due to its improvements on the parameter efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.46" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.46/>Structural Knowledge Distillation : Tractably Distilling Information for Structured Predictor</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/z/zhaohui-yan/>Zhaohui Yan</a>
|
<a href=/people/z/zixia-jia/>Zixia Jia</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--46><div class="card-body p-3 small">Knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a more fine-grained one (the student). The <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> of knowledge distillation is typically the <a href=https://en.wikipedia.org/wiki/Cross-entropy>cross-entropy</a> between the teacher and the student&#8217;s output distributions. However, for structured prediction problems, the output space is exponential in size ; therefore, the cross-entropy objective becomes intractable to compute and optimize directly. In this paper, we derive a factorized form of the knowledge distillation objective for <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a>, which is tractable for many typical choices of the teacher and student models. In particular, we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios : 1) the teacher and student share the same factorization form of the output structure scoring function ; 2) the student factorization produces more fine-grained substructures than the teacher factorization ; 3) the teacher factorization produces more fine-grained substructures than the student factorization ; 4) the factorization forms from the teacher and the student are incompatible.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.47.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--47 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.47 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.47" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.47/>Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks</a></strong><br><a href=/people/r/rabeeh-karimi-mahabadi/>Rabeeh Karimi Mahabadi</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/m/mostafa-dehghani/>Mostafa Dehghani</a>
|
<a href=/people/j/james-henderson/>James Henderson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--47><div class="card-body p-3 small">State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. However, such <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer i d in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> while adding only 0.29 % <a href=https://en.wikipedia.org/wiki/Parameter_(computer_programming)>parameters</a> per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in https://github.com/rabeehk/hyperformer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.49.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.49/>OoMMix : Out-of-manifold Regularization in Contextual Embedding Space for Text Classification<span class=acl-fixed-case>O</span>o<span class=acl-fixed-case>MM</span>ix: Out-of-manifold Regularization in Contextual Embedding Space for Text Classification</a></strong><br><a href=/people/s/seonghyeon-lee/>Seonghyeon Lee</a>
|
<a href=/people/d/dongha-lee/>Dongha Lee</a>
|
<a href=/people/h/hwanjo-yu/>Hwanjo Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--49><div class="card-body p-3 small">Recent studies on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> with pre-trained weights (i.e., BERT) have mainly focused on a low-dimensional subspace, where the <a href=https://en.wikipedia.org/wiki/Embedding>embedding vectors</a> computed from input words (or their contexts) are located. In this work, we propose a new approach, called OoMMix, to finding and regularizing the remainder of the space, referred to as out-of-manifold, which can not be accessed through the words. Specifically, we synthesize the out-of-manifold embeddings based on two <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> obtained from actually-observed words, to utilize them for fine-tuning the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a>. A <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> is trained to detect whether an input embedding is located inside the <a href=https://en.wikipedia.org/wiki/Manifold>manifold</a> or not, and simultaneously, a generator is optimized to produce new embeddings that can be easily identified as out-of-manifold by the <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a>. These two <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> successfully collaborate in a unified and end-to-end manner for regularizing the out-of-manifold. Our extensive evaluation on various text classification benchmarks demonstrates the effectiveness of our approach, as well as its good compatibility with existing data augmentation techniques which aim to enhance the manifold.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.50/>Understanding and Countering Stereotypes : A Computational Approach to the Stereotype Content Model</a></strong><br><a href=/people/k/kathleen-c-fraser/>Kathleen C. Fraser</a>
|
<a href=/people/i/isar-nejadgholi/>Isar Nejadgholi</a>
|
<a href=/people/s/svetlana-kiritchenko/>Svetlana Kiritchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--50><div class="card-body p-3 small">Stereotypical language expresses widely-held beliefs about different social categories. Many <a href=https://en.wikipedia.org/wiki/Stereotype>stereotypes</a> are overtly negative, while others may appear positive on the surface, but still lead to negative consequences. In this work, we present a computational approach to interpreting <a href=https://en.wikipedia.org/wiki/Stereotype>stereotypes</a> in text through the Stereotype Content Model (SCM), a comprehensive <a href=https://en.wikipedia.org/wiki/Causality>causal theory</a> from <a href=https://en.wikipedia.org/wiki/Social_psychology>social psychology</a>. The SCM proposes that <a href=https://en.wikipedia.org/wiki/Stereotype>stereotypes</a> can be understood along two primary dimensions : warmth and <a href=https://en.wikipedia.org/wiki/Competence_(human_resources)>competence</a>. We present a method for defining warmth and competence axes in semantic embedding space, and show that the four quadrants defined by this subspace accurately represent the warmth and competence concepts, according to annotated lexicons. We then apply our computational SCM model to textual stereotype data and show that it compares favourably with survey-based studies in the psychological literature. Furthermore, we explore various <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to counter stereotypical beliefs with <a href=https://en.wikipedia.org/wiki/Anti-stereotype>anti-stereotypes</a>. It is known that countering <a href=https://en.wikipedia.org/wiki/Stereotype>stereotypes</a> with anti-stereotypical examples is one of the most effective ways to reduce biased thinking, yet the problem of generating anti-stereotypes has not been previously studied. Thus, a better understanding of how to generate realistic and effective anti-stereotypes can contribute to addressing pressing societal concerns of <a href=https://en.wikipedia.org/wiki/Stereotype>stereotyping</a>, <a href=https://en.wikipedia.org/wiki/Prejudice>prejudice</a>, and <a href=https://en.wikipedia.org/wiki/Discrimination>discrimination</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.51.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.51/>Structurizing Misinformation Stories via Rationalizing Fact-Checks</a></strong><br><a href=/people/s/shan-jiang/>Shan Jiang</a>
|
<a href=/people/c/christo-wilson/>Christo Wilson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--51><div class="card-body p-3 small">Misinformation has recently become a well-documented matter of public concern. Existing studies on this topic have hitherto adopted a coarse concept of <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a>, which incorporates a broad spectrum of story types ranging from <a href=https://en.wikipedia.org/wiki/List_of_political_conspiracies>political conspiracies</a> to misinterpreted pranks. This paper aims to structurize these misinformation stories by leveraging fact-check articles. Our intuition is that key phrases in a fact-check article that identify the <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation type(s)</a> (e.g., doctored images, urban legends) also act as rationales that determine the verdict of the fact-check (e.g., false). We experiment on rationalized models with <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> as weak supervision to extract these phrases as <a href=https://en.wikipedia.org/wiki/Rationale>rationales</a>, and then cluster semantically similar rationales to summarize prevalent misinformation types. Using archived fact-checks from <a href=https://en.wikipedia.org/wiki/Snopes>Snopes.com</a>, we identify ten types of misinformation stories. We discuss how these types have evolved over the last ten years and compare their prevalence between the 2016/2020 US presidential elections and the <a href=https://en.wikipedia.org/wiki/Influenza_A_virus_subtype_H1N1>H1N1 / COVID-19 pandemics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.57/>Learning from Perturbations : Diverse and Informative Dialogue Generation with Inverse Adversarial Training</a></strong><br><a href=/people/w/wangchunshu-zhou/>Wangchunshu Zhou</a>
|
<a href=/people/q/qifei-li/>Qifei Li</a>
|
<a href=/people/c/chenle-li/>Chenle Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--57><div class="card-body p-3 small">In this paper, we propose Inverse Adversarial Training (IAT) algorithm for training neural dialogue systems to avoid generic responses and model dialogue history better. In contrast to standard adversarial training algorithms, IAT encourages the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to be sensitive to the perturbation in the dialogue history and therefore learning from perturbations. By giving higher rewards for responses whose output probability reduces more significantly when dialogue history is perturbed, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is encouraged to generate more diverse and consistent responses. By penalizing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> when generating the same response given perturbed dialogue history, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is forced to better capture dialogue history and generate more informative responses. Experimental results on two benchmark datasets show that our approach can better model dialogue history and generate more diverse and consistent responses. In addition, we point out a problem of the widely used maximum mutual information (MMI) based methods for improving the diversity of dialogue response generation models and demonstrate it empirically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.59" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.59/>CitationIE : Leveraging the <a href=https://en.wikipedia.org/wiki/Citation_graph>Citation Graph</a> for Scientific Information Extraction<span class=acl-fixed-case>C</span>itation<span class=acl-fixed-case>IE</span>: Leveraging the Citation Graph for Scientific Information Extraction</a></strong><br><a href=/people/v/vijay-viswanathan/>Vijay Viswanathan</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--59><div class="card-body p-3 small">Automatically extracting key information from scientific documents has the potential to help scientists work more efficiently and accelerate the pace of scientific progress. Prior work has considered extracting document-level entity clusters and relations end-to-end from raw scientific text, which can improve literature search and help identify methods and materials for a given problem. Despite the importance of this task, most existing works on scientific information extraction (SciIE) consider <a href=https://en.wikipedia.org/wiki/Information_extraction>extraction</a> solely based on the content of an individual paper, without considering the paper&#8217;s place in the broader literature. In contrast to prior work, we augment our text representations by leveraging a complementary source of document context : the citation graph of referential links between citing and cited papers. On a test set of English-language scientific documents, we show that simple ways of utilizing the structure and content of the <a href=https://en.wikipedia.org/wiki/Citation_graph>citation graph</a> can each lead to significant gains in different scientific information extraction tasks. When these tasks are combined, we observe a sizable improvement in end-to-end information extraction over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>, suggesting the potential for future work along this direction. We release <a href=https://en.wikipedia.org/wiki/Programming_tool>software tools</a> to facilitate citation-aware SciIE development.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.60" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.60/>From Discourse to Narrative : Knowledge Projection for Event Relation Extraction</a></strong><br><a href=/people/j/jialong-tang/>Jialong Tang</a>
|
<a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/m/meng-liao/>Meng Liao</a>
|
<a href=/people/y/yaojie-lu/>Yaojie Lu</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/w/weijian-xie/>Weijian Xie</a>
|
<a href=/people/j/jin-xu/>Jin Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--60><div class="card-body p-3 small">Current event-centric knowledge graphs highly rely on explicit connectives to mine relations between events. Unfortunately, due to the sparsity of connectives, these methods severely undermine the coverage of EventKGs. The lack of high-quality labelled corpora further exacerbates that problem. In this paper, we propose a knowledge projection paradigm for event relation extraction : projecting discourse knowledge to narratives by exploiting the commonalities between them. Specifically, we propose Multi-tier Knowledge Projection Network (MKPNet), which can leverage multi-tier discourse knowledge effectively for event relation extraction. In this way, the labelled data requirement is significantly reduced, and implicit event relations can be effectively extracted. Intrinsic experimental results show that MKPNet achieves the new state-of-the-art performance and extrinsic experimental results verify the value of the extracted event relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.61" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.61/>AdvPicker : Effectively Leveraging Unlabeled Data via Adversarial Discriminator for Cross-Lingual NER<span class=acl-fixed-case>A</span>dv<span class=acl-fixed-case>P</span>icker: <span class=acl-fixed-case>E</span>ffectively <span class=acl-fixed-case>L</span>everaging <span class=acl-fixed-case>U</span>nlabeled <span class=acl-fixed-case>D</span>ata via <span class=acl-fixed-case>A</span>dversarial <span class=acl-fixed-case>D</span>iscriminator for <span class=acl-fixed-case>C</span>ross-<span class=acl-fixed-case>L</span>ingual <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/w/weile-chen/>Weile Chen</a>
|
<a href=/people/h/huiqiang-jiang/>Huiqiang Jiang</a>
|
<a href=/people/q/qianhui-wu/>Qianhui Wu</a>
|
<a href=/people/b/borje-karlsson/>Börje Karlsson</a>
|
<a href=/people/y/yi-guan/>Yi Guan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--61><div class="card-body p-3 small">Neural methods have been shown to achieve high performance in Named Entity Recognition (NER), but rely on costly high-quality labeled data for training, which is not always available across languages. While previous works have shown that unlabeled data in a target language can be used to improve cross-lingual model performance, we propose a novel adversarial approach (AdvPicker) to better leverage such <a href=https://en.wikipedia.org/wiki/Data>data</a> and further improve results. We design an adversarial learning framework in which an encoder learns entity domain knowledge from labeled source-language data and better shared features are captured via adversarial training-where a discriminator selects less language-dependent target-language data via similarity to the source language. Experimental results on standard benchmark datasets well demonstrate that the proposed method benefits strongly from this data selection process and outperforms existing state-of-the-art methods ; without requiring any additional external resources (e.g., gazetteers or via machine translation).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.62" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.62/>Compare to The Knowledge : Graph Neural Fake News Detection with External Knowledge</a></strong><br><a href=/people/l/linmei-hu/>Linmei Hu</a>
|
<a href=/people/t/tianchi-yang/>Tianchi Yang</a>
|
<a href=/people/l/luhao-zhang/>Luhao Zhang</a>
|
<a href=/people/w/wanjun-zhong/>Wanjun Zhong</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/c/chuan-shi/>Chuan Shi</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--62><div class="card-body p-3 small">Nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important. Most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted. In this paper, we propose a novel end-to-end graph neural model called CompareNet, which compares the news to the knowledge base (KB) through entities for fake news detection. Considering that <a href=https://en.wikipedia.org/wiki/Fake_news>fake news detection</a> is correlated with topics, we also incorporate <a href=https://en.wikipedia.org/wiki/Topic_and_comment>topics</a> to enrich the news representation. Specifically, we first construct a directed heterogeneous document graph for each news incorporating topics and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a>. Based on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content. The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed entity comparison network, to capture the consistency between the news content and KB. Finally, the topic-enriched news representation combining the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity comparison features</a> is fed into a fake news classifier. Experimental results on two benchmark datasets demonstrate that <a href=https://en.wikipedia.org/wiki/CompareNet>CompareNet</a> significantly outperforms state-of-the-art methods.<i>directed heterogeneous document graph</i> for each news incorporating topics and entities. Based on the graph, we develop a <i>heterogeneous graph attention network</i> for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content. The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed <i>entity comparison network</i>, to capture the consistency between the news content and KB. Finally, the topic-enriched news representation combining the entity comparison features is fed into a fake news classifier. Experimental results on two benchmark datasets demonstrate that CompareNet significantly outperforms state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.63" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.63/>Discontinuous Named Entity Recognition as Maximal Clique Discovery</a></strong><br><a href=/people/y/yucheng-wang/>Yucheng Wang</a>
|
<a href=/people/b/bowen-yu/>Bowen Yu</a>
|
<a href=/people/h/hongsong-zhu/>Hongsong Zhu</a>
|
<a href=/people/t/tingwen-liu/>Tingwen Liu</a>
|
<a href=/people/n/nan-yu/>Nan Yu</a>
|
<a href=/people/l/limin-sun/>Limin Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--63><div class="card-body p-3 small">Named entity recognition (NER) remains challenging when entity mentions can be discontinuous. Existing methods break the recognition process into several <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>sequential steps</a>. In training, they predict conditioned on the golden intermediate results, while at <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> relying on the model output of the previous steps, which introduces <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a>. To solve this problem, we first construct a segment graph for each sentence, in which each <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>node</a> denotes a segment (a continuous entity on its own, or a part of discontinuous entities), and an <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edge</a> links two nodes that belong to the same entity. The <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> named Mac. Then discontinuous NER can be reformulated as a non-parametric process of discovering maximal cliques in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> and concatenating the spans in each clique. Experiments on three benchmarks show that our method outperforms the state-of-the-art (SOTA) results, with up to 3.5 percentage points improvement on F1, and achieves 5x speedup over the SOTA model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.65" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.65/>Do Context-Aware Translation Models Pay the Right Attention?</a></strong><br><a href=/people/k/kayo-yin/>Kayo Yin</a>
|
<a href=/people/p/patrick-fernandes/>Patrick Fernandes</a>
|
<a href=/people/d/danish-pruthi/>Danish Pruthi</a>
|
<a href=/people/a/aditi-chaudhary/>Aditi Chaudhary</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--65><div class="card-body p-3 small">Context-aware machine translation models are designed to leverage <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>, but often fail to do so. As a result, they inaccurately disambiguate <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a> and <a href=https://en.wikipedia.org/wiki/Polysemy>polysemous words</a> that require <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> for resolution. In this paper, we ask several questions : What contexts do human translators use to resolve ambiguous words? Are <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14 K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model&#8217;s attention scores</a> and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.66" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.66/>Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data<span class=acl-fixed-case>NMT</span> Models to Translate Low-resource Related Languages without Parallel Data</a></strong><br><a href=/people/w/wei-jen-ko/>Wei-Jen Ko</a>
|
<a href=/people/a/ahmed-el-kishky/>Ahmed El-Kishky</a>
|
<a href=/people/a/adithya-renduchintala/>Adithya Renduchintala</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/n/naman-goyal/>Naman Goyal</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--66><div class="card-body p-3 small">The scarcity of parallel data is a major obstacle for training high-quality <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages ; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves <a href=https://en.wikipedia.org/wiki/Translation>translation</a> into low-resource language compared to other <a href=https://en.wikipedia.org/wiki/Translation>translation baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.68/>Multilingual Speech Translation from Efficient Finetuning of Pretrained Models</a></strong><br><a href=/people/x/xian-li/>Xian Li</a>
|
<a href=/people/c/changhan-wang/>Changhan Wang</a>
|
<a href=/people/y/yun-tang/>Yun Tang</a>
|
<a href=/people/c/chau-tran/>Chau Tran</a>
|
<a href=/people/y/yuqing-tang/>Yuqing Tang</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a>
|
<a href=/people/a/alexei-baevski/>Alexei Baevski</a>
|
<a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--68><div class="card-body p-3 small">We present a simple yet effective approach to build multilingual speech-to-text (ST) translation through efficient <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from a pretrained speech encoder and text decoder. Our key finding is that a minimalistic LNA (LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning 10 50 % of the pretrained parameters. This effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic modeling, and mBART for multilingual text generation. This sets a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for 36 translation directions (and surpassing cascaded ST for 26 of them) on the large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on average for En-X directions and +6.7 BLEU for X-En directions). Our approach demonstrates strong zero-shot performance in a many-to-many multilingual model (+5.6 BLEU on average across 28 non-English directions), making it an appealing approach for attaining high-quality <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a> with improved parameter and data efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.70" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.70/>What Context Features Can Transformer Language Models Use?</a></strong><br><a href=/people/j/joe-oconnor/>Joe O’Connor</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--70><div class="card-body p-3 small">Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate <a href=https://en.wikipedia.org/wiki/Prediction>model prediction</a>? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a>. In both mid- and long-range contexts, we find that several extremely destructive context manipulationsincluding shuffling word order within sentences and deleting all words other than nounsremove less than 15 % of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.76/>A Targeted Assessment of Incremental Processing in Neural Language Models and Humans</a></strong><br><a href=/people/e/ethan-wilcox/>Ethan Wilcox</a>
|
<a href=/people/p/pranali-vani/>Pranali Vani</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--76><div class="card-body p-3 small">We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena. Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task. We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set sizes. We find that across many phenomena, both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model &#8216;accuracy&#8217; scores a la Marvin and Linzen (2018) about equal. However, although <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> outputs match humans in direction, we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences. Specifically, when <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> encounter syntactic violations they fail to accurately predict the longer reading times observed in the human data. These results call into question whether contemporary language models are approaching human-like performance for sensitivity to syntactic violations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.77" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.77/>The Possible, the Plausible, and the Desirable : Event-Based Modality Detection for <a href=https://en.wikipedia.org/wiki/Language_processing>Language Processing</a></a></strong><br><a href=/people/v/valentina-pyatkin/>Valentina Pyatkin</a>
|
<a href=/people/s/shoval-sadde/>Shoval Sadde</a>
|
<a href=/people/a/aynat-rubinstein/>Aynat Rubinstein</a>
|
<a href=/people/p/paul-portner/>Paul Portner</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--77><div class="card-body p-3 small">Modality is the linguistic ability to describe vents with added information such as how desirable, plausible, or feasible they are. Modality is important for many NLP downstream tasks such as the detection of hedging, <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a>, <a href=https://en.wikipedia.org/wiki/Speculation>speculation</a>, and more. Previous studies that address modality detection in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> often restrict modal expressions to a closed syntactic class, and the modal sense labels are vastly different across different studies, lacking an accepted standard. Furthermore, these <a href=https://en.wikipedia.org/wiki/Sense>senses</a> are often analyzed independently of the events that they modify. This work builds on the theoretical foundations of the Georgetown Gradable Modal Expressions (GME) work by Rubinstein et al. (2013) to propose an event-based modality detection task where modal expressions can be words of any syntactic class and sense labels are drawn from a comprehensive taxonomy which harmonizes the modal concepts contributed by the different studies. We present experiments on the GME corpus aiming to detect and classify fine-grained modal concepts and associate them with their modified events. We show that detecting and classifying modal expressions is not only feasible, it also improves the detection of modal events in their own right.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block style=text-decoration:line-through><strong><a class=align-middle href=/2021.acl-long.79/>Prosodic segmentation for parsing spoken dialogue</a></strong><br><a href=/people/e/elizabeth-nielsen/>Elizabeth Nielsen</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--79><div class="card-body p-3 small">Parsing spoken dialogue poses unique difficulties, including <a href=https://en.wikipedia.org/wiki/Disfluency>disfluencies</a> and unmarked boundaries between sentence-like units. Previous work has shown that <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosody</a> can help with parsing disfluent speech (Tran et al. 2018), but has assumed that the input to the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is already segmented into sentence-like units (SUs), which is n&#8217;t true in existing speech applications. We investigate how <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosody</a> affects a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> that receives an entire dialogue turn as input (a turn-based model), instead of gold standard pre-segmented SUs (an SU-based model). In experiments on the English Switchboard corpus, we find that when using transcripts alone, the turn-based model has trouble segmenting SUs, leading to worse parse performance than the SU-based model. However, <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosody</a> can effectively replace gold standard SU boundaries : with <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosody</a>, the turn-based model performs as well as the SU-based model (91.38 vs. 91.06 F1 score, respectively), despite performing two tasks (SU segmentation and parsing) rather than one (parsing alone). Analysis shows that pitch and intensity features are the most important for this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, since they allow the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to correctly distinguish an SU boundary from a <a href=https://en.wikipedia.org/wiki/Speech_disfluency>speech disfluency</a> a distinction that the model otherwise struggles to make.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.82" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.82/>Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network</a></strong><br><a href=/people/j/justin-lovelace/>Justin Lovelace</a>
|
<a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/s/shikhar-vashishth/>Shikhar Vashishth</a>
|
<a href=/people/j/jill-fain-lehman/>Jill Fain Lehman</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rosé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--82><div class="card-body p-3 small">Knowledge Graph (KG) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs. We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed. We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG completion methods in this challenging setting. We find that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance improvements stem primarily from its robustness to sparsity. We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities. This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.83.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.83" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.83/>A DQN-based Approach to Finding Precise Evidences for Fact Verification<span class=acl-fixed-case>DQN</span>-based Approach to Finding Precise Evidences for Fact Verification</a></strong><br><a href=/people/h/hai-wan/>Hai Wan</a>
|
<a href=/people/h/haicheng-chen/>Haicheng Chen</a>
|
<a href=/people/j/jianfeng-du/>Jianfeng Du</a>
|
<a href=/people/w/weilin-luo/>Weilin Luo</a>
|
<a href=/people/r/rongzhen-ye/>Rongzhen Ye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--83><div class="card-body p-3 small">Computing precise evidences, namely minimal sets of sentences that support or refute a given claim, rather than larger evidences is crucial in fact verification (FV), since larger evidences may contain conflicting pieces some of which support the claim while the other refute, thereby misleading FV. Despite being important, precise evidences are rarely studied by existing methods for FV. It is challenging to find precise evidences due to a large <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> with lots of <a href=https://en.wikipedia.org/wiki/Local_optimum>local optimums</a>. Inspired by the strong exploration ability of the deep Q-learning network (DQN), we propose a DQN-based approach to retrieval of precise evidences. In addition, to tackle the label bias on Q-values computed by <a href=https://en.wikipedia.org/wiki/DQN>DQN</a>, we design a post-processing strategy which seeks best thresholds for determining the true labels of computed evidences. Experimental results confirm the effectiveness of <a href=https://en.wikipedia.org/wiki/DQN>DQN</a> in computing precise evidences and demonstrate improvements in achieving accurate claim verification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.85" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.85/>Unsupervised Out-of-Domain Detection via Pre-trained Transformers</a></strong><br><a href=/people/k/keyang-xu/>Keyang Xu</a>
|
<a href=/people/t/tongzheng-ren/>Tongzheng Ren</a>
|
<a href=/people/s/shikun-zhang/>Shikun Zhang</a>
|
<a href=/people/y/yihao-feng/>Yihao Feng</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--85><div class="card-body p-3 small">Deployed real-world machine learning applications are often subject to uncontrolled and even potentially malicious inputs. Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety issues. Prior studies on out-of-domain detection require in-domain task labels and are limited to supervised classification scenarios. Our work tackles the problem of detecting out-of-domain samples with only <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised in-domain data</a>. We utilize the latent representations of pre-trained transformers and propose a simple yet effective method to transform <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> across all layers to construct out-of-domain detectors efficiently. Two domain-specific fine-tuning approaches are further proposed to boost detection accuracy. Our empirical evaluations of related methods on two datasets validate that our method greatly improves out-of-domain detection ability in a more general scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.87/>Selecting Informative Contexts Improves Language Model Fine-tuning</a></strong><br><a href=/people/r/richard-antonello/>Richard Antonello</a>
|
<a href=/people/n/nicole-beckage/>Nicole Beckage</a>
|
<a href=/people/j/javier-turek/>Javier Turek</a>
|
<a href=/people/a/alexander-huth/>Alexander Huth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--87><div class="card-body p-3 small">Language model fine-tuning is essential for modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, but is computationally expensive and time-consuming. Further, the effectiveness of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is limited by the inclusion of training examples that negatively affect performance. Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning. We define the <a href=https://en.wikipedia.org/wiki/Information_gain>information gain</a> of an example as the improvement on a <a href=https://en.wikipedia.org/wiki/Validity_(statistics)>validation metric</a> after training on that example. A secondary learner is then trained to approximate this quantity. During <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, this <a href=https://en.wikipedia.org/wiki/Learning>learner</a> selects informative examples and skips uninformative ones. We show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> has consistent improvement across <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning tasks</a>, and language model architectures. For example, we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. We present statistical evidence that offers insight into the improvements of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> over standard <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. The generality of our method leads us to propose a new paradigm for language model fine-tuning we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, thereby improving the performance and reducing the overall energy footprint of language model fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.88/>Explainable Prediction of Text Complexity : The Missing Preliminaries for Text Simplification</a></strong><br><a href=/people/c/cristina-garbacea/>Cristina Garbacea</a>
|
<a href=/people/m/mengtian-guo/>Mengtian Guo</a>
|
<a href=/people/s/samuel-carton/>Samuel Carton</a>
|
<a href=/people/q/qiaozhu-mei/>Qiaozhu Mei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--88><div class="card-body p-3 small">Text simplification reduces the <a href=https://en.wikipedia.org/wiki/Language_complexity>language complexity</a> of professional content for accessibility purposes. End-to-end neural network models have been widely adopted to directly generate the simplified version of input text, usually functioning as a blackbox. We show that text simplification can be decomposed into a compact pipeline of tasks to ensure the transparency and explainability of the process. The first two steps in this <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> are often neglected : 1) to predict whether a given piece of text needs to be simplified, and 2) if yes, to identify complex parts of the text. The two <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> can be solved separately using either lexical or deep learning methods, or solved jointly. Simply applying explainable complexity prediction as a preliminary step, the out-of-sample text simplification performance of the state-of-the-art, black-box simplification models can be improved by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.89/>Multi-Task Retrieval for Knowledge-Intensive Tasks</a></strong><br><a href=/people/j/jean-maillard/>Jean Maillard</a>
|
<a href=/people/v/vladimir-karpukhin/>Vladimir Karpukhin</a>
|
<a href=/people/f/fabio-petroni/>Fabio Petroni</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a>
|
<a href=/people/b/barlas-oguz/>Barlas Oguz</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a>
|
<a href=/people/g/gargi-ghosh/>Gargi Ghosh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--89><div class="card-body p-3 small">Retrieving relevant contexts from a large corpus is a crucial step for tasks such as open-domain question answering and <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a>. Although neural retrieval outperforms traditional methods like tf-idf and <a href=https://en.wikipedia.org/wiki/BM25>BM25</a>, its performance degrades considerably when applied to out-of-domain data. Driven by the question of whether a neural retrieval model can be _ universal _ and perform robustly on a wide variety of problems, we propose a multi-task trained model. Our approach not only outperforms previous methods in the few-shot setting, but also rivals specialised neural retrievers, even when in-domain training data is abundant. With the help of our retriever, we improve existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for downstream tasks and closely match or improve the <a href=https://en.wikipedia.org/wiki/State_(computer_science)>state</a> of the <a href=https://en.wikipedia.org/wiki/Art>art</a> on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>multiple benchmarks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.91" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.91/>Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation</a></strong><br><a href=/people/e/elena-voita/>Elena Voita</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--91><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context : the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP). Its underlying &#8216;conservation principle&#8217; makes relevance propagation unique : differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token&#8217;s influence. We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with more data tend to rely on source information more and to have more sharp token contributions ; the training process is non-monotonic with several stages of different nature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.92/>Comparing Test Sets with Item Response Theory</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/p/phu-mon-htut/>Phu Mon Htut</a>
|
<a href=/people/w/william-huang/>William Huang</a>
|
<a href=/people/d/dhara-mungra/>Dhara Mungra</a>
|
<a href=/people/r/richard-yuanzhe-pang/>Richard Yuanzhe Pang</a>
|
<a href=/people/j/jason-phang/>Jason Phang</a>
|
<a href=/people/h/haokun-liu/>Haokun Liu</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/s/samuel-bowman/>Samuel R. Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--92><div class="card-body p-3 small">Recent years have seen numerous NLP datasets introduced to evaluate the performance of fine-tuned models on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding tasks</a>. Recent results from large pretrained models, though, show that many of these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are largely saturated and unlikely to be able to detect further progress. What kind of datasets are still effective at discriminating among strong models, and what kind of datasets should we expect to be able to detect future improvements? To measure this uniformly across datasets, we draw on <a href=https://en.wikipedia.org/wiki/Item_response_theory>Item Response Theory</a> and evaluate 29 <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> using predictions from 18 pretrained Transformer models on individual test examples. We find that Quoref, HellaSwag, and MC-TACO are best suited for distinguishing among state-of-the-art models, while SNLI, MNLI, and CommitmentBank seem to be saturated for current strong models. We also observe span selection task format, which is used for QA datasets like <a href=https://en.wikipedia.org/wiki/QAMR>QAMR</a> or SQuAD2.0, is effective in differentiating between strong and weak models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.94" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.94/>More Identifiable yet Equally Performant Transformers for Text Classification</a></strong><br><a href=/people/r/rishabh-bhardwaj/>Rishabh Bhardwaj</a>
|
<a href=/people/n/navonil-majumder/>Navonil Majumder</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--94><div class="card-body p-3 small">Interpretability is an important aspect of the trustworthiness of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s predictions. Transformer&#8217;s predictions are widely explained by the attention weights, i.e., a <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> generated at its self-attention unit (head). Current empirical studies provide shreds of evidence that attention weights are not explanations by proving that they are not unique. A recent study showed theoretical justifications to this observation by proving the non-identifiability of attention weights. For a given input to a <a href=https://en.wikipedia.org/wiki/Head>head</a> and its output, if the attention weights generated in it are unique, we call the weights identifiable. In this work, we provide deeper theoretical analysis and empirical observations on the identifiability of attention weights. Ignored in the previous works, we find the attention weights are more identifiable than we currently perceive by uncovering the hidden role of the key vector. However, the <a href=https://en.wikipedia.org/wiki/Weight>weights</a> are still prone to be non-unique attentions that make them unfit for interpretation. To tackle this issue, we provide a variant of the encoder layer that decouples the relationship between key and value vector and provides identifiable weights up to the desired length of the input. We prove the applicability of such variations by providing empirical justifications on varied text classification tasks. The <a href=https://en.wikipedia.org/wiki/Implementation>implementations</a> are available at https://github.com/declare-lab/identifiable-transformers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.95" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.95/>AugNLG : Few-shot Natural Language Generation using Self-trained Data Augmentation<span class=acl-fixed-case>A</span>ug<span class=acl-fixed-case>NLG</span>: Few-shot Natural Language Generation using Self-trained Data Augmentation</a></strong><br><a href=/people/x/xinnuo-xu/>Xinnuo Xu</a>
|
<a href=/people/g/guoyin-wang/>Guoyin Wang</a>
|
<a href=/people/y/young-bum-kim/>Young-Bum Kim</a>
|
<a href=/people/s/sungjin-lee/>Sungjin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--95><div class="card-body p-3 small">Natural Language Generation (NLG) is a key component in a task-oriented dialogue system, which converts the structured meaning representation (MR) to the <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. For large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots, neither template-based approaches nor model-based approaches are scalable. Recently, neural NLGs started leveraging <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and showed promising results in few-shot settings. This paper proposes AugNLG, a novel data augmentation approach that combines a self-trained neural retrieval model with a few-shot learned NLU model, to automatically create MR-to-Text data from open-domain texts. The proposed system mostly outperforms the state-of-the-art methods on the FewshotWOZ data in both BLEU and Slot Error Rate. We further confirm improved results on the FewshotSGD data and provide comprehensive analysis results on key components of our system. Our code and data are available at https://github.com/XinnuoXu/AugNLG.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.97.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--97 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.97 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.97" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.97/>A Dataset and Baselines for Multilingual Reply Suggestion</a></strong><br><a href=/people/m/mozhi-zhang/>Mozhi Zhang</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/b/budhaditya-deb/>Budhaditya Deb</a>
|
<a href=/people/g/guoqing-zheng/>Guoqing Zheng</a>
|
<a href=/people/m/milad-shokouhi/>Milad Shokouhi</a>
|
<a href=/people/a/ahmed-hassan/>Ahmed Hassan Awadallah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--97><div class="card-body p-3 small">Reply suggestion models help users process emails and chats faster. Previous work only studies English reply suggestion. Instead, we present MRS, a multilingual reply suggestion dataset with ten languages. MRS can be used to compare two families of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> : 1) retrieval models that select the reply from a fixed set and 2) generation models that produce the reply from scratch. Therefore, MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence labeling tasks. We build a generation model and a <a href=https://en.wikipedia.org/wiki/Information_retrieval>retrieval model</a> as baselines for MRS. The two models have different strengths in the monolingual setting, and they require different strategies to generalize across languages. MRS is publicly available at https://github.com/zhangmozhi/mrs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.99/>Align Voting Behavior with Public Statements for Legislator Representation Learning</a></strong><br><a href=/people/x/xinyi-mou/>Xinyi Mou</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/l/lei-chen/>Lei Chen</a>
|
<a href=/people/s/shangyi-ning/>Shangyi Ning</a>
|
<a href=/people/y/yancheng-he/>Yancheng He</a>
|
<a href=/people/c/changjian-jiang/>Changjian Jiang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--99><div class="card-body p-3 small">Ideology of legislators is typically estimated by ideal point models from <a href=https://en.wikipedia.org/wiki/Voting_methods_in_deliberative_assemblies>historical records of votes</a>. It represents legislators and legislation as points in a latent space and shows promising results for modeling <a href=https://en.wikipedia.org/wiki/Voting_behavior>voting behavior</a>. However, it fails to capture more specific attitudes of legislators toward emerging issues and is unable to model newly-elected legislators without voting histories. In order to mitigate these two problems, we explore to incorporate both <a href=https://en.wikipedia.org/wiki/Voting_behavior>voting behavior</a> and public statements on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> to jointly model <a href=https://en.wikipedia.org/wiki/Legislator>legislators</a>. In addition, we propose a novel task, namely hashtag usage prediction to model the ideology of legislators on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. In practice, we construct a heterogeneous graph for the legislative context and use relational graph neural networks to learn the representation of legislators with the guidance of historical records of their voting and hashtag usage. Experiment results indicate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields significant improvements for the task of <a href=https://en.wikipedia.org/wiki/Voting_methods_in_deliberative_assemblies>roll call vote prediction</a>. Further analysis further demonstrates that legislator representation we learned captures nuances in statements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.103/>Attention Calibration for Transformer in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yu-lu/>Yu Lu</a>
|
<a href=/people/j/jiali-zeng/>Jiali Zeng</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/s/shuangzhi-wu/>Shuangzhi Wu</a>
|
<a href=/people/m/mu-li/>Mu Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--103><div class="card-body p-3 small">Attention mechanisms have achieved substantial improvements in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> by dynamically selecting relevant inputs for different predictions. However, recent studies have questioned the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>&#8217; capability for discovering decisive inputs. In this paper, we propose to calibrate the attention weights by introducing a mask perturbation model that automatically evaluates each input&#8217;s contribution to the model outputs. We increase the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention weights</a> assigned to the indispensable tokens, whose removal leads to a dramatic performance decrease. The extensive experiments on the Transformer-based translation have demonstrated the effectiveness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We further find that the calibrated attention weights are more uniform at lower layers to collect multiple information while more concentrated on the specific inputs at higher layers. Detailed analyses also show a great need for calibration in the attention weights with high entropy where the model is unconfident about its decision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--107 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.107/>Towards <a href=https://en.wikipedia.org/wiki/Argument_mining>Argument Mining</a> for Social Good : A Survey</a></strong><br><a href=/people/e/eva-maria-vecchi/>Eva Maria Vecchi</a>
|
<a href=/people/n/neele-falk/>Neele Falk</a>
|
<a href=/people/i/iman-jundi/>Iman Jundi</a>
|
<a href=/people/g/gabriella-lapesa/>Gabriella Lapesa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--107><div class="card-body p-3 small">This survey builds an interdisciplinary picture of Argument Mining (AM), with a strong focus on its potential to address issues related to Social and Political Science. More specifically, we focus on AM challenges related to its applications to <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and in the multilingual domain, and then proceed to the widely debated notion of argument quality. We propose a novel definition of argument quality which is integrated with that of deliberative quality from the Social Science literature. Under our definition, the quality of a contribution needs to be assessed at multiple levels : the contribution itself, its preceding context, and the consequential effect on the development of the upcoming discourse. The latter has not received the deserved attention within the community. We finally define an application of AM for Social Good : (semi-)automatic moderation, a highly integrative application which (a) represents a challenging testbed for the integrated notion of quality we advocate, (b) allows the empirical quantification of argument / deliberative quality to benefit from the developments in other NLP fields (i.e. hate speech detection, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a>, debiasing), and (c) has a clearly beneficial potential at the level of its societal thanks to its real-world application (even if extremely ambitious).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--112 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.112/>Factorising Meaning and Form for Intent-Preserving Paraphrasing</a></strong><br><a href=/people/t/tom-hosking/>Tom Hosking</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--112><div class="card-body p-3 small">We propose a method for generating paraphrases of English questions that retain the original intent but use a different surface form. Our model combines a careful choice of training objective with a principled information bottleneck, to induce a latent encoding space that disentangles meaning and form. We train an encoder-decoder model to reconstruct a question from a <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrase</a> with the same meaning and an exemplar with the same surface form, leading to separated encoding spaces. We use a Vector-Quantized Variational Autoencoder to represent the <a href=https://en.wikipedia.org/wiki/Surface_(topology)>surface form</a> as a set of discrete latent variables, allowing us to use a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to select a different <a href=https://en.wikipedia.org/wiki/Surface_(topology)>surface form</a> at test time. Crucially, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> does not require access to an external source of target exemplars. Extensive experiments and a human evaluation show that we are able to generate <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> with a better tradeoff between semantic preservation and syntactic novelty compared to previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--113 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.113" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.113/>AggGen : Ordering and Aggregating while Generating<span class=acl-fixed-case>A</span>gg<span class=acl-fixed-case>G</span>en: Ordering and Aggregating while Generating</a></strong><br><a href=/people/x/xinnuo-xu/>Xinnuo Xu</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/v/verena-rieser/>Verena Rieser</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--113><div class="card-body p-3 small">We present AggGen (pronounced &#8216;again&#8217;) a data-to-text model which re-introduces two explicit sentence planning stages into neural data-to-text systems : input ordering and input aggregation. In contrast to previous work using sentence planning, our model is still end-to-end : AggGen performs sentence planning at the same time as generating text by learning latent alignments (via semantic facts) between input representation and target text. Experiments on the WebNLG and E2E challenge data show that by using fact-based alignments our approach is more interpretable, expressive, robust to noise, and easier to control, while retaining the advantages of end-to-end systems in terms of fluency. Our code is available at https://github.com/XinnuoXu/AggGen.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.116.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.116/>BACO : A Background Knowledge- and Content-Based Framework for Citing Sentence Generation<span class=acl-fixed-case>BACO</span>: A Background Knowledge- and Content-Based Framework for Citing Sentence Generation</a></strong><br><a href=/people/y/yubin-ge/>Yubin Ge</a>
|
<a href=/people/l/ly-dinh/>Ly Dinh</a>
|
<a href=/people/x/xiaofeng-liu/>Xiaofeng Liu</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a>
|
<a href=/people/z/ziyao-lu/>Ziyao Lu</a>
|
<a href=/people/a/ante-wang/>Ante Wang</a>
|
<a href=/people/j/jana-diesner/>Jana Diesner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--116><div class="card-body p-3 small">In this paper, we focus on the problem of citing sentence generation, which entails generating a short text to capture the salient information in a cited paper and the connection between the citing and cited paper. We present BACO, a BAckground knowledge- and COntent-based framework for citing sentence generation, which considers two types of information : (1) background knowledge by leveraging structural information from a citation network ; and (2) content, which represents in-depth information about what to cite and why to cite. First, a <a href=https://en.wikipedia.org/wiki/Citation_network>citation network</a> is encoded to provide background knowledge. Second, we apply salience estimation to identify what to cite by estimating the importance of sentences in the cited paper. During the decoding stage, both types of information are combined to facilitate the text generation, and then we conduct a joint training for the generator and citation function classification to make the model aware of why to cite. Our experimental results show that our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> outperforms comparative <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--123 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.123" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.123/>Directed Acyclic Graph Network for Conversational Emotion Recognition</a></strong><br><a href=/people/w/weizhou-shen/>Weizhou Shen</a>
|
<a href=/people/s/siyue-wu/>Siyue Wu</a>
|
<a href=/people/y/yunyi-yang/>Yunyi Yang</a>
|
<a href=/people/x/xiaojun-quan/>Xiaojun Quan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--123><div class="card-body p-3 small">The modeling of conversational context plays a vital role in emotion recognition from conversation (ERC). In this paper, we put forward a novel idea of encoding the utterances with a <a href=https://en.wikipedia.org/wiki/Directed_acyclic_graph>directed acyclic graph (DAG)</a> to better model the intrinsic structure within a conversation, and design a directed acyclic neural network, namely DAG-ERC, to implement this idea. In an attempt to combine the strengths of conventional graph-based neural models and recurrence-based neural models, DAG-ERC provides a more intuitive way to model the information flow between long-distance conversation background and nearby context. Extensive experiments are conducted on four ERC benchmarks with state-of-the-art models employed as baselines for comparison. The empirical results demonstrate the superiority of this new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and confirm the motivation of the directed acyclic graph architecture for ERC.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.125" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.125/>Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection</a></strong><br><a href=/people/l/lixing-zhu/>Lixing Zhu</a>
|
<a href=/people/g/gabriele-pergola/>Gabriele Pergola</a>
|
<a href=/people/l/lin-gui/>Lin Gui</a>
|
<a href=/people/d/deyu-zhou/>Deyu Zhou</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--125><div class="card-body p-3 small">Emotion detection in dialogues is challenging as it often requires the identification of thematic topics underlying a <a href=https://en.wikipedia.org/wiki/Conversation>conversation</a>, the relevant commonsense knowledge, and the intricate transition patterns between the affective states. In this paper, we propose a Topic-Driven Knowledge-Aware Transformer to handle the challenges above. We firstly design a topic-augmented language model (LM) with an additional layer specialized for topic detection. The topic-augmented LM is then combined with commonsense statements derived from a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> based on the dialogue contextual information. Finally, a transformer-based encoder-decoder architecture fuses the topical and commonsense information, and performs the emotion label sequence prediction. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has been experimented on four <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in dialogue emotion detection, demonstrating its superiority empirically over the existing state-of-the-art approaches. Quantitative and qualitative results show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can discover topics which help in distinguishing emotion categories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.128" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.128/>Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification</a></strong><br><a href=/people/j/jiasheng-si/>Jiasheng Si</a>
|
<a href=/people/d/deyu-zhou/>Deyu Zhou</a>
|
<a href=/people/t/tongzhe-li/>Tongzhe Li</a>
|
<a href=/people/x/xingyu-shi/>Xingyu Shi</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--128><div class="card-body p-3 small">Fact verification is a challenging task that requires simultaneously <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a> and aggregating over multiple retrieved pieces of evidence to evaluate the truthfulness of a claim. Existing approaches typically (i) explore the semantic interaction between the claim and evidence at different granularity levels but fail to capture their topical consistency during the reasoning process, which we believe is crucial for verification ; (ii) aggregate multiple pieces of evidence equally without considering their implicit stances to the claim, thereby introducing spurious information. To alleviate the above issues, we propose a novel topic-aware evidence reasoning and stance-aware aggregation model for more accurate fact verification, with the following four key properties : 1) checking topical consistency between the claim and evidence ; 2) maintaining topical coherence among multiple pieces of evidence ; 3) ensuring semantic similarity between the global topic information and the semantic representation of evidence ; 4) aggregating evidence based on their implicit stances to the claim. Extensive experiments conducted on the two benchmark datasets demonstrate the superiority of the proposed model over several state-of-the-art approaches for fact verification. The source code can be obtained from https://github.com/jasenchn/TARSA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--131 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.131/>A Survey of Code-switching : Linguistic and Social Perspectives for Language Technologies</a></strong><br><a href=/people/a/a-seza-dogruoz/>A. Seza Doğruöz</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/b/barbara-bullock/>Barbara E. Bullock</a>
|
<a href=/people/a/almeida-jacqueline-toribio/>Almeida Jacqueline Toribio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--131><div class="card-body p-3 small">The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years. So far, much of this research focuses mainly on the improvement of <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational methods</a> and largely ignores linguistic and social aspects of C-S discussed across a wide range of languages within the long-established literature in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>. To fill this gap, we offer a survey of code-switching (C-S) covering the literature in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> with a reflection on the key issues in language technologies. From the linguistic perspective, we provide an overview of structural and functional patterns of C-S focusing on the literature from European and Indian contexts as highly multilingual areas. From the language technologies perspective, we discuss how massive language models fail to represent diverse C-S types due to lack of appropriate training data, lack of robust evaluation benchmarks for C-S (across multilingual situations and types of C-S) and lack of end-to- end systems that cover sociolinguistic aspects of C-S as well. Our survey will be a step to- wards an outcome of mutual benefit for computational scientists and linguists with a shared interest in <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingualism</a> and C-S.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--137 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.137" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.137/>Dialogue Response Selection with Hierarchical Curriculum Learning</a></strong><br><a href=/people/y/yixuan-su/>Yixuan Su</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/q/qingyu-zhou/>Qingyu Zhou</a>
|
<a href=/people/z/zibo-lin/>Zibo Lin</a>
|
<a href=/people/s/simon-baker/>Simon Baker</a>
|
<a href=/people/y/yunbo-cao/>Yunbo Cao</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--137><div class="card-body p-3 small">We study the learning of a <a href=https://en.wikipedia.org/wiki/Matching_model>matching model</a> for dialogue response selection. Motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios, we propose a hierarchical curriculum learning framework that trains the matching model in an easy-to-difficult scheme. Our learning framework consists of two complementary curricula : (1) corpus-level curriculum (CC) ; and (2) instance-level curriculum (IC). In CC, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> gradually increases its ability in finding the matching clues between the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>dialogue context</a> and a response candidate. As for IC, it progressively strengthens the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s ability in identifying the mismatching information between the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>dialogue context</a> and a response candidate. Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.138.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--138 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.138 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.138" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.138/>A Joint Model for Dropped Pronoun Recovery and Conversational Discourse Parsing in Chinese Conversational Speech<span class=acl-fixed-case>C</span>hinese Conversational Speech</a></strong><br><a href=/people/j/jingxuan-yang/>Jingxuan Yang</a>
|
<a href=/people/k/kerui-xu/>Kerui Xu</a>
|
<a href=/people/j/jun-xu/>Jun Xu</a>
|
<a href=/people/s/si-li/>Si Li</a>
|
<a href=/people/s/sheng-gao/>Sheng Gao</a>
|
<a href=/people/j/jun-guo/>Jun Guo</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--138><div class="card-body p-3 small">In this paper, we present a neural model for joint dropped pronoun recovery (DPR) and conversational discourse parsing (CDP) in Chinese conversational speech. We show that <a href=https://en.wikipedia.org/wiki/Derivative>DPR</a> and <a href=https://en.wikipedia.org/wiki/Derivative>CDP</a> are closely related, and a joint model benefits both tasks. We refer to our model as DiscProReco, and it first encodes the tokens in each utterance in a conversation with a directed Graph Convolutional Network (GCN). The token states for an utterance are then aggregated to produce a single state for each utterance. The utterance states are then fed into a biaffine classifier to construct a conversational discourse graph. A second (multi-relational) GCN is then applied to the utterance states to produce a discourse relation-augmented representation for the utterances, which are then fused together with token states in each utterance as input to a dropped pronoun recovery layer. The joint model is trained and evaluated on a new Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) data set that we annotated with both two types of information. Experimental results on the SPDPR dataset and other benchmarks show that DiscProReco significantly outperforms the state-of-the-art baselines of both tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.140/>Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data</a></strong><br><a href=/people/h/haoming-jiang/>Haoming Jiang</a>
|
<a href=/people/d/danqing-zhang/>Danqing Zhang</a>
|
<a href=/people/t/tianyu-cao/>Tianyu Cao</a>
|
<a href=/people/b/bing-yin/>Bing Yin</a>
|
<a href=/people/t/tuo-zhao/>Tuo Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--140><div class="card-body p-3 small">Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER). Existing work mainly focuses on learning <a href=https://en.wikipedia.org/wiki/Deep_learning>deep NER models</a> only with <a href=https://en.wikipedia.org/wiki/Supervised_learning>weak supervision</a>, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms <a href=https://en.wikipedia.org/wiki/Supervised_learning>fully supervised NER</a> with manually / strongly labeled data. In this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data. Unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data. To address this issue, we propose a new multi-stage computational framework NEEDLE with three essential ingredients : (1) weak label completion, (2) noise-aware loss function, and (3) final fine-tuning over the strongly labeled data. Through experiments on E-commerce query NER and Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise of the weak labels and outperforms existing methods. In particular, we achieve new SOTA F1-scores on 3 Biomedical NER datasets : BC5CDR-chem 93.74, BC5CDR-disease 90.69, NCBI-disease 92.28.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--142 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.142" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.142/>Improving <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> by External Context Retrieving and Cooperative Learning</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--142><div class="card-body p-3 small">Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a>, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by <a href=https://en.wikipedia.org/wiki/Cooperative_learning>Cooperative Learning</a>, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.145" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.145/>Bird’s Eye : Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach</a></strong><br><a href=/people/y/yifan-hou/>Yifan Hou</a>
|
<a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--145><div class="card-body p-3 small">NLP has a rich history of representing our prior understanding of language in the form of <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>. Recent work on analyzing contextualized text representations has focused on hand-designed probe models to understand how and to what extent do these <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> encode a particular linguistic phenomenon. However, due to the inter-dependence of various phenomena and randomness of training probe models, detecting how these representations encode the rich information in these linguistic graphs remains a challenging problem. In this paper, we propose a new information-theoretic probe, Bird&#8217;s Eye, which is a fairly simple probe method for detecting if and how these representations encode the information in these linguistic graphs. Instead of using model performance, our probe takes an information-theoretic view of probing and estimates the mutual information between the linguistic graph embedded in a continuous space and the contextualized word representations. Furthermore, we also propose an approach to use our probe to investigate localized linguistic information in the linguistic graphs using perturbation analysis. We call this probing setup Worm&#8217;s Eye. Using these probes, we analyze the BERT models on its ability to encode a syntactic and a semantic graph structure, and find that these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> encode to some degree both syntactic as well as semantic information ; albeit syntactic information to a greater extent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.148.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.148" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.148/>Bad Seeds : Evaluating Lexical Methods for Bias Measurement</a></strong><br><a href=/people/m/maria-antoniak/>Maria Antoniak</a>
|
<a href=/people/d/david-mimno/>David Mimno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--148><div class="card-body p-3 small">A common factor in bias measurement methods is the use of hand-curated seed lexicons, but there remains little guidance for their selection. We gather seeds used in prior work, documenting their common sources and rationales, and in case studies of three English-language corpora, we enumerate the different types of social biases and linguistic features that, once encoded in the seeds, can affect subsequent bias measurements. Seeds developed in one context are often re-used in other contexts, but documentation and evaluation remain necessary precursors to relying on <a href=https://en.wikipedia.org/wiki/Seed>seeds</a> for sensitive measurements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--150 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.150/>Intrinsic Bias Metrics Do Not Correlate with Application Bias</a></strong><br><a href=/people/s/seraphina-goldfarb-tarrant/>Seraphina Goldfarb-Tarrant</a>
|
<a href=/people/r/rebecca-marchant/>Rebecca Marchant</a>
|
<a href=/people/r/ricardo-munoz-sanchez/>Ricardo Muñoz Sánchez</a>
|
<a href=/people/m/mugdha-pandya/>Mugdha Pandya</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--150><div class="card-body p-3 small">Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that quantify bias in models. Some of these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that holds in all scenarios across tasks and languages. We urge researchers working on <a href=https://en.wikipedia.org/wiki/Debiasing>debiasing</a> to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release <a href=https://en.wikipedia.org/wiki/Source_code>code</a>, a new intrinsic metric, and an annotated test set focused on gender bias in <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--153 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.153.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.153/>Crafting Adversarial Examples for Neural Machine Translation</a></strong><br><a href=/people/x/xinze-zhang/>Xinze Zhang</a>
|
<a href=/people/j/junzhe-zhang/>Junzhe Zhang</a>
|
<a href=/people/z/zhenhua-chen/>Zhenhua Chen</a>
|
<a href=/people/k/kun-he/>Kun He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--153><div class="card-body p-3 small">Effective adversary generation for neural machine translation (NMT) is a crucial prerequisite for building robust machine translation systems. In this work, we investigate veritable evaluations of NMT adversarial attacks, and propose a novel method to craft NMT adversarial examples. We first show the current NMT adversarial attacks may be improperly estimated by the commonly used mono-directional translation, and we propose to leverage the round-trip translation technique to build valid metrics for evaluating NMT adversarial attacks. Our intuition is that an effective NMT adversarial example, which imposes minor shifting on the source and degrades the translation dramatically, would naturally lead to a semantic-destroyed round-trip translation result. We then propose a promising black-box attack method called Word Saliency speedup Local Search (WSLS) that could effectively attack the mainstream NMT architectures. Comprehensive experiments demonstrate that the proposed metrics could accurately evaluate the attack effectiveness, and the proposed WSLS could significantly break the state-of-art NMT models with small perturbation. Besides, WSLS exhibits strong transferability on attacking Baidu and Bing online translators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--156 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.156" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.156/>Hierarchical Context-aware Network for Dense Video Event Captioning</a></strong><br><a href=/people/l/lei-ji/>Lei Ji</a>
|
<a href=/people/x/xianglin-guo/>Xianglin Guo</a>
|
<a href=/people/h/haoyang-huang/>Haoyang Huang</a>
|
<a href=/people/x/xilin-chen/>Xilin Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--156><div class="card-body p-3 small">Dense video event captioning aims to generate a sequence of descriptive captions for each event in a long untrimmed video. Video-level context provides important information and facilities the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to generate consistent and less redundant captions between events. In this paper, we introduce a novel Hierarchical Context-aware Network for dense video event captioning (HCN) to capture <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> from various aspects. In detail, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> leverages local and global context with different mechanisms to jointly learn to generate coherent captions. The local context module performs full interaction between neighbor frames and the global context module selectively attends to previous or future events. According to our extensive experiment on both Youcook2 and Activitynet Captioning datasets, the video-level HCN model outperforms the event-level context-agnostic model by a large margin. The code is available at https://github.com/KirkGuo/HCN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.157/>Control Image Captioning Spatially and Temporally</a></strong><br><a href=/people/k/kun-yan/>Kun Yan</a>
|
<a href=/people/l/lei-ji/>Lei Ji</a>
|
<a href=/people/h/huaishao-luo/>Huaishao Luo</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/s/shuai-ma/>Shuai Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--157><div class="card-body p-3 small">Generating image captions with user intention is an emerging need. The recently published Localized Narratives dataset takes mouse traces as another input to the image captioning task, which is an intuitive and efficient way for a user to control what to describe in the image. However, how to effectively employ <a href=https://en.wikipedia.org/wiki/Trace_(disambiguation)>traces</a> to improve generation quality and <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a> is still under exploration. This paper aims to solve this problem by proposing a novel model called LoopCAG, which connects Contrastive constraints and Attention Guidance in a Loop manner, engaged explicit spatial and temporal constraints to the generating process. Precisely, each generated sentence is temporally aligned to the corresponding <a href=https://en.wikipedia.org/wiki/Trace_(linear_algebra)>trace sequence</a> through a contrastive learning strategy. Besides, each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance. Comprehensive experimental results demonstrate that our LoopCAG model learns better correspondence among the three modalities (vision, language, and traces) and achieves SOTA performance on trace-controlled image captioning task. Moreover, the <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a> and explainability of LoopCAG are validated by analyzing spatial and temporal sensitivity during the generation process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--158 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.158/>Edited Media Understanding Frames : Reasoning About the Intent and Implications of Visual Misinformation</a></strong><br><a href=/people/j/jeff-da/>Jeff Da</a>
|
<a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/r/rowan-zellers/>Rowan Zellers</a>
|
<a href=/people/a/anthony-zheng/>Anthony Zheng</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--158><div class="card-body p-3 small">Understanding manipulated media, from automatically generated &#8216;deepfakes&#8217; to manually edited ones, raises novel research challenges. Because the vast majority of edited or manipulated images are benign, such as <a href=https://en.wikipedia.org/wiki/Photo_manipulation>photoshopped images</a> for visual enhancements, the key challenge is to understand the complex layers of underlying intents of media edits and their implications with respect to <a href=https://en.wikipedia.org/wiki/Disinformation>disinformation</a>. In this paper, we study Edited Media Frames, a new formalism to understand visual media manipulation as structured annotations with respect to the intents, emotional reactions, attacks on individuals, and the overall implications of <a href=https://en.wikipedia.org/wiki/Disinformation>disinformation</a>. We introduce a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for our task, EMU, with 56k question-answer pairs written in rich natural language. We evaluate a wide variety of vision-and-language models for our task, and introduce a new model PELICAN, which builds upon recent progress in pretrained multimodal representations. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtains promising results on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, with humans rating its answers as accurate 48.2 % of the time. At the same time, there is still much work to be done and we provide analysis that highlights areas for further progress.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--160 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.160" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.160/>Modeling Fine-Grained Entity Types with Box Embeddings</a></strong><br><a href=/people/y/yasumasa-onoe/>Yasumasa Onoe</a>
|
<a href=/people/m/michael-boratko/>Michael Boratko</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--160><div class="card-body p-3 small">Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types&#8217; complex interdependencies. We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> represents both types and entity mentions as boxes. Each mention and its context are fed into a BERT-based model to embed that mention in our box space ; essentially, this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> leverages typological clues present in the surface text to hypothesize a type representation for the mention. Box containment can then be used to derive both the <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior probability</a> of a mention exhibiting a given type and the conditional probability relations between types themselves. We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks. In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and <a href=https://en.wikipedia.org/wiki/Confidence>confidence</a> (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.162/>Weight Distillation : Transferring the Knowledge in Neural Network Parameters</a></strong><br><a href=/people/y/ye-lin/>Ye Lin</a>
|
<a href=/people/y/yanyang-li/>Yanyang Li</a>
|
<a href=/people/z/ziyang-wang/>Ziyang Wang</a>
|
<a href=/people/b/bei-li/>Bei Li</a>
|
<a href=/people/q/quan-du/>Quan Du</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--162><div class="card-body p-3 small">Knowledge distillation has been proven to be effective in model acceleration and compression. It transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network. But this way ignores the knowledge inside the <a href=https://en.wikipedia.org/wiki/Neural_network>large neural networks</a>, e.g., <a href=https://en.wikipedia.org/wiki/Parameter_(computer_programming)>parameters</a>. Our preliminary study as well as the recent success in <a href=https://en.wikipedia.org/wiki/Training>pre-training</a> suggests that transferring parameters are more effective in distilling knowledge. In this paper, we propose Weight Distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator. On the WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks, our experiments show that weight distillation learns a small network that is 1.88 2.94x faster than the large network but with competitive BLEU performance. When fixing the size of small networks, weight distillation outperforms knowledge distillation by 0.51 1.82 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.169/>PlotCoder : Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context<span class=acl-fixed-case>P</span>lot<span class=acl-fixed-case>C</span>oder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context</a></strong><br><a href=/people/x/xinyun-chen/>Xinyun Chen</a>
|
<a href=/people/l/linyuan-gong/>Linyuan Gong</a>
|
<a href=/people/a/alvin-cheung/>Alvin Cheung</a>
|
<a href=/people/d/dawn-song/>Dawn Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--169><div class="card-body p-3 small">Creating effective <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualization</a> is an important part of <a href=https://en.wikipedia.org/wiki/Analytics>data analytics</a>. While there are many libraries for creating <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualization</a>, writing such <a href=https://en.wikipedia.org/wiki/Source_code>code</a> remains difficult given the myriad of parameters that users need to provide. In this paper, we propose the new task of synthesizing visualization programs from a combination of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language utterances</a> and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>code context</a>. To tackle the learning problem, we introduce PlotCoder, a new hierarchical encoder-decoder architecture that models both the code context and the input utterance. We use PlotCoder to first determine the template of the visualization code, followed by predicting the data to be plotted. We use Jupyter notebooks containing visualization programs crawled from GitHub to train PlotCoder. On a comprehensive set of test samples from those notebooks, we show that PlotCoder correctly predicts the plot type of about 70 % samples, and synthesizes the correct programs for 35 % samples, performing 3-4.5 % better than the baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.170/>Changing the World by Changing the Data</a></strong><br><a href=/people/a/anna-rogers/>Anna Rogers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--170><div class="card-body p-3 small">NLP community is currently investing a lot more research and resources into development of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> than training data. While we have made a lot of progress, it is now clear that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> learn all kinds of spurious patterns, social biases, and annotation artifacts. Algorithmic solutions have so far had limited success. An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. This position paper maps out the arguments for and against <a href=https://en.wikipedia.org/wiki/Data_curation>data curation</a>, and argues that fundamentally the point is moot : <a href=https://en.wikipedia.org/wiki/Data_curation>curation</a> already is and will be happening, and it is changing the world. The question is only how much thought we want to invest into that process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--172 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.172/>On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation</a></strong><br><a href=/people/r/ruidan-he/>Ruidan He</a>
|
<a href=/people/l/linlin-liu/>Linlin Liu</a>
|
<a href=/people/h/hai-ye/>Hai Ye</a>
|
<a href=/people/q/qingyu-tan/>Qingyu Tan</a>
|
<a href=/people/b/bosheng-ding/>Bosheng Ding</a>
|
<a href=/people/l/liying-cheng/>Liying Cheng</a>
|
<a href=/people/j/jiawei-low/>Jiawei Low</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--172><div class="card-body p-3 small">Adapter-based tuning has recently arisen as an alternative to <a href=https://en.wikipedia.org/wiki/Musical_tuning>fine-tuning</a>. It works by adding light-weight adapter modules to a pretrained language model (PrLM) and only updating the parameters of adapter modules when learning on a downstream task. As such, it adds only a few trainable parameters per new <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, allowing a high degree of parameter sharing. Prior studies have shown that adapter-based tuning often achieves comparable results to <a href=https://en.wikipedia.org/wiki/Musical_tuning>fine-tuning</a>. However, existing work only focuses on the parameter-efficient aspect of adapter-based tuning while lacking further investigation on its effectiveness. In this paper, we study the latter. We first show that adapter-based tuning better mitigates forgetting issues than <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> since it yields representations with less deviation from those generated by the initial PrLM. We then empirically compare the two tuning methods on several downstream NLP tasks and settings. We demonstrate that 1) adapter-based tuning outperforms <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on low-resource and cross-lingual tasks ; 2) it is more robust to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and less sensitive to changes in learning rates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--174 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.174.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.174" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.174/>Integrating <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> and Neighborhood Information with Graph-Driven Generative Models for Document Retrieval</a></strong><br><a href=/people/z/zijing-ou/>Zijing Ou</a>
|
<a href=/people/q/qinliang-su/>Qinliang Su</a>
|
<a href=/people/j/jianxing-yu/>Jianxing Yu</a>
|
<a href=/people/b/bang-liu/>Bang Liu</a>
|
<a href=/people/j/jingwen-wang/>Jingwen Wang</a>
|
<a href=/people/r/ruihui-zhao/>Ruihui Zhao</a>
|
<a href=/people/c/changyou-chen/>Changyou Chen</a>
|
<a href=/people/y/yefeng-zheng/>Yefeng Zheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--174><div class="card-body p-3 small">With the need of fast retrieval speed and small memory footprint, document hashing has been playing a crucial role in large-scale information retrieval. To generate high-quality hashing code, both <a href=https://en.wikipedia.org/wiki/Semantics_(computer_science)>semantics</a> and <a href=https://en.wikipedia.org/wiki/Neighbourhood_(mathematics)>neighborhood information</a> are crucial. However, most existing methods leverage only one of them or simply combine them via some intuitive criteria, lacking a theoretical principle to guide the integration process. In this paper, we encode the neighborhood information with a graph-induced Gaussian distribution, and propose to integrate the two types of <a href=https://en.wikipedia.org/wiki/Information>information</a> with a graph-driven generative model. To deal with the complicated correlations among documents, we further propose a tree-structured approximation method for <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a>. Under the approximation, we prove that the training objective can be decomposed into terms involving only singleton or pairwise documents, enabling the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to be trained as efficiently as uncorrelated ones. Extensive experimental results on three benchmark datasets show that our method achieves superior performance over state-of-the-art methods, demonstrating the effectiveness of the proposed model for simultaneously preserving semantic and neighborhood information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.176/>KaggleDBQA : Realistic Evaluation of Text-to-SQL Parsers<span class=acl-fixed-case>K</span>aggle<span class=acl-fixed-case>DBQA</span>: Realistic Evaluation of Text-to-<span class=acl-fixed-case>SQL</span> Parsers</a></strong><br><a href=/people/c/chia-hsuan-lee/>Chia-Hsuan Lee</a>
|
<a href=/people/o/oleksandr-polozov/>Oleksandr Polozov</a>
|
<a href=/people/m/matthew-richardson/>Matthew Richardson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--176><div class="card-body p-3 small">The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. Recently, large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases. In this work, we examine the challenges that still prevent these <a href=https://en.wikipedia.org/wiki/List_of_art_media>techniques</a> from practical deployment. First, we present KaggleDBQA, a new cross-domain evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions. Second, we re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in real-life settings. Finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge. We show that KaggleDBQA presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> by over 13.2 %, doubling their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--179 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.179" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.179/>Better than Average : Paired Evaluation of NLP systems<span class=acl-fixed-case>NLP</span> systems</a></strong><br><a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/r/robert-west/>Robert West</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--179><div class="card-body p-3 small">Evaluation in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is usually done by comparing the scores of competing systems independently averaged over a common set of test instances. In this work, we question the use of <a href=https://en.wikipedia.org/wiki/Average>averages</a> for aggregating evaluation scores into a final number used to decide which system is best, since the average, as well as alternatives such as the <a href=https://en.wikipedia.org/wiki/Median>median</a>, ignores the pairing arising from the fact that systems are evaluated on the same test instances. We illustrate the importance of taking the instancelevel pairing of evaluation scores into account and demonstrate, both theoretically and empirically, the advantages of aggregation methods based on pairwise comparisons, such as the BradleyTerry (BT) model, a mechanism based on the estimated probability that a given system scores better than another on the test set. By re-evaluating 296 real NLP evaluation setups across four tasks and 18 evaluation metrics, we show that the choice of aggregation mechanism matters and yields different conclusions as to which systems are state of the art in about 30 % of the setups. To facilitate the adoption of pairwise evaluation, we release a practical tool for performing the full analysis of evaluation scores with the mean, median, BT, and two variants of BT (Elo and TrueSkill), alongside functionality for appropriate statistical testing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.184/>Distributed Representations of Emotion Categories in Emotion Space</a></strong><br><a href=/people/x/xiangyu-wang/>Xiangyu Wang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--184><div class="card-body p-3 small">Emotion category is usually divided into different ones by human beings, but it is indeed difficult to clearly distinguish and define the boundaries between different emotion categories. The existing studies working on <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a> usually focus on how to improve the performance of <a href=https://en.wikipedia.org/wiki/Prediction>model prediction</a>, in which <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> are represented with one-hot vectors. However, emotion relations are ignored in one-hot representations. In this article, we first propose a general framework to learn the <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a> for <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion categories</a> in emotion space from a given <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification dataset</a>. Furthermore, based on the soft labels predicted by the pre-trained neural network model, we derive a simple and effective <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>. Experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.185.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.185" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.185/>Style is NOT a single variable : Case Studies for Cross-Stylistic Language Understanding<span class=acl-fixed-case>NOT</span> a single variable: Case Studies for Cross-Stylistic Language Understanding</a></strong><br><a href=/people/d/dongyeop-kang/>Dongyeop Kang</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--185><div class="card-body p-3 small">Every natural text is written in some style. Style is formed by a complex combination of different stylistic factors, including formality markers, <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a>, <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a>, etc. One can not form a complete understanding of a text without considering these <a href=https://en.wikipedia.org/wiki/Factor_analysis>factors</a>. The <a href=https://en.wikipedia.org/wiki/Factor_analysis>factors</a> combine and co-vary in complex ways to form <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>styles</a>. Studying the nature of the covarying combinations sheds light on stylistic language in general, sometimes called cross-style language understanding. This paper provides the benchmark corpus (XSLUE) that combines existing datasets and collects a new one for sentence-level cross-style language understanding and evaluation. The <a href=https://en.wikipedia.org/wiki/Benchmark_(surveying)>benchmark</a> contains text in 15 different styles under the proposed four theoretical groupings : figurative, personal, affective, and interpersonal groups. For valid evaluation, we collect an additional diagnostic set by annotating all 15 styles on the same text. Using XSLUE, we propose three interesting cross-style applications in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation</a>, and generation. First, our proposed cross-style classifier trained with multiple styles together helps improve overall classification performance against individually-trained style classifiers. Second, our study shows that some styles are highly dependent on each other in human-written text. Finally, we find that combinations of some contradictive styles likely generate stylistically less appropriate text. We believe our benchmark and case studies help explore interesting future directions for cross-style research. The preprocessed datasets and code are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--188 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.188" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.188/>A Unified Generative Framework for Aspect-based Sentiment Analysis</a></strong><br><a href=/people/h/hang-yan/>Hang Yan</a>
|
<a href=/people/j/junqi-dai/>Junqi Dai</a>
|
<a href=/people/t/tuo-ji/>Tuo Ji</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--188><div class="card-body p-3 small">Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms, their corresponding sentiment polarities, and the <a href=https://en.wikipedia.org/wiki/Opinion_poll>opinion terms</a>. There exist seven subtasks in ABSA. Most studies only focus on the subsets of these subtasks, which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework. In this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all ABSA subtasks into a unified generative formulation. Based on the unified formulation, we exploit the pre-training sequence-to-sequence model BART to solve all ABSA subtasks in an end-to-end framework. Extensive experiments on four ABSA datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole ABSA subtasks, which could benefit multiple tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.189/>Discovering Dialogue Slots with Weak Supervision</a></strong><br><a href=/people/v/vojtech-hudecek/>Vojtěch Hudeček</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--189><div class="card-body p-3 small">Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain. We propose a method that eliminates this requirement : We use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms. Furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention. This <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagger</a> is trained solely on the outputs of our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> and thus does not rely on any labeled data. Our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains. Moreover, we find that slot annotations discovered by our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.191.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.191/>PROTAUGMENT : Unsupervised diverse short-texts paraphrasing for intent detection meta-learning<span class=acl-fixed-case>PROTAUGMENT</span>: Unsupervised diverse short-texts paraphrasing for intent detection meta-learning</a></strong><br><a href=/people/t/thomas-dopierre/>Thomas Dopierre</a>
|
<a href=/people/c/christophe-gravier/>Christophe Gravier</a>
|
<a href=/people/w/wilfried-logerais/>Wilfried Logerais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--191><div class="card-body p-3 small">Recent research considers few-shot intent detection as a meta-learning problem : the model is learning to learn from a consecutive set of small tasks named episodes. In this work, we propose ProtAugment, a meta-learning algorithm for short texts classification (the intent detection task). ProtAugment is a novel extension of Prototypical Networks, that limits <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> on the bias introduced by the few-shots classification objective at each episode. It relies on diverse paraphrasing : a conditional language model is first fine-tuned for <a href=https://en.wikipedia.org/wiki/Paraphrasing>paraphrasing</a>, and diversity is later introduced at the decoding stage at each meta-learning episode. The diverse paraphrasing is unsupervised as it is applied to unlabelled data, and then fueled to the Prototypical Network training objective as a consistency loss. ProtAugment is the state-of-the-art method for intent detection meta-learning, at no extra labeling efforts and without the need to fine-tune a conditional language model on a given application domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.195/>Towards Robustness of Text-to-SQL Models against Synonym Substitution<span class=acl-fixed-case>SQL</span> Models against Synonym Substitution</a></strong><br><a href=/people/y/yujian-gan/>Yujian Gan</a>
|
<a href=/people/x/xinyun-chen/>Xinyun Chen</a>
|
<a href=/people/q/qiuping-huang/>Qiuping Huang</a>
|
<a href=/people/m/matthew-purver/>Matthew Purver</a>
|
<a href=/people/j/john-r-woodward/>John R. Woodward</a>
|
<a href=/people/j/jinxia-xie/>Jinxia Xie</a>
|
<a href=/people/p/pengsheng-huang/>Pengsheng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--195><div class="card-body p-3 small">Recently, there has been significant progress in studying <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to translate <a href=https://en.wikipedia.org/wiki/String_(computer_science)>text descriptions</a> into <a href=https://en.wikipedia.org/wiki/SQL>SQL queries</a>. Despite achieving good performance on some public benchmarks, existing text-to-SQL models typically rely on the lexical matching between words in natural language (NL) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism. In this work, we investigate the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of text-to-SQL models to synonym substitution. In particular, we introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-Syn are modified from Spider, by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases. We observe that the accuracy dramatically drops by eliminating such explicit correspondence between NL questions and table schemas, even if the <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> are not adversarially selected to conduct worst-case attacks. Finally, we present two categories of approaches to improve the model robustness. The first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input, while the second category is based on <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a>. We demonstrate that both categories of <a href=https://en.wikipedia.org/wiki/Psychological_warfare>approaches</a> significantly outperform their counterparts without the defense, and the first category of <a href=https://en.wikipedia.org/wiki/Psychological_warfare>approaches</a> are more effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--198 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.198" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.198/>LGESQL : Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations<span class=acl-fixed-case>LGESQL</span>: Line Graph Enhanced Text-to-<span class=acl-fixed-case>SQL</span> Model with Mixed Local and Non-Local Relations</a></strong><br><a href=/people/r/ruisheng-cao/>Ruisheng Cao</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/z/zhi-chen/>Zhi Chen</a>
|
<a href=/people/y/yanbin-zhao/>Yanbin Zhao</a>
|
<a href=/people/s/su-zhu/>Su Zhu</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--198><div class="card-body p-3 small">This work aims to tackle the challenging heterogeneous graph encoding problem in the text-to-SQL task. Previous methods are typically node-centric and merely utilize different weight matrices to parameterize edge types, which 1) ignore the rich semantics embedded in the topological structure of edges, and 2) fail to distinguish local and non-local relations for each node. To this end, we propose a Line Graph Enhanced Text-to-SQL (LGESQL) model to mine the underlying relational features without constructing meta-paths. By virtue of the <a href=https://en.wikipedia.org/wiki/Line_graph>line graph</a>, messages propagate more efficiently through not only connections between nodes, but also the topology of directed edges. Furthermore, both local and non-local relations are integrated distinctively during the graph iteration. We also design an auxiliary task called graph pruning to improve the discriminative capability of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves state-of-the-art results (62.8 % with Glove, 72.0 % with Electra) on the cross-domain text-to-SQL benchmark Spider at the time of writing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.199" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.199/>Multi-stage Pre-training over Simplified Multimodal Pre-training Models</a></strong><br><a href=/people/t/tongtong-liu/>Tongtong Liu</a>
|
<a href=/people/f/fangxiang-feng/>Fangxiang Feng</a>
|
<a href=/people/x/xiaojie-wang/>Xiaojie Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--199><div class="card-body p-3 small">Multimodal pre-training models, such as LXMERT, have achieved excellent results in downstream tasks. However, current pre-trained models require large amounts of training data and have huge model sizes, which make them impossible to apply in low-resource situations. How to obtain similar or even better performance than a larger <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> under the premise of less pre-training data and smaller model size has become an important problem. In this paper, we propose a new Multi-stage Pre-training (MSP) method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train a model in stages. We also design several different pre-training tasks suitable for the <a href=https://en.wikipedia.org/wiki/Granularity>information granularity</a> in different stage in order to efficiently capture the diverse knowledge from a limited corpus. We take a Simplified LXMERT (LXMERT-S) which is with 45.9 % parameters of the original LXMERT model and only 11.44 % of the original pre-training data as the testbed of our MSP method. Experimental results show that our method achieves comparable performance to the original LXMERT model in all downstream tasks, and even outperforms the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in Image-Text Retrieval task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--200 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.200" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.200/>Beyond Sentence-Level End-to-End Speech Translation : Context Helps</a></strong><br><a href=/people/b/biao-zhang/>Biao Zhang</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--200><div class="card-body p-3 small">Document-level contextual information has shown benefits to text-based machine translation, but whether and how <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> helps end-to-end (E2E) speech translation (ST) is still under-studied. We fill this gap through extensive experiments using a simple concatenation-based context-aware ST model, paired with adaptive feature selection on speech encodings for computational efficiency. We investigate several decoding approaches, and introduce in-model ensemble decoding which jointly performs document- and sentence-level translation using the same model. Our results on the MuST-C benchmark with Transformer demonstrate the effectiveness of context to E2E ST. Compared to sentence-level ST, context-aware ST obtains better translation quality (+0.18-2.61 BLEU), improves pronoun and homophone translation, shows better robustness to (artificial) audio segmentation errors, and reduces <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a> and <a href=https://en.wikipedia.org/wiki/Flicker_(screen)>flicker</a> to deliver higher quality for simultaneous translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.203" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.203/>Missing Modality Imagination Network for <a href=https://en.wikipedia.org/wiki/Emotion_recognition>Emotion Recognition</a> with Uncertain Missing Modalities</a></strong><br><a href=/people/j/jinming-zhao/>Jinming Zhao</a>
|
<a href=/people/r/ruichen-li/>Ruichen Li</a>
|
<a href=/people/q/qin-jin/>Qin Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--203><div class="card-body p-3 small">Multimodal fusion has been proved to improve <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any missing modality given available modalities under different missing modality conditions. Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a> performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition. The code will be available at https://github.com/AIM3-RUC/MMIN.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.204/>Stacked Acoustic-and-Textual Encoding : Integrating the Pre-trained Models into Speech Translation Encoders</a></strong><br><a href=/people/c/chen-xu/>Chen Xu</a>
|
<a href=/people/b/bojie-hu/>Bojie Hu</a>
|
<a href=/people/y/yanyang-li/>Yanyang Li</a>
|
<a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/s/shen-huang/>Shen Huang</a>
|
<a href=/people/q/qi-ju/>Qi Ju</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--204><div class="card-body p-3 small">Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a>. Our <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the <a href=https://en.wikipedia.org/wiki/System>system</a>. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves state-of-the-art <a href=https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms>BLEU scores</a> of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.205.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.205" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.205/>N-ary Constituent Tree Parsing with Recursive Semi-Markov Model<span class=acl-fixed-case>M</span>arkov Model</a></strong><br><a href=/people/x/xin-xin/>Xin Xin</a>
|
<a href=/people/j/jinlong-li/>Jinlong Li</a>
|
<a href=/people/z/zeqi-tan/>Zeqi Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--205><div class="card-body p-3 small">In this paper, we study the task of graph-based constituent parsing in the setting that binarization is not conducted as a pre-processing step, where a constituent tree may consist of nodes with more than two children. Previous graph-based methods on this setting typically generate hidden nodes with the dummy label inside the n-ary nodes, in order to transform the <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> into a binary tree for prediction. The limitation is that the hidden nodes break the sibling relations of the n-ary node&#8217;s children. Consequently, the <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>dependencies</a> of such sibling constituents might not be accurately modeled and is being ignored. To solve this limitation, we propose a novel graph-based framework, which is called recursive semi-Markov model. The main idea is to utilize 1-order semi-Markov model to predict the immediate children sequence of a constituent candidate, which then recursively serves as a child candidate of its parent. In this manner, the dependencies of sibling constituents can be described by 1-order transition features, which solves the above limitation. Through experiments, the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> obtains the <a href=https://en.wikipedia.org/wiki/Design_of_experiments>F1</a> of 95.92 % and 92.50 % on the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> of <a href=https://en.wikipedia.org/wiki/Design_of_experiments>PTB</a> and CTB 5.1 respectively. Specially, the recursive semi-Markov model shows advantages in modeling nodes with more than two children, whose average F1 can be improved by 0.3-1.1 points in PTB and 2.3-6.8 points in CTB 5.1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--206 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.206" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.206/>Automated Concatenation of Embeddings for <a href=https://en.wikipedia.org/wiki/Structured_prediction>Structured Prediction</a></a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--206><div class="card-body p-3 small">Pretrained contextualized embeddings are powerful <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representations</a> for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. However, the selection of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, and updates the belief based on a <a href=https://en.wikipedia.org/wiki/Reward_system>reward</a>. We follow strategies in <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.207/>Multi-View Cross-Lingual Structured Prediction with Minimum Supervision</a></strong><br><a href=/people/z/zechuan-hu/>Zechuan Hu</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--207><div class="card-body p-3 small">In structured prediction problems, cross-lingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages. However, not all source models are created equal and some may hurt performance on the target language. Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models. In this paper, we propose a multi-view framework, by leveraging a small number of labeled target sentences, to effectively combine multiple source models into an aggregated source view at different granularity levels (language, sentence, or sub-structure), and transfer it to a target view based on a task-specific model. By encouraging the two views to interact with each other, our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can dynamically adjust the <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence level</a> of each source model and improve the performance of both <a href=https://en.wikipedia.org/wiki/View_model>views</a> during training. Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--212 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.212" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.212/>Assessing the Representations of Idiomaticity in Vector Models with a Noun Compound Dataset Labeled at Type and Token Levels</a></strong><br><a href=/people/m/marcos-garcia/>Marcos Garcia</a>
|
<a href=/people/t/tiago-kramer-vieira/>Tiago Kramer Vieira</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/m/marco-idiart/>Marco Idiart</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--212><div class="card-body p-3 small">Accurate assessment of the ability of embedding models to capture <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a> may require evaluation at token rather than type level, to account for degrees of idiomaticity and possible ambiguity between literal and idiomatic usages. However, most existing <a href=https://en.wikipedia.org/wiki/Web_resource>resources</a> with annotation of idiomaticity include ratings only at type level. This paper presents the Noun Compound Type and Token Idiomaticity (NCTTI) dataset, with human annotations for 280 noun compounds in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and 180 in <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> at both type and token level. We compiled 8,725 and 5,091 token level annotations for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, respectively, which are strongly correlated with the corresponding scores obtained at type level. The NCTTI dataset is used to explore how vector space models reflect the variability of idiomaticity across sentences. Several experiments using state-of-the-art contextualised models suggest that their representations are not capturing the noun compounds idiomaticity as human annotators. This new multilingual resource also contains suggestions for paraphrases of the noun compounds both at type and token levels, with uses for lexical substitution or disambiguation in context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--216 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.216.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.216" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.216/>Locate and Label : A Two-stage Identifier for Nested Named Entity Recognition</a></strong><br><a href=/people/y/yongliang-shen/>Yongliang Shen</a>
|
<a href=/people/x/xinyin-ma/>Xinyin Ma</a>
|
<a href=/people/z/zeqi-tan/>Zeqi Tan</a>
|
<a href=/people/s/shuai-zhang/>Shuai Zhang</a>
|
<a href=/people/w/wen-wang/>Wen Wang</a>
|
<a href=/people/w/weiming-lu/>Weiming Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--216><div class="card-body p-3 small">Named entity recognition (NER) is a well-studied task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Traditional NER research only deals with flat entities and ignores nested entities. The span-based methods treat entity recognition as a span classification task. Although these methods have the innate ability to handle nested NER, they suffer from high computational cost, ignorance of boundary information, under-utilization of the spans that partially match with entities, and difficulties in long entity recognition. To tackle these issues, we propose a two-stage entity identifier. First we generate span proposals by filtering and boundary regression on the seed spans to locate the entities, and then label the boundary-adjusted span proposals with the corresponding categories. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> effectively utilizes the boundary information of entities and partially matched spans during <a href=https://en.wikipedia.org/wiki/Training>training</a>. Through boundary regression, entities of any length can be covered theoretically, which improves the ability to recognize long entities. In addition, many low-quality seed spans are filtered out in the first stage, which reduces the <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a> of <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. Experiments on nested NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--217 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.217" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.217/>Text2Event : Controllable Sequence-to-Structure Generation for End-to-end Event Extraction<span class=acl-fixed-case>T</span>ext2<span class=acl-fixed-case>E</span>vent: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction</a></strong><br><a href=/people/y/yaojie-lu/>Yaojie Lu</a>
|
<a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/j/jin-xu/>Jin Xu</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/j/jialong-tang/>Jialong Tang</a>
|
<a href=/people/a/annan-li/>Annan Li</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/m/meng-liao/>Meng Liao</a>
|
<a href=/people/s/shaoyi-chen/>Shaoyi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--217><div class="card-body p-3 small">Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract <a href=https://en.wikipedia.org/wiki/Event_(computing)>events</a> from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> and transfer learning settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--219 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.219/>A Neural Transition-based Joint Model for Disease Named Entity Recognition and Normalization</a></strong><br><a href=/people/z/zongcheng-ji/>Zongcheng Ji</a>
|
<a href=/people/t/tian-xia/>Tian Xia</a>
|
<a href=/people/m/mei-han/>Mei Han</a>
|
<a href=/people/j/jing-xiao/>Jing Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--219><div class="card-body p-3 small">Disease is one of the fundamental entities in <a href=https://en.wikipedia.org/wiki/Medical_research>biomedical research</a>. Recognizing such entities from biomedical text and then normalizing them to a standardized disease vocabulary offer a tremendous opportunity for many downstream applications. Previous studies have demonstrated that joint modeling of the two sub-tasks has superior performance than the pipelined counterpart. Although the neural joint model based on multi-task learning framework has achieved state-of-the-art performance, it suffers from the boundary inconsistency problem due to the separate decoding procedures. Moreover, it ignores the rich information (e.g., the text surface form) of each candidate concept in the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a>, which is quite essential for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity normalization</a>. In this work, we propose a neural transition-based joint model to alleviate these two issues. We transform the end-to-end disease recognition and normalization task as an action sequence prediction task, which not only jointly learns the model with shared representations of the input, but also jointly searches the output by state transitions in one search space. Moreover, we introduce attention mechanisms to take advantage of the text surface form of each candidate concept for better <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> performance. Experimental results conducted on two publicly available datasets show the effectiveness of the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--224 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.224/>Cascade versus Direct Speech Translation : Do the Differences Still Make a Difference?</a></strong><br><a href=/people/l/luisa-bentivogli/>Luisa Bentivogli</a>
|
<a href=/people/m/mauro-cettolo/>Mauro Cettolo</a>
|
<a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/a/alina-karakanta/>Alina Karakanta</a>
|
<a href=/people/a/alberto-martinelli/>Alberto Martinelli</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--224><div class="card-body p-3 small">Five years after the first published proofs of concept, direct approaches to speech translation (ST) are now competing with traditional cascade solutions. In light of this steady progress, can we claim that the performance gap between the two is closed? Starting from this question, we present a systematic comparison between state-of-the-art <a href=https://en.wikipedia.org/wiki/System>systems</a> representative of the two paradigms. Focusing on three language directions (English-German / Italian / Spanish), we conduct automatic and manual evaluations, exploiting high-quality professional post-edits and annotations. Our multi-faceted analysis on one of the few publicly available ST benchmarks attests for the first time that : i) the gap between the two paradigms is now closed, and ii) the subtle differences observed in their behavior are not sufficient for humans neither to distinguish them nor to prefer one over the other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--227 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.227" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.227/>ERNIE-Doc : A Retrospective Long-Document Modeling Transformer<span class=acl-fixed-case>ERNIE</span>-<span class=acl-fixed-case>D</span>oc: A Retrospective Long-Document Modeling Transformer</a></strong><br><a href=/people/s/siyu-ding/>SiYu Ding</a>
|
<a href=/people/j/junyuan-shang/>Junyuan Shang</a>
|
<a href=/people/s/shuohuan-wang/>Shuohuan Wang</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a>
|
<a href=/people/h/hao-tian/>Hao Tian</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--227><div class="card-body p-3 small">Transformers are not suited for processing long documents, due to their quadratically increasing memory and <a href=https://en.wikipedia.org/wiki/Time_management>time consumption</a>. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on <a href=https://en.wikipedia.org/wiki/Recurrence_relation>Recurrence Transformers</a>. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--228 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.228" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.228/>Marginal Utility Diminishes : Exploring the Minimum Knowledge for BERT Knowledge Distillation<span class=acl-fixed-case>BERT</span> Knowledge Distillation</a></strong><br><a href=/people/y/yuanxin-liu/>Yuanxin Liu</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/z/zheng-lin/>Zheng Lin</a>
|
<a href=/people/w/weiping-wang/>Weiping Wang</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--228><div class="card-body p-3 small">Recently, knowledge distillation (KD) has shown great success in BERT compression. Instead of only learning from the teacher&#8217;s soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student&#8217;s performance. To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher&#8217;s hidden states of all the tokens in a layer-wise manner. In this paper, however, we observe that although distilling the teacher&#8217;s hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled. To understand this effect, we conduct a series of analysis. Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width. We first investigate a variety of <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. In this way, we show that 1) the student&#8217;s performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of <a href=https://en.wikipedia.org/wiki/Hydrogen_sulfide>HSK</a> can achieve the same performance as extensive <a href=https://en.wikipedia.org/wiki/Hydrogen_sulfide>HSK distillation</a>. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training speedup</a> of 2.7x 3.4x.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.229" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.229/>Rational LAMOL : A Rationale-based Lifelong Learning Framework<span class=acl-fixed-case>LAMOL</span>: A Rationale-based Lifelong Learning Framework</a></strong><br><a href=/people/k/kasidis-kanwatchara/>Kasidis Kanwatchara</a>
|
<a href=/people/t/thanapapas-horsuwan/>Thanapapas Horsuwan</a>
|
<a href=/people/p/piyawat-lertvittayakumjorn/>Piyawat Lertvittayakumjorn</a>
|
<a href=/people/b/boonserm-kijsirikul/>Boonserm Kijsirikul</a>
|
<a href=/people/p/peerapon-vateekul/>Peerapon Vateekul</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--229><div class="card-body p-3 small">Lifelong learning (LL) aims to train a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> on a stream of tasks while retaining knowledge from previous tasks. However, many prior attempts in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> still suffer from the catastrophic forgetting issue, where the model completely forgets what it just learned in the previous tasks. In this paper, we introduce Rational LAMOL, a novel end-to-end LL framework for <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. In order to alleviate catastrophic forgetting, Rational LAMOL enhances LAMOL, a recent LL model, by applying critical freezing guided by human rationales. When the <a href=https://en.wikipedia.org/wiki/Rational_choice_theory>human rationales</a> are not available, we propose exploiting unsupervised generated rationales as <a href=https://en.wikipedia.org/wiki/Substitution_reaction>substitutions</a>. In the experiment, we tested Rational LAMOL on permutations of three datasets from the ERASER benchmark. The results show that our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> outperformed vanilla LAMOL on most permutations. Furthermore, unsupervised rationale generation was able to consistently improve the overall LL performance from the baseline without relying on human-annotated rationales.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--231 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.231/>LeeBERT : Learned Early Exit for BERT with cross-level optimization<span class=acl-fixed-case>L</span>ee<span class=acl-fixed-case>BERT</span>: Learned Early Exit for <span class=acl-fixed-case>BERT</span> with cross-level optimization</a></strong><br><a href=/people/w/wei-zhu/>Wei Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--231><div class="card-body p-3 small">Pre-trained language models like BERT are performant in a wide range of natural language tasks. However, they are resource exhaustive and computationally expensive for industrial scenarios. Thus, early exits are adopted at each layer of BERT to perform adaptive computation by predicting easier samples with the first few layers to speed up the <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. In this work, to improve efficiency without performance drop, we propose a novel training scheme called Learned Early Exit for BERT (LeeBERT). First, we ask each exit to learn from each other, rather than learning only from the last layer. Second, the weights of different loss terms are learned, thus balancing off different objectives. We formulate the <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization of LeeBERT</a> as a bi-level optimization problem, and we propose a novel cross-level optimization (CLO) algorithm to improve the <a href=https://en.wikipedia.org/wiki/Mathematical_optimization>optimization</a> results. Experiments on the GLUE benchmark show that our proposed methods improve the performance of the state-of-the-art (SOTA) early exit methods for pre-trained models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--232 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.232" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.232/>Unsupervised Extractive Summarization-Based Representations for Accurate and Explainable Collaborative Filtering</a></strong><br><a href=/people/r/reinald-adrian-pugoy/>Reinald Adrian Pugoy</a>
|
<a href=/people/h/hung-yu-kao/>Hung-Yu Kao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--232><div class="card-body p-3 small">We pioneer the first extractive summarization-based collaborative filtering model called ESCOFILT. Our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> specifically produces extractive summaries for each item and user. Unlike other types of <a href=https://en.wikipedia.org/wiki/Explanation>explanations</a>, summary-level explanations closely resemble real-life explanations. The strength of ESCOFILT lies in the fact that it unifies <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representation</a> and explanation. In other words, extractive summaries both represent and explain the items and users. Our model uniquely integrates BERT, K-Means embedding clustering, and <a href=https://en.wikipedia.org/wiki/Multilayer_perceptron>multilayer perceptron</a> to learn sentence embeddings, representation-explanations, and user-item interactions, respectively. We argue that our approach enhances both rating prediction accuracy and user / item explainability. Our experiments illustrate that ESCOFILT&#8217;s prediction accuracy is better than the other state-of-the-art recommender models. Furthermore, we propose a comprehensive set of criteria that assesses the real-life explainability of explanations. Our explainability study demonstrates the superiority of and preference for summary-level explanations over other explanation types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.234/>Competence-based Multimodal Curriculum Learning for Medical Report Generation</a></strong><br><a href=/people/f/fenglin-liu/>Fenglin Liu</a>
|
<a href=/people/s/shen-ge/>Shen Ge</a>
|
<a href=/people/x/xian-wu/>Xian Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--234><div class="card-body p-3 small">Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> ; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance. The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMCL can be incorporated into existing models to improve their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--236 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.236" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.236/>Meta-KD : A Meta Knowledge Distillation Framework for Language Model Compression across Domains<span class=acl-fixed-case>KD</span>: A Meta Knowledge Distillation Framework for Language Model Compression across Domains</a></strong><br><a href=/people/h/haojie-pan/>Haojie Pan</a>
|
<a href=/people/c/chengyu-wang/>Chengyu Wang</a>
|
<a href=/people/m/minghui-qiu/>Minghui Qiu</a>
|
<a href=/people/y/yichang-zhang/>Yichang Zhang</a>
|
<a href=/people/y/yaliang-li/>Yaliang Li</a>
|
<a href=/people/j/jun-huang/>Jun Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--236><div class="card-body p-3 small">Pre-trained language models have been applied to various NLP tasks with considerable performance gains. However, the large model sizes, together with the long <a href=https://en.wikipedia.org/wiki/Time_complexity>inference time</a>, limit the deployment of such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in <a href=https://en.wikipedia.org/wiki/Real-time_computing>real-time applications</a>. One line of model compression approaches considers knowledge distillation to distill large teacher models into small student models. Most of these studies focus on single-domain only, which ignores the transferable knowledge from other domains. We notice that training a <a href=https://en.wikipedia.org/wiki/Teacher>teacher</a> with transferable knowledge digested across domains can achieve better generalization capability to help knowledge distillation. Hence we propose a Meta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students. Specifically, we explicitly force the meta-teacher to capture transferable knowledge at both instance-level and feature-level from multiple domains, and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher. Experiments on public multi-domain NLP tasks show the effectiveness and superiority of the proposed Meta-KD framework. Further, we also demonstrate the capability of Meta-KD in the settings where the training data is scarce.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.237/>A Semantic-based Method for Unsupervised Commonsense Question Answering</a></strong><br><a href=/people/y/yilin-niu/>Yilin Niu</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/j/jiaming-liang/>Jiaming Liang</a>
|
<a href=/people/w/wenkai-chen/>Wenkai Chen</a>
|
<a href=/people/x/xiaoyan-zhu/>Xiaoyan Zhu</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--237><div class="card-body p-3 small">Unsupervised commonsense question answering is appealing since <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> does not rely on any labeled task data. Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context. However, such scores from <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> can be easily affected by irrelevant factors, such as <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequencies</a>, <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structures</a>, etc. These distracting factors may not only mislead the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers. In this paper, we present a novel SEmantic-based Question Answering method (SEQA) for unsupervised commonsense question answering. Instead of directly scoring each answer choice, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> first generates a set of plausible answers with <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> between each plausible answer and each choice. We devise a simple, yet sound <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalism</a> for this idea and verify its effectiveness and robustness with extensive experiments. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on four <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark datasets</a>, and our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves the best results in <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised settings</a>. Moreover, when attacked by TextFooler with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.242.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.242" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.242/>Online Learning Meets Machine Translation Evaluation : Finding the Best Systems with the Least Human Effort<span class=acl-fixed-case>O</span>nline <span class=acl-fixed-case>L</span>earning Meets <span class=acl-fixed-case>M</span>achine <span class=acl-fixed-case>T</span>ranslation Evaluation: Finding the Best Systems with the Least Human Effort</a></strong><br><a href=/people/v/vania-mendonca/>Vânia Mendonça</a>
|
<a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/l/luisa-coheur/>Luisa Coheur</a>
|
<a href=/people/a/alberto-sardinha/>Alberto Sardinha</a>
|
<a href=/people/a/ana-lucia-santos/>Ana Lúcia Santos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--242><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>, assessing the quality of a large amount of <a href=https://en.wikipedia.org/wiki/Machine_translation>automatic translations</a> can be challenging. Automatic metrics are not reliable when it comes to high performing <a href=https://en.wikipedia.org/wiki/System>systems</a>. In addition, resorting to human evaluators can be expensive, especially when evaluating multiple systems. To overcome the latter challenge, we propose a novel application of <a href=https://en.wikipedia.org/wiki/Educational_technology>online learning</a> that, given an ensemble of Machine Translation systems, dynamically converges to the best systems, by taking advantage of the human feedback available. Our experiments on WMT&#8217;19 datasets show that our online approach quickly converges to the top-3 ranked systems for the language pairs considered, despite the lack of human feedback for many translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--243 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.243" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.243/>How Good is Your <a href=https://en.wikipedia.org/wiki/Tokenizer>Tokenizer</a>? On the Monolingual Performance of Multilingual Language Models</a></strong><br><a href=/people/p/phillip-rust/>Phillip Rust</a>
|
<a href=/people/j/jonas-pfeiffer/>Jonas Pfeiffer</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--243><div class="card-body p-3 small">In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model&#8217;s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.250.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--250 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.250 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.250" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.250/>Human-in-the-Loop for Data Collection : a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech</a></strong><br><a href=/people/m/margherita-fanton/>Margherita Fanton</a>
|
<a href=/people/h/helena-bonaldi/>Helena Bonaldi</a>
|
<a href=/people/s/serra-sinem-tekiroglu/>Serra Sinem Tekiroğlu</a>
|
<a href=/people/m/marco-guerini/>Marco Guerini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--250><div class="card-body p-3 small">Undermining the impact of hateful content with informed and non-aggressive responses, called counter narratives, has emerged as a possible solution for having healthier online communities. Thus, some <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP studies</a> have started addressing the task of counter narrative generation. Although such studies have made an effort to build hate speech / counter narrative (HS / CN) datasets for neural generation, they fall short in reaching either high-quality and/or high-quantity. In this paper, we propose a novel human-in-the-loop data collection methodology in which a generative language model is refined iteratively by using its own data from the previous loops to generate new training samples that experts review and/or post-edit. Our experiments comprised several loops including diverse dynamic variations. Results show that the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is scalable and facilitates diverse, novel, and cost-effective <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a>. To our knowledge, the resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is the only expert-based multi-target HS / CN dataset available to the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--252 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.252/>Joint Models for Answer Verification in Question Answering Systems</a></strong><br><a href=/people/z/zeyu-zhang/>Zeyu Zhang</a>
|
<a href=/people/t/thuy-vu/>Thuy Vu</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--252><div class="card-body p-3 small">This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection (AS2) modules, which are core components of retrieval-based Question Answering (QA) systems. Our work shows that a critical step to effectively exploiting an <a href=https://en.wikipedia.org/wiki/Answer_set>answer set</a> regards modeling the interrelated information between pair of answers. For this purpose, we build a three-way multi-classifier, which decides if an answer supports, refutes, or is neutral with respect to another one. More specifically, our neural architecture integrates a state-of-the-art AS2 module with the multi-classifier, and a joint layer connecting all components. We tested our models on WikiQA, TREC-QA, and a real-world dataset. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> obtain the new state of the art in AS2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--254 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.254/>TAT-QA : A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance<span class=acl-fixed-case>TAT</span>-<span class=acl-fixed-case>QA</span>: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance</a></strong><br><a href=/people/f/fengbin-zhu/>Fengbin Zhu</a>
|
<a href=/people/w/wenqiang-lei/>Wenqiang Lei</a>
|
<a href=/people/y/youcheng-huang/>Youcheng Huang</a>
|
<a href=/people/c/chao-wang/>Chao Wang</a>
|
<a href=/people/s/shuo-zhang/>Shuo Zhang</a>
|
<a href=/people/j/jiancheng-lv/>Jiancheng Lv</a>
|
<a href=/people/f/fuli-feng/>Fuli Feng</a>
|
<a href=/people/t/tat-seng-chua/>Tat-Seng Chua</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--254><div class="card-body p-3 small">Hybrid data combining both <a href=https://en.wikipedia.org/wiki/Table_(information)>tabular and textual content</a> (e.g., financial reports) are quite pervasive in the real world. However, Question Answering (QA) over such hybrid data is largely neglected in existing research. In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as <a href=https://en.wikipedia.org/wiki/Addition>addition</a>, <a href=https://en.wikipedia.org/wiki/Subtraction>subtraction</a>, <a href=https://en.wikipedia.org/wiki/Multiplication>multiplication</a>, <a href=https://en.wikipedia.org/wiki/Division_(mathematics)>division</a>, <a href=https://en.wikipedia.org/wiki/Counting>counting</a>, comparison / sorting, and the compositions. We further propose a novel QA model termed TAGOP, which is capable of reasoning over both tables and text. It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, and then applies <a href=https://en.wikipedia.org/wiki/Computer_algebra>symbolic reasoning</a> over them with a set of aggregation operators to arrive at the final answer. TAGOP achieves 58.0 % inF1, which is an 11.1 % absolute increase over the previous best baseline model, according to our experiments on TAT-QA. But this result still lags far behind performance of expert human, i.e.90.8 % in <a href=https://en.wikipedia.org/wiki/F1_(disambiguation)>F1</a>. It is demonstrated that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid form data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.261.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--261 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.261 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.261" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.261/>Position Bias Mitigation : A Knowledge-Aware Graph Model for Emotion Cause Extraction</a></strong><br><a href=/people/h/hanqi-yan/>Hanqi Yan</a>
|
<a href=/people/l/lin-gui/>Lin Gui</a>
|
<a href=/people/g/gabriele-pergola/>Gabriele Pergola</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--261><div class="card-body p-3 small">The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> expressed in text. We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves. Existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for ECE tend to explore such relative position information and suffer from the dataset bias. To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses. We test the performance of existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on such adversarial examples and observe a significant performance drop. To address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause. Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust against adversarial attacks compared to existing models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.262.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.262/>Every Bite Is an Experience : Key Point Analysis of Business Reviews<span class=acl-fixed-case>K</span>ey <span class=acl-fixed-case>P</span>oint <span class=acl-fixed-case>A</span>nalysis of Business Reviews</a></strong><br><a href=/people/r/roy-bar-haim/>Roy Bar-Haim</a>
|
<a href=/people/l/lilach-eden/>Lilach Eden</a>
|
<a href=/people/y/yoav-kantor/>Yoav Kantor</a>
|
<a href=/people/r/roni-friedman/>Roni Friedman</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--262><div class="card-body p-3 small">Previous work on review summarization focused on measuring the sentiment toward the main aspects of the reviewed product or business, or on creating a textual summary. These approaches provide only a partial view of the data : aspect-based sentiment summaries lack sufficient explanation or justification for the aspect rating, while textual summaries do not quantify the significance of each element, and are not well-suited for representing conflicting views. Recently, Key Point Analysis (KPA) has been proposed as a summarization framework that provides both textual and quantitative summary of the main points in the data. We adapt KPA to review data by introducing Collective Key Point Mining for better key point extraction ; integrating <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> into KPA ; identifying good key point candidates for review summaries ; and leveraging the massive amount of available reviews and their metadata. We show empirically that these novel extensions of <a href=https://en.wikipedia.org/wiki/KPA>KPA</a> substantially improve its performance. We demonstrate that promising results can be achieved without any domain-specific annotation, while human supervision can lead to further improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.264" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.264/>Consistency Regularization for Cross-Lingual Fine-Tuning</a></strong><br><a href=/people/b/bo-zheng/>Bo Zheng</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/w/wenhui-wang/>Wenhui Wang</a>
|
<a href=/people/z/zewen-chi/>Zewen Chi</a>
|
<a href=/people/s/saksham-singhal/>Saksham Singhal</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/x/xia-song/>Xia Song</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--264><div class="card-body p-3 small">Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. In this work, we propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, <a href=https://en.wikipedia.org/wiki/Gaussian_noise>Gaussian noise</a>, code-switch substitution, and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. In addition, we employ <a href=https://en.wikipedia.org/wiki/Consistency_(statistics)>model consistency</a> to regularize the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks, including text classification, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--266 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.266" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.266/>Rejuvenating Low-Frequency Words : Making the Most of Parallel Data in Non-Autoregressive Translation</a></strong><br><a href=/people/l/liang-ding/>Liang Ding</a>
|
<a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/x/xuebo-liu/>Xuebo Liu</a>
|
<a href=/people/d/derek-f-wong/>Derek F. Wong</a>
|
<a href=/people/d/dacheng-tao/>Dacheng Tao</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--266><div class="card-body p-3 small">Knowledge distillation (KD) is commonly used to construct <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a> for training non-autoregressive translation (NAT) models. However, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on predicting low-frequency words. To alleviate the problem, we directly expose the <a href=https://en.wikipedia.org/wiki/Raw_data>raw data</a> into <a href=https://en.wikipedia.org/wiki/Network_address_translation>NAT</a> by leveraging pretraining. By analyzing directed alignments, we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source. Accordingly, we propose reverse KD to rejuvenate more alignments for low-frequency target words. To make the most of authentic and synthetic data, we combine these complementary approaches as a new training strategy for further boosting <a href=https://en.wikipedia.org/wiki/Network_address_translation>NAT</a> performance. We conduct experiments on five translation benchmarks over two advanced <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>. Results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on <a href=https://en.wikipedia.org/wiki/Word_frequency>low-frequency words</a>. Encouragingly, our <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, respectively. Our code, data, and trained models are available at.<url>https://github.com/longyuewangdcu/RLFW-NAT</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--267 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.267" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.267/>G-Transformer for Document-Level Machine Translation<span class=acl-fixed-case>G</span>-Transformer for Document-Level Machine Translation</a></strong><br><a href=/people/g/guangsheng-bao/>Guangsheng Bao</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/z/zhiyang-teng/>Zhiyang Teng</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/w/weihua-luo/>Weihua Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--267><div class="card-body p-3 small">Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a> can fail. In this paper, we find such failure is not caused by <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>, but by sticking around local minima during training. Our analysis shows that the increased <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.268.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--268 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.268 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.268" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.268/>Prevent the <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a> from being Overconfident in Neural Machine Translation</a></strong><br><a href=/people/m/mengqi-miao/>Mengqi Miao</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/y/yijin-liu/>Yijin Liu</a>
|
<a href=/people/x/xiao-hua-zhou/>Xiao-Hua Zhou</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--268><div class="card-body p-3 small">The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation. Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation. Despite its success, <a href=https://en.wikipedia.org/wiki/Nootropic>NMT</a> still suffers from the <a href=https://en.wikipedia.org/wiki/Hallucination>hallucination problem</a>, generating fluent but inadequate translations. The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM. Accordingly, we define the Margin between the NMT and the <a href=https://en.wikipedia.org/wiki/Linear_model>LM</a>, calculated by subtracting the predicted probability of the <a href=https://en.wikipedia.org/wiki/Linear_model>LM</a> from that of the NMT model for each token. The <a href=https://en.wikipedia.org/wiki/Margin>Margin</a> is negatively correlated to the overconfidence degree of the LM. Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentence-level Objective (MSO) to maximize the Margin for preventing the LM from being overconfident. Experiments on WMT14 English-to-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.272.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--272 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.272 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.272" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.272/>Diversifying Dialog Generation via Adaptive Label Smoothing</a></strong><br><a href=/people/y/yida-wang/>Yida Wang</a>
|
<a href=/people/y/yinhe-zheng/>Yinhe Zheng</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--272><div class="card-body p-3 small">Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature. Although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts. In this paper, we propose an Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts. The maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module. The resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be trained in an endto-end manner. Extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--273 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.273/>Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training</a></strong><br><a href=/people/l/li-ming-zhan/>Li-Ming Zhan</a>
|
<a href=/people/h/haowen-liang/>Haowen Liang</a>
|
<a href=/people/b/bo-liu/>Bo Liu</a>
|
<a href=/people/l/lu-fan/>Lu Fan</a>
|
<a href=/people/x/xiao-ming-wu/>Xiao-Ming Wu</a>
|
<a href=/people/a/albert-y-s-lam/>Albert Y.S. Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--273><div class="card-body p-3 small">Out-of-scope intent detection is of practical importance in task-oriented dialogue systems. Since the distribution of outlier utterances is arbitrary and unknown in the training stage, existing methods commonly rely on strong assumptions on data distribution such as <a href=https://en.wikipedia.org/wiki/Mixture_of_Gaussians>mixture of Gaussians</a> to make <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, resulting in either complex multi-step training procedures or hand-crafted rules such as confidence threshold selection for <a href=https://en.wikipedia.org/wiki/Outlier_detection>outlier detection</a>. In this paper, we propose a simple yet effective method to train an out-of-scope intent classifier in a fully end-to-end manner by simulating the test scenario in training, which requires no assumption on data distribution and no additional post-processing or threshold setting. Specifically, we construct a set of pseudo outliers in the training stage, by generating synthetic outliers using inliner features via self-supervision and sampling out-of-scope sentences from easily available open-domain datasets. The pseudo outliers are used to train a discriminative classifier that can be directly applied to and generalize well on the test task. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> extensively on four benchmark dialogue datasets and observe significant improvements over state-of-the-art approaches. Our code has been released at https://github.com/liam0949/DCLOOS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--275 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.275" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.275/>Nested Named Entity Recognition via Explicitly Excluding the Influence of the Best Path</a></strong><br><a href=/people/y/yiran-wang/>Yiran Wang</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--275><div class="card-body p-3 small">This paper presents a novel method for nested named entity recognition. As a layered method, our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> extends the prior second-best path recognition method by explicitly excluding the influence of the best path. Our method maintains a set of hidden states at each time step and selectively leverages them to build a different <a href=https://en.wikipedia.org/wiki/Potential_function>potential function</a> for recognition at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.277.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--277 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.277 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.277" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.277/>Revisiting the Negative Data of Distantly Supervised Relation Extraction</a></strong><br><a href=/people/c/chenhao-xie/>Chenhao Xie</a>
|
<a href=/people/j/jiaqing-liang/>Jiaqing Liang</a>
|
<a href=/people/j/jingping-liu/>Jingping Liu</a>
|
<a href=/people/c/chengsong-huang/>Chengsong Huang</a>
|
<a href=/people/w/wenhao-huang/>Wenhao Huang</a>
|
<a href=/people/y/yanghua-xiao/>Yanghua Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--277><div class="card-body p-3 small">Distantly supervision automatically generates plenty of training samples for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> also incurs two major problems : noisy labels and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>imbalanced training data</a>. Previous works focus more on reducing wrongly labeled relations (false positives) while few explore the missing relations that are caused by incompleteness of knowledge base (false negatives). Furthermore, the quantity of negative labels overwhelmingly surpasses the positive ones in previous problem formulations. In this paper, we first provide a thorough analysis of the above challenges caused by negative data. Next, we formulate the problem of <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> into as a positive unlabeled learning task to alleviate <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false negative problem</a>. Thirdly, we propose a pipeline approach, dubbed ReRe, that first performs sentence classification with relational labels and then extracts the subjects / objects. Experimental results show that the proposed method consistently outperforms existing approaches and remains excellent performance even learned with a large quantity of false positive samples. Source code is available online at https://github.com/redreamality/RERE-relation-extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.278.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--278 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.278 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.278" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.278/>Knowing the No-match : Entity Alignment with Dangling Cases</a></strong><br><a href=/people/z/zequn-sun/>Zequn Sun</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/w/wei-hu/>Wei Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--278><div class="card-body p-3 small">This paper studies a new problem setting of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity alignment</a> for knowledge graphs (KGs). Since KGs possess different sets of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a>, there could be entities that can not find alignment across them, leading to the problem of dangling entities. As the first attempt to this problem, we construct a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and design a multi-task learning framework for both <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity alignment</a> and dangling entity detection. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can opt to abstain from predicting alignment for the detected dangling entities. We propose three techniques for dangling entity detection that are based on the distribution of nearest-neighbor distances, i.e., <a href=https://en.wikipedia.org/wiki/Nearest_neighbor_search>nearest neighbor classification</a>, <a href=https://en.wikipedia.org/wiki/Marginal_distribution>marginal ranking</a> and background ranking. After detecting and removing dangling entities, an incorporated entity alignment model in our framework can provide more robust alignment for remaining entities. Comprehensive experiments and analyses demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>. We further discover that the dangling entity detection module can, in turn, improve alignment learning and the final performance. The contributed resource is publicly available to foster further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--280 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.280" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.280/>BERT is to NLP what AlexNet is to CV : Can Pre-Trained Language Models Identify Analogies?<span class=acl-fixed-case>BERT</span> is to <span class=acl-fixed-case>NLP</span> what <span class=acl-fixed-case>A</span>lex<span class=acl-fixed-case>N</span>et is to <span class=acl-fixed-case>CV</span>: Can Pre-Trained Language Models Identify Analogies?</a></strong><br><a href=/people/a/asahi-ushio/>Asahi Ushio</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--280><div class="card-body p-3 small">Analogies play a central role in human commonsense reasoning. The ability to recognize <a href=https://en.wikipedia.org/wiki/Analogy>analogies</a> such as eye is to seeing what ear is to hearing, sometimes referred to as <a href=https://en.wikipedia.org/wiki/Analogy>analogical proportions</a>, shape how we structure knowledge and understand <a href=https://en.wikipedia.org/wiki/Language>language</a>. Surprisingly, however, the task of identifying such <a href=https://en.wikipedia.org/wiki/Analogy>analogies</a> has not yet received much attention in the <a href=https://en.wikipedia.org/wiki/Language_model>language model era</a>. In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> obtained from educational settings, as well as more commonly used <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters. Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models. Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--281 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.281" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.281/>Exploring the Representation of Word Meanings in Context : A Case Study on Homonymy and Synonymy<span class=acl-fixed-case>A</span> Case Study on Homonymy and Synonymy</a></strong><br><a href=/people/m/marcos-garcia/>Marcos Garcia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--281><div class="card-body p-3 small">This paper presents a multilingual study of word meaning representations in context. We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as <a href=https://en.wikipedia.org/wiki/Homonym>homonymy</a> and <a href=https://en.wikipedia.org/wiki/Synonym>synonymy</a>. To do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses. A systematic assessment on four scenarios shows that the best monolingual models based on <a href=https://en.wikipedia.org/wiki/Transformers_(film)>Transformers</a> can adequately disambiguate <a href=https://en.wikipedia.org/wiki/Homonym>homonyms</a> in context. However, as they rely heavily on context, these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> fail at representing words with different senses when occurring in similar sentences. Experiments are performed in <a href=https://en.wikipedia.org/wiki/Galician_language>Galician</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and both the dataset (with more than 3,000 evaluation items) and new models are freely released with this study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.283" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.283/>HERALD : An Annotation Efficient Method to Detect User Disengagement in Social Conversations<span class=acl-fixed-case>HERALD</span>: An Annotation Efficient Method to Detect User Disengagement in Social Conversations</a></strong><br><a href=/people/w/weixin-liang/>Weixin Liang</a>
|
<a href=/people/k/kai-hui-liang/>Kai-Hui Liang</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--283><div class="card-body p-3 small">Open-domain dialog systems have a user-centric goal : to provide humans with an engaging conversation experience. User engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning. Existing work on detecting user disengagement typically requires hand-labeling many dialog samples. We propose HERALD, an efficient annotation framework that reframes the training data annotation process as a denoising problem. Specifically, instead of manually labeling training samples, we first use a set of labeling heuristics to label training samples automatically. We then denoise the weakly labeled data using the Shapley algorithm. Finally, we use the <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>denoised data</a> to train a user engagement detector. Our experiments show that HERALD improves annotation efficiency significantly and achieves 86 % user disengagement detection accuracy in two dialog corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.284.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--284 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.284 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.284/>Value-Agnostic Conversational Semantic Parsing</a></strong><br><a href=/people/e/emmanouil-antonios-platanios/>Emmanouil Antonios Platanios</a>
|
<a href=/people/a/adam-pauls/>Adam Pauls</a>
|
<a href=/people/s/subhro-roy/>Subhro Roy</a>
|
<a href=/people/y/yuchen-zhang/>Yuchen Zhang</a>
|
<a href=/people/a/alexander-kyte/>Alexander Kyte</a>
|
<a href=/people/a/alan-guo/>Alan Guo</a>
|
<a href=/people/s/sam-thomson/>Sam Thomson</a>
|
<a href=/people/j/jayant-krishnamurthy/>Jayant Krishnamurthy</a>
|
<a href=/people/j/jason-wolfe/>Jason Wolfe</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--284><div class="card-body p-3 small">Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a>, and system responses. Existing <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> typically condition on rich representations of history that include the complete set of values and computations previously discussed. We propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that abstracts over values to focus <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> on type- and function-level context. This approach provides a compact encoding of dialogue histories and predicted programs, improving <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and computational efficiency. Our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime. Trained on the SMCalFlow and TreeDST datasets, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms prior <a href=https://en.wikipedia.org/wiki/Work_(physics)>work</a> by 7.3 % and 10.6 % respectively in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>absolute accuracy</a>. Trained on only a thousand examples from each <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> outperforms strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by 12.4 % and 6.4 %. These results indicate that simple <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> are key to effective <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> in conversational semantic parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--285 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.285" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.285/>MPC-BERT : A Pre-Trained Language Model for Multi-Party Conversation Understanding<span class=acl-fixed-case>MPC</span>-<span class=acl-fixed-case>BERT</span>: A Pre-Trained Language Model for Multi-Party Conversation Understanding</a></strong><br><a href=/people/j/jia-chen-gu/>Jia-Chen Gu</a>
|
<a href=/people/c/chongyang-tao/>Chongyang Tao</a>
|
<a href=/people/z/zhenhua-ling/>Zhenhua Ling</a>
|
<a href=/people/c/can-xu/>Can Xu</a>
|
<a href=/people/x/xiubo-geng/>Xiubo Geng</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--285><div class="card-body p-3 small">Recently, various neural models for multi-party conversation (MPC) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction. However, these existing methods on MPC usually represent interlocutors and utterances individually and ignore the inherent complicated structure in MPC which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process. To this end, we present MPC-BERT, a pre-trained model for MPC understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks. Particularly, these tasks can be generally categorized into (1) interlocutor structure modeling including reply-to utterance recognition, identical speaker searching and pointer consistency distinction, and (2) utterance semantics modeling including masked shared utterance restoration and shared node detection. We evaluate MPC-BERT on three downstream tasks including addressee recognition, <a href=https://en.wikipedia.org/wiki/Speaker_identification>speaker identification</a> and response selection. Experimental results show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--287 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.287" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.287/>NeuralWOZ : Learning to Collect Task-Oriented Dialogue via Model-Based Simulation<span class=acl-fixed-case>N</span>eural<span class=acl-fixed-case>WOZ</span>: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation</a></strong><br><a href=/people/s/sungdong-kim/>Sungdong Kim</a>
|
<a href=/people/m/minsuk-chang/>Minsuk Chang</a>
|
<a href=/people/s/sang-woo-lee/>Sang-Woo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--287><div class="card-body p-3 small">We propose NeuralWOZ, a novel dialogue collection framework that uses model-based dialogue simulation. NeuralWOZ has two pipelined models, Collector and Labeler. Collector generates dialogues from (1) user&#8217;s goal instructions, which are the user context and task constraints in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>, and (2) system&#8217;s API call results, which is a list of possible query responses for user requests from the given <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. Labeler annotates the generated dialogue by formulating the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> as a <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple-choice problem</a>, in which the candidate labels are extracted from goal instructions and API call results. We demonstrate the effectiveness of the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in the zero-shot domain transfer learning for dialogue state tracking. In the evaluation, the synthetic dialogue corpus generated from NeuralWOZ achieves a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> with improvements of 4.4 % point joint goal accuracy on average across domains, and improvements of 5.7 % point of zero-shot coverage against the MultiWOZ 2.1 dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.289.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--289 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.289 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.289" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.289/>Structural Guidance for Transformer Language Models</a></strong><br><a href=/people/p/peng-qian/>Peng Qian</a>
|
<a href=/people/t/tahira-naseem/>Tahira Naseem</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a>
|
<a href=/people/r/ramon-fernandez-astudillo/>Ramón Fernandez Astudillo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--289><div class="card-body p-3 small">Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The Generative Parsing idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The Structural Scaffold idea guides the language model&#8217;s representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models&#8217; syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--290 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.290" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.290/>Surprisal Estimators for Human Reading Times Need Character Models</a></strong><br><a href=/people/b/byung-doh-oh/>Byung-Doh Oh</a>
|
<a href=/people/c/christian-clark/>Christian Clark</a>
|
<a href=/people/w/william-schuler/>William Schuler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--290><div class="card-body p-3 small">While the use of character models has been popular in NLP applications, it has not been explored much in the context of psycholinguistic modeling. This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities. Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, <a href=https://en.wikipedia.org/wiki/Eye_tracking>eye-tracking</a>, and <a href=https://en.wikipedia.org/wiki/Functional_magnetic_resonance_imaging>fMRI data</a> than those from large-scale language models trained on much more data. This may suggest that the proposed processing model provides a more humanlike account of <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence processing</a>, which assumes a larger role of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>, <a href=https://en.wikipedia.org/wiki/Phonotactics>phonotactics</a>, and orthographic complexity than was previously thought.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.294.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--294 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.294 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.294" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.294/>H-Transformer-1D : Fast One-Dimensional Hierarchical Attention for Sequences<span class=acl-fixed-case>H</span>-Transformer-1<span class=acl-fixed-case>D</span>: Fast One-Dimensional Hierarchical Attention for Sequences</a></strong><br><a href=/people/z/zhenhai-zhu/>Zhenhai Zhu</a>
|
<a href=/people/r/radu-soricut/>Radu Soricut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--294><div class="card-body p-3 small">We describe an efficient hierarchical method to compute <a href=https://en.wikipedia.org/wiki/Attention>attention</a> in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--297 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.297" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.297/>Towards Propagation Uncertainty : Edge-enhanced Bayesian Graph Convolutional Networks for Rumor Detection<span class=acl-fixed-case>B</span>ayesian Graph Convolutional Networks for Rumor Detection</a></strong><br><a href=/people/l/lingwei-wei/>Lingwei Wei</a>
|
<a href=/people/d/dou-hu/>Dou Hu</a>
|
<a href=/people/w/wei-zhou/>Wei Zhou</a>
|
<a href=/people/z/zhaojuan-yue/>Zhaojuan Yue</a>
|
<a href=/people/s/songlin-hu/>Songlin Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--297><div class="card-body p-3 small">Detecting rumors on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is a very critical task with significant implications to the economy, public health, etc. Previous works generally capture effective <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> from texts and the propagation structure. However, the uncertainty caused by unreliable relations in the propagation structure is common and inevitable due to wily rumor producers and the limited collection of spread data. Most approaches neglect it and may seriously limit the learning of features. Towards this issue, this paper makes the first attempt to explore propagation uncertainty for rumor detection. Specifically, we propose a novel Edge-enhanced Bayesian Graph Convolutional Network (EBGCN) to capture robust structural features. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> adaptively rethinks the reliability of latent relations by adopting a <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian approach</a>. Besides, we design a new edge-wise consistency training framework to optimize the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by enforcing consistency on relations. Experiments on three public benchmark datasets demonstrate that the proposed model achieves better performance than baseline methods on both rumor detection and early rumor detection tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--301 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.301/>A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections</a></strong><br><a href=/people/d/dimitris-pappas/>Dimitris Pappas</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--301><div class="card-body p-3 small">Question answering (QA) systems for large document collections typically use <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipelines</a> that (i) retrieve possibly relevant documents, (ii) re-rank them, (iii) rank paragraphs or other snippets of the top-ranked documents, and (iv) select spans of the top-ranked snippets as exact answers. Pipelines are conceptually simple, but errors propagate from one component to the next, without later components being able to revise earlier decisions. We present an architecture for joint document and snippet ranking, the two middle stages, which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents. The <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> is general and can be used with any neural text relevance ranker. We experiment with two main instantiations of the architecture, based on POSIT-DRMM (PDRMM) and a BERT-based ranker. Experiments on biomedical data from BIOASQ show that our joint models vastly outperform the pipelines in snippet retrieval, the main goal for QA, with fewer trainable parameters, also remaining competitive in document retrieval. Furthermore, our joint PDRMM-based model is competitive with BERT-based models, despite using orders of magnitude fewer parameters. These claims are also supported by human evaluation on two test batches of BIOASQ. To test our key findings on another <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we modified the Natural Questions dataset so that it can also be used for document and snippet retrieval. Our joint PDRMM-based model again outperforms the corresponding <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> in snippet retrieval on the modified Natural Questions dataset, even though it performs worse than the <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> in <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--303 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.303/>ABCD : A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences<span class=acl-fixed-case>ABCD</span>: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences</a></strong><br><a href=/people/y/yanjun-gao/>Yanjun Gao</a>
|
<a href=/people/t/ting-hao-huang/>Ting-Hao Huang</a>
|
<a href=/people/r/rebecca-j-passonneau/>Rebecca J. Passonneau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--303><div class="card-body p-3 small">Atomic clauses are fundamental text units for understanding complex sentences. Identifying the atomic sentences within complex sentences is important for applications such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>, <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a>, discourse parsing, and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. Previous work mainly relies on rule-based methods dependent on <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. We propose a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to decompose each complex sentence into simple sentences derived from the tensed clauses in the source, and a novel problem formulation as a graph edit task. Our neural model learns to Accept, Break, Copy or Drop elements of a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> that combines word adjacency and grammatical dependencies. The full processing pipeline includes modules for <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph construction</a>, graph editing, and sentence generation from the output graph. We introduce DeSSE, a new dataset designed to train and evaluate complex sentence decomposition, and MinWiki, a subset of MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on MinWiki. On DeSSE, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline. Results include a detailed error analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--308 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.308" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.308/>VECO : Variable and Flexible Cross-lingual Pre-training for <a href=https://en.wikipedia.org/wiki/Language_understanding>Language Understanding</a> and Generation<span class=acl-fixed-case>VECO</span>: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation</a></strong><br><a href=/people/f/fuli-luo/>Fuli Luo</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/j/jiahao-liu/>Jiahao Liu</a>
|
<a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/b/bin-bi/>Bin Bi</a>
|
<a href=/people/s/songfang-huang/>Songfang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--308><div class="card-body p-3 small">Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages. However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages. In this paper, we plug a cross-attention module into the Transformer encoder to explicitly build the interdependence between languages. It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language. More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> to generation. As a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and sentence retrieval. For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to 1 2 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--317 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.317" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.317/>Check It Again : Progressive Visual Question Answering via Visual Entailment</a></strong><br><a href=/people/q/qingyi-si/>Qingyi Si</a>
|
<a href=/people/z/zheng-lin/>Zheng Lin</a>
|
<a href=/people/m/ming-yu-zheng/>Ming yu Zheng</a>
|
<a href=/people/p/peng-fu/>Peng Fu</a>
|
<a href=/people/w/weiping-wang/>Weiping Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--317><div class="card-body p-3 small">While sophisticated neural-based models have achieved remarkable success in Visual Question Answering (VQA), these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> tend to answer questions only according to superficial correlations between question and answer. Several recent approaches have been developed to address this language priors problem. However, most of them predict the correct answer according to one best output without checking the authenticity of answers. Besides, they only explore the interaction between image and question, ignoring the semantics of candidate answers. In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment. Specifically, we first select the candidate answers relevant to the question or the image, then we rerank the candidate answers by a visual entailment task, which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer. Experimental results show the effectiveness of our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, which establishes a new state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on VQA-CP v2 with a 7.55 % improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--325 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.325" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.325/>How is BERT surprised? Layerwise detection of linguistic anomalies<span class=acl-fixed-case>BERT</span> surprised? Layerwise detection of linguistic anomalies</a></strong><br><a href=/people/b/bai-li/>Bai Li</a>
|
<a href=/people/z/zining-zhu/>Zining Zhu</a>
|
<a href=/people/g/guillaume-thomas/>Guillaume Thomas</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--325><div class="card-body p-3 small">Transformer language models have shown remarkable ability in detecting when a word is anomalous in context, but likelihood scores offer no information about the cause of the anomaly. In this work, we use Gaussian models for density estimation at intermediate layers of three language models (BERT, RoBERTa, and XLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark. In lower layers, surprisal is highly correlated to low token frequency, but this correlation diminishes in upper layers. Next, we gather datasets of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphosyntactic</a>, semantic, and commonsense anomalies from psycholinguistic studies ; we find that the best performing model RoBERTa exhibits surprisal in earlier layers when the anomaly is <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphosyntactic</a> than when it is semantic, while commonsense anomalies do not exhibit surprisal at any intermediate layer. These results suggest that language models employ separate mechanisms to detect different types of linguistic anomalies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--326 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.326/>Psycholinguistic Tripartite Graph Network for Personality Detection</a></strong><br><a href=/people/t/tao-yang/>Tao Yang</a>
|
<a href=/people/f/feifan-yang/>Feifan Yang</a>
|
<a href=/people/h/haolan-ouyang/>Haolan Ouyang</a>
|
<a href=/people/x/xiaojun-quan/>Xiaojun Quan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--326><div class="card-body p-3 small">Most of the recent work on personality detection from online posts adopts multifarious deep neural networks to represent the posts and builds predictive models in a data-driven manner, without the exploitation of psycholinguistic knowledge that may unveil the connections between one&#8217;s language use and his psychological traits. In this paper, we propose a psycholinguistic knowledge-based tripartite graph network, TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer. The graph network injects structural psycholinguistic knowledge in LIWC, a computerized instrument for psycholinguistic analysis, by constructing a heterogeneous tripartite graph. The initializer is employed to provide <a href=https://en.wikipedia.org/wiki/Graph_embedding>initial embeddings</a> for the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>graph nodes</a>. To reduce the computational cost in graph learning, we further propose a novel flow graph attention network (GAT) that only transmits messages between neighboring parties in the <a href=https://en.wikipedia.org/wiki/Tripartite_graph>tripartite graph</a>. Benefiting from the <a href=https://en.wikipedia.org/wiki/Tripartite_graph>tripartite graph</a>, TrigNet can aggregate post information from a psychological perspective, which is a novel way of exploiting <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>. Extensive experiments on two datasets show that TrigNet outperforms the existing state-of-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> by 3.47 and 2.10 points in average F1. Moreover, the flow GAT reduces the <a href=https://en.wikipedia.org/wiki/FLOPS>FLOPS</a> and Memory measures by 38 % and 32 %, respectively, in comparison to the original GAT in our setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--329 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.329/>Probing Toxic Content in Large Pre-Trained Language Models</a></strong><br><a href=/people/n/nedjma-ousidhoum/>Nedjma Ousidhoum</a>
|
<a href=/people/x/xinran-zhao/>Xinran Zhao</a>
|
<a href=/people/t/tianqing-fang/>Tianqing Fang</a>
|
<a href=/people/y/yangqiu-song/>Yangqiu Song</a>
|
<a href=/people/d/dit-yan-yeung/>Dit-Yan Yeung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--329><div class="card-body p-3 small">Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>templates</a> are prompted by a name of a social group followed by a <a href=https://en.wikipedia.org/wiki/Causality>cause-effect relation</a>. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to assess and mitigate the <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> transmitted by PTLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.338.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--338 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.338 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.338" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.338/>HiddenCut : Simple Data Augmentation for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a> with Better Generalizability<span class=acl-fixed-case>H</span>idden<span class=acl-fixed-case>C</span>ut: Simple Data Augmentation for Natural Language Understanding with Better Generalizability</a></strong><br><a href=/people/j/jiaao-chen/>Jiaao Chen</a>
|
<a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/w/weizhu-chen/>Weizhu Chen</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--338><div class="card-body p-3 small">Fine-tuning large pre-trained models with task-specific data has achieved great success in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. However, it has been demonstrated that the majority of information within the self-attention networks is redundant and not utilized effectively during the fine-tuning stage. This leads to inferior results when generalizing the obtained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> to out-of-domain distributions. To this end, we propose a simple yet effective data augmentation technique, HiddenCut, to better regularize the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> and encourage it to learn more generalizable features. Specifically, contiguous spans within the hidden space are dynamically and strategically dropped during training. Experiments show that our HiddenCut method outperforms the state-of-the-art augmentation methods on the GLUE benchmark, and consistently exhibits superior generalization performances on out-of-distribution and challenging counterexamples. We have publicly released our code at https://github.com/GT-SALT/HiddenCut.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.339.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--339 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.339 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.339/>Neural Stylistic Response Generation with Disentangled Latent Variables</a></strong><br><a href=/people/q/qingfu-zhu/>Qingfu Zhu</a>
|
<a href=/people/w/weinan-zhang/>Wei-Nan Zhang</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--339><div class="card-body p-3 small">Generating open-domain conversational responses in the desired style usually suffers from the lack of parallel data in the style. Meanwhile, using monolingual stylistic data to increase style intensity often leads to the expense of decreasing content relevance. In this paper, we propose to disentangle the content and style in latent space by diluting sentence-level information in style representations. Combining the desired style representation and a response content representation will then obtain a stylistic response. Our approach achieves a higher BERT-based style intensity score and comparable BLEU scores, compared with baselines. Human evaluation results show that our approach significantly improves style intensity and maintains content relevance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--341 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.341/>RADDLE : An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems<span class=acl-fixed-case>RADDLE</span>: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems</a></strong><br><a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/c/chunyuan-li/>Chunyuan Li</a>
|
<a href=/people/z/zhu-zhang/>Zhu Zhang</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/j/jinchao-li/>Jinchao Li</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--341><div class="card-body p-3 small">For task-oriented dialog systems to be maximally useful, it must be able to process conversations in a way that is (1) generalizable with a small number of training examples for new task domains, and (2) robust to user input in various styles, modalities, or domains. In pursuit of these goals, we introduce the RADDLE benchmark, a collection of corpora and tools for evaluating the performance of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> across a diverse set of domains. By including tasks with limited training data, RADDLE is designed to favor and encourage <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> with a strong <a href=https://en.wikipedia.org/wiki/Generalization>generalization ability</a>. RADDLE also includes a diagnostic checklist that facilitates detailed robustness analysis in aspects such as <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>language variations</a>, <a href=https://en.wikipedia.org/wiki/Speech_error>speech errors</a>, unseen entities, and out-of-domain utterances. We evaluate recent state-of-the-art systems based on pre-training and fine-tuning, and find that grounded pre-training on heterogeneous dialog corpora performs better than training a separate model per domain. Adversarial training is also proposed to improve <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a> against <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy inputs</a>. Overall, existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are less than satisfactory in robustness evaluation, which suggests opportunities for future improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--343 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.343.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.343/>A Pre-training Strategy for Zero-Resource Response Selection in Knowledge-Grounded Conversations</a></strong><br><a href=/people/c/chongyang-tao/>Chongyang Tao</a>
|
<a href=/people/c/changyu-chen/>Changyu Chen</a>
|
<a href=/people/j/jiazhan-feng/>Jiazhan Feng</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--343><div class="card-body p-3 small">Recently, many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge (e.g., documents) when conversing with humans. However, it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents, which hinders the effective and adequate training of knowledge selection and response matching. To overcome the challenge, we consider decomposing the training of the knowledge-grounded response selection into three tasks including : 1) query-passage matching task ; 2) query-dialogue history matching task ; 3) multi-turn response matching task, and joint learning all these tasks in a unified pre-trained language model. The former two tasks could help the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> in knowledge selection and comprehension, while the last task is designed for matching the proper response with the given query and background knowledge (dialogue history). By this means, the model can be learned to select relevant knowledge and distinguish proper response, with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues. Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--344 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.344" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.344/>Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks</a></strong><br><a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/g/guimin-chen/>Guimin Chen</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/x/xiang-wan/>Xiang Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--344><div class="card-body p-3 small">Syntactic information, especially dependency trees, has been widely used by existing studies to improve <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> with better <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic guidance</a> for analyzing the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> associated with the given entities. However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated, so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great importance in this task. In this paper, we propose a dependency-driven approach for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> with attentive graph convolutional networks (A-GCN). In this approach, an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an off-the-shelf dependency parser, to distinguish the importance of different word dependencies. Consider that dependency types among words also contain important contextual guidance, which is potentially helpful for relation extraction, we also include the type information in A-GCN modeling. Experimental results on two English benchmark datasets demonstrate the effectiveness of our A-GCN, which outperforms previous studies and achieves state-of-the-art performance on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--347 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.347/>Claim Matching Beyond English to Scale Global Fact-Checking<span class=acl-fixed-case>E</span>nglish to Scale Global Fact-Checking</a></strong><br><a href=/people/a/ashkan-kazemi/>Ashkan Kazemi</a>
|
<a href=/people/k/kiran-garimella/>Kiran Garimella</a>
|
<a href=/people/d/devin-gaffney/>Devin Gaffney</a>
|
<a href=/people/s/scott-hale/>Scott Hale</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--347><div class="card-body p-3 small">Manual fact-checking does not scale well to serve the needs of the internet. This issue is further compounded in <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>non-English contexts</a>. In this paper, we discuss claim matching as a possible solution to scale <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a>. We define claim matching as the task of identifying pairs of textual messages containing claims that can be served with one <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-check</a>. We construct a novel dataset of WhatsApp tipline and public group messages alongside fact-checked claims that are first annotated for containing claim-like statements and then matched with potentially similar items and annotated for claim matching. Our dataset contains content in high-resource (English, Hindi) and lower-resource (Bengali, Malayalam, Tamil) languages. We train our own embedding model using knowledge distillation and a high-quality teacher model in order to address the imbalance in embedding quality between the low- and high-resource languages in our dataset. We provide evaluations on the performance of our solution and compare with baselines and existing state-of-the-art multilingual embedding models, namely LASER and LaBSE. We demonstrate that our performance exceeds <a href=https://en.wikipedia.org/wiki/LASER>LASER</a> and LaBSE in all settings. We release our annotated datasets, <a href=https://en.wikipedia.org/wiki/Codebook>codebooks</a>, and trained <a href=https://en.wikipedia.org/wiki/Embedding>embedding model</a> to allow for further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--349 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.349" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.349/>Energy-Based Reranking : Improving Neural Machine Translation Using Energy-Based Models</a></strong><br><a href=/people/s/sumanta-bhattacharyya/>Sumanta Bhattacharyya</a>
|
<a href=/people/a/amirmohammad-rooshenas/>Amirmohammad Rooshenas</a>
|
<a href=/people/s/subhajit-naskar/>Subhajit Naskar</a>
|
<a href=/people/s/simeng-sun/>Simeng Sun</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--349><div class="card-body p-3 small">The discrepancy between <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation (MLE)</a> and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016 ; Norouzi et al., 2016 ; Shen et al., 2016 ; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and <a href=https://en.wikipedia.org/wiki/Stability_theory>stability</a>. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energy-based model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in a <a href=https://en.wikipedia.org/wiki/Ranking_(statistics)>re-ranking algorithm</a> based on the samples drawn from NMT : energy-based re-ranking (EBR). We use both marginal energy models (over target sentence) and joint energy models (over both source and target sentences). Our EBR with the joint energy model consistently improves the performance of the Transformer-based NMT : +3.7 BLEU points on IWSLT&#8217;14 German-English, +3.37 BELU points on Sinhala-English, +1.4 BLEU points on WMT&#8217;16 English-German tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--357 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.357/>ForecastQA : A Question Answering Challenge for Event Forecasting with Temporal Text Data<span class=acl-fixed-case>F</span>orecast<span class=acl-fixed-case>QA</span>: A Question Answering Challenge for Event Forecasting with Temporal Text Data</a></strong><br><a href=/people/w/woojeong-jin/>Woojeong Jin</a>
|
<a href=/people/r/rahul-khanna/>Rahul Khanna</a>
|
<a href=/people/s/suji-kim/>Suji Kim</a>
|
<a href=/people/d/dong-ho-lee/>Dong-Ho Lee</a>
|
<a href=/people/f/fred-morstatter/>Fred Morstatter</a>
|
<a href=/people/a/aram-galstyan/>Aram Galstyan</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--357><div class="card-body p-3 small">Event forecasting is a challenging, yet important task, as humans seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, construct a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, and provide benchmarks for developing methods for event forecasting with large volumes of <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text data</a>. To simulate the forecasting scenario on temporal news documents, we formulate the problem as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike existing QA tasks, our <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> limits accessible information, and thus a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has to make a forecasting judgement. To showcase the usefulness of this task formulation, we introduce ForecastQA, a <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering dataset</a> consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. We present our experiments on ForecastQA using BERTbased models and find that our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 61.0 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, which still lags behind human performance by about 19 %. We hope ForecastQA will support future research efforts in bridging this gap.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--360 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.360/>Trigger is Not Sufficient : Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction</a></strong><br><a href=/people/k/kaiwen-wei/>Kaiwen Wei</a>
|
<a href=/people/x/xian-sun/>Xian Sun</a>
|
<a href=/people/z/zequn-zhang/>Zequn Zhang</a>
|
<a href=/people/j/jingyuan-zhang/>Jingyuan Zhang</a>
|
<a href=/people/g/guo-zhi/>Guo Zhi</a>
|
<a href=/people/l/li-jin/>Li Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--360><div class="card-body p-3 small">Implicit Event Argument Extraction seeks to identify <a href=https://en.wikipedia.org/wiki/Argument>arguments</a> that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.362.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--362 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.362 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.362/>AdaTag : Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding<span class=acl-fixed-case>A</span>da<span class=acl-fixed-case>T</span>ag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding</a></strong><br><a href=/people/j/jun-yan/>Jun Yan</a>
|
<a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/y/yan-liang/>Yan Liang</a>
|
<a href=/people/c/christan-grant/>Christan Grant</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/x/xin-luna-dong/>Xin Luna Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--362><div class="card-body p-3 small">Automatic extraction of product attribute values is an important enabling technology in <a href=https://en.wikipedia.org/wiki/E-commerce>e-Commerce platforms</a>. This <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> is usually modeled using sequence labeling architectures, with several extensions to handle multi-attribute extraction. One line of previous work constructs attribute-specific models, through separate <a href=https://en.wikipedia.org/wiki/Code>decoders</a> or entirely separate models. However, this approach constrains <a href=https://en.wikipedia.org/wiki/Knowledge_sharing>knowledge sharing</a> across different attributes. Other contributions use a single multi-attribute model, with different techniques to embed <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attribute information</a>. But sharing the entire network parameters across all attributes can limit the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s capacity to capture attribute-specific characteristics. In this paper we present AdaTag, which uses adaptive decoding to handle <a href=https://en.wikipedia.org/wiki/Data_extraction>extraction</a>. We parameterize the decoder with pretrained attribute embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This allows for separate, but semantically correlated, <a href=https://en.wikipedia.org/wiki/Code>decoders</a> to be generated on the fly for different attributes. This approach facilitates <a href=https://en.wikipedia.org/wiki/Knowledge_sharing>knowledge sharing</a>, while maintaining the specificity of each attribute. Our experiments on a real-world e-Commerce dataset show marked improvements over previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--364 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.364" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.364/>Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference</a></strong><br><a href=/people/r/robert-l-logan-iv/>Robert L Logan IV</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/d/dan-bikel/>Dan Bikel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--364><div class="card-body p-3 small">Streaming cross document entity coreference (CDC) systems disambiguate mentions of named entities in a scalable manner via incremental clustering. Unlike other approaches for named entity disambiguation (e.g., entity linking), streaming CDC allows for the disambiguation of entities that are unknown at inference time. Thus, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is well-suited for processing <a href=https://en.wikipedia.org/wiki/Stream_(computing)>streams of data</a> where new entities are frequently introduced. Despite these benefits, this task is currently difficult to study, as existing approaches are either evaluated on datasets that are no longer available, or omit other crucial details needed to ensure fair comparison. In this work, we address this issue by compiling a large benchmark adapted from existing free datasets, and performing a comprehensive evaluation of a number of novel and existing baseline models. We investigate : how to best encode <a href=https://en.wikipedia.org/wiki/Note_(typography)>mentions</a>, which clustering algorithms are most effective for grouping mentions, how models transfer to different domains, and how bounding the number of mentions tracked during <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> impacts performance. Our results show that the relative performance of neural and feature-based mention encoders varies across different domains, and in most cases the best performance is achieved using a combination of both approaches. We also find that performance is minimally impacted by limiting the number of tracked mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--365 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.365/>Search from History and Reason for Future : Two-stage Reasoning on Temporal Knowledge Graphs</a></strong><br><a href=/people/z/zixuan-li/>Zixuan Li</a>
|
<a href=/people/x/xiaolong-jin/>Xiaolong Jin</a>
|
<a href=/people/s/saiping-guan/>Saiping Guan</a>
|
<a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/j/jiafeng-guo/>Jiafeng Guo</a>
|
<a href=/people/y/yuanzhuo-wang/>Yuanzhuo Wang</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--365><div class="card-body p-3 small">Temporal Knowledge Graphs (TKGs) have been developed and used in many different areas. Reasoning on TKGs that predicts potential facts (events) in the future brings great challenges to existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. When facing a prediction task, human beings usually search useful historical information (i.e., clues) in their memories and then reason for future meticulously. Inspired by this mechanism, we propose CluSTeR to predict future facts in a two-stage manner, Clue Searching and Temporal Reasoning, accordingly. Specifically, at the clue searching stage, CluSTeR learns a beam search policy via reinforcement learning (RL) to induce multiple clues from historical facts. At the temporal reasoning stage, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> adopts a graph convolution network based sequence method to deduce answers from clues. Experiments on four datasets demonstrate the substantial advantages of CluSTeR compared with the state-of-the-art methods. Moreover, the clues found by CluSTeR further provide interpretability for the results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--366 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.366" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.366/>Employing Argumentation Knowledge Graphs for Neural Argument Generation</a></strong><br><a href=/people/k/khalid-al-khatib/>Khalid Al Khatib</a>
|
<a href=/people/l/lukas-trautner/>Lukas Trautner</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--366><div class="card-body p-3 small">Generating high-quality arguments, while being challenging, may benefit a wide range of downstream applications, such as writing assistants and argument search engines. Motivated by the effectiveness of utilizing <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> for supporting general text generation tasks, this paper investigates the usage of argumentation-related knowledge graphs to control the generation of arguments. In particular, we construct and populate three knowledge graphs, employing several compositions of them to encode various knowledge into texts of debate portals and relevant paragraphs from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. Then, the texts with the encoded knowledge are used to fine-tune a pre-trained text generation model, GPT-2. We evaluate the newly created arguments manually and automatically, based on several dimensions important in argumentative contexts, including argumentativeness and plausibility. The results demonstrate the positive impact of encoding the graphs&#8217; knowledge into debate portal texts for generating arguments with superior quality than those generated without knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.370.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--370 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.370 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.370/>GWLAN : General Word-Level AutocompletioN for Computer-Aided Translation<span class=acl-fixed-case>GWLAN</span>: General Word-Level <span class=acl-fixed-case>A</span>utocompletio<span class=acl-fixed-case>N</span> for Computer-Aided Translation</a></strong><br><a href=/people/h/huayang-li/>Huayang Li</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a>
|
<a href=/people/g/guoping-huang/>Guoping Huang</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--370><div class="card-body p-3 small">Computer-aided translation (CAT), the use of <a href=https://en.wikipedia.org/wiki/Software>software</a> to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators. Autocompletion, which suggests <a href=https://en.wikipedia.org/wiki/Translation>translation</a> results according to the text pieces provided by human translators, is a core function of <a href=https://en.wikipedia.org/wiki/Computer-aided_technologies>CAT</a>. There are two limitations in previous research in this line. First, most research works on this topic focus on <a href=https://en.wikipedia.org/wiki/Autocomplete>sentence-level autocompletion</a> (i.e., generating the whole translation as a sentence based on human input), but <a href=https://en.wikipedia.org/wiki/Autocomplete>word-level autocompletion</a> is under-explored so far. Second, almost no public benchmarks are available for the autocompletion task of CAT. This might be among the reasons why research progress in <a href=https://en.wikipedia.org/wiki/Computer-aided_technologies>CAT</a> is much slower compared to automatic MT. In this paper, we propose the task of general word-level autocompletion (GWLAN) from a real-world CAT scenario, and construct the first public benchmark to facilitate research in this topic. In addition, we propose an effective method for <a href=https://en.wikipedia.org/wiki/Wireless_LAN>GWLAN</a> and compare it with several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.373.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--373 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.373 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.373" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.373/>MLBiNet : A Cross-Sentence Collective Event Detection Network<span class=acl-fixed-case>MLB</span>i<span class=acl-fixed-case>N</span>et: A Cross-Sentence Collective Event Detection Network</a></strong><br><a href=/people/d/dongfang-lou/>Dongfang Lou</a>
|
<a href=/people/z/zhilin-liao/>Zhilin Liao</a>
|
<a href=/people/s/shumin-deng/>Shumin Deng</a>
|
<a href=/people/n/ningyu-zhang/>Ningyu Zhang</a>
|
<a href=/people/h/huajun-chen/>Huajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--373><div class="card-body p-3 small">We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results.<b>M</b>ulti-<b>L</b>ayer <b>Bi</b>directional <b>Net</b>work (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--376 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.376/>Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks</a></strong><br><a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/x/xinyu-zuo/>Xinyu Zuo</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/y/yuguang-chen/>Yuguang Chen</a>
|
<a href=/people/w/weihua-peng/>Weihua Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--376><div class="card-body p-3 small">Identifying causal relations of events is an important task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing area</a>. However, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is very challenging, because <a href=https://en.wikipedia.org/wiki/Causality>event causality</a> is usually expressed in diverse forms that often lack explicit causal clues. Existing methods can not handle well the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>, especially in the condition of lacking training data. Nonetheless, humans can make a correct judgement based on their background knowledge, including <a href=https://en.wikipedia.org/wiki/Descriptive_knowledge>descriptive knowledge</a> and relational knowledge. Inspired by it, we propose a novel Latent Structure Induction Network (LSIN) to incorporate the external structural knowledge into this task. Specifically, to make use of the <a href=https://en.wikipedia.org/wiki/Descriptive_knowledge>descriptive knowledge</a>, we devise a Descriptive Graph Induction module to obtain and encode the graph-structured descriptive knowledge. To leverage the relational knowledge, we propose a Relational Graph Induction module which is able to automatically learn a reasoning structure for event causality reasoning. Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--379 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.379" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.379/>R2D2 : Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling<span class=acl-fixed-case>R</span>2<span class=acl-fixed-case>D</span>2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling</a></strong><br><a href=/people/x/xiang-hu/>Xiang Hu</a>
|
<a href=/people/h/haitao-mi/>Haitao Mi</a>
|
<a href=/people/z/zujie-wen/>Zujie Wen</a>
|
<a href=/people/y/yafang-wang/>Yafang Wang</a>
|
<a href=/people/y/yi-su/>Yi Su</a>
|
<a href=/people/j/jing-zheng/>Jing Zheng</a>
|
<a href=/people/g/gerard-de-melo/>Gerard de Melo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--379><div class="card-body p-3 small">Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of <a href=https://en.wikipedia.org/wiki/Abstraction>abstraction</a> that can be hierarchically combined. However, existing <a href=https://en.wikipedia.org/wiki/Deep_learning>deep models</a> with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a> and enable <a href=https://en.wikipedia.org/wiki/Code>encoding</a> in <a href=https://en.wikipedia.org/wiki/Time_complexity>linear time</a>. Experimental results on <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> and unsupervised parsing show the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.383.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--383 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.383 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.383" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.383/>Personalized Transformer for Explainable Recommendation</a></strong><br><a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/y/yongfeng-zhang/>Yongfeng Zhang</a>
|
<a href=/people/l/li-chen/>Li Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--383><div class="card-body p-3 small">Personalization of <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> plays a vital role in a large spectrum of tasks, such as <a href=https://en.wikipedia.org/wiki/Recommender_system>explainable recommendation</a>, review summarization and <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialog systems</a>. In these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, user and item IDs are important identifiers for <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a>. Transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words. To address this problem, we present a PErsonalized Transformer for Explainable Recommendation (PETER), on which we design a simple and effective learning objective that utilizes the IDs to predict the words in the target explanation, so as to endow the IDs with linguistic meanings and to achieve personalized Transformer. Besides generating explanations, PETER can also make recommendations, which makes it a unified model for the whole recommendation-explanation pipeline. Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--384 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.384/>Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques<span class=acl-fixed-case>SOAP</span> Notes from Doctor-Patient Conversations Using Modular Summarization Techniques</a></strong><br><a href=/people/k/kundan-krishna/>Kundan Krishna</a>
|
<a href=/people/s/sopan-khosla/>Sopan Khosla</a>
|
<a href=/people/j/jeffrey-p-bigham/>Jeffrey Bigham</a>
|
<a href=/people/z/zachary-c-lipton/>Zachary C. Lipton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--384><div class="card-body p-3 small">Following each patient visit, physicians draft long semi-structured clinical summaries called <a href=https://en.wikipedia.org/wiki/SOAP_notes>SOAP notes</a>. While invaluable to clinicians and researchers, creating digital SOAP notes is burdensome, contributing to physician burnout. In this paper, we introduce the first complete <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipelines</a> to leverage deep summarization models to generate these notes based on transcripts of conversations between physicians and patients. After exploring a spectrum of methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that (i) extracts important utterances relevant to each summary section ; (ii) clusters together related utterances ; and then (iii) generates one summary sentence per cluster. Cluster2Sent outperforms its purely abstractive counterpart by 8 ROUGE-1 points, and produces significantly more factual and coherent sentences as assessed by expert human evaluators. For <a href=https://en.wikipedia.org/wiki/Reproducibility>reproducibility</a>, we demonstrate similar benefits on the publicly available AMI dataset. Our results speak to the benefits of structuring summaries into sections and annotating supporting evidence when constructing summarization corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.393.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--393 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.393 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.393" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.393/>ConSERT : A Contrastive Framework for Self-Supervised Sentence Representation Transfer<span class=acl-fixed-case>C</span>on<span class=acl-fixed-case>SERT</span>: A Contrastive Framework for Self-Supervised Sentence Representation Transfer</a></strong><br><a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/r/rumei-li/>Rumei Li</a>
|
<a href=/people/s/sirui-wang/>Sirui Wang</a>
|
<a href=/people/f/fuzheng-zhang/>Fuzheng Zhang</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--393><div class="card-body p-3 small">Learning high-quality <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a> benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8 % relative improvement over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.395.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--395 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.395 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.395" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.395/>COINS : Dynamically Generating COntextualized Inference Rules for Narrative Story Completion<span class=acl-fixed-case>COINS</span>: Dynamically Generating <span class=acl-fixed-case>CO</span>ntextualized Inference Rules for Narrative Story Completion</a></strong><br><a href=/people/d/debjit-paul/>Debjit Paul</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--395><div class="card-body p-3 small">Despite recent successes of large pre-trained language models in solving reasoning tasks, their inference capabilities remain opaque. We posit that such models can be made more interpretable by explicitly generating interim inference rules, and using them to guide the generation of task-specific textual outputs. In this paper we present Coins, a recursive inference framework that i) iteratively reads context sentences, ii) dynamically generates contextualized inference rules, encodes them, and iii) uses them to guide task-specific output generation. We apply to a Narrative Story Completion task that asks a model to complete a story with missing sentences, to produce a coherent story with plausible logical connections, causal relationships, and temporal dependencies. By modularizing <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and sentence generation steps in a recurrent model, we aim to make reasoning steps and their effects on next sentence generation transparent. Our automatic and manual evaluations show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates better <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>story sentences</a> than SOTA baselines, especially in terms of <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a>. We further demonstrate improved performance over strong pre-trained LMs in generating commonsense inference rules. The recursive nature of holds the potential for <a href=https://en.wikipedia.org/wiki/Control_theory>controlled generation of longer sequences</a>.<i>Narrative Story Completion</i> task that asks a model to complete a story with missing sentences, to produce a coherent story with plausible logical connections, causal relationships, and temporal dependencies. By modularizing inference and sentence generation steps in a recurrent model, we aim to make reasoning steps and their effects on next sentence generation transparent. Our automatic and manual evaluations show that the model generates better story sentences than SOTA baselines, especially in terms of coherence. We further demonstrate improved performance over strong pre-trained LMs in generating commonsense inference rules. The recursive nature of holds the potential for controlled generation of longer sequences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.397.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--397 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.397 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.397/>From <a href=https://en.wikipedia.org/wiki/Paraphrase>Paraphrasing</a> to Semantic Parsing : Unsupervised Semantic Parsing via Synchronous Semantic Decoding</a></strong><br><a href=/people/s/shan-wu/>Shan Wu</a>
|
<a href=/people/b/bo-chen/>Bo Chen</a>
|
<a href=/people/c/chunlei-xin/>Chunlei Xin</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/w/weipeng-zhang/>Weipeng Zhang</a>
|
<a href=/people/j/jiansong-chen/>Jiansong Chen</a>
|
<a href=/people/f/fan-yang/>Fan Yang</a>
|
<a href=/people/x/xunliang-cai/>Xunliang Cai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--397><div class="card-body p-3 small">Semantic parsing is challenging due to the structure gap and the <a href=https://en.wikipedia.org/wiki/Semantic_gap>semantic gap</a> between utterances and logical forms. In this paper, we propose an unsupervised semantic parsing method-Synchronous Semantic Decoding (SSD), which can simultaneously resolve the <a href=https://en.wikipedia.org/wiki/Semantic_gap>semantic gap</a> and the structure gap by jointly leveraging paraphrasing and grammar-constrained decoding. Specifically, we reformulate <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> as a constrained paraphrasing problem : given an utterance, our model synchronously generates its canonical utterancel and meaning representation. During synchronously decoding : the utterance paraphrasing is constrained by the structure of the logical form, therefore the canonical utterance can be paraphrased controlledly ; the semantic decoding is guided by the semantics of the canonical utterance, therefore its <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> can be generated unsupervisedly. Experimental results show that SSD is a promising approach and can achieve state-of-the-art unsupervised semantic parsing performance on multiple datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.398/>Pre-training Universal Language Representation</a></strong><br><a href=/people/y/yian-li/>Yian Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--398><div class="card-body p-3 small">Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a <a href=https://en.wikipedia.org/wiki/Uniform_space>uniform vector space</a>. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for pre-trained language models. Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--404 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.404/>A Cognitive Regularizer for <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a></a></strong><br><a href=/people/j/jason-wei/>Jason Wei</a>
|
<a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--404><div class="card-body p-3 small">The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> for <a href=https://en.wikipedia.org/wiki/Language_model>statistical language modeling</a>. Specifically, we augment the canonical MLE objective for training language models with a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizer</a> that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--405 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.405" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.405/>Lower Perplexity is Not Always Human-Like</a></strong><br><a href=/people/t/tatsuki-kuribayashi/>Tatsuki Kuribayashi</a>
|
<a href=/people/y/yohei-oseki/>Yohei Oseki</a>
|
<a href=/people/t/takumi-ito/>Takumi Ito</a>
|
<a href=/people/r/ryo-yoshida/>Ryo Yoshida</a>
|
<a href=/people/m/masayuki-asahara/>Masayuki Asahara</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--405><div class="card-body p-3 small">In computational psycholinguistics, various <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, despite the recent trend towards <a href=https://en.wikipedia.org/wiki/Linguistic_universal>linguistic universal</a> within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization the lower perplexity a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> has, the more human-like the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> is in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> with typologically different structures from <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Our experiments demonstrate that this established <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> exhibits a surprising lack of universality ; namely, lower perplexity is not always human-like. Moreover, this discrepancy between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.<i>the lower perplexity a language model has, the more human-like the language model is</i>&#8212; in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--408 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.408" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.408/>Obtaining Better Static Word Embeddings Using Contextual Embedding Models</a></strong><br><a href=/people/p/prakhar-gupta/>Prakhar Gupta</a>
|
<a href=/people/m/martin-jaggi/>Martin Jaggi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--408><div class="card-body p-3 small">The advent of contextual word embeddings representations of words which incorporate semantic and syntactic information from their contexthas led to tremendous improvements on a wide variety of NLP tasks. However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret. In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve computational efficiency of NLP applications, while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods. As a side-effect, our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--409 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.409/>Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation</a></strong><br><a href=/people/y/yingjun-du/>Yingjun Du</a>
|
<a href=/people/n/nithin-holla/>Nithin Holla</a>
|
<a href=/people/x/xiantong-zhen/>Xiantong Zhen</a>
|
<a href=/people/c/cees-snoek/>Cees Snoek</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--409><div class="card-body p-3 small">A critical challenge faced by supervised word sense disambiguation (WSD) is the lack of large annotated datasets with sufficient coverage of words in their diversity of senses. This inspired recent research on few-shot WSD using <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a>. While such work has successfully applied <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a> to learn new word senses from very few examples, its performance still lags behind its fully-supervised counterpart. Aiming to further close this gap, we propose a model of <a href=https://en.wikipedia.org/wiki/Semantic_memory>semantic memory</a> for WSD in a meta-learning setting. Semantic memory encapsulates prior experiences seen throughout the lifetime of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, which aids better <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> in limited data settings. Our model is based on hierarchical variational inference and incorporates an adaptive memory update rule via a hypernetwork. We show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> advances the state of the art in few-shot WSD, supports effective <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> in extremely data scarce (e.g. one-shot) scenarios and produces meaning prototypes that capture similar senses of distinct words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--411 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.411/>Text-Free Image-to-Speech Synthesis Using Learned Segmental Units</a></strong><br><a href=/people/w/wei-ning-hsu/>Wei-Ning Hsu</a>
|
<a href=/people/d/david-harwath/>David Harwath</a>
|
<a href=/people/t/tyler-miller/>Tyler Miller</a>
|
<a href=/people/c/christopher-song/>Christopher Song</a>
|
<a href=/people/j/james-glass/>James Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--411><div class="card-body p-3 small">In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe. We investigate several different intermediate speech representations, and empirically find that the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a> must satisfy several important properties to serve as drop-in replacements for text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--412 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.412.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.412/>CTFN : Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network<span class=acl-fixed-case>CTFN</span>: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network</a></strong><br><a href=/people/j/jiajia-tang/>Jiajia Tang</a>
|
<a href=/people/k/kang-li/>Kang Li</a>
|
<a href=/people/x/xuanyu-jin/>Xuanyu Jin</a>
|
<a href=/people/a/andrzej-cichocki/>Andrzej Cichocki</a>
|
<a href=/people/q/qibin-zhao/>Qibin Zhao</a>
|
<a href=/people/w/wanzeng-kong/>Wanzeng Kong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--412><div class="card-body p-3 small">Multimodal sentiment analysis is the challenging research area that attends to the fusion of multiple heterogeneous modalities. The main challenge is the occurrence of some missing modalities during the multimodal fusion procedure. However, the existing techniques require all modalities as input, thus are sensitive to missing modalities at predicting time. In this work, the coupled-translation fusion network (CTFN) is firstly proposed to model bi-direction interplay via couple learning, ensuring the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> in respect to missing modalities. Specifically, the cyclic consistency constraint is presented to improve the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance, allowing us directly to discard decoder and only embraces encoder of Transformer. This could contribute to a much lighter model. Due to the couple learning, CTFN is able to conduct bi-direction cross-modality intercorrelation parallelly. Based on CTFN, a hierarchical architecture is further established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings compared with traditional translation methods. Moreover, the convolution block is utilized to further highlight explicit interactions among those <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translations</a>. For evaluation, CTFN was verified on two multimodal benchmarks with extensive ablation studies. The experiments demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves state-of-the-art or often competitive performance. Additionally, CTFN still maintains <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> when considering missing modality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--414 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.414.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.414/>Language Model Evaluation Beyond Perplexity</a></strong><br><a href=/people/c/clara-meister/>Clara Meister</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--414><div class="card-body p-3 small">We propose an alternate approach to quantifying how well language models learn <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> : we ask how well they match the statistical tendencies of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a frameworkpaired with significance testsfor evaluating the fit of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different <a href=https://en.wikipedia.org/wiki/Probability_distribution>distributions</a> is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the typetoken relationship of natural language than text produced using standard ancestral sampling ; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--417 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.417" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.417/>Alignment Rationale for Natural Language Inference</a></strong><br><a href=/people/z/zhongtao-jiang/>Zhongtao Jiang</a>
|
<a href=/people/y/yuanzhe-zhang/>Yuanzhe Zhang</a>
|
<a href=/people/z/zhao-yang/>Zhao Yang</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--417><div class="card-body p-3 small">Deep learning models have achieved great success on the task of Natural Language Inference (NLI), though only a few attempts try to explain their behaviors. Existing explanation methods usually pick prominent features such as words or phrases from the input text. However, for NLI, alignments among words or phrases are more enlightening clues to explain the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. To this end, this paper presents AREC, a post-hoc approach to generate alignment rationale explanations for co-attention based models in NLI. The explanation is based on <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection</a>, which keeps few but sufficient alignments while maintaining the same prediction of the target model. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is more faithful and human-readable compared with many existing approaches. We further study and re-evaluate three typical <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> through our explanation beyond accuracy, and propose a simple method that greatly improves the model robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--419 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.419.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.419/>On Sample Based Explanation Methods for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> : Faithfulness, Efficiency and Semantic Evaluation<span class=acl-fixed-case>NLP</span>: Faithfulness, Efficiency and Semantic Evaluation</a></strong><br><a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/z/ziming-huang/>Ziming Huang</a>
|
<a href=/people/y/yada-zhu/>Yada Zhu</a>
|
<a href=/people/g/guangnan-ye/>Guangnan Ye</a>
|
<a href=/people/x/xiaodong-cui/>Xiaodong Cui</a>
|
<a href=/people/f/fan-zhang/>Fan Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--419><div class="card-body p-3 small">In the recent advances of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, the scale of the state-of-the-art models and datasets is usually extensive, which challenges the application of sample-based explanation methods in many aspects, such as <a href=https://en.wikipedia.org/wiki/Interpretability>explanation interpretability</a>, <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>, and <a href=https://en.wikipedia.org/wiki/Faithfulness>faithfulness</a>. In this work, for the first time, we can improve the interpretability of explanations by allowing arbitrary text sequences as the explanation unit. On top of this, we implement a hessian-free method with a model faithfulness guarantee. Finally, to compare our method with the others, we propose a semantic-based evaluation metric that can better align with humans&#8217; judgment of explanations than the widely adopted diagnostic or re-training measures. The empirical results on multiple real data sets demonstrate the proposed method&#8217;s superior performance to popular explanation techniques such as <a href=https://en.wikipedia.org/wiki/Influence_function>Influence Function</a> or TracIn on semantic evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--422 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.422" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.422/>Counterfactual Inference for Text Classification Debiasing</a></strong><br><a href=/people/c/chen-qian/>Chen Qian</a>
|
<a href=/people/f/fuli-feng/>Fuli Feng</a>
|
<a href=/people/l/lijie-wen/>Lijie Wen</a>
|
<a href=/people/c/chunping-ma/>Chunping Ma</a>
|
<a href=/people/p/pengjun-xie/>Pengjun Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--422><div class="card-body p-3 small">Today&#8217;s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models&#8217; generalization. Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases. Unfortunately, they either suffer from the extra cost of data collection / selection / annotation or need an elaborate design of balancing strategies. Different from traditional factual inference in which <a href=https://en.wikipedia.org/wiki/Debiasing>debiasing</a> occurs before or during training, counterfactual inference mitigates the influence brought by unintended confounders after training, which can make unbiased decisions with biased observations. Inspired by this, we propose a model-agnostic text classification debiasing framework Corsair, which can effectively avoid employing data manipulations or designing balancing mechanisms. Concretely, Corsair first trains a base model on a training set directly, allowing the dataset biases &#8216;poison&#8217; the trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. In <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, given a factual input document, Corsair imagines its two counterfactual counterparts to distill and mitigate the two <a href=https://en.wikipedia.org/wiki/Bias>biases</a> captured by the poisonous model. Extensive experiments demonstrate <a href=https://en.wikipedia.org/wiki/Corsair>Corsair</a>&#8217;s effectiveness, <a href=https://en.wikipedia.org/wiki/Generalizability>generalizability</a> and <a href=https://en.wikipedia.org/wiki/Equity_(law)>fairness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.423.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--423 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.423 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.423/>HieRec : Hierarchical User Interest Modeling for Personalized News Recommendation<span class=acl-fixed-case>H</span>ie<span class=acl-fixed-case>R</span>ec: Hierarchical User Interest Modeling for Personalized News Recommendation</a></strong><br><a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/p/peiru-yang/>Peiru Yang</a>
|
<a href=/people/y/yang-yu/>Yang Yu</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--423><div class="card-body p-3 small">User interest modeling is critical for personalized news recommendation. Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest. However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding. In this paper, we propose a news recommendation method with hierarchical user interest modeling, named HieRec. Instead of a single user embedding, in our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news. We use a three-level hierarchy to represent 1) overall user interest ; 2) <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>user interest</a> in coarse-grained topics like <a href=https://en.wikipedia.org/wiki/Sport>sports</a> ; and 3) <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>user interest</a> in fine-grained topics like <a href=https://en.wikipedia.org/wiki/Association_football>football</a>. Moreover, we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting. Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--424 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.424/>PP-Rec : News Recommendation with Personalized User Interest and Time-aware News Popularity<span class=acl-fixed-case>PP</span>-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity</a></strong><br><a href=/people/t/tao-qi/>Tao Qi</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--424><div class="card-body p-3 small">Personalized news recommendation methods are widely used in <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news services</a>. These methods usually recommend news based on the matching between <a href=https://en.wikipedia.org/wiki/Content_(media)>news content</a> and <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>user interest</a> inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure time-aware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can effectively improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Diversity_(politics)>diversity</a> for <a href=https://en.wikipedia.org/wiki/News_aggregator>news recommendation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--428 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.428.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.428/>BanditMTL : Bandit-based Multi-task Learning for Text Classification<span class=acl-fixed-case>B</span>andit<span class=acl-fixed-case>MTL</span>: Bandit-based Multi-task Learning for Text Classification</a></strong><br><a href=/people/y/yuren-mao/>Yuren Mao</a>
|
<a href=/people/z/zekai-wang/>Zekai Wang</a>
|
<a href=/people/w/weiwei-liu/>Weiwei Liu</a>
|
<a href=/people/x/xuemin-lin/>Xuemin Lin</a>
|
<a href=/people/w/wenbin-hu/>Wenbin Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--428><div class="card-body p-3 small">Task variance regularization, which can be used to improve the generalization of Multi-task Learning (MTL) models, remains unexplored in multi-task text classification. Accordingly, to fill this gap, this paper investigates how the task might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multi-armed bandit. The proposed method, named BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm. Adopting BanditMTL in the multi-task text classification context is found to achieve state-of-the-art performance. The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--433 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.433/>Exploring Distantly-Labeled Rationales in Neural Network Models</a></strong><br><a href=/people/q/quzhe-huang/>Quzhe Huang</a>
|
<a href=/people/s/shengqi-zhu/>Shengqi Zhu</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--433><div class="card-body p-3 small">Recent studies strive to incorporate various <a href=https://en.wikipedia.org/wiki/Rationality>human rationales</a> into <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to improve <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> performance, but few pay attention to the quality of the rationales. Most existing methods distribute their models&#8217; focus to distantly-labeled rationale words entirely and equally, while ignoring the potential important non-rationale words and not distinguishing the importance of different rationale words. In this paper, we propose two novel auxiliary loss functions to make better use of distantly-labeled rationales, which encourage models to maintain their focus on important words beyond labeled rationales (PINs) and alleviate redundant training on non-helpful rationales (NoIRs). Experiments on two representative classification tasks show that our proposed methods can push a classification model to effectively learn crucial clues from non-perfect rationales while maintaining the ability to spread its focus to other unlabeled important words, thus significantly outperform existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--436 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.436" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.436/>A Human-machine Collaborative Framework for Evaluating Malevolence in Dialogues</a></strong><br><a href=/people/y/yangjun-zhang/>Yangjun Zhang</a>
|
<a href=/people/p/pengjie-ren/>Pengjie Ren</a>
|
<a href=/people/m/maarten-de-rijke/>Maarten de Rijke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--436><div class="card-body p-3 small">Conversational dialogue systems (CDSs) are hard to evaluate due to the <a href=https://en.wikipedia.org/wiki/Complexity>complexity of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a></a>. Automatic evaluation of <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> often shows insufficient correlation with <a href=https://en.wikipedia.org/wiki/Judgement>human judgements</a>. Human evaluation is reliable but labor-intensive. We introduce a human-machine collaborative framework, HMCEval, that can guarantee reliability of the evaluation outcomes with reduced human effort. HMCEval casts dialogue evaluation as a sample assignment problem, where we need to decide to assign a sample to a human or a machine for evaluation. HMCEval includes a model confidence estimation module to estimate the confidence of the predicted sample assignment, and a human effort estimation module to estimate the human effort should the sample be assigned to human evaluation, as well as a sample assignment execution module that finds the optimum assignment solution based on the estimated confidence and effort. We assess the performance of HMCEval on the task of evaluating malevolence in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a>. The experimental results show that HMCEval achieves around 99 % evaluation accuracy with half of the <a href=https://en.wikipedia.org/wiki/Human_resources>human effort</a> spared, showing that HMCEval provides reliable evaluation outcomes while reducing <a href=https://en.wikipedia.org/wiki/Human_resources>human effort</a> by a large amount.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--438 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.438" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.438/>Learning to Ask Conversational Questions by Optimizing Levenshtein Distance<span class=acl-fixed-case>L</span>evenshtein Distance</a></strong><br><a href=/people/z/zhongkun-liu/>Zhongkun Liu</a>
|
<a href=/people/p/pengjie-ren/>Pengjie Ren</a>
|
<a href=/people/z/zhumin-chen/>Zhumin Chen</a>
|
<a href=/people/z/zhaochun-ren/>Zhaochun Ren</a>
|
<a href=/people/m/maarten-de-rijke/>Maarten de Rijke</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--438><div class="card-body p-3 small">Conversational Question Simplification (CQS) aims to simplify self-contained questions into conversational ones by incorporating some <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>conversational characteristics</a>, e.g., <a href=https://en.wikipedia.org/wiki/Anaphora_(rhetoric)>anaphora</a> and <a href=https://en.wikipedia.org/wiki/Ellipsis_(linguistics)>ellipsis</a>. Existing maximum likelihood estimation based methods often get trapped in easily learned tokens as all tokens are treated equally during training. In this work, we introduce a Reinforcement Iterative Sequence Editing (RISE) framework that optimizes the minimum Levenshtein distance through explicit editing actions. RISE is able to pay attention to tokens that are related to conversational characteristics. To train RISE, we devise an Iterative Reinforce Training (IRT) algorithm with a Dynamic Programming based Sampling (DPS) process to improve exploration. Experimental results on two benchmark datasets show that RISE significantly outperforms state-of-the-art methods and generalizes well on unseen data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--439 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.439" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.439/>DVD : A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue<span class=acl-fixed-case>DVD</span>: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue</a></strong><br><a href=/people/h/hung-le/>Hung Le</a>
|
<a href=/people/c/chinnadhurai-sankar/>Chinnadhurai Sankar</a>
|
<a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/a/ahmad-beirami/>Ahmad Beirami</a>
|
<a href=/people/a/alborz-geramifard/>Alborz Geramifard</a>
|
<a href=/people/s/satwik-kottur/>Satwik Kottur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--439><div class="card-body p-3 small">A video-grounded dialogue system is required to understand both <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>, which contains semantic dependencies from turn to turn, and video, which contains visual cues of spatial and temporal scene variations. Building such <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> is a challenging <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>, involving various reasoning types on both visual and language inputs. Existing benchmarks do not have enough annotations to thoroughly analyze dialogue systems and understand their capabilities and limitations in isolation. These <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> are also not explicitly designed to minimise biases that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can exploit without actual reasoning. To address these limitations, in this paper, we present <a href=https://en.wikipedia.org/wiki/DVD>DVD</a>, a Diagnostic Dataset for Video-grounded Dialogue. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is designed to contain minimal biases and has detailed annotations for the different types of reasoning over the spatio-temporal space of video. Dialogues are synthesized over multiple question turns, each of which is injected with a set of cross-turn semantic relationships. We use <a href=https://en.wikipedia.org/wiki/DVD>DVD</a> to analyze existing approaches, providing interesting insights into their abilities and limitations. In total, <a href=https://en.wikipedia.org/wiki/DVD>DVD</a> is built from 11k CATER synthetic videos and contains 10 instances of 10-round dialogues for each video, resulting in more than 100k dialogues and 1 M question-answer pairs. Our code and dataset are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--442 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.442" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.442/>CoSQA : 20,000 + Web Queries for Code Search and Question Answering<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>SQA</span>: 20,000+ Web Queries for Code Search and Question Answering</a></strong><br><a href=/people/j/junjie-huang/>Junjie Huang</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/l/linjun-shou/>Linjun Shou</a>
|
<a href=/people/m/ming-gong/>Ming Gong</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--442><div class="card-body p-3 small">Finding codes given <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language query</a> is beneficial to the productivity of software developers. Future progress towards better <a href=https://en.wikipedia.org/wiki/Semantic_matching>semantic matching</a> between query and code requires richer supervised training resources. To remedy this, we introduce CoSQA dataset. It includes 20,604 labels for pairs of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language queries</a> and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed CoCLR to enhance text-code matching, which works as a data augmenter to bring more artificially generated training instances. We show that, evaluated on CodeXGLUE with the same CodeBERT model, training on CoSQA improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of code question answering by 5.1 % and incorporating CoCLR brings a further improvement of 10.5 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.443.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--443 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.443 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.443/>Rewriter-Evaluator Architecture for Neural Machine Translation</a></strong><br><a href=/people/y/yangming-li/>Yangming Li</a>
|
<a href=/people/k/kaisheng-yao/>Kaisheng Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--443><div class="card-body p-3 small">A few approaches have been developed to improve neural machine translation (NMT) models with multiple passes of decoding. However, their performance gains are limited because of lacking proper policies to terminate the multi-pass process. To address this issue, we introduce a novel architecture of Rewriter-Evaluator. Translating a source sentence involves multiple rewriting passes. In every pass, a rewriter generates a new translation to improve the past translation. Termination of this multi-pass process is determined by a score of translation quality estimated by an evaluator. We also propose prioritized gradient descent (PGD) to jointly and efficiently train the rewriter and the evaluator. Extensive experiments on three machine translation tasks show that our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> notably improves the performances of NMT models and significantly outperforms prior methods. An oracle experiment reveals that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can largely reduce performance gaps to the oracle policy. Experiments confirm that the evaluator trained with PGD is more accurate than prior methods in determining proper numbers of rewriting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--445 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.445" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.445/>Importance-based Neuron Allocation for Multilingual Neural Machine Translation</a></strong><br><a href=/people/w/wanying-xie/>Wanying Xie</a>
|
<a href=/people/y/yang-feng/>Yang Feng</a>
|
<a href=/people/s/shuhao-gu/>Shuhao Gu</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--445><div class="card-body p-3 small">Multilingual neural machine translation with a single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has drawn much attention due to its capability to deal with multiple languages. However, the current multilingual translation paradigm often makes the model tend to preserve the <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a>, but ignore the language-specific knowledge. Some previous works try to solve this problem by adding various kinds of language-specific modules to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, but they suffer from the parameter explosion problem and require specialized manual design. To solve these problems, we propose to divide the model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a> and participating in the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> of all the languages, while the language-specific part is responsible for preserving the language-specific knowledge and participating in the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--448 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.448.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.448" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.448/>Coreference Reasoning in Machine Reading Comprehension</a></strong><br><a href=/people/m/mingzhu-wu/>Mingzhu Wu</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--448><div class="card-body p-3 small">Coreference resolution is essential for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> and has been long studied in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. In recent years, as the format of Question Answering (QA) became a standard for machine reading comprehension (MRC), there have been data collection efforts, e.g., Dasigi et al. (2019), that attempt to evaluate the ability of MRC models to reason about <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>. However, as we show, coreference reasoning in <a href=https://en.wikipedia.org/wiki/Medical_classification>MRC</a> is a greater challenge than earlier thought ; <a href=https://en.wikipedia.org/wiki/Medical_classification>MRC datasets</a> do not reflect the natural distribution and, consequently, the challenges of coreference reasoning. Specifically, success on these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> does not reflect a model&#8217;s proficiency in <a href=https://en.wikipedia.org/wiki/Coreference>coreference reasoning</a>. We propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for creating MRC datasets that better reflect the challenges of coreference reasoning and use it to create a sample evaluation set. The results on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> show that state-of-the-art models still struggle with these phenomena. Furthermore, we develop an effective way to use naturally occurring coreference phenomena from existing coreference resolution datasets when training MRC models. This allows us to show an improvement in the coreference reasoning abilities of state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--451 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.451" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.451/>A Unified Generative Framework for Various NER Subtasks<span class=acl-fixed-case>NER</span> Subtasks</a></strong><br><a href=/people/h/hang-yan/>Hang Yan</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/j/junqi-dai/>Junqi Dai</a>
|
<a href=/people/q/qipeng-guo/>Qipeng Guo</a>
|
<a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--451><div class="card-body p-3 small">Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these <a href=https://en.wikipedia.org/wiki/Solution>solutions</a> can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity representations</a> to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.453.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--453 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.453 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.453/>MulDA : A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER<span class=acl-fixed-case>M</span>ul<span class=acl-fixed-case>DA</span>: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual <span class=acl-fixed-case>NER</span></a></strong><br><a href=/people/l/linlin-liu/>Linlin Liu</a>
|
<a href=/people/b/bosheng-ding/>Bosheng Ding</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/l/luo-si/>Luo Si</a>
|
<a href=/people/c/chunyan-miao/>Chunyan Miao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--453><div class="card-body p-3 small">Named Entity Recognition (NER) for low-resource languages is a both practical and challenging research problem. This paper addresses zero-shot transfer for cross-lingual NER, especially when the amount of source-language training data is also limited. The paper first proposes a simple but effective labeled sequence translation method to translate source-language training data to target languages and avoids problems such as word order change and entity span determination. With the source-language data as well as the translated data, a generation-based multilingual data augmentation method is introduced to further increase diversity by generating synthetic labeled data in multiple languages. These augmented data enable the language model based NER models to generalize better with both the language-specific features from the target-language synthetic data and the language-independent features from multilingual synthetic data. An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new <a href=https://en.wikipedia.org/wiki/Research>research</a> on a wide variety of target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.455.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--455 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.455 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.455.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.455" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.455/>Math Word Problem Solving with Explicit Numerical Values</a></strong><br><a href=/people/q/qinzhuo-wu/>Qinzhuo Wu</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--455><div class="card-body p-3 small">In recent years, math word problem solving has received considerable attention and achieved promising results, but previous methods rarely take numerical values into consideration. Most methods treat the <a href=https://en.wikipedia.org/wiki/Number>numerical values</a> in the problems as <a href=https://en.wikipedia.org/wiki/Symbol_(formal)>number symbols</a>, and ignore the prominent role of the <a href=https://en.wikipedia.org/wiki/Number>numerical values</a> in solving the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. In this paper, we propose a novel approach called NumS2 T, which enhances math word problem solving performance by explicitly incorporating numerical values into a sequence-to-tree network. In addition, a numerical properties prediction mechanism is used to capture the category and comparison information of numerals and measure their importance in global expressions. Experimental results on the Math23 K and APE datasets demonstrate that our model achieves better performance than existing state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.457.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--457 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.457 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.457" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.457/>SMedBERT : A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining<span class=acl-fixed-case>SM</span>ed<span class=acl-fixed-case>BERT</span>: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining</a></strong><br><a href=/people/t/taolin-zhang/>Taolin Zhang</a>
|
<a href=/people/z/zerui-cai/>Zerui Cai</a>
|
<a href=/people/c/chengyu-wang/>Chengyu Wang</a>
|
<a href=/people/m/minghui-qiu/>Minghui Qiu</a>
|
<a href=/people/b/bite-yang/>Bite Yang</a>
|
<a href=/people/x/xiaofeng-he/>Xiaofeng He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--457><div class="card-body p-3 small">Recently, the performance of Pre-trained Language Models (PLMs) has been significantly improved by injecting knowledge facts to enhance their abilities of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>. For medical domains, the background knowledge sources are especially useful, due to the massive medical terms and their complicated relations are difficult to understand in text. In this work, we introduce SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantic knowledge from neighbours of linked-entity. In SMedBERT, the mention-neighbour hybrid attention is proposed to learn heterogeneous-entity information, which infuses the semantic representations of entity types into the homogeneous neighbouring entity structure. Apart from <a href=https://en.wikipedia.org/wiki/Knowledge_integration>knowledge integration</a> as external features, we propose to employ the neighbors of linked-entities in the <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> as additional global contexts of text mentions, allowing them to communicate via shared neighbors, thus enrich their semantic representations. Experiments demonstrate that SMedBERT significantly outperforms strong baselines in various knowledge-intensive Chinese medical tasks. It also improves the performance of other tasks such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, question matching and natural language inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--460 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.460.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.460/>Controversy and Conformity : from Generalized to Personalized Aggressiveness Detection</a></strong><br><a href=/people/k/kamil-kanclerz/>Kamil Kanclerz</a>
|
<a href=/people/a/alicja-figas/>Alicja Figas</a>
|
<a href=/people/m/marcin-gruza/>Marcin Gruza</a>
|
<a href=/people/t/tomasz-kajdanowicz/>Tomasz Kajdanowicz</a>
|
<a href=/people/j/jan-kocon/>Jan Kocon</a>
|
<a href=/people/d/daria-puchalska/>Daria Puchalska</a>
|
<a href=/people/p/przemyslaw-kazienko/>Przemyslaw Kazienko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--460><div class="card-body p-3 small">There is content such as <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, offensive, toxic or aggressive documents, which are perceived differently by their consumers. They are commonly identified using <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> solely based on textual content that generalize pre-agreed meanings of difficult problems. Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> provide the same results for each user, which leads to high misclassification rate observable especially for contentious, aggressive documents. Both document controversy and user nonconformity require new solutions. Therefore, we propose novel personalized approaches that respect individual beliefs expressed by either user conformity-based measures or various embeddings of their previous text annotations. We found that only a few annotations of most controversial documents are enough for all our personalization methods to significantly outperform classic, generalized solutions. The more controversial the content, the greater the gain. The personalized solutions may be used to efficiently filter unwanted aggressive content in the way adjusted to a given person.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.462.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--462 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.462 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.462" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.462/>Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding</a></strong><br><a href=/people/x/xin-sun/>Xin Sun</a>
|
<a href=/people/t/tao-ge/>Tao Ge</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--462><div class="card-body p-3 small">In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC). SAD optimizes the online inference efficiency for GEC by two innovations : 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism ; 2) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield identical predictions to greedy decoding but with significant speedup for online inference. Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss. Not only does our approach allow a single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to achieve the state-of-the-art results in English GEC benchmarks : 66.4 F0.5 in the CoNLL-14 and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at https://github.com/AutoTemp/Shallow-Aggressive-Decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--464 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.464/>PHMOSpell : Phonological and Morphological Knowledge Guided Chinese Spelling Check<span class=acl-fixed-case>PHMOS</span>pell: Phonological and Morphological Knowledge Guided <span class=acl-fixed-case>C</span>hinese Spelling Check</a></strong><br><a href=/people/l/li-huang/>Li Huang</a>
|
<a href=/people/j/junjie-li/>Junjie Li</a>
|
<a href=/people/w/weiwei-jiang/>Weiwei Jiang</a>
|
<a href=/people/z/zhiyu-zhang/>Zhiyu Zhang</a>
|
<a href=/people/m/minchuan-chen/>Minchuan Chen</a>
|
<a href=/people/s/shaojun-wang/>Shaojun Wang</a>
|
<a href=/people/j/jing-xiao/>Jing Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--464><div class="card-body p-3 small">Chinese Spelling Check (CSC) is a challenging task due to the complex characteristics of <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a>. Statistics reveal that most Chinese spelling errors belong to phonological or visual errors. However, previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> rarely utilize phonological and morphological knowledge of Chinese characters or heavily rely on external resources to model their similarities. To address the above issues, we propose a novel end-to-end trainable model called PHMOSpell, which promotes the performance of CSC with multi-modal information. Specifically, we derive pinyin and glyph representations for <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a> from audio and visual modalities respectively, which are integrated into a pre-trained language model by a well-designed adaptive gating mechanism. To verify its effectiveness, we conduct comprehensive experiments and ablation tests. Experimental results on three shared benchmarks demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> consistently outperforms previous state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.466.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--466 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.466 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.466" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.466/>Improving <a href=https://en.wikipedia.org/wiki/Encoder>Encoder</a> by Auxiliary Supervision Tasks for Table-to-Text Generation</a></strong><br><a href=/people/l/liang-li/>Liang Li</a>
|
<a href=/people/c/can-ma/>Can Ma</a>
|
<a href=/people/y/yinliang-yue/>Yinliang Yue</a>
|
<a href=/people/d/dayong-hu/>Dayong Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--466><div class="card-body p-3 small">Table-to-text generation aims at automatically generating <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural text</a> to help people conveniently obtain salient information in tables. Although <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural models</a> for <a href=https://en.wikipedia.org/wiki/Table_(information)>table-to-text</a> have achieved remarkable progress, some problems are still overlooked. Previous methods can not deduce the factual results from the entity&#8217;s (player or team) performance and the relations between entities. To solve this issue, we first build an entity graph from the input tables and introduce a <a href=https://en.wikipedia.org/wiki/Automated_reasoning>reasoning module</a> to perform <a href=https://en.wikipedia.org/wiki/Automated_reasoning>reasoning</a> on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. Moreover, there are different relations (e.g., the numeric size relation and the importance relation) between records in different dimensions. And these <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> may contribute to the data-to-text generation. However, it is hard for a vanilla encoder to capture these. Consequently, we propose to utilize two auxiliary tasks, Number Ranking (NR) and Importance Ranking (IR), to supervise the encoder to capture the different relations. Experimental results on ROTOWIRE and RW-FG show that our method not only has a good generalization but also outperforms previous methods on several metrics : BLEU, Content Selection, Content Ordering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.468.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--468 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.468 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.468/>Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation</a></strong><br><a href=/people/x/xin-liu/>Xin Liu</a>
|
<a href=/people/b/baosong-yang/>Baosong Yang</a>
|
<a href=/people/d/dayiheng-liu/>Dayiheng Liu</a>
|
<a href=/people/h/haibo-zhang/>Haibo Zhang</a>
|
<a href=/people/w/weihua-luo/>Weihua Luo</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/haiying-zhang/>Haiying Zhang</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--468><div class="card-body p-3 small">A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary. This potentially weakens the effect when applying pretrained models into natural language generation (NLG) tasks, especially for the subword distributions between upstream and downstream tasks with significant discrepancy. Towards approaching this problem, we extend the vanilla pretrain-finetune pipeline with an extra embedding transfer step. Specifically, a plug-and-play embedding generator is introduced to produce the representation of any input token, according to pre-trained embeddings of its morphologically similar ones. Thus, embeddings of mismatch tokens in downstream tasks can also be efficiently initialized. We conduct experiments on a variety of NLG tasks under the pretrain-finetune fashion. Experimental results and extensive analyses show that the proposed <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> offers us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.473.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--473 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.473 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.473" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.473/>Capturing Relations between Scientific Papers : An Abstractive Model for Related Work Section Generation</a></strong><br><a href=/people/x/xiuying-chen/>Xiuying Chen</a>
|
<a href=/people/h/hind-alamro/>Hind Alamro</a>
|
<a href=/people/m/mingzhe-li/>Mingzhe Li</a>
|
<a href=/people/s/shen-gao/>Shen Gao</a>
|
<a href=/people/x/xiangliang-zhang/>Xiangliang Zhang</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--473><div class="card-body p-3 small">Given a set of related publications, related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order. Most of existing related work generation models follow the inflexible extractive style, which directly extract sentences from multiple original papers to form a related work discussion. Hence, in this paper, we propose a Relation-aware Related work Generator (RRG), which generates an abstractive related work from the given multiple scientific papers in the same research area. Concretely, we propose a relation-aware multi-document encoder that relates one document to another according to their content dependency in a relation graph. The relation graph and the document representation are interacted and polished iteratively, complementing each other in the training process. We also contribute two public datasets composed of related work sections and their corresponding papers. Extensive experiments on the two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> brings substantial improvements over several strong baselines. We hope that this work will promote advances in related work generation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.479.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--479 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.479 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.479/>PhotoChat : A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling<span class=acl-fixed-case>P</span>hoto<span class=acl-fixed-case>C</span>hat: A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling</a></strong><br><a href=/people/x/xiaoxue-zang/>Xiaoxue Zang</a>
|
<a href=/people/l/lijuan-liu/>Lijuan Liu</a>
|
<a href=/people/m/maria-wang/>Maria Wang</a>
|
<a href=/people/y/yang-song/>Yang Song</a>
|
<a href=/people/h/hao-zhang/>Hao Zhang</a>
|
<a href=/people/j/jindong-chen/>Jindong Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--479><div class="card-body p-3 small">We present a new human-human dialogue dataset-PhotoChat, the first dataset that casts light on the photo sharing behavior in online messaging. PhotoChat contains 12k dialogues, each of which is paired with a user photo that is shared during the conversation. Based on this dataset, we propose two tasks to facilitate research on image-text modeling : a photo-sharing intent prediction task that predicts whether one intends to share a photo in the next conversation turn, and a photo retrieval task that retrieves the most relevant photo according to the dialogue context. In addition, for both tasks, we provide <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline models</a> using the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art models</a> and report their benchmark performances. The best <a href=https://en.wikipedia.org/wiki/Image_retrieval>image retrieval model</a> achieves 10.4 % <a href=https://en.wikipedia.org/wiki/Precision_and_recall>recall@1</a> (out of 1000 candidates) and the best photo intent prediction model achieves 58.1 % <a href=https://en.wikipedia.org/wiki/Precision_and_recall>F1 score</a>, indicating that the dataset presents interesting yet challenging real-world problems. We are releasing PhotoChat to facilitate future research work among the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.480.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--480 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.480 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.480/>Good for Misconceived Reasons : An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation</a></strong><br><a href=/people/z/zhiyong-wu/>Zhiyong Wu</a>
|
<a href=/people/l/lingpeng-kong/>Lingpeng Kong</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/b/ben-kao/>Ben Kao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--480><div class="card-body p-3 small">A neural multimodal machine translation (MMT) system is one that aims to perform better <a href=https://en.wikipedia.org/wiki/Translation>translation</a> by extending conventional text-only translation models with multimodal information. Many recent studies report improvements when equipping their <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with the multimodal module, despite the controversy of whether such improvements indeed come from the multimodal part. We revisit the contribution of <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multimodal information</a> in <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>MMT</a> by devising two interpretable MMT models. To our surprise, although our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> replicate similar gains as recently developed multimodal-integrated systems achieved, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> learn to ignore the multimodal information. Upon further investigation, we discover that the improvements achieved by the multimodal models over text-only counterparts are in fact results of the regularization effect. We report empirical findings that highlight the importance of MMT models&#8217; interpretability, and discuss how our findings will benefit future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--482 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.482" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.482/>BERTifying the <a href=https://en.wikipedia.org/wiki/Hidden_Markov_model>Hidden Markov Model</a> for Multi-Source Weakly Supervised Named Entity Recognition<span class=acl-fixed-case>BERT</span>ifying the Hidden <span class=acl-fixed-case>M</span>arkov Model for Multi-Source Weakly Supervised Named Entity Recognition</a></strong><br><a href=/people/y/yinghao-li/>Yinghao Li</a>
|
<a href=/people/p/pranav-shetty/>Pranav Shetty</a>
|
<a href=/people/l/lucas-liu/>Lucas Liu</a>
|
<a href=/people/c/chao-zhang/>Chao Zhang</a>
|
<a href=/people/l/le-song/>Le Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--482><div class="card-body p-3 small">We study the problem of learning a named entity recognition (NER) tagger using noisy labels from multiple weak supervision sources. Though cheap to obtain, the labels from weak supervision sources are often incomplete, inaccurate, and contradictory, making it difficult to learn an accurate NER model. To address this challenge, we propose a conditional hidden Markov model (CHMM), which can effectively infer true labels from multi-source noisy labels in an unsupervised way. CHMM enhances the classic <a href=https://en.wikipedia.org/wiki/Hidden_Markov_model>hidden Markov model</a> with the contextual representation power of pre-trained language models. Specifically, CHMM learns token-wise transition and emission probabilities from the BERT embeddings of the input tokens to infer the latent true labels from noisy observations. We further refine CHMM with an alternate-training approach (CHMM-ALT). It fine-tunes a BERT-NER model with the labels inferred by CHMM, and this BERT-NER&#8217;s output is regarded as an additional weak source to train the CHMM in return. Experiments on four NER benchmarks from various domains show that our method outperforms state-of-the-art weakly supervised NER models by wide margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.483.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--483 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.483 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.483" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.483/>CIL : Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction<span class=acl-fixed-case>CIL</span>: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction</a></strong><br><a href=/people/t/tao-chen/>Tao Chen</a>
|
<a href=/people/h/haizhou-shi/>Haizhou Shi</a>
|
<a href=/people/s/siliang-tang/>Siliang Tang</a>
|
<a href=/people/z/zhigang-chen/>Zhigang Chen</a>
|
<a href=/people/f/fei-wu/>Fei Wu</a>
|
<a href=/people/y/yueting-zhuang/>Yueting Zhuang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--483><div class="card-body p-3 small">The journey of reducing noise from distant supervision (DS) generated training data has been started since the DS was first introduced into the relation extraction (RE) task. For the past decade, researchers apply the multi-instance learning (MIL) framework to find the most reliable <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> from a bag of sentences. Although the pattern of MIL bags can greatly reduce DS noise, it fails to represent many other useful <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence features</a> in the datasets. In many cases, these sentence features can only be acquired by extra sentence-level human annotation with heavy costs. Therefore, the performance of distantly supervised RE models is bounded. In this paper, we go beyond typical MIL framework and propose a novel contrastive instance learning (CIL) framework. Specifically, we regard the initial MIL as the relational triple encoder and constraint positive pairs against negative pairs for each instance. Experiments demonstrate the effectiveness of our proposed framework, with significant improvements over the previous methods on NYT10, GDS and KBP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.485.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--485 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.485 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.485" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.485/>An End-to-End Progressive Multi-Task Learning Framework for Medical Named Entity Recognition and Normalization</a></strong><br><a href=/people/b/baohang-zhou/>Baohang Zhou</a>
|
<a href=/people/x/xiangrui-cai/>Xiangrui Cai</a>
|
<a href=/people/y/ying-zhang/>Ying Zhang</a>
|
<a href=/people/x/xiaojie-yuan/>Xiaojie Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--485><div class="card-body p-3 small">Medical named entity recognition (NER) and normalization (NEN) are fundamental for constructing knowledge graphs and building QA systems. Existing implementations for medical NER and NEN are suffered from the error propagation between the two tasks. The mispredicted mentions from NER will directly influence the results of NEN. Therefore, the NER module is the bottleneck of the whole <a href=https://en.wikipedia.org/wiki/System>system</a>. Besides, the learnable features for both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> are beneficial to improving the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. To avoid the disadvantages of existing models and exploit the generalized representation across the two tasks, we design an end-to-end progressive multi-task learning model for jointly modeling medical NER and NEN in an effective way. There are three level tasks with <a href=https://en.wikipedia.org/wiki/Degree_of_difficulty>progressive difficulty</a> in the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. The progressive tasks can reduce the error propagation with the incremental task settings which implies the lower level tasks gain the supervised signals other than errors from the higher level tasks to improve their performances. Besides, the <a href=https://en.wikipedia.org/wiki/Context_(computing)>context features</a> are exploited to enrich the semantic information of entity mentions extracted by NER. The performance of NEN profits from the enhanced entity mention features. The standard entities from <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> are introduced into the NER module for extracting corresponding entity mentions correctly. The empirical results on two publicly available medical literature datasets demonstrate the superiority of our <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> over nine typical methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.487.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--487 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.487 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.487" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.487/>Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition</a></strong><br><a href=/people/m/meihan-tong/>Meihan Tong</a>
|
<a href=/people/s/shuai-wang/>Shuai Wang</a>
|
<a href=/people/b/bin-xu/>Bin Xu</a>
|
<a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/m/minghui-liu/>Minghui Liu</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--487><div class="card-body p-3 small">Few-shot Named Entity Recognition (NER) exploits only a handful of annotations to iden- tify and classify named entity mentions. Pro- totypical network shows superior performance on few-shot NER. However, existing prototyp- ical methods fail to differentiate rich seman- tics in other-class words, which will aggravate <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> under few shot scenario. To address the issue, we propose a novel model, Mining Undefined Classes from Other-class (MUCO), that can automatically induce different unde- fined classes from the other class to improve few-shot NER. With these extra-labeled unde- fined classes, our method will improve the dis- criminative ability of NER classifier and en- hance the understanding of predefined classes with stand-by semantic knowledge. Experi- mental results demonstrate that our model out- performs five state-of-the-art models in both 1- shot and 5-shots settings on four NER bench- marks. We will release the code upon accep- tance. The source code is released on https : //github.com / shuaiwa16 / OtherClassNER.git.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.495.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--495 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.495 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.495.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.495/>Multi-Label Few-Shot Learning for Aspect Category Detection</a></strong><br><a href=/people/m/mengting-hu/>Mengting Hu</a>
|
<a href=/people/s/shiwan-zhao/>Shiwan Zhao</a>
|
<a href=/people/h/honglei-guo/>Honglei Guo</a>
|
<a href=/people/c/chao-xue/>Chao Xue</a>
|
<a href=/people/h/hang-gao/>Hang Gao</a>
|
<a href=/people/t/tiegang-gao/>Tiegang Gao</a>
|
<a href=/people/r/renhong-cheng/>Renhong Cheng</a>
|
<a href=/people/z/zhong-su/>Zhong Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--495><div class="card-body p-3 small">Aspect category detection (ACD) in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> aims to identify the <a href=https://en.wikipedia.org/wiki/Aspect_(grammar)>aspect categories</a> mentioned in a sentence. In this paper, we formulate <a href=https://en.wikipedia.org/wiki/Autocorrelation>ACD</a> in the few-shot learning scenario. However, existing few-shot learning approaches mainly focus on single-label predictions. These methods can not work well for the ACD task since a sentence may contain multiple <a href=https://en.wikipedia.org/wiki/Aspect_(grammar)>aspect categories</a>. Therefore, we propose a multi-label few-shot learning method based on the prototypical network. To alleviate the noise, we design two effective attention mechanisms. The support-set attention aims to extract better <a href=https://en.wikipedia.org/wiki/Prototype>prototypes</a> by removing irrelevant aspects. The query-set attention computes multiple prototype-specific representations for each query instance, which are then used to compute accurate distances with the corresponding prototypes. To achieve multi-label inference, we further learn a dynamic threshold per instance by a policy network. Extensive experimental results on three datasets demonstrate that the proposed method significantly outperforms strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.496.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--496 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.496 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.496" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.496/>Argument Pair Extraction via Attention-guided Multi-Layer Multi-Cross Encoding</a></strong><br><a href=/people/l/liying-cheng/>Liying Cheng</a>
|
<a href=/people/t/tianyu-wu/>Tianyu Wu</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--496><div class="card-body p-3 small">Argument pair extraction (APE) is a research task for extracting arguments from two passages and identifying potential argument pairs. Prior research work treats this task as a sequence labeling problem and a binary classification problem on two passages that are directly concatenated together, which has a limitation of not fully utilizing the unique characteristics and inherent relations of two different passages. This paper proposes a novel attention-guided multi-layer multi-cross encoding scheme to address the challenges. The new model processes two passages with two individual sequence encoders and updates their <a href=https://en.wikipedia.org/wiki/Representation_theory>representations</a> using each other&#8217;s representations through <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. In addition, the pair prediction part is formulated as a table-filling problem by updating the representations of two sequences&#8217; Cartesian product. Furthermore, an auxiliary attention loss is introduced to guide each argument to align to its paired argument. An extensive set of experiments show that the new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improves the APE performance over several alternatives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--500 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.500 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.500" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.500/>OpenMEVA : A Benchmark for Evaluating Open-ended Story Generation Metrics<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>MEVA</span>: A Benchmark for Evaluating Open-ended Story Generation Metrics</a></strong><br><a href=/people/j/jian-guan/>Jian Guan</a>
|
<a href=/people/z/zhexin-zhang/>Zhexin Zhang</a>
|
<a href=/people/z/zhuoer-feng/>Zhuoer Feng</a>
|
<a href=/people/z/zitao-liu/>Zitao Liu</a>
|
<a href=/people/w/wenbiao-ding/>Wenbiao Ding</a>
|
<a href=/people/x/xiaoxi-mao/>Xiaoxi Mao</a>
|
<a href=/people/c/changjie-fan/>Changjie Fan</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--500><div class="card-body p-3 small">Automatic metrics are essential for developing natural language generation (NLG) models, particularly for open-ended language generation tasks such as story generation. However, existing automatic metrics are observed to correlate poorly with <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a>. The lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a <a href=https://en.wikipedia.org/wiki/Performance_metric>metric</a> and fairly compare different <a href=https://en.wikipedia.org/wiki/Performance_metric>metrics</a>. Therefore, we propose OpenMEVA, a <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for evaluating open-ended story generation metrics. OpenMEVA provides a comprehensive test suite to assess the capabilities of metrics, including (a) the correlation with human judgments, (b) the generalization to different model outputs and datasets, (c) the ability to judge story coherence, and (d) the robustness to perturbations. To this end, OpenMEVA includes both manually annotated stories and auto-constructed test examples. We evaluate existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> on OpenMEVA and observe that they have poor correlation with human judgments, fail to recognize discourse-level incoherence, and lack inferential knowledge (e.g., causal order between events), the generalization ability and robustness. Our study presents insights for developing NLG models and <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> in further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--504 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.504" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.504/>Selective Knowledge Distillation for Neural Machine Translation</a></strong><br><a href=/people/f/fusheng-wang/>Fusheng Wang</a>
|
<a href=/people/j/jianhao-yan/>Jianhao Yan</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--504><div class="card-body p-3 small">Neural Machine Translation (NMT) models achieve state-of-the-art performance on many translation benchmarks. As an active research field in NMT, knowledge distillation is widely applied to enhance the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s performance by transferring teacher model&#8217;s knowledge on each training sample. However, previous work rarely discusses the different impacts and connections among these <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>samples</a>, which serve as the medium for transferring teacher knowledge. In this paper, we design a novel <a href=https://en.wikipedia.org/wiki/Communication_protocol>protocol</a> that can effectively analyze the different impacts of samples by comparing various samples&#8217; partitions. Based on above <a href=https://en.wikipedia.org/wiki/Protocol_(science)>protocol</a>, we conduct extensive experiments and find that the teacher&#8217;s knowledge is not the more, the better. Knowledge over specific samples may even hurt the whole performance of knowledge distillation. Finally, to address these issues, we propose two simple yet effective strategies, i.e., batch-level and global-level selections, to pick suitable samples for distillation. We evaluate our approaches on two large-scale machine translation tasks, WMT&#8217;14 English-German and WMT&#8217;19 Chinese-English. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>approaches</a> yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--505 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.505" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.505/>Measuring and Increasing Context Usage in Context-Aware Machine Translation</a></strong><br><a href=/people/p/patrick-fernandes/>Patrick Fernandes</a>
|
<a href=/people/k/kayo-yin/>Kayo Yin</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--505><div class="card-body p-3 small">Recent work in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> at translation time. In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, conditional cross-mutual information, to quantify usage of context by these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Using this <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--508 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.508" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.508/>Length-Adaptive Transformer : Train Once with Length Drop, Use Anytime with Search</a></strong><br><a href=/people/g/gyuwan-kim/>Gyuwan Kim</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--508><div class="card-body p-3 small">Despite <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a>&#8217; impressive accuracy, their <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> is often prohibitive to use with limited computational resources. Most previous approaches to improve inference efficiency require a separate <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for each possible computational budget. In this paper, we extend PoWER-BERT (Goyal et al., 2020) and propose Length-Adaptive Transformer that can be used for various inference scenarios after one-shot training. We train a <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a> with LengthDrop, a structural variant of dropout, which stochastically determines a sequence length at each layer. We then conduct a multi-objective evolutionary search to find a length configuration that maximizes the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and minimizes the efficiency metric under any given computational budget. Additionally, we significantly extend the applicability of PoWER-BERT beyond sequence-level classification into token-level classification with Drop-and-Restore process that drops word-vectors temporarily in intermediate layers and restores at the last layer if necessary. We empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups, including span-based question answering and text classification. Code is available at https://github.com/clovaai/lengthadaptive-transformer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--515 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.515/>Regression Bugs Are In Your Model ! Measuring, Reducing and Analyzing Regressions In NLP Model Updates<span class=acl-fixed-case>NLP</span> Model Updates</a></strong><br><a href=/people/y/yuqing-xie/>Yuqing Xie</a>
|
<a href=/people/y/yi-an-lai/>Yi-An Lai</a>
|
<a href=/people/y/yuanjun-xiong/>Yuanjun Xiong</a>
|
<a href=/people/y/yi-zhang/>Yi Zhang</a>
|
<a href=/people/s/stefano-soatto/>Stefano Soatto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--515><div class="card-body p-3 small">Behavior of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> can be inconsistent between different versions. Regressions during model update are a common cause of concern that often over-weigh the benefits in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> or efficiency gain. This work focuses on quantifying, reducing and analyzing <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression errors</a> in the NLP model updates. Using negative flip rate as <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression measure</a>, we show that <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> has a prevalent presence across tasks in the GLUE benchmark. We formulate the regression-free model updates into a constrained optimization problem, and further reduce it into a relaxed form which can be approximately optimized through knowledge distillation training method. We empirically analyze how <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>model ensemble</a> reduces <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a>. Finally, we conduct CheckList behavioral testing to understand the distribution of regressions across linguistic phenomena, and the efficacy of ensemble and distillation methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.517.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--517 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.517 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.517" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.517/>On the Efficacy of Adversarial Data Collection for <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> : Results from a Large-Scale Randomized Study</a></strong><br><a href=/people/d/divyansh-kaushik/>Divyansh Kaushik</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/z/zachary-c-lipton/>Zachary C. Lipton</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--517><div class="card-body p-3 small">In adversarial data collection (ADC), a human workforce interacts with a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in real time, attempting to produce examples that elicit incorrect predictions. Researchers hope that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on these more challenging <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> will rely less on superficial patterns, and thus be less brittle. However, despite ADC&#8217;s intuitive appeal, it remains unclear when training on adversarial datasets produces more robust models. In this paper, we conduct a large-scale controlled study focused on <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, assigning workers at random to compose questions either (i) adversarially (with a model in the loop) ; or (ii) in the standard fashion (without a model). Across a variety of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> and datasets, we find that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on adversarial data usually perform better on other adversarial datasets but worse on a diverse collection of out-of-domain evaluation sets. Finally, we provide a qualitative analysis of adversarial (vs standard) data, identifying key differences and offering guidance for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--520 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.520" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.520/>Question Answering Over Temporal Knowledge Graphs</a></strong><br><a href=/people/a/apoorv-saxena/>Apoorv Saxena</a>
|
<a href=/people/s/soumen-chakrabarti/>Soumen Chakrabarti</a>
|
<a href=/people/p/partha-talukdar/>Partha Talukdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--520><div class="card-body p-3 small">Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broad coverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by a factor of 340x. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. In response, we also propose CRONKGQA, a transformer-based solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120 % in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we have released our code as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.521.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--521 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.521 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.521/>Language Model Augmented Relevance Score</a></strong><br><a href=/people/r/ruibo-liu/>Ruibo Liu</a>
|
<a href=/people/j/jason-wei/>Jason Wei</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--521><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automated metrics</a> are commonly used to evaluate NLG systems, <a href=https://en.wikipedia.org/wiki/Information_technology>they</a> often correlate poorly with <a href=https://en.wikipedia.org/wiki/Judgement>human judgements</a>. Newer metrics such as BERTScore have addressed many weaknesses in prior metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and ROUGE, which rely on n-gram matching. These newer methods, however, are still limited in that they do not consider the generation context, so they can not properly reward generated text that is correct but deviates from the given reference. In this paper, we propose Language Model Augmented Relevance Score (MARS), a new context-aware metric for NLG evaluation. MARS leverages off-the-shelf language models, guided by <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, to create augmented references that consider both the generation context and available human references, which are then used as additional references to score generated text. Compared with seven existing metrics in three common NLG tasks, MARS not only achieves higher correlation with human reference judgements, but also differentiates well-formed candidates from adversarial samples to a larger degree.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.522.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--522 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.522 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.522" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.522/>DExperts : Decoding-Time Controlled Text Generation with Experts and Anti-Experts<span class=acl-fixed-case>DE</span>xperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts</a></strong><br><a href=/people/a/alisa-liu/>Alisa Liu</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/x/ximing-lu/>Ximing Lu</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/c/chandra-bhagavatula/>Chandra Bhagavatula</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--522><div class="card-body p-3 small">Despite recent advances in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>, it remains challenging to control attributes of generated text. We propose DExperts : Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with expert LMs and/or anti-expert LMs in a product of experts. Intuitively, under the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a>, tokens only get high probability if they are considered likely by the <a href=https://en.wikipedia.org/wiki/Expert>experts</a>, and unlikely by the <a href=https://en.wikipedia.org/wiki/Expert>anti-experts</a>. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--523 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.523" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.523/>Polyjuice : Generating Counterfactuals for Explaining, Evaluating, and Improving Models</a></strong><br><a href=/people/t/tongshuang-wu/>Tongshuang Wu</a>
|
<a href=/people/m/marco-tulio-ribeiro/>Marco Tulio Ribeiro</a>
|
<a href=/people/j/jeffrey-heer/>Jeffrey Heer</a>
|
<a href=/people/d/daniel-s-weld/>Daniel Weld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--523><div class="card-body p-3 small">While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> or word substitutions. We present <a href=https://en.wikipedia.org/wiki/Polyjuice>Polyjuice</a>, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that <a href=https://en.wikipedia.org/wiki/Polyjuice>Polyjuice</a> produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications : improving training and evaluation on three different tasks (with around 70 % less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--527 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.527" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.527/>Mid-Air Hand Gestures for Post-Editing of Machine Translation</a></strong><br><a href=/people/r/rashad-albo-jamara/>Rashad Albo Jamara</a>
|
<a href=/people/n/nico-herbig/>Nico Herbig</a>
|
<a href=/people/a/antonio-kruger/>Antonio Krüger</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--527><div class="card-body p-3 small">To translate large volumes of text in a globally connected world, more and more translators are integrating <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> and <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing (PE)</a> into their translation workflows to generate publishable quality translations. While this process has been shown to save time and reduce errors, the task of <a href=https://en.wikipedia.org/wiki/Translation>translation</a> is changing from mostly text production from scratch to fixing errors within useful but partly incorrect MT output. This is affecting the interface design of <a href=https://en.wikipedia.org/wiki/Machine_translation>translation tools</a>, where better support for <a href=https://en.wikipedia.org/wiki/Text_editor>text editing tasks</a> is required. Here, we present the first study that investigates the usefulness of mid-air hand gestures in combination with the keyboard (GK) for text editing in PE of MT. Guided by a gesture elicitation study with 14 freelance translators, we develop a prototype supporting mid-air hand gestures for cursor placement, text selection, deletion, and reordering. These gestures combined with the <a href=https://en.wikipedia.org/wiki/Computer_keyboard>keyboard</a> facilitate all editing types required for PE. An evaluation of the <a href=https://en.wikipedia.org/wiki/Prototype>prototype</a> shows that the average editing duration of GK is only slightly slower than the standard mouse and keyboard (MK), even though participants are very familiar with the latter, and relative novices to the former. Furthermore, the <a href=https://en.wikipedia.org/wiki/Qualitative_research>qualitative analysis</a> shows positive attitudes towards <a href=https://en.wikipedia.org/wiki/List_of_gestures>hand gestures</a> for <a href=https://en.wikipedia.org/wiki/Physical_education>PE</a>, especially when manipulating <a href=https://en.wikipedia.org/wiki/Phrase>single words</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--529 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.529/>Joint Verification and Reranking for Open Fact Checking Over Tables</a></strong><br><a href=/people/m/michael-sejr-schlichtkrull/>Michael Sejr Schlichtkrull</a>
|
<a href=/people/v/vladimir-karpukhin/>Vladimir Karpukhin</a>
|
<a href=/people/b/barlas-oguz/>Barlas Oguz</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--529><div class="card-body p-3 small">Structured information is an important knowledge source for automatic verification of factual claims. Nevertheless, the majority of existing research into this task has focused on textual data, and the few recent inquiries into <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> have been for the closed-domain setting where appropriate evidence for each claim is assumed to have already been retrieved. In this paper, we investigate <a href=https://en.wikipedia.org/wiki/Verification_and_validation>verification</a> over structured data in the open-domain setting, introducing a joint reranking-and-verification model which fuses evidence documents in the <a href=https://en.wikipedia.org/wiki/Verification_and_validation>verification component</a>. Our open-domain model achieves performance comparable to the closed-domain state-of-the-art on the TabFact dataset, and demonstrates performance gains from the inclusion of multiple tables as well as a significant improvement over a heuristic retrieval baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--541 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.541/>Verb Knowledge Injection for Multilingual Event Processing</a></strong><br><a href=/people/o/olga-majewska/>Olga Majewska</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--541><div class="card-body p-3 small">Linguistic probing of pretrained Transformer-based language models (LMs) revealed that they encode a range of syntactic and semantic properties of a language. However, they are still prone to fall back on superficial cues and simple <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> to solve downstream tasks, rather than leverage deeper linguistic information. In this paper, we target a specific facet of <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic knowledge</a>, the interplay between <a href=https://en.wikipedia.org/wiki/Semantics>verb meaning</a> and <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>argument structure</a>. We investigate whether injecting explicit information on verbs&#8217; semantic-syntactic behaviour improves the performance of pretrained LMs in event extraction tasks, where accurate verb processing is paramount. Concretely, we impart the verb knowledge from curated lexical resources into dedicated adapter modules (verb adapters), allowing it to complement, in downstream tasks, the language knowledge obtained during LM-pretraining. We first demonstrate that injecting verb knowledge leads to performance gains in English event extraction. We then explore the utility of verb adapters for event extraction in other languages : we investigate 1) zero-shot language transfer with multilingual Transformers and 2) transfer via (noisy automatic) translation of English verb-based lexical knowledge. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when relying on noisily translated lexical knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--543 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.543" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.543/>Lexical Semantic Change Discovery</a></strong><br><a href=/people/s/sinan-kurtyigit/>Sinan Kurtyigit</a>
|
<a href=/people/m/maike-park/>Maike Park</a>
|
<a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--543><div class="card-body p-3 small">While there is a large amount of research in the field of Lexical Semantic Change Detection, only few approaches go beyond a standard benchmark evaluation of existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. In this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for both <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> and <a href=https://en.wikipedia.org/wiki/Discovery_(observation)>discovery</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.544.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--544 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.544 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.544/>The R-U-A-Robot Dataset : Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity<span class=acl-fixed-case>R</span>-<span class=acl-fixed-case>U</span>-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity</a></strong><br><a href=/people/d/david-gros/>David Gros</a>
|
<a href=/people/y/yu-li/>Yu Li</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--544><div class="card-body p-3 small">Humans are increasingly interacting with machines through <a href=https://en.wikipedia.org/wiki/Language>language</a>, sometimes in contexts where the user may not know they are talking to a machine (like over the phone or a text chatbot). We aim to understand how system designers and researchers might allow their systems to confirm its <a href=https://en.wikipedia.org/wiki/Non-human>non-human identity</a>. We collect over 2,500 phrasings related to the intent of Are you a robot?. This is paired with over 2,500 adversarially selected utterances where only confirming the system is non-human would be insufficient or disfluent. We compare <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> to recognize the intent and discuss the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> / recall and model complexity tradeoffs. Such classifiers could be integrated into <a href=https://en.wikipedia.org/wiki/Dialog_(software)>dialog systems</a> to avoid undesired deception. We then explore how both a generative research model (Blender) as well as two deployed systems (Amazon Alexa, Google Assistant) handle this intent, finding that systems often fail to confirm their non-human identity. Finally, we try to understand what a good response to the intent would be, and conduct a user study to compare the important aspects when responding to this intent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--545 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.545/>Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Conversational Systems</a></strong><br><a href=/people/c/claudio-pinhanez/>Claudio Pinhanez</a>
|
<a href=/people/p/paulo-cavalin/>Paulo Cavalin</a>
|
<a href=/people/v/victor-henrique-alves-ribeiro/>Victor Henrique Alves Ribeiro</a>
|
<a href=/people/a/ana-appel/>Ana Appel</a>
|
<a href=/people/h/heloisa-candello/>Heloisa Candello</a>
|
<a href=/people/j/julio-nogima/>Julio Nogima</a>
|
<a href=/people/m/mauro-pichiliani/>Mauro Pichiliani</a>
|
<a href=/people/m/melina-guerra/>Melina Guerra</a>
|
<a href=/people/m/maira-de-bayser/>Maira de Bayser</a>
|
<a href=/people/g/gabriel-malfatti/>Gabriel Malfatti</a>
|
<a href=/people/h/henrique-ferreira/>Henrique Ferreira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--545><div class="card-body p-3 small">In this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers. Developers often include such <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a>, structure as <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomies</a>, in the documentation of chatbots. By using neuro-symbolic algorithms to incorporate those <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomies</a> into embeddings of the output space, we were able to improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in intent recognition. In datasets with intents and example utterances from 200 professional chatbots, we saw decreases in the equal error rate (EER) in more than 40 % of the <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> in comparison to the baseline of the same <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> without the meta-knowledge. The meta-knowledge proved also to be effective in detecting out-of-scope utterances, improving the false acceptance rate (FAR) in two thirds of the <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>, with decreases of 0.05 or more in FAR in almost 40 % of the <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>. When considering only the well-developed workspaces with a high level use of <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomies</a>, FAR decreased more than 0.05 in 77 % of them, and more than 0.1 in 39 % of the <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.551.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--551 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.551 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.551/>ARBERT & MARBERT : Deep Bidirectional Transformers for Arabic<span class=acl-fixed-case>ARBERT</span> & <span class=acl-fixed-case>MARBERT</span>: Deep Bidirectional Transformers for <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/a/abdelrahim-elmadany/>AbdelRahim Elmadany</a>
|
<a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--551><div class="card-body p-3 small">Pre-trained language models (LMs) are currently integral to many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a>. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> including XLM-R Large (3.4x larger size). Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.552.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--552 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.552 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.552" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.552/>Improving <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>Paraphrase Detection</a> with the Adversarial Paraphrasing Task</a></strong><br><a href=/people/a/animesh-nighojkar/>Animesh Nighojkar</a>
|
<a href=/people/j/john-licato/>John Licato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--552><div class="card-body p-3 small">If two sentences have the same meaning, it should follow that they are equivalent in their <a href=https://en.wikipedia.org/wiki/Property_(philosophy)>inferential properties</a>, i.e., each sentence should textually entail the other. However, many <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrase datasets</a> currently in widespread use rely on a sense of <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrase</a> based on word overlap and <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>. Can we teach them instead to identify paraphrases in a way that draws on the inferential properties of the sentences, and is not over-reliant on lexical and syntactic similarities of a sentence pair? We apply the adversarial paradigm to this question, and introduce a new adversarial method of dataset creation for paraphrase identification : the Adversarial Paraphrasing Task (APT), which asks participants to generate semantically equivalent (in the sense of mutually implicative) but lexically and syntactically disparate paraphrases. These sentence pairs can then be used both to test paraphrase identification models (which get barely random accuracy) and then improve their performance. To accelerate <a href=https://en.wikipedia.org/wiki/Data_set>dataset generation</a>, we explore automation of APT using T5, and show that the resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> also improves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We discuss implications for <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a> and release our dataset in the hope of making <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection models</a> better able to detect sentence-level meaning equivalence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.553.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--553 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.553 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.553/>ADEPT : An Adjective-Dependent Plausibility Task<span class=acl-fixed-case>ADEPT</span>: An Adjective-Dependent Plausibility Task</a></strong><br><a href=/people/a/ali-emami/>Ali Emami</a>
|
<a href=/people/i/ian-porada/>Ian Porada</a>
|
<a href=/people/a/alexandra-olteanu/>Alexandra Olteanu</a>
|
<a href=/people/k/kaheer-suleman/>Kaheer Suleman</a>
|
<a href=/people/a/adam-trischler/>Adam Trischler</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--553><div class="card-body p-3 small">A false contract is more likely to be rejected than a contract is, yet a false key is less likely than a key to open doors. While correctly interpreting and assessing the effects of such adjective-noun pairs (e.g., false key) on the plausibility of given events (e.g., opening doors) underpins many natural language understanding tasks, doing so often requires a significant degree of <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> and common-sense reasoning. We introduce ADEPT a large-scale semantic plausibility task consisting of over 16 thousand sentences that are paired with slightly modified versions obtained by adding an <a href=https://en.wikipedia.org/wiki/Adjective>adjective</a> to a noun. Overall, we find that while the task appears easier for human judges (85 % accuracy), it proves more difficult for transformer-based models like RoBERTa (71 % accuracy). Our experiments also show that neither the adjective itself nor its taxonomic class suffice in determining the correct plausibility judgement, emphasizing the importance of endowing automatic natural language understanding systems with more context sensitivity and common-sense reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.555.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--555 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.555 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.555/>Conditional Generation of Temporally-ordered Event Sequences</a></strong><br><a href=/people/s/shih-ting-lin/>Shih-Ting Lin</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--555><div class="card-body p-3 small">Models of narrative schema knowledge have proven useful for a range of event-related tasks, but they typically do not capture the temporal relationships between events. We propose a single model that addresses both temporal ordering, sorting given events into the order they occurred, and event infilling, predicting new events which fit into an existing temporally-ordered sequence. We use a BART-based conditional generation model that can capture both temporality and common event co-occurrence, meaning it can be flexibly applied to different tasks in this space. Our model is trained as a denoising autoencoder : we take temporally-ordered event sequences, shuffle them, delete some events, and then attempt to recover the original event sequence. This task teaches the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to make inferences given incomplete knowledge about the events in an underlying scenario. On the temporal ordering task, we show that our model is able to unscramble event sequences from existing datasets without access to explicitly labeled temporal training data, outperforming both a BERT-based pairwise model and a BERT-based pointer network. On event infilling, human evaluation shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to generate events that fit better temporally into the input events when compared to GPT-2 story completion models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.558.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--558 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.558 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.558" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.558/>SpanNER : Named Entity Re-/Recognition as Span Prediction<span class=acl-fixed-case>S</span>pan<span class=acl-fixed-case>NER</span>: Named Entity Re-/Recognition as Span Prediction</a></strong><br><a href=/people/j/jinlan-fu/>Jinlan Fu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--558><div class="card-body p-3 small">Recent years have seen the paradigm shift of Named Entity Recognition (NER) systems from <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> to span prediction. Despite its preliminary effectiveness, the span prediction model&#8217;s architectural bias has not been fully understood. In this paper, we first investigate the strengths and weaknesses when the span prediction model is used for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary advantages of systems based on different paradigms. We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems&#8217; outputs. We experimentally implement 154 <a href=https://en.wikipedia.org/wiki/System>systems</a> on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners. We make all codes and datasets available :, as well as an online system demo :. Our model also has been deployed into the ExplainaBoard platform, which allows users to flexibly perform a system combination of top-scoring systems in an interactive way :.<url>https://github.com/neulab/spanner</url>, as well as an online system demo: <url>http://spanner.sh</url>. Our model also has been deployed into the ExplainaBoard platform, which allows users to flexibly perform a system combination of top-scoring systems in an interactive way: <url>http://explainaboard.nlpedia.ai/leaderboard/task-ner/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.562.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--562 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.562 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.562" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.562/>Beyond Noise : Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation</a></strong><br><a href=/people/e/eleftheria-briakou/>Eleftheria Briakou</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--562><div class="card-body p-3 small">While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN-FR tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.566.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--566 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.566 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-long.566.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material">
<i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.566" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-long.566/>Scientific Credibility of Machine Translation Research : A Meta-Evaluation of 769 Papers</a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/r/raphael-rubino/>Raphael Rubino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--566><div class="card-body p-3 small">This paper presents the first large-scale meta-evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>. We annotated MT evaluations conducted in 769 research papers published from 2010 to 2020. Our study shows that practices for automatic MT evaluation have dramatically changed during the past decade and follow concerning trends. An increasing number of MT evaluations exclusively rely on differences between <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> to draw conclusions, without performing any kind of statistical significance testing nor human evaluation, while at least 108 metrics claiming to be better than <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> have been proposed. MT evaluations in recent papers tend to copy and compare automatic metric scores from previous work to claim the superiority of a method or an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> without confirming neither exactly the same training, validating, and testing data have been used nor the metric scores are comparable. Furthermore, tools for reporting standardized metric scores are still far from being widely adopted by the MT community. After showing how the accumulation of these pitfalls leads to dubious evaluation, we propose a guideline to encourage better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.567.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--567 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.567 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.567" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.567/>Neural Machine Translation with Monolingual Translation Memory</a></strong><br><a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/h/huayang-li/>Huayang Li</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--567><div class="card-body p-3 small">Prior work has proved that Translation Memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for <a href=https://en.wikipedia.org/wiki/Memory_retrieval>memory retrieval</a>, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner. Our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the <a href=https://en.wikipedia.org/wiki/Memory_retrieval>memory retriever</a> and NMT model can be jointly optimized for the ultimate translation goal. Experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.568.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--568 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.568 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.568" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.568/>Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</a></strong><br><a href=/people/a/armen-aghajanyan/>Armen Aghajanyan</a>
|
<a href=/people/s/sonal-gupta/>Sonal Gupta</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--568><div class="card-body p-3 small">Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively <a href=https://en.wikipedia.org/wiki/Gradient_descent>vanilla gradient descent algorithms</a> (e.g., without strong regularization) to tune a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with hundreds of millions of parameters on <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> through the lens of <a href=https://en.wikipedia.org/wiki/Intrinsic_dimension>intrinsic dimension</a> provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension ; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90 % of the full parameter performance levels on MRPC. Furthermore, we empirically show that <a href=https://en.wikipedia.org/wiki/Training>pre-training</a> implicitly minimizes <a href=https://en.wikipedia.org/wiki/Intrinsic_dimension>intrinsic dimension</a> and, perhaps surprisingly, larger models tend to have lower <a href=https://en.wikipedia.org/wiki/Intrinsic_dimension>intrinsic dimension</a> after a fixed number of <a href=https://en.wikipedia.org/wiki/Training>pre-training updates</a>, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.569.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--569 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.569 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Outstanding Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.569" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.569/>UnNatural Language Inference<span class=acl-fixed-case>UnNatural</span> <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>I</span>nference</a></strong><br><a href=/people/k/koustuv-sinha/>Koustuv Sinha</a>
|
<a href=/people/p/prasanna-parthasarathi/>Prasanna Parthasarathi</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--569><div class="card-body p-3 small">Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to understand human-like syntax, at least to some extent. We provide novel evidence that complicates this claim : we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are invariant to random word-order permutations. This behavior notably differs from that of humans ; we struggle to understand the meaning of ungrammatical sentences. To measure the severity of this issue, we propose a suite of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> and investigate which properties of particular <a href=https://en.wikipedia.org/wiki/Permutation>permutations</a> lead <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to be word order invariant. For example, in MNLI dataset we find almost all (98.7 %) examples contain at least one <a href=https://en.wikipedia.org/wiki/Permutation>permutation</a> which elicits the gold label. Models are even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists in pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-long.571.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-long--571 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-long.571 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-long.571" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-long.571/>Vocabulary Learning via <a href=https://en.wikipedia.org/wiki/Optimal_transport>Optimal Transport</a> for Neural Machine Translation</a></strong><br><a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/c/chun-gan/>Chun Gan</a>
|
<a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-long--571><div class="card-body p-3 small">The choice of token vocabulary affects the performance of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. This paper aims to figure out what is a good vocabulary and whether we can find the optimal vocabulary without <a href=https://en.wikipedia.org/wiki/Trial_and_error>trial training</a>. To answer these questions, we first provide an alternative understanding of <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> from the perspective of <a href=https://en.wikipedia.org/wiki/Information_theory>information theory</a>. It motivates us to formulate the quest of vocabularization finding the best token dictionary with a proper size as an optimal transport (OT) problem. We propose VOLT, a simple and efficient solution without trial training. Empirical results show that VOLT beats widely-used vocabularies in diverse scenarios, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation. For example, VOLT achieves 70 % vocabulary size reduction and 0.5 BLEU gain on English-German translation. Also, compared to BPE-search, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https://github.com/Jingjing-NLP/VOLT.</div></div></div><hr><div id=2021acl-short><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.acl-short/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.0/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></strong><br><a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.1/>Catchphrase : Automatic Detection of Cultural References</a></strong><br><a href=/people/n/nir-sweed/>Nir Sweed</a>
|
<a href=/people/d/dafna-shahaf/>Dafna Shahaf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--1><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Snowclone>snowclone</a> is a customizable phrasal template that can be realized in multiple, instantly recognized variants. For example, * is the new * (Orange is the new black, 40 is the new 30). Snowclones are extensively used in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In this paper, we study snowclones originating from pop-culture quotes ; our goal is to automatically detect cultural references in text. We introduce a new, publicly available data set of pop-culture quotes and their corresponding snowclone usages and train models on them. We publish code for Catchphrase, an internet browser plugin to automatically detect and mark references in real-time, and examine its performance via a user study. Aside from assisting people to better comprehend cultural references, we hope that detecting snowclones can complement work on <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> and help tackling long-standing questions in <a href=https://en.wikipedia.org/wiki/Social_science>social science</a> about the dynamics of information propagation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.5/>Difficulty-Aware Machine Translation Evaluation</a></strong><br><a href=/people/r/runzhe-zhan/>Runzhe Zhan</a>
|
<a href=/people/x/xuebo-liu/>Xuebo Liu</a>
|
<a href=/people/d/derek-f-wong/>Derek F. Wong</a>
|
<a href=/people/l/lidia-s-chao/>Lidia S. Chao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--5><div class="card-body p-3 small">The high-quality translation results produced by machine translation (MT) systems still pose a huge challenge for automatic evaluation. Current MT evaluation pays the same attention to each sentence component, while the questions of real-world examinations (e.g., university examinations) have different difficulties and weightings. In this paper, we propose a novel difficulty-aware MT evaluation metric, expanding the evaluation dimension by taking translation difficulty into consideration. A translation that fails to be predicted by most MT systems will be treated as a difficult one and assigned a large weight in the final score function, and conversely. Experimental results on the WMT19 English-German Metrics shared tasks show that our proposed method outperforms commonly used MT metrics in terms of human correlation. In particular, our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> performs well even when all the MT systems are very competitive, which is when most existing <a href=https://en.wikipedia.org/wiki/Performance_metric>metrics</a> fail to distinguish between them. The source code is freely available at https://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.10/>Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions<span class=acl-fixed-case>VQA</span> Systems <span class=acl-fixed-case>RAD</span>? <span class=acl-fixed-case>M</span>easuring Robustness to Augmented Data with Focused Interventions</a></strong><br><a href=/people/d/daniel-rosenberg/>Daniel Rosenberg</a>
|
<a href=/people/i/itai-gat/>Itai Gat</a>
|
<a href=/people/a/amir-feder/>Amir Feder</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--10><div class="card-body p-3 small">Deep learning algorithms have shown promising results in visual question answering (VQA) tasks, but a more careful look reveals that they often do not understand the rich signal they are being fed with. To understand and better measure the generalization capabilities of VQA systems, we look at their <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to counterfactually augmented data. Our proposed augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes. Using these augmentations, we propose a new robustness measure, Robustness to Augmented Data (RAD), which measures the consistency of model predictions between original and augmented examples. Through extensive experimentation, we show that RAD, unlike classical accuracy measures, can quantify when state-of-the-art systems are not robust to <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a>. We find substantial failure cases which reveal that current VQA systems are still brittle. Finally, we connect between <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>, demonstrating the predictive power of RAD for performance on unseen augmentations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.14/>N-Best ASR Transformer : Enhancing SLU Performance using Multiple ASR Hypotheses<span class=acl-fixed-case>ASR</span> Transformer: Enhancing <span class=acl-fixed-case>SLU</span> Performance using Multiple <span class=acl-fixed-case>ASR</span> Hypotheses</a></strong><br><a href=/people/k/karthik-ganesan/>Karthik Ganesan</a>
|
<a href=/people/p/pakhi-bamdev/>Pakhi Bamdev</a>
|
<a href=/people/j/jaivarsan-b/>Jaivarsan B</a>
|
<a href=/people/a/amresh-venugopal/>Amresh Venugopal</a>
|
<a href=/people/a/abhinav-tushar/>Abhinav Tushar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--14><div class="card-body p-3 small">Spoken Language Understanding (SLU) systems parse speech into semantic structures like <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>dialog acts</a> and <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>slots</a>. This involves the use of an Automatic Speech Recognizer (ASR) to transcribe speech into multiple text alternatives (hypotheses). Transcription errors, ordinary in ASRs, impact downstream SLU performance negatively. Common approaches to mitigate such errors involve using richer information from the ASR, either in form of N-best hypotheses or word-lattices. We hypothesize that transformer models will learn better with a simpler utterance representation using the concatenation of the N-best ASR alternatives, where each alternative is separated by a special delimiter [ SEP ]. In our work, we test our hypothesis by using the concatenated N-best ASR alternatives as the input to the transformer encoder models, namely BERT and XLM-RoBERTa, and achieve equivalent performance to the prior state-of-the-art model on DSTC2 dataset. We also show that our approach significantly outperforms the prior state-of-the-art when subjected to the low data regime. Additionally, this <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is accessible to users of third-party ASR APIs which do not provide word-lattice information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.21/>AND does not mean OR : Using Formal Languages to Study Language Models’ Representations<span class=acl-fixed-case>AND</span> does not mean <span class=acl-fixed-case>OR</span>: Using Formal Languages to Study Language Models’ Representations</a></strong><br><a href=/people/a/aaron-traylor/>Aaron Traylor</a>
|
<a href=/people/r/roman-feiman/>Roman Feiman</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--21><div class="card-body p-3 small">A current open question in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> is to what extent language models, which are trained with access only to the form of language, are able to capture the meaning of language. This question is challenging to answer in general, as there is no clear line between <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning</a> and <a href=https://en.wikipedia.org/wiki/Theory_of_forms>form</a>, but rather <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning</a> constrains <a href=https://en.wikipedia.org/wiki/Theory_of_forms>form</a> in consistent ways. The goal of this study is to offer insights into a narrower but critical subquestion : Under what conditions should we expect that <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning</a> and <a href=https://en.wikipedia.org/wiki/Theory_of_forms>form</a> covary sufficiently, such that a language model with access only to <a href=https://en.wikipedia.org/wiki/Theory_of_forms>form</a> might nonetheless succeed in emulating <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning</a>? Focusing on several formal languages (propositional logic and a set of programming languages), we generate training corpora using a variety of motivated constraints, and measure a distributional language model&#8217;s ability to differentiate logical symbols (AND, OR, and NOT). Our findings are largely negative : none of our simulated training corpora result in <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> which definitively differentiate meaningfully different symbols (e.g., AND vs. OR), suggesting a limitation to the types of semantic signals that current <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are able to exploit.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.25/>Continual Quality Estimation with Online Bayesian Meta-Learning<span class=acl-fixed-case>B</span>ayesian Meta-Learning</a></strong><br><a href=/people/a/abiola-obamuyide/>Abiola Obamuyide</a>
|
<a href=/people/m/marina-fomicheva/>Marina Fomicheva</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--25><div class="card-body p-3 small">Most current quality estimation (QE) models for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> are trained and evaluated in a static setting where <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and test data</a> are assumed to be from a fixed distribution. However, in real-life settings, the test data that a deployed QE model would be exposed to may differ from its training data. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of end-users, who will employ predictions in practice. To address this challenge, we propose an online Bayesian meta-learning framework for the continuous training of QE models that is able to adapt them to the needs of different users, while being robust to distributional shifts in training and test data. Experiments on <a href=https://en.wikipedia.org/wiki/Data>data</a> with varying number of users and language characteristics validate the effectiveness of the proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.31/>Multilingual Agreement for Multilingual Neural Machine Translation</a></strong><br><a href=/people/j/jian-yang/>Jian Yang</a>
|
<a href=/people/y/yuwei-yin/>Yuwei Yin</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/h/haoyang-huang/>Haoyang Huang</a>
|
<a href=/people/d/dongdong-zhang/>Dongdong Zhang</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--31><div class="card-body p-3 small">Although multilingual neural machine translation (MNMT) enables multiple language translations, the training process is based on independent multilingual objectives. Most multilingual models can not explicitly exploit different language pairs to assist each other, ignoring the relationships among them. In this work, we propose a novel agreement-based method to encourage multilingual agreement among different translation directions, which minimizes the differences among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves significant improvements over the previous multilingual baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.32/>Higher-order Derivatives of Weighted Finite-state Machines</a></strong><br><a href=/people/r/ran-zmigrod/>Ran Zmigrod</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--32><div class="card-body p-3 small">Weighted finite-state machines are a fundamental building block of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>. They have withstood the test of timefrom their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the <a href=https://en.wikipedia.org/wiki/Normalization_constant>normalization constant</a> for weighted finite-state machines. We provide a general <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for evaluating <a href=https://en.wikipedia.org/wiki/Derivative_(finance)>derivatives</a> of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(A2 N4) time where A is the alphabet size and N is the number of states. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for computing second-order expectations, such as <a href=https://en.wikipedia.org/wiki/Covariance_matrix>covariance matrices</a> and gradients of first-order expectations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.33" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.33/>Reinforcement Learning for Abstractive Question Summarization with Question-aware Semantic Rewards</a></strong><br><a href=/people/s/shweta-yadav/>Shweta Yadav</a>
|
<a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/a/asma-ben-abacha/>Asma Ben Abacha</a>
|
<a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--33><div class="card-body p-3 small">The growth of online consumer health questions has led to the necessity for reliable and accurate <a href=https://en.wikipedia.org/wiki/Question_answering>question answering systems</a>. A recent study showed that manual summarization of consumer health questions brings significant improvement in retrieving relevant answers. However, the automatic summarization of long questions is a challenging task due to the lack of training data and the complexity of the related subtasks, such as the question focus and type recognition. In this paper, we introduce a reinforcement learning-based framework for abstractive question summarization. We propose two novel rewards obtained from the downstream tasks of (i) question-type identification and (ii) question-focus recognition to regularize the question generation model. These rewards ensure the generation of semantically valid questions and encourage the inclusion of key medical entities / foci in the question summary. We evaluated our proposed method on two benchmark datasets and achieved higher performance over <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art models</a>. The manual evaluation of the summaries reveals that the generated questions are more diverse and have fewer factual inconsistencies than the baseline summaries. The source code is available here : https://github.com/shwetanlp/CHQ-Summ.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.34/>A Semantics-aware Transformer Model of Relation Linking for Knowledge Base Question Answering</a></strong><br><a href=/people/t/tahira-naseem/>Tahira Naseem</a>
|
<a href=/people/s/srinivas-ravishankar/>Srinivas Ravishankar</a>
|
<a href=/people/n/nandana-mihindukulasooriya/>Nandana Mihindukulasooriya</a>
|
<a href=/people/i/ibrahim-abdelaziz/>Ibrahim Abdelaziz</a>
|
<a href=/people/y/young-suk-lee/>Young-Suk Lee</a>
|
<a href=/people/p/pavan-kapanipathi/>Pavan Kapanipathi</a>
|
<a href=/people/s/salim-roukos/>Salim Roukos</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a>
|
<a href=/people/a/alexander-gray/>Alexander Gray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--34><div class="card-body p-3 small">Relation linking is a crucial component of Knowledge Base Question Answering systems. Existing <a href=https://en.wikipedia.org/wiki/System>systems</a> use a wide variety of <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>, or ensembles of multiple systems, heavily relying on the surface question text. However, the explicit semantic parse of the question is a rich source of relation information that is not taken advantage of. We propose a simple transformer-based neural model for relation linking that leverages the AMR semantic parse of a sentence. Our <a href=https://en.wikipedia.org/wiki/System>system</a> significantly outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on 4 popular <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. These are based on either <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a> or <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a>, demonstrating that our approach is effective across KGs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.35/>Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation</a></strong><br><a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/n/ning-jin/>Ning Jin</a>
|
<a href=/people/k/kuo-lin/>Kuo Lin</a>
|
<a href=/people/m/mandy-guo/>Mandy Guo</a>
|
<a href=/people/d/daniel-cer/>Daniel Cer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--35><div class="card-body p-3 small">Early fusion models with cross-attention have shown better-than-human performance on some question answer benchmarks, while it is a poor fit for retrieval since it prevents pre-computation of the answer representations. We present a supervised data mining method using an accurate early fusion model to improve the training of an efficient late fusion retrieval model. We first train an accurate <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a> with cross-attention between questions and answers. The cross-attention model is then used to annotate additional passages in order to generate weighted training examples for a neural retrieval model. The resulting retrieval model with additional data significantly outperforms retrieval models directly trained with gold annotations on Precision at N (P@N) and Mean Reciprocal Rank (MRR).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.36" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.36/>Enhancing Descriptive Image Captioning with Natural Language Inference</a></strong><br><a href=/people/z/zhan-shi/>Zhan Shi</a>
|
<a href=/people/h/hui-liu/>Hui Liu</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--36><div class="card-body p-3 small">Generating descriptive sentences that convey non-trivial, detailed, and salient information about <a href=https://en.wikipedia.org/wiki/Image>images</a> is an important goal of image captioning. In this paper we propose a novel approach to encourage captioning models to produce more detailed captions using natural language inference, based on the motivation that, among different captions of an image, descriptive captions are more likely to entail less descriptive captions. Specifically, we construct directed inference graphs for <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>reference captions</a> based on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>. A <a href=https://en.wikipedia.org/wiki/PageRank>PageRank algorithm</a> is then employed to estimate the descriptiveness score of each <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>node</a>. Built on that, we use reference sampling and weighted designated rewards to guide <a href=https://en.wikipedia.org/wiki/Closed_captioning>captioning</a> to generate descriptive captions. The results on MSCOCO show that the proposed method outperforms the baselines significantly on a wide range of conventional and descriptiveness-related evaluation metrics.<i>descriptive</i> sentences that convey non-trivial, detailed, and salient information about images is an important goal of image captioning. In this paper we propose a novel approach to encourage captioning models to produce more detailed captions using natural language inference, based on the motivation that, among different captions of an image, descriptive captions are more likely to entail less descriptive captions. Specifically, we construct directed inference graphs for reference captions based on natural language inference. A PageRank algorithm is then employed to estimate the descriptiveness score of each node. Built on that, we use reference sampling and weighted designated rewards to guide captioning to generate descriptive captions. The results on MSCOCO show that the proposed method outperforms the baselines significantly on a wide range of conventional and descriptiveness-related evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.39" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.39/>On Positivity Bias in Negative Reviews</a></strong><br><a href=/people/m/madhusudhan-aithal/>Madhusudhan Aithal</a>
|
<a href=/people/c/chenhao-tan/>Chenhao Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--39><div class="card-body p-3 small">Prior work has revealed that positive words occur more frequently than negative words in human expressions, which is typically attributed to positivity bias, a tendency for people to report positive views of reality. But what about the language used in negative reviews? Consistent with prior work, we show that English negative reviews tend to contain more positive words than negative words, using a variety of datasets. We reconcile this observation with prior findings on the pragmatics of negation, and show that negations are commonly associated with positive words in negative reviews. Furthermore, in negative reviews, the majority of sentences with positive words express negative opinions based on sentiment classifiers, indicating some form of <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.40/>PRAL : A Tailored Pre-Training Model for Task-Oriented Dialog Generation<span class=acl-fixed-case>PRAL</span>: A Tailored Pre-Training Model for Task-Oriented Dialog Generation</a></strong><br><a href=/people/j/jing-gu/>Jing Gu</a>
|
<a href=/people/q/qingyang-wu/>Qingyang Wu</a>
|
<a href=/people/c/chongruo-wu/>Chongruo Wu</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--40><div class="card-body p-3 small">Large pre-trained language generation models such as GPT-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks. However, the performance of pre-trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> on task-oriented dialog tasks is still under-explored. We propose a Pre-trainedRole Alternating Language model (PRAL), explicitly designed for task-oriented conversational systems. We design several techniques : start position randomization, knowledge distillation, and history discount to improve pre-training performance. In addition, we introduce a high-quality large-scale task-oriented dialog pre-training dataset by post-prossessing13 dialog datasets. We effectively adapt PRALon three downstream tasks. The results show that PRAL outperforms or is on par with state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.41/>ROPE : Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction<span class=acl-fixed-case>ROPE</span>: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction</a></strong><br><a href=/people/c/chen-yu-lee/>Chen-Yu Lee</a>
|
<a href=/people/c/chun-liang-li/>Chun-Liang Li</a>
|
<a href=/people/c/chu-wang/>Chu Wang</a>
|
<a href=/people/r/renshen-wang/>Renshen Wang</a>
|
<a href=/people/y/yasuhisa-fujii/>Yasuhisa Fujii</a>
|
<a href=/people/s/siyang-qin/>Siyang Qin</a>
|
<a href=/people/a/ashok-popat/>Ashok Popat</a>
|
<a href=/people/t/tomas-pfister/>Tomas Pfister</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--41><div class="card-body p-3 small">Natural reading orders of words are crucial for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> from form-like documents. Despite recent advances in Graph Convolutional Networks (GCNs) on modeling spatial layout patterns of documents, they have limited ability to capture reading orders of given word-level node representations in a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. We propose Reading Order Equivariant Positional Encoding (ROPE), a new positional encoding technique designed to apprehend the sequential presentation of words in documents. ROPE generates unique reading order codes for neighboring words relative to the target word given a word-level graph connectivity. We study two fundamental document entity extraction tasks including word labeling and word grouping on the public FUNSD dataset and a large-scale payment dataset. We show that ROPE consistently improves existing GCNs with a margin up to 8.4 % <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.44.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-short.44/>Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing</a></strong><br><a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--44><div class="card-body p-3 small">Extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the U.S. federal minimum wage. Meanwhile, research on collecting high quality annotations suggests using a qualification that requires workers to have previously completed a certain number of tasks. If most requesters who pay fairly require workers to have completed a large number of tasks already then workers need to complete a substantial amount of poorly paid work before they can earn a fair wage. Through analysis of worker discussions and guidance for researchers, we estimate that workers spend approximately 2.25 months of full time effort on poorly paid tasks in order to get the <a href=https://en.wikipedia.org/wiki/Professional_certification>qualifications</a> needed for better paid tasks. We discuss alternatives to this qualification and conduct a study of the correlation between <a href=https://en.wikipedia.org/wiki/Professional_certification>qualifications</a> and work quality on two NLP tasks. We find that it is possible to reduce the burden on workers while still collecting <a href=https://en.wikipedia.org/wiki/Data_quality>high quality data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.45.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.45" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.45/>Men Are Elected, Women Are Married : Events Gender Bias on Wikipedia<span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/j/jiao-sun/>Jiao Sun</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--45><div class="card-body p-3 small">Human activities can be seen as sequences of events, which are crucial to understanding societies. Disproportional event distribution for different demographic groups can manifest and amplify <a href=https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States>social stereotypes</a>, and potentially jeopardize the ability of members in some groups to pursue certain goals. In this paper, we present the first event-centric study of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender biases</a> in a <a href=https://en.wikipedia.org/wiki/Web_of_Science>Wikipedia corpus</a>. To facilitate the study, we curate a corpus of career and personal life descriptions with demographic information consisting of 7,854 fragments from 10,412 celebrities. Then we detect <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> with a state-of-the-art event detection model, calibrate the results using strategically generated templates, and extract <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> that have asymmetric associations with genders. Our study discovers that the Wikipedia pages tend to intermingle personal life events with professional events for females but not for males, which calls for the awareness of the Wikipedia community to formalize guidelines and train the editors to mind the implicit biases that contributors carry. Our work also lays the foundation for future works on quantifying and discovering event biases at the corpus level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.46/>Modeling Task-Aware MIMO Cardinality for Efficient Multilingual Neural Machine Translation<span class=acl-fixed-case>MIMO</span> Cardinality for Efficient Multilingual Neural Machine Translation</a></strong><br><a href=/people/h/hongfei-xu/>Hongfei Xu</a>
|
<a href=/people/q/qiuhui-liu/>Qiuhui Liu</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--46><div class="card-body p-3 small">Neural machine translation has achieved great success in <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual settings</a>, as well as in <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual settings</a>. With the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying <a href=https://en.wikipedia.org/wiki/Linguistic_typology>typological characteristics</a>. Previous work increases the modeling capacity by deepening or widening the <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a>. However, modeling cardinality based on aggregating a set of transformations with the same <a href=https://en.wikipedia.org/wiki/Topological_space>topology</a> has been proven more effective than going deeper or wider when increasing <a href=https://en.wikipedia.org/wiki/Capacity_of_a_set>capacity</a>. In this paper, we propose to efficiently increase the capacity for multilingual NMT by increasing the <a href=https://en.wikipedia.org/wiki/Cardinality>cardinality</a>. Unlike previous work which feeds the same input to several transformations and merges their outputs into one, we present a Multi-Input-Multi-Output (MIMO) architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> surpasses previous work and establishes a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on the large scale OPUS-100 corpus while being 1.31 times as fast.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.56.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.56" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.56/>Exploration and Exploitation : Two Ways to Improve Chinese Spelling Correction Models<span class=acl-fixed-case>C</span>hinese Spelling Correction Models</a></strong><br><a href=/people/c/chong-li/>Chong Li</a>
|
<a href=/people/c/cenyuan-zhang/>Cenyuan Zhang</a>
|
<a href=/people/x/xiaoqing-zheng/>Xiaoqing Zheng</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--56><div class="card-body p-3 small">A sequence-to-sequence learning with <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> has empirically proven to be an effective framework for Chinese Spelling Correction (CSC), which takes a sentence with some spelling errors as input and outputs the corrected one. However, CSC models may fail to correct spelling errors covered by the confusion sets, and also will encounter unseen ones. We propose a method, which continually identifies the weak spots of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to generate more valuable training instances, and apply a task-specific pre-training strategy to enhance the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. The generated adversarial examples are gradually added to the training set. Experimental results show that such an adversarial training method combined with the pre-training strategy can improve both the <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of multiple CSC models across three different datasets, achieving state-of-the-art performance for CSC task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.58/>An Empirical Study on Adversarial Attack on NMT : Languages and Positions Matter<span class=acl-fixed-case>NMT</span>: Languages and Positions Matter</a></strong><br><a href=/people/z/zhiyuan-zeng/>Zhiyuan Zeng</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--58><div class="card-body p-3 small">In this paper, we empirically investigate adversarial attack on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> from two aspects : languages (the source vs. the target language) and positions (front vs. rear). For autoregressive NMT models that generate target words from left to right, we observe that adversarial attack on the source language is more effective than on the target language, and that attacking front positions of target sentences or positions of source sentences aligned to the front positions of corresponding target sentences is more effective than attacking other positions. We further exploit the attention distribution of the victim model to attack source sentences at positions that have a strong association with front target words. Experiment results demonstrate that our attention-based adversarial attack is more effective than adversarial attacks by sampling positions randomly or according to gradients.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.59" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.59/>OntoGUM : Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres<span class=acl-fixed-case>O</span>nto<span class=acl-fixed-case>GUM</span>: Evaluating Contextualized <span class=acl-fixed-case>SOTA</span> Coreference Resolution on 12 More Genres</a></strong><br><a href=/people/y/yilun-zhu/>Yilun Zhu</a>
|
<a href=/people/s/sameer-pradhan/>Sameer Pradhan</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--59><div class="card-body p-3 small">SOTA coreference resolution produces increasingly impressive scores on the OntoNotes benchmark. However lack of comparable data following the same <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a> for more genres makes it difficult to evaluate generalizability to <a href=https://en.wikipedia.org/wiki/Open_domain>open domain data</a>. This paper provides a dataset and comprehensive evaluation showing that the latest neural LM based end-to-end systems degrade very substantially out of domain. We make an OntoNotes-like coreference dataset called OntoGUM publicly available, converted from GUM, an English corpus covering 12 genres, using deterministic rules, which we evaluate. Thanks to the rich syntactic and discourse annotations in GUM, we are able to create the largest human-annotated coreference corpus following the OntoNotes guidelines, and the first to be evaluated for consistency with the OntoNotes scheme. Out-of-domain evaluation across 12 genres shows nearly 15-20 % degradation for both deterministic and deep learning systems, indicating a lack of generalizability or covert <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> in existing coreference resolution models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.61" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.61/>Zero-shot Fact Verification by Claim Generation</a></strong><br><a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--61><div class="card-body p-3 small">Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires <a href=https://en.wikipedia.org/wiki/Fact-checking>fact verification</a>, creating a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model&#8217;s F1 from 50 % to 77 %, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.62" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.62/>Thank you BART ! Rewarding Pre-Trained Models Improves Formality Style Transfer<span class=acl-fixed-case>BART</span>! Rewarding Pre-Trained Models Improves Formality Style Transfer</a></strong><br><a href=/people/h/huiyuan-lai/>Huiyuan Lai</a>
|
<a href=/people/a/antonio-toral/>Antonio Toral</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--62><div class="card-body p-3 small">Scarcity of parallel data causes formality style transfer models to have scarce success in <a href=https://en.wikipedia.org/wiki/Data_preservation>preserving content</a>. We show that fine-tuning pre-trained language (GPT-2) and sequence-to-sequence (BART) models boosts content preservation, and that this is possible even with limited amounts of parallel data. Augmenting these models with rewards that target style and content the two core aspects of the task we achieve a new state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.63/>Deep Context- and Relation-Aware Learning for Aspect-based Sentiment Analysis</a></strong><br><a href=/people/s/shinhyeok-oh/>Shinhyeok Oh</a>
|
<a href=/people/d/dongyub-lee/>Dongyub Lee</a>
|
<a href=/people/t/taesun-whang/>Taesun Whang</a>
|
<a href=/people/i/ilnam-park/>IlNam Park</a>
|
<a href=/people/s/seo-gaeun/>Seo Gaeun</a>
|
<a href=/people/e/eunggyun-kim/>EungGyun Kim</a>
|
<a href=/people/h/harksoo-kim/>Harksoo Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--63><div class="card-body p-3 small">Existing works for aspect-based sentiment analysis (ABSA) have adopted a unified approach, which allows the interactive relations among subtasks. However, we observe that these methods tend to predict polarities based on the literal meaning of aspect and opinion terms and mainly consider relations implicitly among subtasks at the word level. In addition, identifying multiple aspectopinion pairs with their polarities is much more challenging. Therefore, a comprehensive understanding of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> w.r.t. the aspect and opinion are further required in ABSA. In this paper, we propose Deep Contextualized Relation-Aware Network (DCRAN), which allows interactive relations among subtasks with deep contextual information based on two modules (i.e., Aspect and Opinion Propagation and Explicit Self-Supervised Strategies). Especially, we design novel self-supervised strategies for ABSA, which have strengths in dealing with multiple aspects. Experimental results show that DCRAN significantly outperforms previous state-of-the-art methods by large margins on three widely used benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.67/>TIMERS : Document-level Temporal Relation Extraction<span class=acl-fixed-case>TIMERS</span>: Document-level Temporal Relation Extraction</a></strong><br><a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/r/rajiv-jain/>Rajiv Jain</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/v/vlad-morariu/>Vlad Morariu</a>
|
<a href=/people/q/quan-hung-tran/>Quan Hung Tran</a>
|
<a href=/people/d/dinesh-manocha/>Dinesh Manocha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--67><div class="card-body p-3 small">We present TIMERS-a TIME, Rhetorical and Syntactic-aware model for document-level temporal relation classification in the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. Our proposed method leverages rhetorical discourse features and temporal arguments from semantic role labels, in addition to traditional local syntactic features, trained through a Gated Relational-GCN. Extensive experiments show that the proposed model outperforms previous methods by 5-18 % on the TDDiscourse, TimeBank-Dense, and MATRES datasets due to our discourse-level modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.70" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.70/>More than Text : Multi-modal Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/d/dong-zhang/>Dong Zhang</a>
|
<a href=/people/z/zheng-hu/>Zheng Hu</a>
|
<a href=/people/s/shoushan-li/>Shoushan Li</a>
|
<a href=/people/h/hanqian-wu/>Hanqian Wu</a>
|
<a href=/people/q/qiaoming-zhu/>Qiaoming Zhu</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--70><div class="card-body p-3 small">Chinese word segmentation (CWS) is undoubtedly an important basic task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Previous works only focus on the textual modality, but there are often audio and video utterances (such as news broadcast and face-to-face dialogues), where textual, acoustic and visual modalities normally exist. To this end, we attempt to combine the multi-modality (mainly the converted text and actual voice information) to perform CWS. In this paper, we annotate a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for <a href=https://en.wikipedia.org/wiki/Content_Scramble_System>CWS</a> containing text and audio. Moreover, we propose a time-dependent multi-modal interactive model based on Transformer framework to integrate multi-modal information for word sequence labeling. The experimental results on three different training sets show the effectiveness of our approach with fusing text and audio.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.71.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.71" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.71/>A Mixture-of-Experts Model for Antonym-Synonym Discrimination</a></strong><br><a href=/people/z/zhipeng-xie/>Zhipeng Xie</a>
|
<a href=/people/n/nan-zeng/>Nan Zeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--71><div class="card-body p-3 small">Discrimination between <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> and <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> is an important and challenging NLP task. Antonyms and <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> often share the same or similar contexts and thus are hard to make a distinction. This paper proposes two underlying hypotheses and employs the mixture-of-experts framework as a solution. It works on the basis of a divide-and-conquer strategy, where a number of localized experts focus on their own domains (or subspaces) to learn their specialties, and a gating mechanism determines the space partitioning and the expert mixture. Experimental results have shown that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves the state-of-the-art performance on the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.72.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--72 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.72 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.72" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.72/>Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking</a></strong><br><a href=/people/f/fangyu-liu/>Fangyu Liu</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--72><div class="card-body p-3 small">Injecting external domain-specific knowledge (e.g., UMLS) into pretrained language models (LMs) advances their capability to handle specialised in-domain tasks such as biomedical entity linking (BEL). However, such abundant expert knowledge is available only for a handful of languages (e.g., English). In this work, by proposing a novel cross-lingual biomedical entity linking task (XL-BEL) and establishing a new XL-BEL benchmark spanning 10 typologically diverse languages, we first investigate the ability of standard knowledge-agnostic as well as knowledge-enhanced monolingual and multilingual LMs beyond the standard monolingual English BEL task. The scores indicate large gaps to English performance. We then address the challenge of transferring domain-specific knowledge in resource-rich languages to resource-poor ones. To this end, we propose and evaluate a series of cross-lingual transfer methods for the XL-BEL task, and demonstrate that general-domain bitext helps propagate the available English knowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.73" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.73/>A Cluster-based Approach for Improving <a href=https://en.wikipedia.org/wiki/Isotropy>Isotropy</a> in Contextual Embedding Space</a></strong><br><a href=/people/s/sara-rajaee/>Sara Rajaee</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--73><div class="card-body p-3 small">The representation degeneration problem in Contextual Word Representations (CWRs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study <a href=https://en.wikipedia.org/wiki/Isotropy>isotropy</a>. Our quantitative analysis over <a href=https://en.wikipedia.org/wiki/Isotropy>isotropy</a> shows that a local assessment could be more accurate due to the clustered structure of CWRs. Based on this observation, we propose a local cluster-based method to address the <a href=https://en.wikipedia.org/wiki/Degeneracy_(mathematics)>degeneration issue</a> in contextual embedding spaces. We show that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve CWRs performance on semantic tasks. Moreover, we find that tense information in <a href=https://en.wikipedia.org/wiki/Grammatical_tense>verb representations</a> dominates sense semantics. We show that removing dominant directions of verb representations can transform the <a href=https://en.wikipedia.org/wiki/Semantic_space>space</a> to better suit semantic applications. Our experiments demonstrate that the proposed cluster-based method can mitigate the <a href=https://en.wikipedia.org/wiki/Degeneracy_(mathematics)>degeneration problem</a> on multiple tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.76/>Do n’t Let Discourse Confine Your Model : Sequence Perturbations for Improved Event Language Models</a></strong><br><a href=/people/m/mahnaz-koupaee/>Mahnaz Koupaee</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--76><div class="card-body p-3 small">Event language models represent plausible sequences of events. Most existing approaches train <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a> on text, which successfully capture <a href=https://en.wikipedia.org/wiki/Co-occurrence>event co-occurrence</a> but unfortunately constrain the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to follow the <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse order</a> in which events are presented. Other domains may employ different discourse orders, and for many applications, we may care about different notions of <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>ordering</a> (e.g., temporal) or not care about <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>ordering</a> at all (e.g., when predicting related events in a schema). We propose a simple yet surprisingly effective strategy for improving event language models by perturbing event sequences so we can relax model dependence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> and out-of-domain events data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.77/>The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes</a></strong><br><a href=/people/n/nils-reimers/>Nils Reimers</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--77><div class="card-body p-3 small">Information Retrieval using dense low-dimensional representations recently became popular and showed out-performance to traditional sparse-representations like <a href=https://en.wikipedia.org/wiki/BM25>BM25</a>. However, no previous work investigated how dense representations perform with large index sizes. We show theoretically and empirically that the performance for <a href=https://en.wikipedia.org/wiki/Sparse_matrix>dense representations</a> decreases quicker than <a href=https://en.wikipedia.org/wiki/Sparse_matrix>sparse representations</a> for increasing index sizes. In extreme cases, this can even lead to a tipping point where at a certain index size <a href=https://en.wikipedia.org/wiki/Sparse_matrix>sparse representations</a> outperform <a href=https://en.wikipedia.org/wiki/Dense_matrix>dense representations</a>. We show that this behavior is tightly connected to the number of dimensions of the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> : The lower the <a href=https://en.wikipedia.org/wiki/Dimension>dimension</a>, the higher the chance for <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positives</a>, i.e. returning irrelevant documents</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.78.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--78 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.78 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.78" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.78/>Cross-lingual Text Classification with Heterogeneous Graph Neural Network</a></strong><br><a href=/people/z/ziyun-wang/>Ziyun Wang</a>
|
<a href=/people/x/xuan-liu/>Xuan Liu</a>
|
<a href=/people/p/peiji-yang/>Peiji Yang</a>
|
<a href=/people/s/shixing-liu/>Shixing Liu</a>
|
<a href=/people/z/zhisheng-wang/>Zhisheng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--78><div class="card-body p-3 small">Cross-lingual text classification aims at training a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> on the source language and transferring the knowledge to target languages, which is very useful for low-resource languages. Recent multilingual pretrained language models (mPLM) achieve impressive results in cross-lingual classification tasks, but rarely consider factors beyond semantic similarity, causing performance degradation between some language pairs. In this paper we propose a simple yet effective method to incorporate heterogeneous information within and across languages for cross-lingual text classification using graph convolutional networks (GCN). In particular, we construct a heterogeneous graph by treating documents and words as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a>, and linking <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> with different relations, which include part-of-speech roles, <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>, and document translations. Extensive experiments show that our graph-based method significantly outperforms state-of-the-art models on all tasks, and also achieves consistent performance gain over baselines in low-resource settings where external tools like translators are unavailable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.80/>Embedding Time Differences in Context-sensitive Neural Networks for Learning Time to Event</a></strong><br><a href=/people/n/nazanin-dehghani/>Nazanin Dehghani</a>
|
<a href=/people/h/hassan-hajipoor/>Hassan Hajipoor</a>
|
<a href=/people/h/hadi-amiri/>Hadi Amiri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--80><div class="card-body p-3 small">We propose an effective context-sensitive neural model for time to event (TTE) prediction task, which aims to predict the amount of time to / from the occurrence of given events in <a href=https://en.wikipedia.org/wiki/Streaming_media>streaming content</a>. We investigate this problem in the context of a multi-task learning framework, which we enrich with time difference embeddings. In addition, we develop a multi-genre dataset of English events about <a href=https://en.wikipedia.org/wiki/Association_football>soccer competitions</a> and academy awards ceremonies, and their relevant tweets obtained from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is 1.4 and 3.3 hours more accurate than the current state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in estimating <a href=https://en.wikipedia.org/wiki/Time_to_live>TTE</a> on <a href=https://en.wikipedia.org/wiki/Twitter>English and Dutch tweets</a> respectively. We examine different aspects of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to illustrate its source of improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.84/>Domain-Adaptive Pretraining Methods for Dialogue Understanding</a></strong><br><a href=/people/h/han-wu/>Han Wu</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/l/lifeng-jin/>Lifeng Jin</a>
|
<a href=/people/h/haisong-zhang/>Haisong Zhang</a>
|
<a href=/people/l/linqi-song/>Linqi Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--84><div class="card-body p-3 small">Language models like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and SpanBERT pretrained on open-domain data have obtained impressive gains on various NLP tasks. In this paper, we probe the effectiveness of domain-adaptive pretraining objectives on downstream tasks. In particular, three objectives, including a novel objective focusing on modeling predicate-argument relations, are evaluated on two challenging dialogue understanding tasks. Experimental results demonstrate that domain-adaptive pretraining with proper objectives can significantly improve the performance of a strong baseline on these tasks, achieving the new state-of-the-art performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.85/>Targeting the Benchmark : On Methodology in Current Natural Language Processing Research</a></strong><br><a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--85><div class="card-body p-3 small">It has become a common pattern in our field : One group introduces a language task, exemplified by a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, which they argue is challenging enough to serve as a benchmark. They also provide a <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline model</a> for it, which then soon is improved upon by other groups. Often, research efforts then move on, and the <a href=https://en.wikipedia.org/wiki/Pattern>pattern</a> repeats itself. What is typically left implicit is the argumentation for why this constitutes progress, and progress towards what. In this paper, we try to step back for a moment from this pattern and work out possible argumentations and their parts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.87/>nmT5-Is <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> still relevant for pre-training massively multilingual language models?<span class=acl-fixed-case>T</span>5 - Is parallel data still relevant for pre-training massively multilingual language models?</a></strong><br><a href=/people/m/mihir-kale/>Mihir Kale</a>
|
<a href=/people/a/aditya-siddhant/>Aditya Siddhant</a>
|
<a href=/people/r/rami-al-rfou/>Rami Al-Rfou</a>
|
<a href=/people/l/linting-xue/>Linting Xue</a>
|
<a href=/people/n/noah-constant/>Noah Constant</a>
|
<a href=/people/m/melvin-johnson/>Melvin Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--87><div class="card-body p-3 small">Recently, mT5-a massively multilingual version of T5-leveraged a unified text-to-text format to attain state-of-the-art results on a wide variety of multilingual NLP tasks. In this paper, we investigate the impact of incorporating parallel data into mT5 pre-training. We find that multi-tasking language modeling with objectives such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> during pre-training is a straightforward way to improve performance on downstream multilingual and cross-lingual tasks. However, the gains start to diminish as the model capacity increases, suggesting that parallel data might not be as essential for larger <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. At the same time, even at larger model sizes, we find that pre-training with parallel data still provides benefits in the limited labelled data regime</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.93" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.93/>Explicitly Capturing Relations between Entity Mentions via Graph Neural Networks for Domain-specific Named Entity Recognition</a></strong><br><a href=/people/p/pei-chen/>Pei Chen</a>
|
<a href=/people/h/haibo-ding/>Haibo Ding</a>
|
<a href=/people/j/jun-araki/>Jun Araki</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--93><div class="card-body p-3 small">Named entity recognition (NER) is well studied for the general domain, and recent systems have achieved human-level performance for identifying common entity types. However, the NER performance is still moderate for <a href=https://en.wikipedia.org/wiki/Domain-specific_language>specialized domains</a> that tend to feature complicated contexts and jargonistic entity types. To address these challenges, we propose explicitly connecting entity mentions based on both global coreference relations and local dependency relations for building better entity mention representations. In our experiments, we incorporate <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity mention relations</a> by Graph Neural Networks and show that our system noticeably improves the NER performance on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from different domains. We further show that the proposed lightweight system can effectively elevate the <a href=https://en.wikipedia.org/wiki/Network_topology>NER</a> performance to a higher level even when only a tiny amount of labeled data is available, which is desirable for domain-specific NER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.94" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.94/>Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction</a></strong><br><a href=/people/g/gyubok-lee/>Gyubok Lee</a>
|
<a href=/people/s/seongjun-yang/>Seongjun Yang</a>
|
<a href=/people/e/edward-choi/>Edward Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--94><div class="card-body p-3 small">Accurate terminology translation is crucial for ensuring the practicality and reliability of neural machine translation (NMT) systems. To address this, lexically constrained NMT explores various methods to ensure pre-specified words and phrases appear in the translation output. However, in many cases, those methods are studied on general domain corpora, where the terms are mostly uni- and bi-grams (98 %). In this paper, we instead tackle a more challenging setup consisting of domain-specific corpora with much longer <a href=https://en.wikipedia.org/wiki/N-gram>n-gram</a> and highly specialized terms. Inspired by the recent success of masked span prediction models, we propose a simple and effective training strategy that achieves consistent improvements on both terminology and sentence-level translation for three domain-specific corpora in two language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.95" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.95/>Quotation Recommendation and Interpretation Based on Transformation from Queries to Quotations</a></strong><br><a href=/people/l/lingzhi-wang/>Lingzhi Wang</a>
|
<a href=/people/x/xingshan-zeng/>Xingshan Zeng</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--95><div class="card-body p-3 small">To help individuals express themselves better, quotation recommendation is receiving growing attention. Nevertheless, most prior efforts focus on modeling <a href=https://en.wikipedia.org/wiki/Quotation>quotations</a> and queries separately and ignore the relationship between the <a href=https://en.wikipedia.org/wiki/Quotation>quotations</a> and the queries. In this work, we introduce a <a href=https://en.wikipedia.org/wiki/Transformation_matrix>transformation matrix</a> that directly maps the query representations to quotation representations. To better learn the mapping relationship, we employ a mapping loss that minimizes the distance of two semantic spaces (one for <a href=https://en.wikipedia.org/wiki/Quotation>quotation</a> and another for mapped-query). Furthermore, we explore using the words in history queries to interpret the figurative language of quotations, where quotation-aware attention is applied on top of history queries to highlight the indicator words. Experiments on two datasets in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--100 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.100/>Neural-Symbolic Commonsense Reasoner with Relation Predictors</a></strong><br><a href=/people/f/farhad-moghimifar/>Farhad Moghimifar</a>
|
<a href=/people/l/lizhen-qu/>Lizhen Qu</a>
|
<a href=/people/t/terry-yue-zhuo/>Terry Yue Zhuo</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/m/mahsa-baktashmotlagh/>Mahsa Baktashmotlagh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--100><div class="card-body p-3 small">Commonsense reasoning aims to incorporate sets of commonsense facts, retrieved from Commonsense Knowledge Graphs (CKG), to draw conclusion about ordinary situations. The dynamic nature of <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> postulates <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> capable of performing multi-hop reasoning over new situations. This feature also results in having large-scale sparse Knowledge Graphs, where such reasoning process is needed to predict relations between new events. However, existing approaches in this area are limited by considering CKGs as a limited set of facts, thus rendering them unfit for reasoning over new unseen situations and events. In this paper, we present a neural-symbolic reasoner, which is capable of reasoning over large-scale dynamic CKGs. The <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> for reasoning over CKGs are learned during training by our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. In addition to providing interpretable explanation, the learned <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> help to generalise <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> to newly introduced events. Experimental results on the task of link prediction on CKGs prove the effectiveness of our model by outperforming the state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.104/>Parameter Selection : Why We Should Pay More Attention to It</a></strong><br><a href=/people/j/jie-jyun-liu/>Jie-Jyun Liu</a>
|
<a href=/people/t/tsung-han-yang/>Tsung-Han Yang</a>
|
<a href=/people/s/si-an-chen/>Si-An Chen</a>
|
<a href=/people/c/chih-jen-lin/>Chih-Jen Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--104><div class="card-body p-3 small">The importance of parameter selection in <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> is well known. However, due to the many parameter combinations, an incomplete or an insufficient procedure is often applied. This situation may cause misleading or confusing conclusions. In this opinion paper, through an intriguing example we point out that the seriousness goes beyond what is generally recognized. In the topic of multilabel classification for medical code prediction, one influential paper conducted a proper parameter selection on a set, but when moving to a subset of frequently occurring labels, the authors used the same parameters without a separate tuning. The set of frequent labels became a popular benchmark in subsequent studies, which kept pushing the state of the art. However, we discovered that most of the results in these studies can not surpass the approach in the original paper if a parameter tuning had been conducted at the time. Thus it is unclear how much progress the subsequent developments have actually brought. The lesson clearly indicates that without enough attention on parameter selection, the research progress in our field can be uncertain or even illusive.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.110" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.110/>Modeling Discriminative Representations for Out-of-Domain Detection with Supervised Contrastive Learning</a></strong><br><a href=/people/z/zhiyuan-zeng/>Zhiyuan Zeng</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/z/zijun-liu/>Zijun Liu</a>
|
<a href=/people/y/yanan-wu/>Yanan Wu</a>
|
<a href=/people/h/hong-xu/>Hong Xu</a>
|
<a href=/people/h/huixing-jiang/>Huixing Jiang</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--110><div class="card-body p-3 small">Detecting Out-of-Domain (OOD) or unknown intents from user queries is essential in a task-oriented dialog system. A key challenge of OOD detection is to learn discriminative semantic features. Traditional cross-entropy loss only focuses on whether a sample is correctly classified, and does not explicitly distinguish the margins between categories. In this paper, we propose a supervised contrastive learning objective to minimize intra-class variance by pulling together in-domain intents belonging to the same class and maximize inter-class variance by pushing apart samples from different classes. Besides, we employ an adversarial augmentation mechanism to obtain pseudo diverse views of a sample in the latent space. Experiments on two public datasets prove the effectiveness of our method capturing discriminative representations for OOD detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.114/>Exposing the limits of Zero-shot Cross-lingual Hate Speech Detection</a></strong><br><a href=/people/d/debora-nozza/>Debora Nozza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--114><div class="card-body p-3 small">Reducing and counter-acting hate speech on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> is a significant concern. Most of the proposed automatic methods are conducted exclusively on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and very few consistently labeled, non-English resources have been proposed. Learning to detect <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and transferring to unseen languages seems an immediate solution. This work is the first to shed light on the limits of this zero-shot, cross-lingual transfer learning framework for hate speech detection. We use benchmark data sets in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> to detect <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> towards immigrants and women. Investigating post-hoc explanations of the model, we discover that non-hateful, language-specific taboo interjections are misinterpreted as signals of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>. Our findings demonstrate that zero-shot, cross-lingual models can not be used as they are, but need to be carefully designed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.118/>Addressing Semantic Drift in Generative Question Answering with Auxiliary Extraction</a></strong><br><a href=/people/c/chenliang-li/>Chenliang Li</a>
|
<a href=/people/b/bin-bi/>Bin Bi</a>
|
<a href=/people/m/ming-yan/>Ming Yan</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/s/songfang-huang/>Songfang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--118><div class="card-body p-3 small">Recently, question answering (QA) based on machine reading comprehension has become popular. This work focuses on generative QA which aims to generate an abstractive answer to a given question instead of extracting an answer span from a provided passage. Generative QA often suffers from two critical problems : (1) summarizing content irrelevant to a given question, (2) drifting away from a correct answer during generation. In this paper, we address these problems by a novel Rationale-Enriched Answer Generator (REAG), which incorporates an extractive mechanism into a generative model. Specifically, we add an extraction task on the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to obtain the rationale for an answer, which is the most relevant piece of text in an input document to a given question. Based on the extracted rationale and original input, the decoder is expected to generate an answer with high confidence. We jointly train REAG on the MS MARCO QA+NLG task and the experimental results show that REAG improves the quality and semantic accuracy of answers over baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.125/>Improving Model Generalization : A Chinese Named Entity Recognition Case Study<span class=acl-fixed-case>C</span>hinese Named Entity Recognition Case Study</a></strong><br><a href=/people/g/guanqing-liang/>Guanqing Liang</a>
|
<a href=/people/c/cane-wing-ki-leung/>Cane Wing-Ki Leung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--125><div class="card-body p-3 small">Generalization is an important ability that helps to ensure that a machine learning model can perform well on unseen data. In this paper, we study the effect of data bias on model generalization, using Chinese Named Entity Recognition (NER) as a case study. Specifically, we analyzed five benchmarking datasets for Chinese NER, and observed the following two types of data bias that can compromise model generalization ability. Firstly, the test sets of all the five <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> contain a significant proportion of entities that have been seen in the training sets. Such test data would therefore not be able to reflect the true generalization ability of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Secondly, all <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are dominated by a few fat-head entities, i.e., entities appearing with particularly high frequency. As a result, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> might be able to produce high prediction accuracy simply by keyword memorization without leveraging <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context knowledge</a>. To address these data biases, we first refine each test set by excluding seen entities from it, so as to better evaluate a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s generalization ability. Then, we propose a simple yet effective entity resampling method to make entities within the same category distributed equally, encouraging a model to leverage both name and context knowledge in the training process. Experimental results demonstrate that the proposed entity resampling method significantly improves a model&#8217;s ability in detecting unseen entities, especially for company, organization and position categories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.128.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-short.128/>Attentive Multiview Text Representation for <a href=https://en.wikipedia.org/wiki/Differential_diagnosis>Differential Diagnosis</a></a></strong><br><a href=/people/h/hadi-amiri/>Hadi Amiri</a>
|
<a href=/people/m/mitra-mohtarami/>Mitra Mohtarami</a>
|
<a href=/people/i/isaac-kohane/>Isaac Kohane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--128><div class="card-body p-3 small">We present a text representation approach that can combine different views (representations) of the same input through effective <a href=https://en.wikipedia.org/wiki/Data_fusion>data fusion</a> and attention strategies for ranking purposes. We apply our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to the problem of <a href=https://en.wikipedia.org/wiki/Differential_diagnosis>differential diagnosis</a>, which aims to find the most probable diseases that match with clinical descriptions of patients, using data from the <a href=https://en.wikipedia.org/wiki/Undiagnosed_Diseases_Network>Undiagnosed Diseases Network</a>. Our model outperforms several ranking approaches (including a commercially-supported system) by effectively prioritizing and combining representations obtained from traditional and recent text representation techniques. We elaborate on several aspects of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and shed light on its improved performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.129.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.129" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.129/>MedNLI Is Not Immune : Natural Language Inference Artifacts in the Clinical Domain<span class=acl-fixed-case>M</span>ed<span class=acl-fixed-case>NLI</span> Is Not Immune: <span class=acl-fixed-case>N</span>atural Language Inference Artifacts in the Clinical Domain</a></strong><br><a href=/people/c/christine-herlihy/>Christine Herlihy</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--129><div class="card-body p-3 small">Crowdworker-constructed natural language inference (NLI) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis-only classifiers to achieve better-than-random performance (CITATION). We investigate whether MedNLI, a physician-annotated dataset with premises extracted from clinical notes, contains such artifacts (CITATION). We find that entailed hypotheses contain generic versions of specific concepts in the premise, as well as modifiers related to <a href=https://en.wikipedia.org/wiki/Responsiveness>responsiveness</a>, duration, and <a href=https://en.wikipedia.org/wiki/Probability>probability</a>. Neutral hypotheses feature conditions and behaviors that co-occur with, or cause, the condition(s) in the premise. Contradiction hypotheses feature explicit negation of the premise and implicit negation via assertion of good health. Adversarial filtering demonstrates that performance degrades when evaluated on the difficult subset. We provide <a href=https://en.wikipedia.org/wiki/Partition_of_a_set>partition information</a> and recommendations for alternative dataset construction strategies for knowledge-intensive domains.<i>difficult</i> subset. We provide partition information and recommendations for alternative dataset construction strategies for knowledge-intensive domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.130/>Towards a more Robust Evaluation for Conversational Question Answering</a></strong><br><a href=/people/w/wissam-siblini/>Wissam Siblini</a>
|
<a href=/people/b/baris-sayil/>Baris Sayil</a>
|
<a href=/people/y/yacine-kessaci/>Yacine Kessaci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--130><div class="card-body p-3 small">With the explosion of chatbot applications, Conversational Question Answering (CQA) has generated a lot of interest in recent years. Among proposals, reading comprehension models which take advantage of the conversation history (previous QA) seem to answer better than those which only consider the current question. Nevertheless, we note that the CQA evaluation protocol has a major limitation. In particular, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are allowed, at each turn of the conversation, to access the ground truth answers of the previous turns. Not only does this severely prevent their applications in fully autonomous chatbots, it also leads to unsuspected biases in their behavior. In this paper, we highlight this effect and propose new tools for <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> and <a href=https://en.wikipedia.org/wiki/Training>training</a> in order to guard against the noted issues. The new results that we bring come to reinforce <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> of the current state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.132/>Avoiding Overlap in <a href=https://en.wikipedia.org/wiki/Data_augmentation>Data Augmentation</a> for AMR-to-Text Generation<span class=acl-fixed-case>AMR</span>-to-Text Generation</a></strong><br><a href=/people/w/wenchao-du/>Wenchao Du</a>
|
<a href=/people/j/jeffrey-flanigan/>Jeffrey Flanigan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--132><div class="card-body p-3 small">Leveraging additional unlabeled data to boost model performance is common practice in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. For generation tasks, if there is overlap between the additional data and the target text evaluation data, then training on the additional <a href=https://en.wikipedia.org/wiki/Data>data</a> is training on answers of the test set. This leads to overly-inflated scores with the additional <a href=https://en.wikipedia.org/wiki/Data>data</a> compared to real-world testing scenarios and problems when comparing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We study the AMR dataset and Gigaword, which is popularly used for improving AMR-to-text generators, and find significant overlap between Gigaword and a subset of the AMR dataset. We propose methods for excluding parts of Gigaword to remove this overlap, and show that our approach leads to a more realistic evaluation of the task of AMR-to-text generation. Going forward, we give simple best-practice recommendations for leveraging additional data in AMR-to-text generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.133" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.133/>Weakly-Supervised Methods for <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>Suicide Risk Assessment</a> : Role of Related Domains</a></strong><br><a href=/people/c/chenghao-yang/>Chenghao Yang</a>
|
<a href=/people/y/yudong-zhang/>Yudong Zhang</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--133><div class="card-body p-3 small">Social media has become a valuable resource for the study of <a href=https://en.wikipedia.org/wiki/Suicidal_ideation>suicidal ideation</a> and the <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>assessment of suicide risk</a>. Among social media platforms, <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> has emerged as the most promising one due to its anonymity and its focus on topic-based communities (subreddits) that can be indicative of someone&#8217;s state of mind or interest regarding mental health disorders such as <a href=https://en.wikipedia.org/wiki/Reddit>r / SuicideWatch</a>, <a href=https://en.wikipedia.org/wiki/Reddit>r / Anxiety</a>, <a href=https://en.wikipedia.org/wiki/Reddit>r / depression</a>. A challenge for previous work on <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>suicide risk assessment</a> has been the small amount of labeled data. We propose an empirical investigation into several classes of weakly-supervised approaches, and show that using pseudo-labeling based on related issues around <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a> (e.g., <a href=https://en.wikipedia.org/wiki/Anxiety>anxiety</a>, depression) helps improve model performance for <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>suicide risk assessment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.134/>Can Transformer Models Measure Coherence In Text : Re-Thinking the Shuffle Test</a></strong><br><a href=/people/p/philippe-laban/>Philippe Laban</a>
|
<a href=/people/l/luke-dai/>Luke Dai</a>
|
<a href=/people/l/lucas-bandarkar/>Lucas Bandarkar</a>
|
<a href=/people/m/marti-a-hearst/>Marti A. Hearst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--134><div class="card-body p-3 small">The Shuffle Test is the most common task to evaluate whether <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a> can measure coherence in text. Most recent work uses direct supervision on the task ; we show that by simply finetuning a RoBERTa model, we can achieve a near perfect <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 97.8 %, a state-of-the-art. We argue that this outstanding performance is unlikely to lead to a good model of <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>text coherence</a>, and suggest that the Shuffle Test should be approached in a Zero-Shot setting : <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> should be evaluated without being trained on the task itself. We evaluate common models in this setting, such as Generative and Bi-directional Transformers, and find that larger architectures achieve high-performance out-of-the-box. Finally, we suggest the k-Block Shuffle Test, a modification of the original by increasing the size of blocks shuffled. Even though human reader performance remains high (around 95 % accuracy), model performance drops from 94 % to 78 % as block size increases, creating a conceptually simple challenge to benchmark NLP models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.136/>SaRoCo : Detecting Satire in a Novel Romanian Corpus of News Articles<span class=acl-fixed-case>S</span>a<span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>C</span>o: Detecting Satire in a Novel <span class=acl-fixed-case>R</span>omanian Corpus of News Articles</a></strong><br><a href=/people/a/ana-cristina-rogoz/>Ana-Cristina Rogoz</a>
|
<a href=/people/g/gaman-mihaela/>Gaman Mihaela</a>
|
<a href=/people/r/radu-tudor-ionescu/>Radu Tudor Ionescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--136><div class="card-body p-3 small">In this work, we introduce a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for satire detection in <a href=https://en.wikipedia.org/wiki/Media_of_Romania>Romanian news</a>. We gathered 55,608 public news articles from multiple real and satirical news sources, composing one of the largest <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> for satire detection regardless of language and the only one for the <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian language</a>. We provide an official split of the text samples, such that training news articles belong to different sources than test news articles, thus ensuring that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> do not achieve high performance simply due to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. We conduct experiments with two state-of-the-art deep neural models, resulting in a set of strong baselines for our novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. Our results show that the machine-level accuracy for satire detection in <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a> is quite low (under 73 % on the test set) compared to the human-level accuracy (87 %), leaving enough room for improvement in future research.</div></div></div><hr><div id=2021acl-srw><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.acl-srw/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.0/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop</a></strong><br><a href=/people/j/jad-kabbara/>Jad Kabbara</a>
|
<a href=/people/h/haitao-lin/>Haitao Lin</a>
|
<a href=/people/a/amandalynne-paullada/>Amandalynne Paullada</a>
|
<a href=/people/j/jannis-vamvas/>Jannis Vamvas</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.3/>Transformer-Based Direct Hidden Markov Model for Machine Translation<span class=acl-fixed-case>M</span>arkov Model for Machine Translation</a></strong><br><a href=/people/w/weiyue-wang/>Weiyue Wang</a>
|
<a href=/people/z/zijian-yang/>Zijian Yang</a>
|
<a href=/people/y/yingbo-gao/>Yingbo Gao</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--3><div class="card-body p-3 small">The neural hidden Markov model has been proposed as an alternative to attention mechanism in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. However, since the introduction of the transformer models, its performance has been surpassed. This work proposes to introduce the concept of the <a href=https://en.wikipedia.org/wiki/Hidden_Markov_model>hidden Markov model</a> to the transformer architecture, which outperforms the transformer baseline. Interestingly, we find that the zero-order model already provides promising performance, giving it an edge compared to a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with first-order dependency, which performs similarly but is significantly slower in <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and decoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-srw.5.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-srw.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.5/>How Low is Too Low? A Computational Perspective on Extremely Low-Resource Languages</a></strong><br><a href=/people/r/rachit-bansal/>Rachit Bansal</a>
|
<a href=/people/h/himanshu-choudhary/>Himanshu Choudhary</a>
|
<a href=/people/r/ravneet-punia/>Ravneet Punia</a>
|
<a href=/people/n/niko-schenk/>Niko Schenk</a>
|
<a href=/people/e/emilie-page-perron/>Émilie Pagé-Perron</a>
|
<a href=/people/j/jacob-dahl/>Jacob Dahl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--5><div class="card-body p-3 small">Despite the recent advancements of attention-based deep learning architectures across a majority of Natural Language Processing tasks, their application remains limited in a low-resource setting because of a lack of pre-trained models for such languages. In this study, we make the first attempt to investigate the challenges of adapting these techniques to an extremely low-resource language Sumerian cuneiform one of the world&#8217;s oldest written languages attested from at least the beginning of the 3rd millennium BC. Specifically, we introduce the first cross-lingual information extraction pipeline for <a href=https://en.wikipedia.org/wiki/Sumerian_language>Sumerian</a>, which includes <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We introduce InterpretLR, an interpretability toolkit for low-resource NLP and use it alongside human evaluations to gauge the trained models. Notably, all our techniques and most components of our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> can be generalised to any low-resource language. We publicly release all our implementations including a novel <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> with domain-specific pre-processing to promote further research in this domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-srw.7.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-srw.7/>Long Document Summarization in a Low Resource Setting using Pretrained Language Models</a></strong><br><a href=/people/a/ahsaas-bajaj/>Ahsaas Bajaj</a>
|
<a href=/people/p/pavitra-dangati/>Pavitra Dangati</a>
|
<a href=/people/k/kalpesh-krishna/>Kalpesh Krishna</a>
|
<a href=/people/p/pradhiksha-ashok-kumar/>Pradhiksha Ashok Kumar</a>
|
<a href=/people/r/rheeya-uppaal/>Rheeya Uppaal</a>
|
<a href=/people/b/bradford-windsor/>Bradford Windsor</a>
|
<a href=/people/e/eliot-brenner/>Eliot Brenner</a>
|
<a href=/people/d/dominic-dotterrer/>Dominic Dotterrer</a>
|
<a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--7><div class="card-body p-3 small">Abstractive summarization is the task of compressing a long document into a coherent short document while retaining <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>salient information</a>. Modern abstractive summarization methods are based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pre-trained abstractive summarizer BART, which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> based on GPT-2 language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to <a href=https://en.wikipedia.org/wiki/Bay_Area_Rapid_Transit>BART</a>, we observe a 6.0 ROUGE-L improvement. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with independent human labeling by domain experts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.14/>Situation-Based Multiparticipant Chat Summarization : a Concept, an Exploration-Annotation Tool and an Example Collection</a></strong><br><a href=/people/a/anna-smirnova/>Anna Smirnova</a>
|
<a href=/people/e/evgeniy-slobodkin/>Evgeniy Slobodkin</a>
|
<a href=/people/g/george-chernishev/>George Chernishev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--14><div class="card-body p-3 small">Currently, <a href=https://en.wikipedia.org/wiki/Text_messaging>text chatting</a> is one of the primary means of communication. However, modern <a href=https://en.wikipedia.org/wiki/Online_chat>text chat</a> still in general does not offer any <a href=https://en.wikipedia.org/wiki/Navigation>navigation</a> or even full-featured search, although the high volumes of messages demand it. In order to mitigate these inconveniences, we formulate the problem of situation-based summarization and propose a special data annotation tool intended for developing <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and gold-standard data</a>. A <a href=https://en.wikipedia.org/wiki/Situation>situation</a> is a subset of messages revolving around a single event in both temporal and contextual senses : e.g, a group of friends arranging a meeting in chat, agreeing on date, time, and place. Situations can be extracted via <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning techniques</a>. Since the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is novel, neither training nor gold-standard datasets for it have been created yet. In this paper, we present the formulation of the situation-based summarization problem. Next, we describe Chat Corpora Annotator (CCA): the first annotation system designed specifically for exploring and annotating chat log data. We also introduce a custom <a href=https://en.wikipedia.org/wiki/Query_language>query language</a> for semi-automatic situation extraction. Finally, we present the first <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold-standard dataset</a> for situation-based summarization. The software source code and the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.15/>Modeling Text using the Continuous Space Topic Model with Pre-Trained Word Embeddings</a></strong><br><a href=/people/s/seiichi-inoue/>Seiichi Inoue</a>
|
<a href=/people/t/taichi-aida/>Taichi Aida</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/m/manabu-asai/>Manabu Asai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--15><div class="card-body p-3 small">In this study, we propose a model that extends the continuous space topic model (CSTM), which flexibly controls word probability in a document, using pre-trained word embeddings. To develop the proposed model, we pre-train word embeddings, which capture the semantics of words and plug them into the CSTM. Intrinsic experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exhibits a superior performance over the CSTM in terms of <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> and <a href=https://en.wikipedia.org/wiki/Convergence_of_random_variables>convergence speed</a>. Furthermore, extrinsic experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is useful for a document classification task when compared with the baseline model. We qualitatively show that the latent coordinates obtained by training the proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> are better than those of the baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.19/>Hold on honey, men at work : A semi-supervised approach to detecting sexism in sitcoms</a></strong><br><a href=/people/s/smriti-singh/>Smriti Singh</a>
|
<a href=/people/t/tanvi-anand/>Tanvi Anand</a>
|
<a href=/people/a/arijit-ghosh-chowdhury/>Arijit Ghosh Chowdhury</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--19><div class="card-body p-3 small">Television shows play an important role inpropagating societal norms. Owing to the popularity of the situational comedy (sitcom) genre, it contributes significantly to the over-all development of society. In an effort to analyze the content of <a href=https://en.wikipedia.org/wiki/Television_show>television shows</a> belong-ing to this <a href=https://en.wikipedia.org/wiki/Genre>genre</a>, we present a dataset of dialogue turns from popular sitcoms annotated for the presence of <a href=https://en.wikipedia.org/wiki/Sexism>sexist remarks</a>. We train a text classification model to detect <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> using domain adaptive learning. We apply the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to analyze the evolution of sexist content over the years. We propose a domain-specific semi-supervised architecture for the aforementioned detection of sexism. Through extensive experiments, we show that our model often yields better <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance over generic deep learn-ing based sentence classification that does not employ domain-specific training. We find that while <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> decreases over time on average, the proportion of sexist dialogue for the most sexist sitcom actually increases. A quantitative analysis along with a detailed error analysis presents the case for our proposed methodology</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-srw.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.21/>Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation<span class=acl-fixed-case>QA</span> Models to Challenge Sets with Variational Question-Answer Pair Generation</a></strong><br><a href=/people/k/kazutoshi-shinoda/>Kazutoshi Shinoda</a>
|
<a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--21><div class="card-body p-3 small">Question answering (QA) models for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> have achieved human-level accuracy on in-distribution test sets. However, they have been demonstrated to lack <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> to challenge sets, whose <a href=https://en.wikipedia.org/wiki/Probability_distribution>distribution</a> is different from that of training sets. Existing data augmentation methods mitigate this problem by simply augmenting training sets with synthetic examples sampled from the same distribution as the challenge sets. However, these methods assume that the <a href=https://en.wikipedia.org/wiki/Probability_distribution>distribution</a> of a challenge set is known a priori, making them less applicable to unseen challenge sets. In this study, we focus on question-answer pair generation (QAG) to mitigate this problem. While most existing QAG methods aim to improve the quality of synthetic examples, we conjecture that diversity-promoting QAG can mitigate the sparsity of training sets and lead to better <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>. We present a variational QAG model that generates multiple diverse QA pairs from a paragraph. Our experiments show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 12 challenge sets, as well as the in-distribution accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.24/>Edit Distance Based Curriculum Learning for Paraphrase Generation</a></strong><br><a href=/people/s/sora-kadotani/>Sora Kadotani</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/y/yuki-arase/>Yuki Arase</a>
|
<a href=/people/m/makoto-onizuka/>Makoto Onizuka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--24><div class="card-body p-3 small">Curriculum learning has improved the quality of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, where only source-side features are considered in the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to determine the difficulty of <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In this study, we apply <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum learning</a> to <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> for the first time. Different from <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> allows a certain level of discrepancy in <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> between source and target, which results in diverse transformations from <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> to reordering of clauses. Hence, the difficulty of transformations requires considering both source and target contexts. Experiments on formality transfer using GYAFC showed that our curriculum learning with <a href=https://en.wikipedia.org/wiki/Edit_distance>edit distance</a> improves the quality of <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>. Additionally, the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> improves the quality of difficult samples, which was not possible for previous <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-srw.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-srw--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-srw.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-srw.28/>CMTA : COVID-19 Misinformation Multilingual Analysis on Twitter<span class=acl-fixed-case>CMTA</span>: <span class=acl-fixed-case>COVID</span>-19 Misinformation Multilingual Analysis on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/r/raj-pranesh/>Raj Pranesh</a>
|
<a href=/people/m/mehrdad-farokhenajd/>Mehrdad Farokhenajd</a>
|
<a href=/people/a/ambesh-shekhar/>Ambesh Shekhar</a>
|
<a href=/people/g/genoveva-vargas-solar/>Genoveva Vargas-Solar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-srw--28><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Internet>internet</a> has actually come to be an essential resource of health knowledge for individuals around the world in the present situation of the coronavirus condition pandemic(COVID-19). During pandemic situations, myths, <a href=https://en.wikipedia.org/wiki/Sensationalism>sensationalism</a>, <a href=https://en.wikipedia.org/wiki/Rumor>rumours</a> and <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a>, generated intentionally or unintentionally, spread rapidly through <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. Twitter is one of these popular social networks people use to share COVID-19 related news, information, and thoughts that reflect their perception and opinion about the pandemic. Evaluation of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> for recognizing <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> can create beneficial understanding to review the top quality and also the readability of online information concerning the COVID-19. This paper presents a multilingual COVID-19 related tweet analysis method, CMTA, that uses BERT, a deep learning model for multilingual tweet misinformation detection and classification. CMTA extracts features from multilingual textual data, which is then categorized into specific information classes. Classification is done by a Dense-CNN model trained on tweets manually annotated into information classes (i.e., &#8216;false&#8217;, &#8216;partly false&#8217;, &#8216;misleading&#8217;). The paper presents an analysis of multilingual tweets from February to June, showing the distribution type of information spread across different languages. To access the performance of the CMTA multilingual model, we performed a comparative analysis of 8 monolingual model and CMTA for the misinformation detection task. The results show that our proposed CMTA model has surpassed various monolingual models which consolidated the fact that through <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> a multilingual framework could be developed.</div></div></div><hr><div id=2021acl-demo><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.acl-demo/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.0/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</a></strong><br><a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jong-c-park/>Jong C. Park</a>
|
<a href=/people/r/rui-xia/>Rui Xia</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.2/>IntelliCAT : Intelligent Machine Translation Post-Editing with Quality Estimation and Translation Suggestion<span class=acl-fixed-case>I</span>ntelli<span class=acl-fixed-case>CAT</span>: Intelligent Machine Translation Post-Editing with Quality Estimation and Translation Suggestion</a></strong><br><a href=/people/d/dongjun-lee/>Dongjun Lee</a>
|
<a href=/people/j/junhyeong-ahn/>Junhyeong Ahn</a>
|
<a href=/people/h/heesoo-park/>Heesoo Park</a>
|
<a href=/people/j/jaemin-jo/>Jaemin Jo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--2><div class="card-body p-3 small">We present IntelliCAT, an interactive translation interface with neural models that streamline the post-editing process on machine translation output. We leverage two quality estimation (QE) models at different granularities : sentence-level QE, to predict the quality of each machine-translated sentence, and word-level QE, to locate the parts of the machine-translated sentence that need correction. Additionally, we introduce a novel translation suggestion model conditioned on both the left and right contexts, providing alternatives for specific words or phrases for correction. Finally, with word alignments, IntelliCAT automatically preserves the original document&#8217;s styles in the translated document. The experimental results show that <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a> based on the proposed QE and translation suggestions can significantly improve translation quality. Furthermore, a user study reveals that three <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> provided in IntelliCAT significantly accelerate the <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing task</a>, achieving a 52.9 % speedup in translation time compared to translating from scratch. The interface is publicly available at https://intellicat.beringlab.com/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-demo.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.4/>TextBox : A Unified, Modularized, and Extensible Framework for Text Generation<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>B</span>ox: A Unified, Modularized, and Extensible Framework for Text Generation</a></strong><br><a href=/people/j/junyi-li/>Junyi Li</a>
|
<a href=/people/t/tianyi-tang/>Tianyi Tang</a>
|
<a href=/people/g/gaole-he/>Gaole He</a>
|
<a href=/people/j/jinhao-jiang/>Jinhao Jiang</a>
|
<a href=/people/x/xiaoxuan-hu/>Xiaoxuan Hu</a>
|
<a href=/people/p/puzhao-xie/>Puzhao Xie</a>
|
<a href=/people/z/zhipeng-chen/>Zhipeng Chen</a>
|
<a href=/people/z/zhuohao-yu/>Zhuohao Yu</a>
|
<a href=/people/w/wayne-xin-zhao/>Wayne Xin Zhao</a>
|
<a href=/people/j/ji-rong-wen/>Ji-Rong Wen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--4><div class="card-body p-3 small">In this paper, we release an open-source library, called <a href=https://en.wikipedia.org/wiki/TextBox>TextBox</a>, to provide a unified, modularized, and extensible text generation framework. TextBox aims to support a broad set of text generation tasks and models. In our <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a>, we implement 21 text generation models on 9 benchmark datasets, covering the categories of VAE, GAN, and pretrained language models. Meanwhile, our <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a> maintains sufficient modularity and extensibility by properly decomposing the model architecture, <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, and learning process into highly reusable modules, which allows users to easily incorporate new models into our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. The above features make <a href=https://en.wikipedia.org/wiki/TextBox>TextBox</a> especially suitable for researchers and practitioners to quickly reproduce baseline models and develop new <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. TextBox is implemented based on <a href=https://en.wikipedia.org/wiki/PyTorch>PyTorch</a>, and released under Apache License 2.0 at the link.<url>https://github.com/RUCAIBox/TextBox</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.5/>Inside ASCENT : Exploring a Deep Commonsense Knowledge Base and its Usage in Question Answering<span class=acl-fixed-case>ASCENT</span>: Exploring a Deep Commonsense Knowledge Base and its Usage in Question Answering</a></strong><br><a href=/people/t/tuan-phong-nguyen/>Tuan-Phong Nguyen</a>
|
<a href=/people/s/simon-razniewski/>Simon Razniewski</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--5><div class="card-body p-3 small">ASCENT is a fully automated methodology for extracting and consolidating commonsense assertions from web contents (Nguyen et al., 2021). It advances traditional triple-based commonsense knowledge representation by capturing semantic facets like locations and purposes, and <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>composite concepts</a>, i.e., subgroups and related aspects of subjects. In this demo, we present a <a href=https://en.wikipedia.org/wiki/Web_portal>web portal</a> that allows users to understand its construction process, explore its content, and observe its impact in the use case of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. The demo website (https://ascent.mpi-inf.mpg.de) and an introductory video (https://youtu.be/qMkJXqu_Yd4) are both available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-demo.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.7/>NeurST : Neural Speech Translation Toolkit<span class=acl-fixed-case>N</span>eur<span class=acl-fixed-case>ST</span>: Neural Speech Translation Toolkit</a></strong><br><a href=/people/c/chengqi-zhao/>Chengqi Zhao</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/q/qianqian-dong/>Qianqian Dong</a>
|
<a href=/people/r/rong-ye/>Rong Ye</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--7><div class="card-body p-3 small">NeurST is an open-source toolkit for neural speech translation. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> mainly focuses on end-to-end speech translation, which is easy to use, modify, and extend to advanced speech translation research and products. NeurST aims at facilitating the speech translation research for NLP researchers and building reliable <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarks</a> for this field. It provides step-by-step recipes for <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a>, <a href=https://en.wikipedia.org/wiki/Data_preprocessing>data preprocessing</a>, distributed training, and <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>. In this paper, we will introduce the framework design of NeurST and show experimental results for different benchmark datasets, which can be regarded as reliable baselines for future research. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is publicly available at and we will continuously update the performance of with other counterparts and studies at.<url>https://github.com/bytedance/neurst</url> and we will continuously update the performance of with other counterparts and studies at <url>https://st-benchmark.github.io/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.9/>MT-Telescope : An interactive platform for contrastive evaluation of MT systems<span class=acl-fixed-case>MT</span>-<span class=acl-fixed-case>T</span>elescope: <span class=acl-fixed-case>A</span>n interactive platform for contrastive evaluation of <span class=acl-fixed-case>MT</span> systems</a></strong><br><a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/ana-c-farinha/>Ana C Farinha</a>
|
<a href=/people/c/craig-stewart/>Craig Stewart</a>
|
<a href=/people/l/luisa-coheur/>Luisa Coheur</a>
|
<a href=/people/a/alon-lavie/>Alon Lavie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--9><div class="card-body p-3 small">We present MT-Telescope, a visualization platform designed to facilitate comparative analysis of the output quality of two Machine Translation (MT) systems. While automated MT evaluation metrics are commonly used to evaluate MT systems at a corpus-level, our platform supports fine-grained segment-level analysis and interactive visualisations that expose the fundamental differences in the performance of the compared systems. MT-Telescope also supports dynamic corpus filtering to enable focused analysis on specific phenomena such as ; translation of named entities, handling of terminology, and the impact of input segment length on translation quality. Furthermore, the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> provides a bootstrapped t-test for <a href=https://en.wikipedia.org/wiki/Statistical_significance>statistical significance</a> as a means of evaluating the rigor of the resulting system ranking. MT-Telescope is open source, written in <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a>, and is built around a user friendly and dynamic web interface. Complementing other existing tools, our platform is designed to facilitate and promote the broader adoption of more rigorous analysis practices in the evaluation of MT quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-demo.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.11/>CogIE : An Information Extraction Toolkit for Bridging Texts and CogNet<span class=acl-fixed-case>C</span>og<span class=acl-fixed-case>IE</span>: An Information Extraction Toolkit for Bridging Texts and <span class=acl-fixed-case>C</span>og<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/z/zhuoran-jin/>Zhuoran Jin</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/d/dianbo-sui/>Dianbo Sui</a>
|
<a href=/people/c/chenhao-wang/>Chenhao Wang</a>
|
<a href=/people/z/zhipeng-xue/>Zhipeng Xue</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--11><div class="card-body p-3 small">CogNet is a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> that integrates three types of knowledge : <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic knowledge</a>, <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> and commonsense knowledge. In this paper, we propose an information extraction toolkit, called CogIE, which is a bridge connecting raw texts and CogNet. CogIE has three features : versatile, knowledge-grounded and extensible. First, CogIE is a versatile toolkit with a rich set of functional modules, including <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, entity typing, entity linking, relation extraction, event extraction and frame-semantic parsing. Second, as a knowledge-grounded toolkit, CogIE can ground the extracted facts to CogNet and leverage different types of knowledge to enrich extracted results. Third, for <a href=https://en.wikipedia.org/wiki/Extensibility>extensibility</a>, owing to the design of <a href=https://en.wikipedia.org/wiki/Multitier_architecture>three-tier architecture</a>, CogIE is not only a plug-and-play toolkit for developers but also an extensible programming framework for researchers. We release an open-access online system to visually extract information from texts. Source code, datasets and pre-trained models are publicly available at GitHub, with a short instruction video.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-demo.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.12/>fastHan : A BERT-based Multi-Task Toolkit for Chinese NLP<span class=acl-fixed-case>H</span>an: A <span class=acl-fixed-case>BERT</span>-based Multi-Task Toolkit for <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/z/zhichao-geng/>Zhichao Geng</a>
|
<a href=/people/h/hang-yan/>Hang Yan</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--12><div class="card-body p-3 small">We present fastHan, an open-source toolkit for four basic tasks in Chinese natural language processing : Chinese word segmentation (CWS), Part-of-Speech (POS) tagging, named entity recognition (NER), and dependency parsing. The backbone of fastHan is a multi-task model based on a pruned BERT, which uses the first 8 layers in BERT. We also provide a 4-layer base model compressed from the 8-layer model. The joint-model is trained and evaluated on 13 corpora of four tasks, yielding near state-of-the-art (SOTA) performance in dependency parsing and NER, achieving SOTA performance in CWS and POS. Besides, fastHan&#8217;s transferability is also strong, performing much better than popular segmentation tools on a <a href=https://en.wikipedia.org/wiki/Text_corpus>non-training corpus</a>. To better meet the need of practical application, we allow users to use their own labeled data to further fine-tune fastHan. In addition to its small size and excellent performance, fastHan is user-friendly. Implemented as a <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>python package</a>, fastHan isolates users from the internal technical details and is convenient to use. The project is released on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.13/>Erase and Rewind : Manual Correction of NLP Output through a Web Interface<span class=acl-fixed-case>NLP</span> Output through a Web Interface</a></strong><br><a href=/people/v/valentino-frasnelli/>Valentino Frasnelli</a>
|
<a href=/people/l/lorenzo-bocchi/>Lorenzo Bocchi</a>
|
<a href=/people/a/alessio-palmero-aprosio/>Alessio Palmero Aprosio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--13><div class="card-body p-3 small">In this paper, we present Tintful, an NLP annotation software that can be used both to manually annotate texts and to fix mistakes in NLP pipelines, such as Stanford CoreNLP. Using a <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> similar to wiki-like systems, a user who notices some wrong annotation can easily fix it and submit the resulting (and right) entry back to the tool developers. Moreover, Tintful can be used to easily annotate data from scratch. The input documents do not need to be in a particular format : starting from the plain text, the sentences are first annotated with CoreNLP, then the user can edit the annotations and submit everything back through a user-friendly interface.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Demonstration Runner-up"><i class="fas fa-award"></i></span></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-demo.14/>ESRA : Explainable Scientific Research Assistant<span class=acl-fixed-case>ESRA</span>: Explainable Scientific Research Assistant</a></strong><br><a href=/people/p/pollawat-hongwimol/>Pollawat Hongwimol</a>
|
<a href=/people/p/peeranuth-kehasukcharoen/>Peeranuth Kehasukcharoen</a>
|
<a href=/people/p/pasit-laohawarutchai/>Pasit Laohawarutchai</a>
|
<a href=/people/p/piyawat-lertvittayakumjorn/>Piyawat Lertvittayakumjorn</a>
|
<a href=/people/a/aik-beng-ng/>Aik Beng Ng</a>
|
<a href=/people/z/zhangsheng-lai/>Zhangsheng Lai</a>
|
<a href=/people/t/timothy-liu/>Timothy Liu</a>
|
<a href=/people/p/peerapon-vateekul/>Peerapon Vateekul</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--14><div class="card-body p-3 small">We introduce Explainable Scientific Research Assistant (ESRA), a literature discovery platform that augments search results with relevant details and explanations, aiding users in understanding more about their queries and the returned papers beyond existing literature search systems. Enabled by a knowledge graph we extracted from abstracts of 23k papers on the arXiv&#8217;s cs. CL category, ESRA provides three main features : explanation (for why a paper is returned to the user), list of facts (that are relevant to the query), and graph visualization (drawing connections between the query and each paper with surrounding related entities). The experimental results with humans involved show that ESRA can accelerate the users&#8217; search process with paper explanations and helps them better explore the landscape of the topics of interest by exploiting the underlying <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>. We provide the ESRA web application at http://esra.cp.eng.chula.ac.th/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.17/>REM : Efficient Semi-Automated Real-Time Moderation of Online Forums<span class=acl-fixed-case>REM</span>: Efficient Semi-Automated Real-Time Moderation of Online Forums</a></strong><br><a href=/people/j/jakob-smedegaard-andersen/>Jakob Smedegaard Andersen</a>
|
<a href=/people/o/olaf-zukunft/>Olaf Zukunft</a>
|
<a href=/people/w/walid-maalej/>Walid Maalej</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--17><div class="card-body p-3 small">This paper presents REM, a novel tool for the semi-automated real-time moderation of large scale online forums. The growing demand for <a href=https://en.wikipedia.org/wiki/Online_participation>online participation</a> and the increasing number of user comments raise challenges in filtering out harmful and undesirable content from <a href=https://en.wikipedia.org/wiki/Public_debate>public debates</a> in <a href=https://en.wikipedia.org/wiki/Internet_forum>online forums</a>. Since a manual moderation does not scale well and pure automated approaches often lack the required level of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, we suggest a semi-automated moderation approach. Our approach maximizes the efficiency of manual efforts by targeting only those comments for which human intervention is needed, e.g. due to high <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification uncertainty</a>. Our <a href=https://en.wikipedia.org/wiki/Tool>tool</a> offers a rich visual interactive environment enabling the exploration of online debates. We conduct a preliminary evaluation experiment to demonstrate the suitability of our approach and publicly release the source code of <a href=https://en.wikipedia.org/wiki/Rapid_eye_movement_sleep>REM</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.19/>A Graphical Interface for Curating Schemas</a></strong><br><a href=/people/p/piyush-mishra/>Piyush Mishra</a>
|
<a href=/people/a/akanksha-malhotra/>Akanksha Malhotra</a>
|
<a href=/people/s/susan-windisch-brown/>Susan Windisch Brown</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/g/ghazaleh-kazeminejad/>Ghazaleh Kazeminejad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--19><div class="card-body p-3 small">Much past work has focused on extracting information like <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a>, <a href=https://en.wikipedia.org/wiki/Legal_person>entities</a>, and <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>relations</a> from <a href=https://en.wikipedia.org/wiki/Document>documents</a>. Very little work has focused on analyzing these results for better <a href=https://en.wikipedia.org/wiki/Mathematical_model>model understanding</a>. In this paper, we introduce a curation interface that takes an Information Extraction (IE) system&#8217;s output in a pre-defined format and generates a graphical representation of its elements. The <a href=https://en.wikipedia.org/wiki/User_interface>interface</a> supports editing while curating schemas for complex events like Improvised Explosive Device (IED) based scenarios. We identify various schemas that either have linear event chains or contain parallel events with complicated temporal ordering. We iteratively update an induced schema to uniquely identify events specific to it, add optional events around them, and prune unnecessary events. The resulting schemas are improved and enriched versions of the machine-induced versions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.24/>CLTR : An End-to-End, Transformer-Based System for Cell-Level Table Retrieval and Table Question Answering<span class=acl-fixed-case>CLTR</span>: An End-to-End, Transformer-Based System for Cell-Level Table Retrieval and Table Question Answering</a></strong><br><a href=/people/f/feifei-pan/>Feifei Pan</a>
|
<a href=/people/m/mustafa-canim/>Mustafa Canim</a>
|
<a href=/people/m/michael-glass/>Michael Glass</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a>
|
<a href=/people/p/peter-fox/>Peter Fox</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--24><div class="card-body p-3 small">We present the first end-to-end, transformer-based table question answering (QA) system that takes natural language questions and massive table corpora as inputs to retrieve the most relevant tables and locate the correct table cells to answer the question. Our system, CLTR, extends the current state-of-the-art QA over tables model to build an end-to-end table QA architecture. This <a href=https://en.wikipedia.org/wiki/System>system</a> has successfully tackled many real-world table QA problems with a simple, unified pipeline. Our proposed system can also generate a <a href=https://en.wikipedia.org/wiki/Heat_map>heatmap</a> of candidate columns and rows over complex tables and allow users to quickly identify the correct cells to answer questions. In addition, we introduce two new open domain benchmarks, E2E_WTQ and E2E_GNQ, consisting of 2,005 <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language questions</a> over 76,242 tables. The <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> are designed to validate CLTR as well as accommodate future table retrieval and end-to-end table QA research and experiments. Our experiments demonstrate that our system is the current state-of-the-art model on the table retrieval task and produces promising results for end-to-end table QA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-demo.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.26/>FastSeq : Make Sequence Generation Faster<span class=acl-fixed-case>F</span>ast<span class=acl-fixed-case>S</span>eq: Make Sequence Generation Faster</a></strong><br><a href=/people/y/yu-yan/>Yu Yan</a>
|
<a href=/people/f/fei-hu/>Fei Hu</a>
|
<a href=/people/j/jiusheng-chen/>Jiusheng Chen</a>
|
<a href=/people/n/nikhil-bhendawade/>Nikhil Bhendawade</a>
|
<a href=/people/t/ting-ye/>Ting Ye</a>
|
<a href=/people/y/yeyun-gong/>Yeyun Gong</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/d/desheng-cui/>Desheng Cui</a>
|
<a href=/people/b/bingyu-chi/>Bingyu Chi</a>
|
<a href=/people/r/ruofei-zhang/>Ruofei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--26><div class="card-body p-3 small">Transformer-based models have made tremendous impacts in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. However the inference speed is a bottleneck due to large model size and intensive computing involved in auto-regressive decoding process. We develop FastSeq framework to accelerate sequence generation without <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy loss</a>. The proposed optimization techniques include an attention cache optimization, an efficient <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for detecting repeated n-grams, and an asynchronous generation pipeline with parallel I / O. These <a href=https://en.wikipedia.org/wiki/Optimizing_compiler>optimizations</a> are general enough to be applicable to Transformer-based models (e.g., T5, GPT2, and UniLM). Our benchmark results on a set of widely used and diverse <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> demonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use with a simple one-line code change. The source code is available at https://github.com/microsoft/fastseq.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-demo.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.30/>Ecco : An Open Source Library for the Explainability of Transformer Language Models</a></strong><br><a href=/people/j/j-alammar/>J Alammar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--30><div class="card-body p-3 small">Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. To increase the transparency of Transformer-based language models, we present Ecco an open-source library for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and interactively explore the inner mechanics of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. This includes (1) gradient-based feature attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and examination tools for neuron activations in the under-explored Feed-Forward Neural Network sublayer of Transformer layers. (4) convenient examination of activation vectors via canonical correlation analysis (CCA), non-negative matrix factorization (NMF), and probing classifiers. We find that <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> can be retrieved from BERT&#8217;s FFNN representations in levels comparable to those in hidden state representations. More curiously, we find that the model builds up <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> in its hidden states even when intermediate FFNNs indicate diminished levels of <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>. Ecco is available at https://www.eccox.io/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.32/>TweeNLP : A Twitter Exploration Portal for Natural Language Processing<span class=acl-fixed-case>T</span>wee<span class=acl-fixed-case>NLP</span>: A <span class=acl-fixed-case>T</span>witter Exploration Portal for Natural Language Processing</a></strong><br><a href=/people/v/viraj-shah/>Viraj Shah</a>
|
<a href=/people/s/shruti-singh/>Shruti Singh</a>
|
<a href=/people/m/mayank-singh/>Mayank Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--32><div class="card-body p-3 small">We present TweeNLP, a one-stop portal that organizes Twitter&#8217;s natural language processing (NLP) data and builds a visualization and exploration platform. It curates 19,395 tweets (as of April 2021) from various NLP conferences and general NLP discussions. It supports multiple features such as TweetExplorer to explore tweets by topics, visualize insights from Twitter activity throughout the organization cycle of conferences, discover popular research papers and researchers. It also builds a timeline of conference and workshop submission deadlines. We envision TweeNLP to function as a collective memory unit for the NLP community by integrating the <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> pertaining to research papers with the NLPExplorer scientific literature search engine. The current <a href=https://en.wikipedia.org/wiki/System>system</a> is hosted at http://nlpexplorer.org/twitter/CFP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-demo.39" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.39/>ReTraCk : A Flexible and Efficient Framework for Knowledge Base Question Answering<span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>T</span>ra<span class=acl-fixed-case>C</span>k: A Flexible and Efficient Framework for Knowledge Base Question Answering</a></strong><br><a href=/people/s/shuang-chen/>Shuang Chen</a>
|
<a href=/people/q/qian-liu/>Qian Liu</a>
|
<a href=/people/z/zhiwei-yu/>Zhiwei Yu</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/f/feng-jiang/>Feng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--39><div class="card-body p-3 small">We present Retriever-Transducer-Checker (ReTraCk), a neural semantic parsing framework for large scale knowledge base question answering (KBQA). ReTraCk is designed as a <a href=https://en.wikipedia.org/wiki/Modular_programming>modular framework</a> to maintain high flexibility. It includes a retriever to retrieve relevant KB items efficiently, a transducer to generate <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> with syntax correctness guarantees and a checker to improve transduction procedure. ReTraCk is ranked at top1 overall performance on the GrailQA leaderboard and obtains highly competitive performance on the typical WebQuestionsSP benchmark. Our <a href=https://en.wikipedia.org/wiki/System>system</a> can interact with users timely, demonstrating the efficiency of the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.41/>TextFlint : Unified Multilingual Robustness Evaluation Toolkit for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a><span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>F</span>lint: Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing</a></strong><br><a href=/people/x/xiao-wang/>Xiao Wang</a>
|
<a href=/people/q/qin-liu/>Qin Liu</a>
|
<a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/y/yicheng-zou/>Yicheng Zou</a>
|
<a href=/people/x/xin-zhou/>Xin Zhou</a>
|
<a href=/people/j/jiacheng-ye/>Jiacheng Ye</a>
|
<a href=/people/y/yongxin-zhang/>Yongxin Zhang</a>
|
<a href=/people/r/rui-zheng/>Rui Zheng</a>
|
<a href=/people/z/zexiong-pang/>Zexiong Pang</a>
|
<a href=/people/q/qinzhuo-wu/>Qinzhuo Wu</a>
|
<a href=/people/z/zhengyan-li/>Zhengyan Li</a>
|
<a href=/people/c/chong-zhang/>Chong Zhang</a>
|
<a href=/people/r/ruotian-ma/>Ruotian Ma</a>
|
<a href=/people/z/zichu-fei/>Zichu Fei</a>
|
<a href=/people/r/ruijian-cai/>Ruijian Cai</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/x/xingwu-hu/>Xingwu Hu</a>
|
<a href=/people/z/zhiheng-yan/>Zhiheng Yan</a>
|
<a href=/people/y/yiding-tan/>Yiding Tan</a>
|
<a href=/people/y/yuan-hu/>Yuan Hu</a>
|
<a href=/people/q/qiyuan-bian/>Qiyuan Bian</a>
|
<a href=/people/z/zhihua-liu/>Zhihua Liu</a>
|
<a href=/people/s/shan-qin/>Shan Qin</a>
|
<a href=/people/b/bolin-zhu/>Bolin Zhu</a>
|
<a href=/people/x/xiaoyu-xing/>Xiaoyu Xing</a>
|
<a href=/people/j/jinlan-fu/>Jinlan Fu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/x/xiaoqing-zheng/>Xiaoqing Zheng</a>
|
<a href=/people/y/yaqian-zhou/>Yaqian Zhou</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--41><div class="card-body p-3 small">TextFlint is a multilingual robustness evaluation toolkit for NLP tasks that incorporates universal text transformation, task-specific transformation, adversarial attack, subpopulation, and their combinations to provide comprehensive robustness analyses. This enables practitioners to automatically evaluate their <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> from various aspects or to customize their evaluations as desired with just a few lines of code. TextFlint also generates complete analytical reports as well as targeted augmented data to address the shortcomings of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in terms of its <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a>. To guarantee acceptability, all the text transformations are linguistically based and all the transformed data selected (up to 100,000 texts) scored highly under human evaluation. To validate the utility, we performed large-scale empirical evaluations (over 67,000) on state-of-the-art <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, classic <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a>, and real-world systems. The toolkit is already available at https://github.com/textflint with all the evaluation results demonstrated at textflint.io.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-demo.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-demo--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-demo.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-demo.43" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-demo.43/>OpenAttack : An Open-source Textual Adversarial Attack Toolkit<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>A</span>ttack: An Open-source Textual Adversarial Attack Toolkit</a></strong><br><a href=/people/g/guoyang-zeng/>Guoyang Zeng</a>
|
<a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/q/qianrui-zhou/>Qianrui Zhou</a>
|
<a href=/people/t/tingji-zhang/>Tingji Zhang</a>
|
<a href=/people/z/zixian-ma/>Zixian Ma</a>
|
<a href=/people/b/bairu-hou/>Bairu Hou</a>
|
<a href=/people/y/yuan-zang/>Yuan Zang</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-demo--43><div class="card-body p-3 small">Textual adversarial attacking has received wide and increasing attention in recent years. Various <a href=https://en.wikipedia.org/wiki/Attack_model>attack models</a> have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named OpenAttack to solve these issues. Compared with existing other textual adversarial attack toolkits, OpenAttack has its unique strengths in support for all attack types, <a href=https://en.wikipedia.org/wiki/Multilinguality>multilinguality</a>, and <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel processing</a>. Currently, OpenAttack includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating <a href=https://en.wikipedia.org/wiki/Attack_model>attack models</a>, measuring robustness of a model, assisting in developing new <a href=https://en.wikipedia.org/wiki/Attack_model>attack models</a>, and <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a>. Source code and documentation can be obtained at https://github.com/thunlp/OpenAttack.</div></div></div><hr><div id=2021acl-tutorials><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-tutorials.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.acl-tutorials/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-tutorials.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-tutorials.0/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts</a></strong><br><a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-tutorials.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-tutorials--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-tutorials.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-tutorials.1/>Advances in Debating Technologies : Building AI That Can Debate Humans<span class=acl-fixed-case>AI</span> That Can Debate Humans</a></strong><br><a href=/people/r/roy-bar-haim/>Roy Bar-Haim</a>
|
<a href=/people/l/liat-ein-dor/>Liat Ein-Dor</a>
|
<a href=/people/m/matan-orbach/>Matan Orbach</a>
|
<a href=/people/e/elad-venezian/>Elad Venezian</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-tutorials--1><div class="card-body p-3 small">The tutorial focuses on Debating Technologies, a sub-field of computational argumentation defined as computational technologies developed directly to enhance, support, and engage with human debating (Gurevych et al., 2016). A recent milestone in this field is <a href=https://en.wikipedia.org/wiki/Project_Debater>Project Debater</a>, which was revealed in 2019 as the first AI system that can debate human experts on complex topics. Project Debater is the third in the series of IBM Research AI&#8217;s grand challenges, following Deep Blue and Watson. It has been developed for over six years by a large team of researchers and engineers, and its live demonstration in February 2019 received massive media attention. This research effort has resulted in more than 50 scientific papers to date, and many datasets freely available for research purposes. We discuss the scientific challenges that arise when building such a system, including <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>, argument quality assessment, stance classification, principled argument detection, narrative generation, and rebutting a human opponent. Many of the underlying capabilities of <a href=https://en.wikipedia.org/wiki/Project_Debater>Project Debater</a> have been made freely available for academic research, and the tutorial will include a detailed explanation of how to use and leverage these tools. In addition to discussing individual components, the tutorial also provides a holistic view of a <a href=https://en.wikipedia.org/wiki/Debate>debating system</a>. Such a view is largely missing in the academic literature, where each paper typically addresses a specific problem in isolation. We present a complete pipeline of a <a href=https://en.wikipedia.org/wiki/Debate>debating system</a>, and discuss the <a href=https://en.wikipedia.org/wiki/Information_flow>information flow</a> and the interaction between the various components. Finally, we discuss practical applications and future challenges of debating technologies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-tutorials.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-tutorials--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-tutorials.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-tutorials.4/>Pre-training Methods for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-tutorials--4><div class="card-body p-3 small">This tutorial provides a comprehensive guide to make the most of pre-training for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. Firstly, we will briefly introduce the background of NMT, pre-training methodology, and point out the main challenges when applying pre-training for NMT. Then we will focus on analysing the role of pre-training in enhancing the performance of <a href=https://en.wikipedia.org/wiki/Neuropsychological_test>NMT</a>, how to design a better pre-training model for executing specific <a href=https://en.wikipedia.org/wiki/Neuropsychological_test>NMT tasks</a> and how to better integrate the pre-trained model into <a href=https://en.wikipedia.org/wiki/Neuropsychological_test>NMT system</a>. In each part, we will provide examples, discuss training techniques and analyse what is transferred when applying pre-training.</div></div></div><hr><div id=2021bppf-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bppf-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.bppf-1/>Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bppf-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bppf-1.0/>Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future</a></strong><br><a href=/people/k/kenneth-church/>Kenneth Church</a>
|
<a href=/people/m/mark-liberman/>Mark Liberman</a>
|
<a href=/people/v/valia-kordoni/>Valia Kordoni</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bppf-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bppf-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bppf-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bppf-1.4/>How Might We Create Better Benchmarks for <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Recognition</a>?</a></strong><br><a href=/people/a/alena-aksenova/>Alëna Aksënova</a>
|
<a href=/people/d/daan-van-esch/>Daan van Esch</a>
|
<a href=/people/j/james-flynn/>James Flynn</a>
|
<a href=/people/p/pavel-golik/>Pavel Golik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bppf-1--4><div class="card-body p-3 small">The applications of automatic speech recognition (ASR) systems are proliferating, in part due to recent significant quality improvements. However, as recent work indicates, even state-of-the-art speech recognition systems some which deliver impressive benchmark results, struggle to generalize across use cases. We review relevant work, and, hoping to inform future benchmark development, outline a taxonomy of speech recognition use cases, proposed for the next generation of ASR benchmarks. We also survey work on <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>, in addition to the de facto standard Word Error Rate (WER) metric, and we introduce a versatile framework designed to describe interactions between linguistic variation and ASR performance metrics.</div></div></div><hr><div id=2021case-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.case-1/>Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.0/>Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021)</a></strong><br><a href=/people/a/ali-hurriyetoglu/>Ali Hürriyetoğlu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.1/>Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021): Workshop and Shared Task Report<span class=acl-fixed-case>CASE</span> 2021): Workshop and Shared Task Report</a></strong><br><a href=/people/a/ali-hurriyetoglu/>Ali Hürriyetoğlu</a>
|
<a href=/people/h/hristo-tanev/>Hristo Tanev</a>
|
<a href=/people/v/vanni-zavarella/>Vanni Zavarella</a>
|
<a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/r/reyyan-yeniterzi/>Reyyan Yeniterzi</a>
|
<a href=/people/o/osman-mutlu/>Osman Mutlu</a>
|
<a href=/people/d/deniz-yuret/>Deniz Yuret</a>
|
<a href=/people/a/aline-villavicencio/>Aline Villavicencio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--1><div class="card-body p-3 small">This workshop is the fourth issue of a series of workshops on automatic extraction of socio-political events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission and with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development of reliable, valid, robust, and practical solutions for automatically detecting descriptions of socio-political events, such as <a href=https://en.wikipedia.org/wiki/Protest>protests</a>, <a href=https://en.wikipedia.org/wiki/Riot>riots</a>, <a href=https://en.wikipedia.org/wiki/War>wars</a> and <a href=https://en.wikipedia.org/wiki/War>armed conflicts</a>, in text streams. This year workshop contributors make use of the state-of-the-art NLP technologies, such as <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a>, Word Embeddings and Transformers and cover a wide range of topics from text classification to news bias detection. Around 40 teams have registered and 15 teams contributed to three tasks that are i) multilingual protest news detection detection, ii) fine-grained classification of socio-political events, and iii) discovering Black Lives Matter protest events. The workshop also highlights two keynote and four invited talks about various aspects of creating event data sets and multi- and cross-lingual machine learning in few- and zero-shot settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.2/>Keynote Abstract : Events on a Global Scale : Towards Language-Agnostic Event Extraction</a></strong><br><a href=/people/e/elizabeth-boschee/>Elizabeth Boschee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--2><div class="card-body p-3 small">Event extraction is a challenging and exciting task in the world of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> & <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. The breadth of events of possible interest, the speed at which surrounding socio-political event contexts evolve, and the complexities involved in generating representative annotated data all contribute to this challenge. One particular dimension of difficulty is the intrinsically global nature of events : many downstream use cases for <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> involve reporting not just in a few major languages but in a much broader context. The languages of interest for even a fixed <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> may still shift from day to day, e.g. when a disease emerges in an unexpected location. Early <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a> to multi-lingual event extraction (e.g. ACE) relied wholly on supervised data provided in each language of interest. Later approaches leveraged the success of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to side-step the issue, simply translating foreign-language content to <a href=https://en.wikipedia.org/wiki/English_language>English</a> and deploying <a href=https://en.wikipedia.org/wiki/English_language>English models</a> on the result (often leaving some significant portion of the original content behind). Most recently, however, the community has begun to shown significant progress applying zero-shot transfer techniques to the problem, developing models using supervised English data but decoding in a foreign language without translation, typically using embedding spaces specifically designed to capture multi-lingual semantic content. In this talk I will discuss multiple dimensions of these promising new approaches and the linguistic representations that underlie them. I will compare them with approaches based on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> (as well as with models trained using in-language training data, where available), and discuss their strengths and weaknesses in different contexts, including the amount of English / foreign bitext available and the nature of the target event ontology. I will also discuss possible future directions with an eye to improving the quality of <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> no matter its source around the globe.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.3/>Keynote Abstract : <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning</a> in Conflict Studies : Reflections on Ethics, Collaboration, and Ongoing Challenges</a></strong><br><a href=/people/k/kristine-eck/>Kristine Eck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--3><div class="card-body p-3 small">Advances in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> are nothing short of revolutionary in their potential to analyze massive amounts of data and in doing so, create new <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>. But there is a responsibility in wielding the power to analyze these <a href=https://en.wikipedia.org/wiki/Data>data</a> since the public attributes a high degree of confidence to results which are based on <a href=https://en.wikipedia.org/wiki/Big_data>big datasets</a>. In this keynote, I will first address our ethical imperative as scholars to get it right. This imperative relates not only to model precision but also to the quality of the underlying data, and to whether the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> inadvertently reproduce or obscure political biases in the source material. In considering the ethical imperative to get it right, it is also important to define what is right : what is considered an acceptable threshold for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> success needs to be understood in light of the project&#8217;s objectives. I then reflect on the different topics and data which are sourced in this field. Much of the existing research has focused on identifying <a href=https://en.wikipedia.org/wiki/Conflict_(process)>conflict events</a> (e.g. battles), but scholars are also increasingly turning to ML approaches to address other facets of the conflict environment. Conflict event extraction has long been a challenge for the natural language processing (NLP) community because it requires sophisticated methods for defining <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>event ontologies</a>, creating language resources, and developing algorithmic approaches. NLP machine-learning tools are ill-adapted to the complex, often messy, and diverse data generated during conflicts. Relative to other types of NLP text corpora, conflicts tend to generate less textual data, and texts are generated non-systematically. Conflict-related texts are often lexically idiosyncratic and tend to be written differently across actors, periods, and conflicts. Event definition and <a href=https://en.wikipedia.org/wiki/Adjudication>adjudication</a> present tough challenges in the context of conflict corpora. Topics which rely on other types of <a href=https://en.wikipedia.org/wiki/Data>data</a> may be better-suited to NLP and machine learning methods. For example, <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and other social media data lend themselves well to studying <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, <a href=https://en.wikipedia.org/wiki/Public_opinion>public opinion</a>, <a href=https://en.wikipedia.org/wiki/Social_polarization>social polarization</a>, or discursive aspects of conflictual environments. Likewise, government-produced policy documents have typically been analyzed with historical, qualitative methods but their standardized formats and quantity suggest that ML methods can provide new traction. ML approaches may also allow scholars to exploit <a href=https://en.wikipedia.org/wiki/Primary_source>local sources</a> and <a href=https://en.wikipedia.org/wiki/Multilingualism>multi-language sources</a> to a greater degree than has been possible. Many challenges remain, and these are best addressed in collaborative projects which build on interdisciplinary expertise. Classification projects need to be anchored in the theoretical interests of scholars of <a href=https://en.wikipedia.org/wiki/Political_violence>political violence</a> if the data they produce are to be put to analytical use. There are few <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontologies</a> for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> that adequately reflect conflict researchers&#8217; interests, which highlights the need for conceptual as well as technical development.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.9/>Extracting Events from Industrial Incident Reports</a></strong><br><a href=/people/n/nitin-ramrakhiyani/>Nitin Ramrakhiyani</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/s/sangameshwar-patil/>Sangameshwar Patil</a>
|
<a href=/people/a/alok-kumar/>Alok Kumar</a>
|
<a href=/people/g/girish-palshikar/>Girish Palshikar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--9><div class="card-body p-3 small">Incidents in industries have huge social and political impact and minimizing the consequent damage has been a high priority. However, automated analysis of repositories of incident reports has remained a challenge. In this paper, we focus on automatically extracting <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> from <a href=https://en.wikipedia.org/wiki/Incident_report>incident reports</a>. Due to absence of event annotated datasets for industrial incidents we employ a transfer learning based approach which is shown to outperform several baselines. We further provide detailed analysis regarding effect of increase in pre-training data and provide explainability of why pre-training improves the performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.11/>Multilingual Protest News Detection-Shared Task 1, CASE 2021<span class=acl-fixed-case>CASE</span> 2021</a></strong><br><a href=/people/a/ali-hurriyetoglu/>Ali Hürriyetoğlu</a>
|
<a href=/people/o/osman-mutlu/>Osman Mutlu</a>
|
<a href=/people/e/erdem-yoruk/>Erdem Yörük</a>
|
<a href=/people/f/farhana-ferdousi-liza/>Farhana Ferdousi Liza</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/s/shyam-ratan/>Shyam Ratan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--11><div class="card-body p-3 small">Benchmarking state-of-the-art text classification and information extraction systems in multilingual, cross-lingual, few-shot, and zero-shot settings for socio-political event information collection is achieved in the scope of the shared task Socio-political and Crisis Events Detection at the workshop CASE @ ACL-IJCNLP 2021. Socio-political event data is utilized for national and international policy- and decision-making. Therefore, the reliability and validity of these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are of the utmost importance. We split the shared task into three parts to address the three aspects of <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> (Task 1), fine-grained semantic classification (Task 2), and <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> (Task 3). Task 1, which is the focus of this report, is on multilingual protest news detection and comprises four subtasks that are document classification (subtask 1), sentence classification (subtask 2), event sentence coreference identification (subtask 3), and event extraction (subtask 4). All subtasks had <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> for both training and evaluation data. Data in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi language</a> was available only for the evaluation of subtask 1. The majority of the submissions, which are 238 in total, are created using multi- and cross-lingual approaches. Best scores are above 77.27 F1-macro for subtask 1, above 85.32 F1-macro for subtask 2, above 84.23 CoNLL 2012 average score for subtask 3, and above 66.20 F1-macro for subtask 4 in all evaluation settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.case-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.13/>IIITT at CASE 2021 Task 1 : Leveraging Pretrained Language Models for Multilingual Protest Detection<span class=acl-fixed-case>IIITT</span> at <span class=acl-fixed-case>CASE</span> 2021 Task 1: Leveraging Pretrained Language Models for Multilingual Protest Detection</a></strong><br><a href=/people/p/pawan-kalyan/>Pawan Kalyan</a>
|
<a href=/people/d/duddukunta-reddy/>Duddukunta Reddy</a>
|
<a href=/people/a/adeep-hande/>Adeep Hande</a>
|
<a href=/people/r/ruba-priyadharshini/>Ruba Priyadharshini</a>
|
<a href=/people/r/ratnasingam-sakuntharaj/>Ratnasingam Sakuntharaj</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--13><div class="card-body p-3 small">In a world abounding in constant protests resulting from events like a global pandemic, <a href=https://en.wikipedia.org/wiki/Climate_change>climate change</a>, religious or political conflicts, there has always been a need to detect events / protests before getting amplified by <a href=https://en.wikipedia.org/wiki/News_media>news media</a> or <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This paper demonstrates our work on the sentence classification subtask of multilingual protest detection in CASE@ACL-IJCNLP 2021. We approached this task by employing various multilingual pre-trained transformer models to classify if any sentence contains information about an event that has transpired or not. We performed soft voting over the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, achieving the best results among the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, accomplishing a macro F1-Score of 0.8291, 0.7578, and 0.7951 in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, Spanish, and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.case-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.14/>NUS-IDS at CASE 2021 Task 1 : Improving Multilingual Event Sentence Coreference Identification With Linguistic Information<span class=acl-fixed-case>NUS</span>-<span class=acl-fixed-case>IDS</span> at <span class=acl-fixed-case>CASE</span> 2021 Task 1: Improving Multilingual Event Sentence Coreference Identification With Linguistic Information</a></strong><br><a href=/people/f/fiona-anting-tan/>Fiona Anting Tan</a>
|
<a href=/people/s/sujatha-das-gollapalli/>Sujatha Das Gollapalli</a>
|
<a href=/people/s/see-kiong-ng/>See-Kiong Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--14><div class="card-body p-3 small">Event Sentence Coreference Identification (ESCI) aims to cluster event sentences that refer to the same event together for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. We describe our ESCI solution developed for the ACL-CASE 2021 shared tasks on the detection and classification of socio-political and crisis event information in a multilingual setting. For a given article, our proposed pipeline comprises of an accurate sentence pair classifier that identifies coreferent sentence pairs and subsequently uses these predicted probabilities to cluster sentences into groups. Sentence pair representations are constructed from fine-tuned BERT embeddings plus POS embeddings fed through a BiLSTM model, and combined with linguistic-based lexical and semantic similarities between sentences. Our best <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> ranked 2nd, 1st and 2nd and obtained CoNLL F1 scores of 81.20 %, 93.03 %, 83.15 % for the English, Portuguese and Spanish test sets respectively in the <a href=https://en.wikipedia.org/wiki/American_Lacrosse_League>ACL-CASE 2021 competition</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.18/>IBM MNLP IE at CASE 2021 Task 1 : Multigranular and Multilingual Event Detection on Protest News<span class=acl-fixed-case>IBM</span> <span class=acl-fixed-case>MNLP</span> <span class=acl-fixed-case>IE</span> at <span class=acl-fixed-case>CASE</span> 2021 Task 1: Multigranular and Multilingual Event Detection on Protest News</a></strong><br><a href=/people/p/parul-awasthy/>Parul Awasthy</a>
|
<a href=/people/j/jian-ni/>Jian Ni</a>
|
<a href=/people/k/ken-barker/>Ken Barker</a>
|
<a href=/people/r/radu-florian/>Radu Florian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--18><div class="card-body p-3 small">In this paper, we present the event detection models and systems we have developed for Multilingual Protest News Detection-Shared Task 1 at CASE 2021. The shared task has 4 subtasks which cover event detection at different granularity levels (from document level to token level) and across multiple languages (English, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and Spanish). To handle data from multiple languages, we use a multilingual transformer-based language model (XLM-R) as the input text encoder. We apply a variety of techniques and build several transformer-based models that perform consistently well across all the subtasks and languages. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> achieve an average <a href=https://en.wikipedia.org/wiki/F1_(classification)>F_1 score</a> of 81.2. Out of thirteen subtask-language tracks, our submissions rank 1st in nine and 2nd in four tracks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.19/>ALEM at CASE 2021 Task 1 : Multilingual Text Classification on News Articles<span class=acl-fixed-case>ALEM</span> at <span class=acl-fixed-case>CASE</span> 2021 Task 1: Multilingual Text Classification on News Articles</a></strong><br><a href=/people/a/alaeddin-gurel/>Alaeddin Gürel</a>
|
<a href=/people/e/emre-emin/>Emre Emin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--19><div class="card-body p-3 small">We participated CASE shared task in ACL-IJCNLP 2021. This paper is a summary of our experiments and ideas about this shared task. For each subtask we shared our <a href=https://en.wikipedia.org/wiki/Methodology>approach</a>, successful and failed methods and our thoughts about them. We submit our results once for every subtask, except for subtask3, in task submission system and present scores based on our validation set formed from given training samples in this paper. Techniques and models we mentioned includes BERT, Multilingual BERT, <a href=https://en.wikipedia.org/wiki/Oversampling>oversampling</a>, <a href=https://en.wikipedia.org/wiki/Undersampling>undersampling</a>, <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> and their implications with each other. Most of the experiments we came up with were not completed, as time did not permit, but we share them here as we plan to do them as suggested in the future work part of document.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.case-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.21/>AMU-EURANOVA at CASE 2021 Task 1 : Assessing the stability of multilingual BERT<span class=acl-fixed-case>AMU</span>-<span class=acl-fixed-case>EURANOVA</span> at <span class=acl-fixed-case>CASE</span> 2021 Task 1: Assessing the stability of multilingual <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/l/leo-bouscarrat/>Léo Bouscarrat</a>
|
<a href=/people/a/antoine-bonnefoy/>Antoine Bonnefoy</a>
|
<a href=/people/c/cecile-capponi/>Cécile Capponi</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--21><div class="card-body p-3 small">This paper explains our participation in task 1 of the CASE 2021 shared task. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is about multilingual event extraction from <a href=https://en.wikipedia.org/wiki/News>news</a>. We focused on sub-task 4, event information extraction. This sub-task has a small training dataset and we fine-tuned a multilingual BERT to solve this <a href=https://en.wikipedia.org/wiki/Task_(computing)>sub-task</a>. We studied the instability problem on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and tried to mitigate it.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.22/>Team DaDeFrNi at CASE 2021 Task 1 : Document and Sentence Classification for Protest Event Detection<span class=acl-fixed-case>D</span>a<span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>F</span>r<span class=acl-fixed-case>N</span>i” at <span class=acl-fixed-case>CASE</span> 2021 Task 1: Document and Sentence Classification for Protest Event Detection</a></strong><br><a href=/people/f/francesco-re/>Francesco Re</a>
|
<a href=/people/d/daniel-vegh/>Daniel Vegh</a>
|
<a href=/people/d/dennis-atzenhofer/>Dennis Atzenhofer</a>
|
<a href=/people/n/niklas-stoehr/>Niklas Stoehr</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--22><div class="card-body p-3 small">This paper accompanies our top-performing submission to the CASE 2021 shared task, which is hosted at the workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text. Subtasks 1 and 2 of Task 1 concern the classification of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>newspaper articles</a> and sentences into conflict versus not conflict-related in four different languages. Our model performs competitively in both subtasks (up to 0.8662 macro F1), obtaining the highest score of all contributions for subtask 1 on Hindi articles (0.7877 macro F1). We describe all experiments conducted with the XLM-RoBERTa (XLM-R) model and report results obtained in each binary classification task. We propose supplementing the original training data with additional data on political conflict events. In addition, we provide an analysis of <a href=https://en.wikipedia.org/wiki/Grammatical_number>unigram probability estimates</a> and <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>geospatial references</a> contained within the original training corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.23/>Fine-grained Event Classification in News-like Text Snippets-Shared Task 2, CASE 2021<span class=acl-fixed-case>CASE</span> 2021</a></strong><br><a href=/people/j/jacek-haneczok/>Jacek Haneczok</a>
|
<a href=/people/g/guillaume-jacquet/>Guillaume Jacquet</a>
|
<a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/n/nicolas-stefanovitch/>Nicolas Stefanovitch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--23><div class="card-body p-3 small">This paper describes the Shared Task on Fine-grained Event Classification in News-like Text Snippets. The Shared Task is divided into three sub-tasks : (a) classification of text snippets reporting socio-political events (25 classes) for which vast amount of training data exists, although exhibiting different structure and style vis-a-vis test data, (b) enhancement to a generalized zero-shot learning problem, where 3 additional event types were introduced in advance, but without any training data (&#8216;unseen&#8217; classes), and (c) further extension, which introduced 2 additional event types, announced shortly prior to the evaluation phase. The reported Shared Task focuses on classification of events in English texts and is organized as part of the Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021), co-located with the ACL-IJCNLP 2021 Conference. Four teams participated in the task. Best performing systems for the three aforementioned sub-tasks achieved 83.9 %, 79.7 % and 77.1 % weighted F1 scores respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.case-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--case-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.case-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.case-1.25/>CASE 2021 Task 2 : Zero-Shot Classification of Fine-Grained Sociopolitical Events with Transformer Models<span class=acl-fixed-case>CASE</span> 2021 Task 2: Zero-Shot Classification of Fine-Grained Sociopolitical Events with Transformer Models</a></strong><br><a href=/people/b/benjamin-j-radford/>Benjamin J. Radford</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--case-1--25><div class="card-body p-3 small">We introduce a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for the classification of texts into fine-grained categories of sociopolitical events. This particular method is responsive to all three Subtasks of Task 2, Fine-Grained Classification of Socio-Political Events, introduced at the CASE workshop of ACL-IJCNLP 2021. We frame Task 2 as textual entailment : given an input text and a candidate event class (query), the model predicts whether the text describes an event of the given type. The <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is able to correctly classify in-sample event types with an average <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 0.74 but struggles with some out-of-sample event types. Despite this, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> shows promise for the zero-shot identification of certain sociopolitical events by achieving an F1-score of 0.52 on one wholly out-of-sample event class.</div></div></div><hr><div id=2021dialdoc-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dialdoc-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.dialdoc-1/>Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dialdoc-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dialdoc-1.0/>Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)</a></strong><br><a href=/people/s/song-feng/>Song Feng</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/m/malihe-alikhani/>Malihe Alikhani</a>
|
<a href=/people/h/he-he/>He He</a>
|
<a href=/people/y/yangfeng-ji/>Yangfeng Ji</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dialdoc-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dialdoc-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dialdoc-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dialdoc-1.4/>Automatic Learning Assistant in Telugu<span class=acl-fixed-case>T</span>elugu</a></strong><br><a href=/people/m/meghana-bommadi/>Meghana Bommadi</a>
|
<a href=/people/s/shreya-terupally/>Shreya Terupally</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dialdoc-1--4><div class="card-body p-3 small">This paper presents a learning assistant that tests one&#8217;s knowledge and gives feedback that helps a person learn at a faster pace. A learning assistant (based on automated question generation) has extensive uses in <a href=https://en.wikipedia.org/wiki/Education>education</a>, information websites, <a href=https://en.wikipedia.org/wiki/Self-assessment>self-assessment</a>, FAQs, testing ML agents, <a href=https://en.wikipedia.org/wiki/Research>research</a>, etc. Multiple researchers, and companies have worked on <a href=https://en.wikipedia.org/wiki/Virtual_Assistance>Virtual Assistance</a>, but majorly in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We built our learning assistant for <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu language</a> to help with teaching in the mother tongue, which is the most efficient way of learning. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is built primarily based on Question Generation in <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>. Many experiments were conducted on Question Generation in <a href=https://en.wikipedia.org/wiki/English_language>English</a> in multiple ways. We have built the first hybrid machine learning and rule-based solution in <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, which proves efficient for <a href=https://en.wikipedia.org/wiki/Short_story>short stories</a> or short passages in children&#8217;s books. Our work covers the fundamental question forms with question types : <a href=https://en.wikipedia.org/wiki/Adjective>adjective</a>, yes / no, <a href=https://en.wikipedia.org/wiki/Adverb>adverb</a>, <a href=https://en.wikipedia.org/wiki/Verb>verb</a>, when, where, whose, quotative, and quantitative (how many / how much). We constructed rules for question generation using Part of Speech (POS) tags and Universal Dependency (UD) tags along with linguistic information of the surrounding relevant context of the word. We used keyword matching, multilingual sentence embedding to evaluate the answer. Our system is primarily built on question generation in <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, and is also capable of evaluating the user&#8217;s answers to the generated questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dialdoc-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dialdoc-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dialdoc-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dialdoc-1.5/>Combining Open Domain Question Answering with a Task-Oriented Dialog System</a></strong><br><a href=/people/j/jan-nehring/>Jan Nehring</a>
|
<a href=/people/n/nils-feldhus/>Nils Feldhus</a>
|
<a href=/people/h/harleen-kaur/>Harleen Kaur</a>
|
<a href=/people/a/akhyar-ahmed/>Akhyar Ahmed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dialdoc-1--5><div class="card-body p-3 small">We apply the modular dialog system framework to combine open-domain question answering with a task-oriented dialog system. This meta dialog system can answer questions from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and at the same time act as a personal assistant. The aim of this <a href=https://en.wikipedia.org/wiki/System>system</a> is to combine the strength of an open-domain question answering system with the conversational power of task-oriented dialog systems. After explaining the technical details of the <a href=https://en.wikipedia.org/wiki/System>system</a>, we combined a new dataset out of standard datasets to evaluate the <a href=https://en.wikipedia.org/wiki/System>system</a>. We further introduce an evaluation method for this <a href=https://en.wikipedia.org/wiki/System>system</a>. Using this method, we compare the performance of the non-modular system with the performance of the modular system and show that the modular dialog system framework is very suitable for this combination of conversational agents and that the performance of each agent decreases only marginally through the modular setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dialdoc-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dialdoc-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dialdoc-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.dialdoc-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.dialdoc-1.6/>CAiRE in DialDoc21 : Data Augmentation for Information Seeking Dialogue System<span class=acl-fixed-case>CA</span>i<span class=acl-fixed-case>RE</span> in <span class=acl-fixed-case>D</span>ial<span class=acl-fixed-case>D</span>oc21: Data Augmentation for Information Seeking Dialogue System</a></strong><br><a href=/people/y/yan-xu/>Yan Xu</a>
|
<a href=/people/e/etsuko-ishii/>Etsuko Ishii</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/z/zihan-liu/>Zihan Liu</a>
|
<a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dialdoc-1--6><div class="card-body p-3 small">Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users&#8217; needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the task and thus achieve promising performance. In DialDoc21 competition, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved 74.95 F1 score and 60.74 Exact Match score in subtask 1, and 37.72 SacreBLEU score in subtask 2. Empirical analysis is provided to explain the effectiveness of our approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dialdoc-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dialdoc-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dialdoc-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dialdoc-1.13/>Team JARS : DialDoc Subtask 1-Improved Knowledge Identification with Supervised Out-of-Domain Pretraining<span class=acl-fixed-case>JARS</span>: <span class=acl-fixed-case>D</span>ial<span class=acl-fixed-case>D</span>oc Subtask 1 - Improved Knowledge Identification with Supervised Out-of-Domain Pretraining</a></strong><br><a href=/people/s/sopan-khosla/>Sopan Khosla</a>
|
<a href=/people/j/justin-lovelace/>Justin Lovelace</a>
|
<a href=/people/r/ritam-dutt/>Ritam Dutt</a>
|
<a href=/people/a/adithya-pratapa/>Adithya Pratapa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dialdoc-1--13><div class="card-body p-3 small">In this paper, we discuss our submission for DialDoc subtask 1. The subtask requires systems to extract knowledge from FAQ-type documents vital to reply to a user&#8217;s query in a conversational setting. We experiment with pretraining a BERT-based question-answering model on different QA datasets from MRQA, as well as conversational QA datasets like CoQA and QuAC. Our results show that models pretrained on CoQA and QuAC perform better than their counterparts that are pretrained on MRQA datasets. Our results also indicate that adding more pretraining data does not necessarily result in improved performance. Our final model, which is an ensemble of AlBERT-XL pretrained on CoQA and QuAC independently, with the chosen answer having the highest average probability score, achieves an F1-Score of 70.9 % on the official test-set.</div></div></div><hr><div id=2021ecnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ecnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.ecnlp-1/>Proceedings of The 4th Workshop on e-Commerce and NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ecnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ecnlp-1.0/>Proceedings of The 4th Workshop on e-Commerce and NLP</a></strong><br><a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/s/surya-kallumadi/>Surya Kallumadi</a>
|
<a href=/people/n/nicola-ueffing/>Nicola Ueffing</a>
|
<a href=/people/o/oleg-rokhlenko/>Oleg Rokhlenko</a>
|
<a href=/people/e/eugene-agichtein/>Eugene Agichtein</a>
|
<a href=/people/i/ido-guy/>Ido Guy</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ecnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ecnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ecnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ecnlp-1.4/>Turn-Level User Satisfaction Estimation in E-commerce Customer Service<span class=acl-fixed-case>E</span>-commerce Customer Service</a></strong><br><a href=/people/r/runze-liang/>Runze Liang</a>
|
<a href=/people/r/ryuichi-takanobu/>Ryuichi Takanobu</a>
|
<a href=/people/f/feng-lin-li/>Feng-Lin Li</a>
|
<a href=/people/j/ji-zhang/>Ji Zhang</a>
|
<a href=/people/h/haiqing-chen/>Haiqing Chen</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ecnlp-1--4><div class="card-body p-3 small">User satisfaction estimation in the dialogue-based customer service is critical not only for helping developers find the system defects, but also making it possible to get timely human intervention for dissatisfied customers. In this paper, we investigate the problem of user satisfaction estimation in E-commerce customer service. In order to apply the <a href=https://en.wikipedia.org/wiki/Estimator>estimator</a> to <a href=https://en.wikipedia.org/wiki/Online_service_provider>online services</a> for timely human intervention, we need to estimate the <a href=https://en.wikipedia.org/wiki/Customer_satisfaction>satisfaction score</a> at each turn. However, in actual scenario we can only collect the satisfaction labels for the whole <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue sessions</a> via <a href=https://en.wikipedia.org/wiki/User-generated_content>user feedback</a>. To this end, we formalize the turn-level satisfaction estimation as a reinforcement learning problem, in which the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be optimized with only session-level satisfaction labels. We conduct experiments on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> collected from a commercial customer service system, and compare our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning models</a>. Extensive experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> outperforms all the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ecnlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ecnlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ecnlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ecnlp-1.11/>Unsupervised Class-Specific Abstractive Summarization of Customer Reviews</a></strong><br><a href=/people/t/thi-nhat-anh-nguyen/>Thi Nhat Anh Nguyen</a>
|
<a href=/people/m/mingwei-shen/>Mingwei Shen</a>
|
<a href=/people/k/karen-hovsepian/>Karen Hovsepian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ecnlp-1--11><div class="card-body p-3 small">Large-scale unsupervised abstractive summarization is sorely needed to automatically scan millions of customer reviews in today&#8217;s fast-paced e-commerce landscape. We address a key challenge in unsupervised abstractive summarization reducing generic and uninformative content and producing useful information that relates to specific product aspects. To do so, we propose to model reviews in the context of some topical classes of interest. In particular, for any arbitrary set of topical classes of interest, the proposed model can learn to generate a set of class-specific summaries from multiple reviews of each product without ground-truth summaries, and the only required signal is class probabilities or class label for each review. The model combines a generative variational autoencoder, with an integrated class-correlation gating mechanism and a hierarchical structure capturing dependence among products, reviews and classes. Human evaluation shows that generated summaries are highly relevant, fluent, and representative. Evaluation using a reference dataset shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art abstractive and extractive baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.ecnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--ecnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.ecnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.ecnlp-1.15/>Detect Profane Language in <a href=https://en.wikipedia.org/wiki/Streaming_media>Streaming Services</a> to Protect Young Audiences</a></strong><br><a href=/people/j/jingxiang-chen/>Jingxiang Chen</a>
|
<a href=/people/k/kai-wei/>Kai Wei</a>
|
<a href=/people/x/xiang-hao/>Xiang Hao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--ecnlp-1--15><div class="card-body p-3 small">With the rapid growth of <a href=https://en.wikipedia.org/wiki/Streaming_media>online video streaming</a>, recent years have seen increasing concerns about <a href=https://en.wikipedia.org/wiki/Profanity>profane language</a> in their content. Detecting <a href=https://en.wikipedia.org/wiki/Profanity>profane language</a> in <a href=https://en.wikipedia.org/wiki/Streaming_media>streaming services</a> is challenging due to the long sentences appeared in a video. While recent research on handling long sentences has focused on developing deep learning modeling techniques, little work has focused on techniques on improving <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>data pipelines</a>. In this work, we develop a data collection pipeline to address long sequence of texts and integrate this <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> with a multi-head self-attention model. With this <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a>, our experiments show the self-attention model offers 12.5 % relative accuracy improvement over state-of-the-art distilBERT model on profane language detection while requiring only 3 % of parameters. This research designs a better <a href=https://en.wikipedia.org/wiki/System>system</a> for informing users of <a href=https://en.wikipedia.org/wiki/Profanity>profane language</a> in <a href=https://en.wikipedia.org/wiki/Streaming_media>video streaming services</a>.</div></div></div><hr><div id=2021gebnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.gebnlp-1/>Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.0/>Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing</a></strong><br><a href=/people/m/marta-costa-jussa/>Marta Costa-jussa</a>
|
<a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/k/kellie-webster/>Kellie Webster</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.2/>Gender Bias Hidden Behind Chinese Word Embeddings : The Case of Chinese Adjectives<span class=acl-fixed-case>C</span>hinese Word Embeddings: The Case of <span class=acl-fixed-case>C</span>hinese Adjectives</a></strong><br><a href=/people/m/meichun-jiao/>Meichun Jiao</a>
|
<a href=/people/z/ziyang-luo/>Ziyang Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--2><div class="card-body p-3 small">Gender bias in word embeddings gradually becomes a vivid research field in recent years. Most studies in this field aim at measurement and debiasing methods with <a href=https://en.wikipedia.org/wiki/English_language>English</a> as the target language. This paper investigates <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in static word embeddings from a unique perspective, <a href=https://en.wikipedia.org/wiki/Chinese_adjectives>Chinese adjectives</a>. By training word representations with different <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, the <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> behind the vectors of adjectives is assessed. Through a comparison between the produced results and a human scored data set, we demonstrate how gender bias encoded in word embeddings differentiates from people&#8217;s attitudes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.gebnlp-1.3.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.3/>Evaluating Gender Bias in Hindi-English Machine Translation<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Machine Translation</a></strong><br><a href=/people/k/krithika-ramesh/>Krithika Ramesh</a>
|
<a href=/people/g/gauri-gupta/>Gauri Gupta</a>
|
<a href=/people/s/sanjay-singh/>Sanjay Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--3><div class="card-body p-3 small">With <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indic languages</a>. In our work, we attempt to evaluate and quantify the <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. We also compare and contrast the resulting bias measurements across multiple <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for pre-trained embeddings and the ones learned by our machine translation model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.gebnlp-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.4/>Alexa, Google, Siri : What are Your Pronouns? Gender and Anthropomorphism in the Design and Perception of Conversational Assistants<span class=acl-fixed-case>A</span>lexa, <span class=acl-fixed-case>G</span>oogle, <span class=acl-fixed-case>S</span>iri: What are Your Pronouns? Gender and Anthropomorphism in the Design and Perception of Conversational Assistants</a></strong><br><a href=/people/g/gavin-abercrombie/>Gavin Abercrombie</a>
|
<a href=/people/a/amanda-cercas-curry/>Amanda Cercas Curry</a>
|
<a href=/people/m/mugdha-pandya/>Mugdha Pandya</a>
|
<a href=/people/v/verena-rieser/>Verena Rieser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--4><div class="card-body p-3 small">Technology companies have produced varied responses to concerns about the effects of the design of their conversational AI systems. Some have claimed that their voice assistants are in fact not gendered or human-likedespite design features suggesting the contrary. We compare these claims to user perceptions by analysing the pronouns they use when referring to AI assistants. We also examine systems&#8217; responses and the extent to which they generate output which is gendered and anthropomorphic. We find that, while some companies appear to be addressing the ethical concerns raised, in some cases, their claims do not seem to hold true. In particular, our results show that system outputs are ambiguous as to the humanness of the systems, and that users tend to personify and gender them as a result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.5/>Gender Bias in Text : Origin, Taxonomy, and Implications</a></strong><br><a href=/people/j/jad-doughman/>Jad Doughman</a>
|
<a href=/people/w/wael-khreich/>Wael Khreich</a>
|
<a href=/people/m/maya-el-gharib/>Maya El Gharib</a>
|
<a href=/people/m/maha-wiss/>Maha Wiss</a>
|
<a href=/people/z/zahraa-berjawi/>Zahraa Berjawi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--5><div class="card-body p-3 small">Gender inequality represents a considerable loss of human potential and perpetuates a culture of violence, higher gender wage gaps, and a lack of representation of women in higher and leadership positions. Applications powered by Artificial Intelligence (AI) are increasingly being used in the real world to provide critical decisions about who is going to be hired, granted a loan, admitted to college, etc. However, the main pillars of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI</a>, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning (ML)</a> have been shown to reflect and even amplify <a href=https://en.wikipedia.org/wiki/Gender_bias>gender biases</a> and stereotypes, which are mainly inherited from historical training data. In an effort to facilitate the identification and mitigation of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in English text, we develop a comprehensive taxonomy that relies on the following <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias types</a> : Generic Pronouns, <a href=https://en.wikipedia.org/wiki/Sexism>Sexism</a>, Occupational Bias, Exclusionary Bias, and <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a>. We also provide a bottom-up overview of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a>, from its societal origin to its spillover onto language. Finally, we link the societal implications of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> to their corresponding type(s) in the proposed <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomy</a>. The underlying motivation of our work is to help enable the technical community to identify and mitigate relevant biases from training corpora for improved fairness in NLP systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.6/>Sexism in the Judiciary : The Importance of Bias Definition in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> and In Our Courts<span class=acl-fixed-case>NLP</span> and In Our Courts</a></strong><br><a href=/people/n/noa-baker-gillis/>Noa Baker Gillis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--6><div class="card-body p-3 small">We analyze 6.7 million case law documents to determine the presence of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> within our <a href=https://en.wikipedia.org/wiki/Judiciary>judicial system</a>. We find that current bias detection methods in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> are insufficient to determine gender bias in our case law database and propose an alternative approach. We show that existing algorithms&#8217; inconsistent results are consequences of prior research&#8217;s inconsistent definitions of biases themselves. Bias detection algorithms rely on groups of words to represent <a href=https://en.wikipedia.org/wiki/Bias>bias</a> (e.g., &#8216;salary,&#8217; &#8216;job,&#8217; and &#8216;boss&#8217; to represent <a href=https://en.wikipedia.org/wiki/Employment>employment</a> as a potentially biased theme against women in text). However, the methods to build these groups of words have several weaknesses, primarily that the word lists are based on the researchers&#8217; own intuitions. We suggest two new methods of automating the creation of word lists to represent <a href=https://en.wikipedia.org/wiki/Bias>biases</a>. We find that our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> outperform current NLP bias detection methods. Our research improves the capabilities of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP technology</a> to detect <a href=https://en.wikipedia.org/wiki/Bias>bias</a> and highlights <a href=https://en.wikipedia.org/wiki/Sexism>gender biases</a> present in influential case law. In order to test our NLP bias detection method&#8217;s performance, we regress our results of bias in case law against <a href=https://en.wikipedia.org/wiki/United_States_Census>U.S census data</a> of women&#8217;s participation in the workforce in the last 100 years.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.10/>Investigating the Impact of <a href=https://en.wikipedia.org/wiki/Gender_representation>Gender Representation</a> in ASR Training Data : a Case Study on Librispeech<span class=acl-fixed-case>ASR</span> Training Data: a Case Study on Librispeech</a></strong><br><a href=/people/m/mahault-garnerin/>Mahault Garnerin</a>
|
<a href=/people/s/solange-rossato/>Solange Rossato</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--10><div class="card-body p-3 small">In this paper we question the impact of <a href=https://en.wikipedia.org/wiki/Gender_representation>gender representation</a> in training data on the performance of an end-to-end ASR system. We create an experiment based on the Librispeech corpus and build 3 different training corpora varying only the proportion of data produced by each gender category. We observe that if our system is overall robust to the gender balance or imbalance in training data, it is nonetheless dependant of the adequacy between the individuals present in the training and testing sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.gebnlp-1.11.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.gebnlp-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.11/>Generating Gender Augmented Data for NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/n/nishtha-jain/>Nishtha Jain</a>
|
<a href=/people/m/maja-popovic/>Maja Popović</a>
|
<a href=/people/d/declan-groves/>Declan Groves</a>
|
<a href=/people/e/eva-vanmassenhove/>Eva Vanmassenhove</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--11><div class="card-body p-3 small">Gender bias is a frequent occurrence in NLP-based applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of <a href=https://en.wikipedia.org/wiki/Bias>bias</a> becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable re-writing approach for short conversational sentences. The <a href=https://en.wikipedia.org/wiki/Rewriting>rewriting method</a> can be applied to sentences that, without <a href=https://en.wikipedia.org/wiki/Context_(language_use)>extra-sentential context</a>, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation system trained to &#8216;translate&#8217; from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results with respect to the automatic generation of gender alternatives for conversational sentences in Spanish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gebnlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gebnlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gebnlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gebnlp-1.12/>Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias Detection in Coreference Resolution<span class=acl-fixed-case>W</span>ino<span class=acl-fixed-case>B</span>ias (<span class=acl-fixed-case>S</span>o<span class=acl-fixed-case>W</span>ino<span class=acl-fixed-case>B</span>ias) Test Set for Latent Gender Bias Detection in Coreference Resolution</a></strong><br><a href=/people/h/hillary-dawkins/>Hillary Dawkins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gebnlp-1--12><div class="card-body p-3 small">We observe an instance of gender-induced bias in a downstream application, despite the absence of explicit gender words in the test cases. We provide a test set, SoWinoBias, for the purpose of measuring such latent gender bias in coreference resolution systems. We evaluate the performance of current debiasing methods on the SoWinoBias test set, especially in reference to the method&#8217;s design and altered embedding space properties. See https://github.com/hillary-dawkins/SoWinoBias.</div></div></div><hr><div id=2021gem-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gem-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.gem-1/>Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gem-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gem-1.0/>Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)</a></strong><br><a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/e/esin-durmus/>Esin Durmus</a>
|
<a href=/people/v/varun-prashant-gangal/>Varun Prashant Gangal</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/y/yacine-jernite/>Yacine Jernite</a>
|
<a href=/people/l/laura-perez-beltrachini/>Laura Perez-Beltrachini</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gem-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gem-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gem-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.gem-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.gem-1.3/>Semantic Similarity Based Evaluation for Abstractive News Summarization</a></strong><br><a href=/people/f/figen-beken-fikri/>Figen Beken Fikri</a>
|
<a href=/people/k/kemal-oflazer/>Kemal Oflazer</a>
|
<a href=/people/b/berrin-yanikoglu/>Berrin Yanikoglu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gem-1--3><div class="card-body p-3 small">ROUGE is a widely used evaluation metric in <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is not suitable for the evaluation of abstractive summarization systems as <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> relies on lexical overlap between the gold standard and the generated summaries. This limitation becomes more apparent for <a href=https://en.wikipedia.org/wiki/Agglutinative_language>agglutinative languages</a> with very large vocabularies and high type / token ratios. In this paper, we present semantic similarity models for <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> and apply them as evaluation metrics for an abstractive summarization task. To achieve this, we translated the English STSb dataset into <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> and presented the first semantic textual similarity dataset for <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a> as well. We showed that our best similarity models have better alignment with average human judgments compared to ROUGE in both Pearson and Spearman correlations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gem-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gem-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gem-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gem-1.10/>The GEM Benchmark : <a href=https://en.wikipedia.org/wiki/Natural-language_generation>Natural Language Generation</a>, its Evaluation and Metrics<span class=acl-fixed-case>GEM</span> Benchmark: Natural Language Generation, its Evaluation and Metrics</a></strong><br><a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/t/tosin-adewumi/>Tosin Adewumi</a>
|
<a href=/people/k/karmanya-aggarwal/>Karmanya Aggarwal</a>
|
<a href=/people/p/pawan-sasanka-ammanamanchi/>Pawan Sasanka Ammanamanchi</a>
|
<a href=/people/a/anuoluwapo-aremu/>Anuoluwapo Aremu</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/k/khyathi-raghavi-chandu/>Khyathi Raghavi Chandu</a>
|
<a href=/people/m/miruna-clinciu/>Miruna-Adriana Clinciu</a>
|
<a href=/people/d/dipanjan-das/>Dipanjan Das</a>
|
<a href=/people/k/kaustubh-dhole/>Kaustubh Dhole</a>
|
<a href=/people/w/wanyu-du/>Wanyu Du</a>
|
<a href=/people/e/esin-durmus/>Esin Durmus</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/c/chris-chinenye-emezue/>Chris Chinenye Emezue</a>
|
<a href=/people/v/varun-gangal/>Varun Gangal</a>
|
<a href=/people/c/cristina-garbacea/>Cristina Garbacea</a>
|
<a href=/people/t/tatsunori-b-hashimoto/>Tatsunori Hashimoto</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/y/yacine-jernite/>Yacine Jernite</a>
|
<a href=/people/h/harsh-jhamtani/>Harsh Jhamtani</a>
|
<a href=/people/y/yangfeng-ji/>Yangfeng Ji</a>
|
<a href=/people/s/shailza-jolly/>Shailza Jolly</a>
|
<a href=/people/m/mihir-kale/>Mihir Kale</a>
|
<a href=/people/d/dhruv-kumar/>Dhruv Kumar</a>
|
<a href=/people/f/faisal-ladhak/>Faisal Ladhak</a>
|
<a href=/people/a/aman-madaan/>Aman Madaan</a>
|
<a href=/people/m/mounica-maddela/>Mounica Maddela</a>
|
<a href=/people/k/khyati-mahajan/>Khyati Mahajan</a>
|
<a href=/people/s/saad-mahamood/>Saad Mahamood</a>
|
<a href=/people/b/bodhisattwa-prasad-majumder/>Bodhisattwa Prasad Majumder</a>
|
<a href=/people/p/pedro-henrique-martins/>Pedro Henrique Martins</a>
|
<a href=/people/a/angelina-mcmillan-major/>Angelina McMillan-Major</a>
|
<a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/e/emiel-van-miltenburg/>Emiel van Miltenburg</a>
|
<a href=/people/m/moin-nadeem/>Moin Nadeem</a>
|
<a href=/people/s/shashi-narayan/>Shashi Narayan</a>
|
<a href=/people/v/vitaly-nikolaev/>Vitaly Nikolaev</a>
|
<a href=/people/a/andre-niyongabo-rubungo/>Andre Niyongabo Rubungo</a>
|
<a href=/people/s/salomey-osei/>Salomey Osei</a>
|
<a href=/people/a/ankur-parikh/>Ankur Parikh</a>
|
<a href=/people/l/laura-perez-beltrachini/>Laura Perez-Beltrachini</a>
|
<a href=/people/n/niranjan-ramesh-rao/>Niranjan Ramesh Rao</a>
|
<a href=/people/v/vikas-raunak/>Vikas Raunak</a>
|
<a href=/people/j/juan-diego-rodriguez/>Juan Diego Rodriguez</a>
|
<a href=/people/s/sashank-santhanam/>Sashank Santhanam</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/t/thibault-sellam/>Thibault Sellam</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a>
|
<a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a>
|
<a href=/people/m/marco-antonio-sobrevilla-cabezudo/>Marco Antonio Sobrevilla Cabezudo</a>
|
<a href=/people/h/hendrik-strobelt/>Hendrik Strobelt</a>
|
<a href=/people/n/nishant-subramani/>Nishant Subramani</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a>
|
<a href=/people/a/akhila-yerukola/>Akhila Yerukola</a>
|
<a href=/people/j/jiawei-zhou/>Jiawei Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gem-1--10><div class="card-body p-3 small">We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, and human evaluation standards. Due to this moving target, new <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> often still evaluate on divergent anglo-centric corpora with well-established, but flawed, <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. This disconnect makes it challenging to identify the limitations of current <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and opportunities for progress. Addressing this limitation, <a href=https://en.wikipedia.org/wiki/Graphics_Environment_Manager>GEM</a> provides an environment in which <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> can easily be applied to a wide set of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and in which <a href=https://en.wikipedia.org/wiki/Evaluation_strategy>evaluation strategies</a> can be tested. Regular updates to the <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> will help NLG research become more multilingual and evolve the challenge alongside <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gem-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gem-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gem-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.gem-1.11/>Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation : A Case Study of the HuggingFace and GEM Data and Model Cards<span class=acl-fixed-case>H</span>ugging<span class=acl-fixed-case>F</span>ace and <span class=acl-fixed-case>GEM</span> Data and Model Cards</a></strong><br><a href=/people/a/angelina-mcmillan-major/>Angelina McMillan-Major</a>
|
<a href=/people/s/salomey-osei/>Salomey Osei</a>
|
<a href=/people/j/juan-diego-rodriguez/>Juan Diego Rodriguez</a>
|
<a href=/people/p/pawan-sasanka-ammanamanchi/>Pawan Sasanka Ammanamanchi</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/y/yacine-jernite/>Yacine Jernite</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gem-1--11><div class="card-body p-3 small">Developing documentation guidelines and easy-to-use templates for datasets and models is a challenging task, especially given the variety of backgrounds, skills, and incentives of the people involved in the building of natural language processing (NLP) tools. Nevertheless, the adoption of standard documentation practices across the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> promotes more accessible and detailed descriptions of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP datasets</a> and models, while supporting researchers and developers in reflecting on their work. To help with the standardization of documentation, we present two case studies of efforts that aim to develop reusable documentation templates the HuggingFace data card, a general purpose card for datasets in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, and the GEM benchmark data and model cards with a focus on natural language generation. We describe our process for developing these <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>templates</a>, including the identification of relevant stakeholder groups, the definition of a set of guiding principles, the use of existing <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>templates</a> as our foundation, and iterative revisions based on feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.gem-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--gem-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.gem-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.gem-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.gem-1.16/>Decoding Methods for Neural Narrative Generation</a></strong><br><a href=/people/a/alexandra-delucia/>Alexandra DeLucia</a>
|
<a href=/people/a/aaron-mueller/>Aaron Mueller</a>
|
<a href=/people/x/xiang-lisa-li/>Xiang Lisa Li</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--gem-1--16><div class="card-body p-3 small">Narrative generation is an open-ended NLP task in which a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> generates a story given a prompt. The task is similar to neural response generation for <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> ; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparametersspecifically, maximum mutual informationanalyzing results over multiple criteria with automatic and human evaluation. We find that (1) <a href=https://en.wikipedia.org/wiki/Nucleic_acid_test>nucleus sampling</a> is generally best with <a href=https://en.wikipedia.org/wiki/Statistical_threshold>thresholds</a> between 0.7 and 0.9 ; (2) a maximum mutual information objective can improve the quality of generated stories ; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any <a href=https://en.wikipedia.org/wiki/Qualitative_property>qualitative metric</a>.</div></div></div><hr><div id=2021internlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.internlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.internlp-1/>Proceedings of the First Workshop on Interactive Learning for Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.internlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.internlp-1.0/>Proceedings of the First Workshop on Interactive Learning for Natural Language Processing</a></strong><br><a href=/people/k/kiante-brantley/>Kianté Brantley</a>
|
<a href=/people/s/soham-dan/>Soham Dan</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/j/ji-ung-lee/>Ji-Ung Lee</a>
|
<a href=/people/f/filip-radlinski/>Filip Radlinski</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a>
|
<a href=/people/e/edwin-simpson/>Edwin Simpson</a>
|
<a href=/people/l/lili-yu/>Lili Yu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.internlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--internlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.internlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.internlp-1.2/>Apple Core-dination : Linguistic Feedback and Learning in a Speech-to-Action Shared World Game</a></strong><br><a href=/people/s/susann-boy/>Susann Boy</a>
|
<a href=/people/a/ariaray-brown/>AriaRay Brown</a>
|
<a href=/people/m/morgan-wixted/>Morgan Wixted</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--internlp-1--2><div class="card-body p-3 small">We investigate the question of how adaptive feedback from a <a href=https://en.wikipedia.org/wiki/Virtual_agent>virtual agent</a> impacts the linguistic input of the user in a shared world game environment. To do so, we carry out an exploratory pilot study to observe how individualized linguistic feedback affects the user&#8217;s speech input. We introduce a speech-controlled game, Apple Core-dination, in which an agent learns complex tasks using a base knowledge of simple actions. The <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> is equipped with a <a href=https://en.wikipedia.org/wiki/Machine_learning>learning mechanism</a> for mapping new commands to sequences of simple actions, as well as the ability to incorporate <a href=https://en.wikipedia.org/wiki/User_(computing)>user input</a> into written responses. The <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> repeatedly shares its internal knowledge state by responding to what it knows and does not know about <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>language meaning</a> and the shared environment. Our paper focuses on the linguistic feedback loop in order to analyze the nature of <a href=https://en.wikipedia.org/wiki/Input_(computer_science)>user input</a>. Feedback from the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> is provided in the form of <a href=https://en.wikipedia.org/wiki/Motion>visual movement</a> and <a href=https://en.wikipedia.org/wiki/Writing>written linguistic responses</a>. Particular attention is given to incorporating user input into agent responses and updating the speech-to-action mappings based on commands provided by the user. Through our pilot study, we analyze task success and compare the <a href=https://en.wikipedia.org/wiki/Lexicon>lexical features</a> of user input. Results show variation in input length and <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>lexical variety</a> across users, suggesting a correlation between the two that can be studied further.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.internlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--internlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.internlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.internlp-1.4/>A Proposal : Interactively Learning to Summarise Timelines by <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/y/yuxuan-ye/>Yuxuan Ye</a>
|
<a href=/people/e/edwin-simpson/>Edwin Simpson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--internlp-1--4><div class="card-body p-3 small">Timeline Summarisation (TLS) aims to generate a concise, time-ordered list of events described in sources such as <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. However, current <a href=https://en.wikipedia.org/wiki/System>systems</a> do not provide an adequate way to adapt to new domains nor to focus on the aspects of interest to a particular user. Therefore, we propose a method for interactively learning abstractive TLS using Reinforcement Learning (RL). We define a compound reward function and use RL to fine-tune an abstractive Multi-document Summarisation (MDS) model, which avoids the need to train using reference summaries. One of the sub-reward functions will be learned interactively from user feedback to ensure the consistency between users&#8217; demands and the generated <a href=https://en.wikipedia.org/wiki/Timeline>timeline</a>. The other sub-reward functions contribute to topical coherence and linguistic fluency. We plan experiments to evaluate whether our approach could generate accurate and precise <a href=https://en.wikipedia.org/wiki/Timeline>timelines</a> tailored for each user.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.internlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--internlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.internlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.internlp-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.internlp-1.5/>Dynamic Facet Selection by Maximizing Graded Relevance</a></strong><br><a href=/people/m/michael-glass/>Michael Glass</a>
|
<a href=/people/m/md-faisal-mahbub-chowdhury/>Md Faisal Mahbub Chowdhury</a>
|
<a href=/people/y/yu-deng/>Yu Deng</a>
|
<a href=/people/r/ruchi-mahindru/>Ruchi Mahindru</a>
|
<a href=/people/n/nicolas-rodolfo-fauceglia/>Nicolas Rodolfo Fauceglia</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a>
|
<a href=/people/n/nandana-mihindukulasooriya/>Nandana Mihindukulasooriya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--internlp-1--5><div class="card-body p-3 small">Dynamic faceted search (DFS), an interactive query refinement technique, is a form of Humancomputer information retrieval (HCIR) approach. It allows users to narrow down search results through facets, where the facets-documents mapping is determined at runtime based on the context of user query instead of pre-indexing the facets statically. In this paper, we propose a new unsupervised approach for dynamic facet generation, namely optimistic facets, which attempts to generate the best possible subset of facets, hence maximizing expected Discounted Cumulative Gain (DCG), a measure of ranking quality that uses a graded relevance scale. We also release code to generate a new evaluation dataset. Through empirical results on two datasets, we show that the proposed DFS approach considerably improves the document ranking in the search results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.internlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--internlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.internlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.internlp-1.6/>Active Curriculum Learning</a></strong><br><a href=/people/b/borna-jafarpour/>Borna Jafarpour</a>
|
<a href=/people/d/dawn-sepehr/>Dawn Sepehr</a>
|
<a href=/people/n/nick-pogrebnyakov/>Nick Pogrebnyakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--internlp-1--6><div class="card-body p-3 small">This paper investigates and reveals the relationship between two closely related machine learning disciplines, namely Active Learning (AL) and Curriculum Learning (CL), from the lens of several novel <a href=https://en.wikipedia.org/wiki/Curriculum>curricula</a>. This paper also introduces Active Curriculum Learning (ACL) which improves AL by combining AL with CL to benefit from the dynamic nature of the AL informativeness concept as well as the human insights used in the design of the curriculum heuristics. Comparison of the performance of ACL and AL on two public datasets for the Named Entity Recognition (NER) task shows the effectiveness of combining AL and CL using our proposed framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.internlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--internlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.internlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.internlp-1.7/>Tackling Fake News Detection by Interactively Learning Representations using Graph Neural Networks</a></strong><br><a href=/people/n/nikhil-mehta/>Nikhil Mehta</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--internlp-1--7><div class="card-body p-3 small">Easy access, variety of content, and fast widespread interactions are some of the reasons that have made <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> increasingly popular in today&#8217;s society. However, this has also enabled the widespread propagation of fake news, text that is published with an intent to spread <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> and sway beliefs. Detecting fake news is important to prevent <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> and maintain a healthy society. While prior works have tackled this problem by building <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning systems</a>, automatedly modeling the social media landscape that enables the spread of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> is challenging. On the contrary, having humans fact check all news is not scalable. Thus, in this paper, we propose to approach this problem interactively, where human insight can be continually combined with an automated system, enabling better social media representation quality. Our experiments show performance improvements in this setting.<i>interactively</i>, where human insight can be continually combined with an automated system, enabling better social media representation quality. Our experiments show performance improvements in this setting.</div></div></div><hr><div id=2021iwpt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwpt-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.iwpt-1/>Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwpt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwpt-1.0/>Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021)</a></strong><br><a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/k/kenji-sagae/>Kenji Sagae</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a>
|
<a href=/people/g/gosse-bouma/>Gosse Bouma</a>
|
<a href=/people/d/djame-seddah/>Djamé Seddah</a>
|
<a href=/people/d/daniel-zeman/>Daniel Zeman</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwpt-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwpt-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwpt-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwpt-1.8/>A Falta de Pan, Buenas Son Tortas : The Efficacy of Predicted UPOS Tags for Low Resource UD Parsing<span class=acl-fixed-case>UPOS</span> Tags for Low Resource <span class=acl-fixed-case>UD</span> Parsing</a></strong><br><a href=/people/m/mark-anderson/>Mark Anderson</a>
|
<a href=/people/m/mathieu-dehouck/>Mathieu Dehouck</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwpt-1--8><div class="card-body p-3 small">We evaluate the efficacy of predicted UPOS tags as input features for dependency parsers in lower resource settings to evaluate how treebank size affects the impact tagging accuracy has on parsing performance. We do this for real low resource universal dependency treebanks, artificially low resource data with varying treebank sizes, and for very small treebanks with varying amounts of augmented data. We find that predicted UPOS tags are somewhat helpful for low resource treebanks, especially when fewer fully-annotated trees are available. We also find that this positive impact diminishes as the amount of data increases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwpt-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwpt-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwpt-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwpt-1.9/>Multilingual Dependency Parsing for Low-Resource African Languages : Case Studies on <a href=https://en.wikipedia.org/wiki/Bambara_language>Bambara</a>, <a href=https://en.wikipedia.org/wiki/Wolof_language>Wolof</a>, and <a href=https://en.wikipedia.org/wiki/Yoruba_language>Yoruba</a><span class=acl-fixed-case>A</span>frican Languages: Case Studies on <span class=acl-fixed-case>B</span>ambara, <span class=acl-fixed-case>W</span>olof, and <span class=acl-fixed-case>Y</span>oruba</a></strong><br><a href=/people/c/cheikh-m-bamba-dione/>Cheikh M. Bamba Dione</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwpt-1--9><div class="card-body p-3 small">This paper describes a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for syntactic knowledge transfer between high-resource languages to extremely low-resource languages. The methodology consists in leveraging multilingual BERT self-attention model pretrained on large datasets to develop a multilingual multi-task model that can predict Universal Dependencies annotations for three African low-resource languages. The UD annotations include universal part-of-speech, <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a>, <a href=https://en.wikipedia.org/wiki/Lemma_(morphology)>lemmas</a>, and dependency trees. In our experiments, we used multilingual word embeddings and a total of 11 Universal Dependencies treebanks drawn from three high-resource languages (English, French, Norwegian) and three low-resource languages (Bambara, Wolof and Yoruba). We developed various <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to test specific language combinations involving <a href=https://en.wikipedia.org/wiki/Language_contact>contemporary contact languages</a> or <a href=https://en.wikipedia.org/wiki/Genetic_relationship_(linguistics)>genetically related languages</a>. The results of the experiments show that multilingual models that involve high-resource languages and low-resource languages with contemporary contact between each other can provide better results than combinations that only include unrelated languages. As far genetic relationships are concerned, we could not draw any conclusion regarding the impact of language combinations involving the selected low-resource languages, namely <a href=https://en.wikipedia.org/wiki/Wolof_language>Wolof</a> and <a href=https://en.wikipedia.org/wiki/Yoruba_language>Yoruba</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwpt-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwpt-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwpt-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwpt-1.16/>COMBO : A New Module for EUD Parsing<span class=acl-fixed-case>COMBO</span>: A New Module for <span class=acl-fixed-case>EUD</span> Parsing</a></strong><br><a href=/people/m/mateusz-klimaszewski/>Mateusz Klimaszewski</a>
|
<a href=/people/a/alina-wroblewska/>Alina Wróblewska</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwpt-1--16><div class="card-body p-3 small">We introduce the COMBO-based approach for EUD parsing and its implementation, which took part in the IWPT 2021 EUD shared task. The goal of this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is to parse raw texts in 17 languages into Enhanced Universal Dependencies (EUD). The proposed approach uses COMBO to predict UD trees and EUD graphs. These <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>structures</a> are then merged into the final EUD graphs. Some EUD edge labels are extended with <a href=https://en.wikipedia.org/wiki/Case_(linguistics)>case information</a> using a single language-independent expansion rule. In the official evaluation, the solution ranked fourth, achieving an average ELAS of 83.79 %. The source code is available at https://gitlab.clarin-pl.eu/syntactic-tools/combo.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwpt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwpt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwpt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwpt-1.17/>Splitting EUD Graphs into Trees : A Quick and Clatty Approach<span class=acl-fixed-case>EUD</span> Graphs into Trees: A Quick and Clatty Approach</a></strong><br><a href=/people/m/mark-anderson/>Mark Anderson</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwpt-1--17><div class="card-body p-3 small">We present the system submission from the FASTPARSE team for the EUD Shared Task at IWPT 2021. We engaged in the task last year by focusing on <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>. This year we have focused on experimenting with new ideas on a limited time budget. Our system is based on splitting the EUD graph into several <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a>, based on <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic criteria</a>. We predict these <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a> using a sequence-labelling parser and combine them into an EUD graph. The results were relatively poor, although not a total disaster and could probably be improved with some polishing of the system&#8217;s rough edges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwpt-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwpt-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwpt-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwpt-1.18/>Graph Rewriting for Enhanced Universal Dependencies<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/b/bruno-guillaume/>Bruno Guillaume</a>
|
<a href=/people/g/guy-perrier/>Guy Perrier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwpt-1--18><div class="card-body p-3 small">This paper describes a system proposed for the IWPT 2021 Shared Task on <a href=https://en.wikipedia.org/wiki/Parsing>Parsing</a> into Enhanced Universal Dependencies (EUD). We propose a Graph Rewriting based system for computing Enhanced Universal Dependencies, given the Basic Universal Dependencies (UD).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwpt-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwpt-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwpt-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwpt-1.19/>Biaffine Dependency and Semantic Graph Parsing for EnhancedUniversal Dependencies<span class=acl-fixed-case>E</span>nhanced<span class=acl-fixed-case>U</span>niversal Dependencies</a></strong><br><a href=/people/g/giuseppe-attardi/>Giuseppe Attardi</a>
|
<a href=/people/d/daniele-sartiano/>Daniele Sartiano</a>
|
<a href=/people/m/maria-simi/>Maria Simi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwpt-1--19><div class="card-body p-3 small">This paper presents the <a href=https://en.wikipedia.org/wiki/System>system</a> used in our submission to the IWPT 2021 Shared Task. This year the official evaluation metrics was <a href=https://en.wikipedia.org/wiki/ELAS>ELAS</a>, therefore dependency parsing might have been avoided as well as other pipeline stages like POS tagging and <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a>. We nevertheless chose to deploy a combination of a dependency parser and a graph parser. The dependency parser is a biaffine parser, that uses transformers for representing input sentences, with no other feature. The graph parser is a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> that exploits a similar architecture except for using a sigmoid crossentropy loss function to return multiple values for the predicted arcs. The final output is obtained by merging the output of the two <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. The dependency parser achieves top or close to top LAS performance with respect to other systems that report results on such metrics, except on low resource languages (Tamil, Estonian, Latvian).<i>IWPT 2021 Shared Task</i>. This year the official evaluation metrics was ELAS, therefore dependency parsing might have been avoided as well as other pipeline stages like POS tagging and lemmatization. We nevertheless chose to deploy a combination of a dependency parser and a graph parser. The dependency parser is a biaffine parser, that uses transformers for representing input sentences, with no other feature. The graph parser is a semantic parser that exploits a similar architecture except for using a sigmoid crossentropy loss function to return multiple values for the predicted arcs. The final output is obtained by merging the output of the two parsers. The dependency parser achieves top or close to top LAS performance with respect to other systems that report results on such metrics, except on low resource languages (Tamil, Estonian, Latvian).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwpt-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwpt-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwpt-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwpt-1.21/>RobertNLP at the IWPT 2021 Shared Task : Simple Enhanced UD Parsing for 17 Languages<span class=acl-fixed-case>R</span>obert<span class=acl-fixed-case>NLP</span> at the <span class=acl-fixed-case>IWPT</span> 2021 Shared Task: Simple Enhanced <span class=acl-fixed-case>UD</span> Parsing for 17 Languages</a></strong><br><a href=/people/s/stefan-grunewald/>Stefan Grünewald</a>
|
<a href=/people/f/frederik-tobias-oertel/>Frederik Tobias Oertel</a>
|
<a href=/people/a/annemarie-friedrich/>Annemarie Friedrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwpt-1--21><div class="card-body p-3 small">This paper presents our multilingual dependency parsing system as used in the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies. Our system consists of an unfactorized biaffine classifier that operates directly on fine-tuned XLM-R embeddings and generates enhanced UD graphs by predicting the best dependency label (or absence of a dependency) for each pair of tokens. To avoid sparsity issues resulting from lexicalized dependency labels, we replace lexical items in relations with placeholders at training and prediction time, later retrieving them from the parse via a hybrid rule-based / machine-learning system. In addition, we utilize model ensembling at prediction time. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves high parsing accuracy on the <a href=https://en.wikipedia.org/wiki/Blinded_experiment>blind test data</a>, ranking 3rd out of 9 with an average ELAS F1 score of 86.97.</div></div></div><hr><div id=2021iwslt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.iwslt-1/>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.0/>Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021)</a></strong><br><a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stuker</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.1/>FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN<span class=acl-fixed-case>FINDINGS</span> <span class=acl-fixed-case>OF</span> <span class=acl-fixed-case>THE</span> <span class=acl-fixed-case>IWSLT</span> 2021 <span class=acl-fixed-case>EVALUATION</span> <span class=acl-fixed-case>CAMPAIGN</span></a></strong><br><a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/j/jacob-bremerman/>Jacob Bremerman</a>
|
<a href=/people/r/roldano-cattoni/>Roldano Cattoni</a>
|
<a href=/people/m/maha-elbayad/>Maha Elbayad</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/x/xutai-ma/>Xutai Ma</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a>
|
<a href=/people/c/changhan-wang/>Changhan Wang</a>
|
<a href=/people/m/matthew-wiesner/>Matthew Wiesner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--1><div class="card-body p-3 small">The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2021) featured this year four shared tasks : (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Multilingual speech translation, (iv) Low-resource speech translation. A total of 22 teams participated in at least one of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. This paper describes each shared task, data and evaluation metrics, and reports results of the received submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.3/>NAIST English-to-Japanese Simultaneous Translation System for IWSLT 2021 Simultaneous Text-to-text Task<span class=acl-fixed-case>NAIST</span> <span class=acl-fixed-case>E</span>nglish-to-<span class=acl-fixed-case>J</span>apanese Simultaneous Translation System for <span class=acl-fixed-case>IWSLT</span> 2021 Simultaneous Text-to-text Task</a></strong><br><a href=/people/r/ryo-fukuda/>Ryo Fukuda</a>
|
<a href=/people/y/yui-oka/>Yui Oka</a>
|
<a href=/people/y/yasumasa-kano/>Yasumasa Kano</a>
|
<a href=/people/y/yuki-yano/>Yuki Yano</a>
|
<a href=/people/y/yuka-ko/>Yuka Ko</a>
|
<a href=/people/h/hirotaka-tokuyama/>Hirotaka Tokuyama</a>
|
<a href=/people/k/kosuke-doi/>Kosuke Doi</a>
|
<a href=/people/s/sakriani-sakti/>Sakriani Sakti</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--3><div class="card-body p-3 small">This paper describes NAIST&#8217;s system for the English-to-Japanese Simultaneous Text-to-text Translation Task in IWSLT 2021 Evaluation Campaign. Our primary submission is based on wait-k neural machine translation with sequence-level knowledge distillation to encourage <a href=https://en.wikipedia.org/wiki/Literal_translation>literal translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.7/>THE IWSLT 2021 BUT SPEECH TRANSLATION SYSTEMS<span class=acl-fixed-case>THE</span> <span class=acl-fixed-case>IWSLT</span> 2021 <span class=acl-fixed-case>BUT</span> <span class=acl-fixed-case>SPEECH</span> <span class=acl-fixed-case>TRANSLATION</span> <span class=acl-fixed-case>SYSTEMS</span></a></strong><br><a href=/people/h/hari-krishna-vydana/>hari Krishna Vydana</a>
|
<a href=/people/m/martin-karafiat/>Martin Karafiat</a>
|
<a href=/people/l/lukas-burget/>Lukas Burget</a>
|
<a href=/people/j/jan-cernocky/>Jan Černocký</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--7><div class="card-body p-3 small">The paper describes BUT&#8217;s English to German offline speech translation (ST) systems developed for IWSLT2021. They are based on jointly trained Automatic Speech Recognition-Machine Translation models. Their performances is evaluated on MustC-Common test set. In this work, we study their efficiency from the perspective of having a large amount of separate ASR training data and MT training data, and a smaller amount of speech-translation training data. Large amounts of ASR and MT training data are utilized for pre-training the ASR and MT models. Speech-translation data is used to jointly optimize ASR-MT models by defining an end-to-end differentiable path from speech to translations. For this purpose, we use the internal continuous representations from the ASR-decoder as the input to MT module. We show that <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a> can be further improved by training the ASR-decoder jointly with the MT-module using large amount of text-only MT training data. We also show significant improvements by training an ASR module capable of generating punctuated text, rather than leaving the punctuation task to the MT module.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.8/>Dealing with training and test segmentation mismatch : FBK@IWSLT2021<span class=acl-fixed-case>FBK</span>@<span class=acl-fixed-case>IWSLT</span>2021</a></strong><br><a href=/people/s/sara-papi/>Sara Papi</a>
|
<a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--8><div class="card-body p-3 small">This paper describes FBK&#8217;s system submission to the IWSLT 2021 Offline Speech Translation task. We participated with a direct model, which is a Transformer-based architecture trained to translate English speech audio data into German texts. The training pipeline is characterized by knowledge distillation and a two-step fine-tuning procedure. Both knowledge distillation and the first fine-tuning step are carried out on manually segmented real and synthetic data, the latter being generated with an MT system trained on the available corpora. Differently, the second fine-tuning step is carried out on a random segmentation of the MuST-C v2 En-De dataset. Its main goal is to reduce the performance drops occurring when a <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech translation model</a> trained on manually segmented data (i.e. an ideal, sentence-like segmentation) is evaluated on <a href=https://en.wikipedia.org/wiki/Audio_signal_processing>automatically segmented audio</a> (i.e. actual, more realistic testing conditions). For the same purpose, a custom hybrid segmentation procedure that accounts for both audio content (pauses) and for the length of the produced segments is applied to the test data before passing them to the system. At inference time, we compared this <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedure</a> with a baseline segmentation method based on Voice Activity Detection (VAD). Our results indicate the effectiveness of the proposed hybrid approach, shown by a reduction of the gap with manual segmentation from 8.3 to 1.4 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.iwslt-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.11/>End-to-End Speech Translation with Pre-trained Models and Adapters : UPC at IWSLT 2021<span class=acl-fixed-case>UPC</span> at <span class=acl-fixed-case>IWSLT</span> 2021</a></strong><br><a href=/people/g/gerard-i-gallego/>Gerard I. Gállego</a>
|
<a href=/people/i/ioannis-tsiamas/>Ioannis Tsiamas</a>
|
<a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--11><div class="card-body p-3 small">This paper describes the submission to the IWSLT 2021 offline speech translation task by the UPC Machine Translation group. The task consists of building a <a href=https://en.wikipedia.org/wiki/System>system</a> capable of translating <a href=https://en.wikipedia.org/wiki/English_language>English audio recordings</a> extracted from <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED talks</a> into <a href=https://en.wikipedia.org/wiki/German_language>German text</a>. Submitted systems can be either cascade or end-to-end and use a custom or given segmentation. Our submission is an end-to-end speech translation system, which combines pre-trained models (Wav2Vec 2.0 and mBART) with coupling modules between the encoder and decoder, and uses an efficient fine-tuning technique, which trains only 20 % of its total parameters. We show that adding an Adapter to the system and pre-training it, can increase the <a href=https://en.wikipedia.org/wiki/Convergence_rate>convergence speed</a> and the final result, with which we achieve a BLEU score of 27.3 on the MuST-C test set. Our final <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> that obtains 28.22 BLEU score on the same set. Our submission also uses a custom segmentation algorithm that employs pre-trained Wav2Vec 2.0 for identifying periods of untranscribable text and can bring improvements of 2.5 to 3 BLEU score on the IWSLT 2019 test set, as compared to the result with the given segmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.15/>Maastricht University’s Multilingual Speech Translation System for IWSLT 2021<span class=acl-fixed-case>IWSLT</span> 2021</a></strong><br><a href=/people/d/danni-liu/>Danni Liu</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--15><div class="card-body p-3 small">This paper describes Maastricht University&#8217;s participation in the IWSLT 2021 multilingual speech translation track. The task in this track is to build multilingual speech translation systems in supervised and zero-shot directions. Our primary system is an <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end model</a> that performs both <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>speech transcription</a> and <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We observe that the joint training for the two tasks is complementary especially when the speech translation data is scarce. On the source and target side, we use <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> and pseudo-labels respectively to improve the performance of our <a href=https://en.wikipedia.org/wiki/System>systems</a>. We also introduce an ensembling technique that consistently improves the quality of transcriptions and <a href=https://en.wikipedia.org/wiki/Translation>translations</a>. The experiments show that the <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end system</a> is competitive with its cascaded counterpart especially in zero-shot conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.16/>ZJU’s IWSLT 2021 Speech Translation System<span class=acl-fixed-case>ZJU</span>’s <span class=acl-fixed-case>IWSLT</span> 2021 Speech Translation System</a></strong><br><a href=/people/l/linlin-zhang/>Linlin Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--16><div class="card-body p-3 small">In this paper, we describe Zhejiang University&#8217;s submission to the IWSLT2021 Multilingual Speech Translation Task. This task focuses on speech translation (ST) research across many non-English source languages. Participants can decide whether to work on constrained systems or unconstrained systems which can using external data. We create both cascaded and end-to-end speech translation constrained systems, using the provided data only. In the cascaded approach, we combine Conformer-based automatic speech recognition (ASR) with the Transformer-based neural machine translation (NMT). Our end-to-end direct speech translation systems use ASR pretrained encoder and multi-task decoders. The submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> are ensembled by different cascaded models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.17/>Multilingual Speech Translation with Unified Transformer : Huawei Noah’s Ark Lab at IWSLT 2021<span class=acl-fixed-case>N</span>oah’s Ark Lab at <span class=acl-fixed-case>IWSLT</span> 2021</a></strong><br><a href=/people/x/xingshan-zeng/>Xingshan Zeng</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--17><div class="card-body p-3 small">This paper describes the system submitted to the IWSLT 2021 Multilingual Speech Translation (MultiST) task from Huawei Noah&#8217;s Ark Lab. We use a unified transformer architecture for our MultiST model, so that the data from different modalities (i.e., speech and text) and different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> (i.e., <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Recognition</a>, <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>, and Speech Translation) can be exploited to enhance the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s ability. Specifically, speech and text inputs are firstly fed to different feature extractors to extract acoustic and textual features, respectively. Then, these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> are processed by a shared encoderdecoder architecture. We apply several training techniques to improve the performance, including <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, task-level curriculum learning, <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, etc. Our final system achieves significantly better results than bilingual baselines on supervised language pairs and yields reasonable results on zero-shot language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.18/>Multilingual Speech Translation KIT @ IWSLT2021<span class=acl-fixed-case>KIT</span> @ <span class=acl-fixed-case>IWSLT</span>2021</a></strong><br><a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/t/tuan-nam-nguyen/>Tuan Nam Nguyen</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a>
|
<a href=/people/d/dan-he/>Dan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--18><div class="card-body p-3 small">This paper contains the description for the submission of Karlsruhe Institute of Technology (KIT) for the multilingual TEDx translation task in the IWSLT 2021 evaluation campaign. Our main approach is to develop both cascade and end-to-end systems and eventually combine them together to achieve the best possible results for this extremely low-resource setting. The report also confirms certain consistent architectural improvement added to the Transformer architecture, for all tasks : <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcription</a> and speech translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.iwslt-1.26" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.26/>Between Flexibility and Consistency : Joint Generation of Captions and Subtitles</a></strong><br><a href=/people/a/alina-karakanta/>Alina Karakanta</a>
|
<a href=/people/m/marco-gaido/>Marco Gaido</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--26><div class="card-body p-3 small">Speech translation (ST) has lately received growing interest for the generation of subtitles without the need for an intermediate source language transcription and timing (i.e. captions). However, the joint generation of source captions and target subtitles does not only bring potential output quality advantages when the two decoding processes inform each other, but it is also often required in multilingual scenarios. In this work, we focus on ST models which generate consistent captions-subtitles in terms of <a href=https://en.wikipedia.org/wiki/Structure>structure</a> and <a href=https://en.wikipedia.org/wiki/Content_(media)>lexical content</a>. We further introduce new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for evaluating subtitling consistency. Our findings show that joint decoding leads to increased performance and consistency between the generated captions and <a href=https://en.wikipedia.org/wiki/Subtitle_(titling)>subtitles</a> while still allowing for sufficient flexibility to produce <a href=https://en.wikipedia.org/wiki/Subtitle_(titling)>subtitles</a> conforming to language-specific needs and norms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.28/>Inverted Projection for Robust Speech Translation</a></strong><br><a href=/people/d/dirk-padfield/>Dirk Padfield</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--28><div class="card-body p-3 small">Traditional translation systems trained on written documents perform well for text-based translation but not as well for speech-based applications. We aim to adapt translation models to speech by introducing actual lexical errors from ASR and segmentation errors from automatic punctuation into our translation training data. We introduce an inverted projection approach that projects automatically detected system segments onto human transcripts and then re-segments the gold translations to align with the projected human transcripts. We demonstrate that this overcomes the train-test mismatch present in other training approaches. The new projection approach achieves gains of over 1 BLEU point over a baseline that is exposed to the human transcripts and segmentations, and these gains hold for both IWSLT data and <a href=https://en.wikipedia.org/wiki/YouTube>YouTube data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.iwslt-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--iwslt-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.iwslt-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.iwslt-1.29/>Towards the evaluation of automatic simultaneous speech translation from a communicative perspective</a></strong><br><a href=/people/c/claudio-fantinuoli/>Claudio Fantinuoli</a>
|
<a href=/people/b/bianca-prandi/>Bianca Prandi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--iwslt-1--29><div class="card-body p-3 small">In recent years, automatic speech-to-speech and speech-to-text translation has gained momentum thanks to advances in <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a>, especially in the domains of <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The quality of such <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> is commonly tested with automatic metrics, such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, primarily with the goal of assessing improvements of releases or in the context of evaluation campaigns. However, little is known about how the output of such <a href=https://en.wikipedia.org/wiki/System>systems</a> is perceived by end users or how they compare to human performances in similar communicative tasks. In this paper, we present the results of an experiment aimed at evaluating the quality of a real-time speech translation engine by comparing it to the performance of professional simultaneous interpreters. To do so, we adopt a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> developed for the assessment of <a href=https://en.wikipedia.org/wiki/Language_interpretation>human interpreters</a> and use it to perform a manual evaluation on both <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human and machine performances</a>. In our sample, we found better performance for the <a href=https://en.wikipedia.org/wiki/Language_interpretation>human interpreters</a> in terms of <a href=https://en.wikipedia.org/wiki/Intelligibility_(communication)>intelligibility</a>, while the <a href=https://en.wikipedia.org/wiki/Machine>machine</a> performs slightly better in terms of <a href=https://en.wikipedia.org/wiki/Informatics>informativeness</a>. The limitations of the study and the possible enhancements of the chosen <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> are discussed. Despite its intrinsic limitations, the use of this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> represents a first step towards a user-centric and communication-oriented methodology for evaluating real-time automatic speech translation.</div></div></div><hr><div id=2021lchange-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.lchange-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.lchange-1/>Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.lchange-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.lchange-1.0/>Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021</a></strong><br><a href=/people/n/nina-tahmasebi/>Nina Tahmasebi</a>
|
<a href=/people/a/adam-jatowt/>Adam Jatowt</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a>
|
<a href=/people/s/simon-hengchen/>Simon Hengchen</a>
|
<a href=/people/s/syrielle-montariol/>Syrielle Montariol</a>
|
<a href=/people/h/haim-dubossarsky/>Haim Dubossarsky</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.lchange-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--lchange-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.lchange-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.lchange-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.lchange-1.1/>Time-Aware Ancient Chinese Text Translation and Inference<span class=acl-fixed-case>A</span>ncient <span class=acl-fixed-case>C</span>hinese Text Translation and Inference</a></strong><br><a href=/people/e/ernie-chang/>Ernie Chang</a>
|
<a href=/people/y/yow-ting-shiue/>Yow-Ting Shiue</a>
|
<a href=/people/h/hui-syuan-yeh/>Hui-Syuan Yeh</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--lchange-1--1><div class="card-body p-3 small">In this paper, we aim to address the challenges surrounding the translation of ancient Chinese text : (1) The linguistic gap due to the difference in eras results in translations that are poor in quality, and (2) most translations are missing the contextual information that is often very crucial to understanding the text. To this end, we improve upon past translation techniques by proposing the following : We reframe the task as a multi-label prediction task where the model predicts both the translation and its particular era. We observe that this helps to bridge the linguistic gap as <a href=https://en.wikipedia.org/wiki/Context_(language_use)>chronological context</a> is also used as auxiliary information. We validate our framework on a <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> annotated with chronology information and show experimentally its efficacy in producing quality translation outputs. We release both the code and the data for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.lchange-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--lchange-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.lchange-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.lchange-1.2/>Three-part diachronic semantic change dataset for Russian<span class=acl-fixed-case>R</span>ussian</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--lchange-1--2><div class="card-body p-3 small">We present a manually annotated lexical semantic change dataset for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> : RuShiftEval. Its novelty is ensured by a single set of target words annotated for their diachronic semantic shifts across three time periods, while the previous work either used only two time periods, or different sets of target words. The paper describes the composition and annotation procedure for the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. In addition, it is shown how the ternary nature of RuShiftEval allows to trace specific diachronic trajectories : &#8216;changed at a particular time period and stable afterwards&#8217; or &#8216;was changing throughout all time periods&#8217;. Based on the analysis of the submissions to the recent shared task on semantic change detection for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, we argue that correctly identifying such trajectories can be an interesting sub-task itself.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.lchange-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--lchange-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.lchange-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.lchange-1.5/>A diachronic evaluation of gender asymmetry in euphemism</a></strong><br><a href=/people/a/anna-kapron-king/>Anna Kapron-King</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--lchange-1--5><div class="card-body p-3 small">The use of <a href=https://en.wikipedia.org/wiki/Euphemism>euphemisms</a> is a known driver of <a href=https://en.wikipedia.org/wiki/Language_change>language change</a>. It has been proposed that women use <a href=https://en.wikipedia.org/wiki/Euphemism>euphemisms</a> more than men. Although there have been several studies investigating gender differences in language, the claim about euphemism usage has not been tested comprehensively through time. If women do use <a href=https://en.wikipedia.org/wiki/Euphemism>euphemisms</a> more, this could mean that women also lead the formation of new <a href=https://en.wikipedia.org/wiki/Euphemism>euphemisms</a> and <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> over time. Using four large diachronic text corpora of English, we evaluate the claim that women use <a href=https://en.wikipedia.org/wiki/Euphemism>euphemisms</a> more than men through a <a href=https://en.wikipedia.org/wiki/Quantitative_research>quantitative analysis</a>. We assembled a list of 106 euphemism-taboo pairs to analyze their relative use through time by each gender in the corpora. Contrary to the existing belief, our results show that women do not use <a href=https://en.wikipedia.org/wiki/Euphemism>euphemisms</a> with a higher proportion than men. We repeated the <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> using different subsets of the euphemism-taboo pairs list and found that our result was robust. Our study indicates that in a broad range of settings involving both speech and writing, and with varying degrees of formality, women do not use or form <a href=https://en.wikipedia.org/wiki/Euphemism>euphemisms</a> more than men.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.lchange-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--lchange-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.lchange-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.lchange-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.lchange-1.7/>Bhcitra : Visualising the dialect geography of South Asia<span class=acl-fixed-case>S</span>outh <span class=acl-fixed-case>A</span>sia</a></strong><br><a href=/people/a/aryaman-arora/>Aryaman Arora</a>
|
<a href=/people/a/adam-farris/>Adam Farris</a>
|
<a href=/people/g/gopalakrishnan-r/>Gopalakrishnan R</a>
|
<a href=/people/s/samopriya-basu/>Samopriya Basu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--lchange-1--7><div class="card-body p-3 small">We present Bhcitra, a dialect mapping system for <a href=https://en.wikipedia.org/wiki/South_Asia>South Asia</a> built on a database of linguistic studies of languages of the region annotated for topic and location data. We analyse language coverage and look towards applications to <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typology</a> by visualising example datasets. The <a href=https://en.wikipedia.org/wiki/Application_software>application</a> is not only meant to be useful for feature mapping, but also serves as a new kind of interactive bibliography for linguists of South Asian languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.lchange-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--lchange-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.lchange-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.lchange-1.8/>Modeling the Evolution of Word Senses with Force-Directed Layouts of Co-occurrence Networks</a></strong><br><a href=/people/t/tim-reke/>Tim Reke</a>
|
<a href=/people/r/robert-schwanhold/>Robert Schwanhold</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--lchange-1--8><div class="card-body p-3 small">Languages evolve over time and the <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning of words</a> can shift. Furthermore, individual words can have multiple senses. However, existing <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> often only reflect one word sense per word and do not reflect <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic changes</a> over time. While there are <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> that can either model semantic change of words or multiple word senses, none of them cover both aspects simultaneously. We propose a novel force-directed graph layout algorithm to draw a network of frequently co-occurring words. In this way, we are able to use the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>drawn graph</a> to visualize the evolution of word senses. In addition, we hope that jointly modeling <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a> and multiple senses of words results in improvements for the individual <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div></div><hr><div id=2021metanlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.metanlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.metanlp-1/>Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.metanlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.metanlp-1.0/>Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing</a></strong><br><a href=/people/h/hung-yi-lee/>Hung-Yi Lee</a>
|
<a href=/people/m/mitra-mohtarami/>Mitra Mohtarami</a>
|
<a href=/people/s/shang-wen-li/>Shang-Wen Li</a>
|
<a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/m/mandy-korpusik/>Mandy Korpusik</a>
|
<a href=/people/s/shuyan-dong/>Shuyan Dong</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.metanlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--metanlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.metanlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.metanlp-1.7/>Multi-accent Speech Separation with One Shot Learning</a></strong><br><a href=/people/k/kuan-po-huang/>Kuan Po Huang</a>
|
<a href=/people/y/yuan-kuei-wu/>Yuan-Kuei Wu</a>
|
<a href=/people/h/hung-yi-lee/>Hung-yi Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--metanlp-1--7><div class="card-body p-3 small">Speech separation is a problem in the field of <a href=https://en.wikipedia.org/wiki/Speech_processing>speech processing</a> that has been studied in full swing recently. However, there has not been much work studying a multi-accent speech separation scenario. Unseen speakers with new accents and <a href=https://en.wikipedia.org/wiki/Noise>noise</a> aroused the domain mismatch problem which can not be easily solved by conventional joint training methods. Thus, we applied MAML and FOMAML to tackle this problem and obtained higher average Si-SNRi values than joint training on almost all the unseen accents. This proved that these two methods do have the ability to generate well-trained parameters for adapting to speech mixtures of new speakers and accents. Furthermore, we found out that FOMAML obtains similar performance compared to <a href=https://en.wikipedia.org/wiki/MAML>MAML</a> while saving a lot of time.</div></div></div><hr><div id=2021mwe-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mwe-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.mwe-1/>Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mwe-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mwe-1.0/>Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021)</a></strong><br><a href=/people/p/paul-cook/>Paul Cook</a>
|
<a href=/people/j/jelena-mitrovic/>Jelena Mitrović</a>
|
<a href=/people/c/carla-parra-escartin/>Carla Parra Escartín</a>
|
<a href=/people/a/ashwini-vaidya/>Ashwini Vaidya</a>
|
<a href=/people/p/petya-osenova/>Petya Osenova</a>
|
<a href=/people/s/shiva-taslimipoor/>Shiva Taslimipoor</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mwe-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mwe-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mwe-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mwe-1.2/>Where Do Aspectual Variants of Light Verb Constructions Belong?</a></strong><br><a href=/people/a/aggeliki-fotopoulou/>Aggeliki Fotopoulou</a>
|
<a href=/people/e/eric-laporte/>Eric Laporte</a>
|
<a href=/people/t/takuya-nakamura/>Takuya Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mwe-1--2><div class="card-body p-3 small">Expressions with an <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspectual variant</a> of a <a href=https://en.wikipedia.org/wiki/Light_verb>light verb</a>, e.g. &#8216;take on debt&#8217; vs. &#8216;have debt&#8217;, are frequent in texts but often difficult to classify between <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>verbal idioms</a>, <a href=https://en.wikipedia.org/wiki/Light_verb_construction>light verb constructions</a> or compositional phrases. We investigate the properties of such <a href=https://en.wikipedia.org/wiki/Expression_(mathematics)>expressions</a> with a disputed membership and propose a selection of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> that determine more satisfactory boundaries between the three categories in this zone, assigning the <a href=https://en.wikipedia.org/wiki/Expression_(mathematics)>expressions</a> to one of them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mwe-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mwe-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mwe-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mwe-1.3/>Data-driven Identification of Idioms in Song Lyrics</a></strong><br><a href=/people/m/miriam-amin/>Miriam Amin</a>
|
<a href=/people/p/peter-fankhauser/>Peter Fankhauser</a>
|
<a href=/people/m/marc-kupietz/>Marc Kupietz</a>
|
<a href=/people/r/roman-schneider/>Roman Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mwe-1--3><div class="card-body p-3 small">The automatic recognition of idioms poses a challenging problem for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>. Whereas <a href=https://en.wikipedia.org/wiki/First_language>native speakers</a> can intuitively handle multiword expressions whose compositional meanings are hard to trace back to individual <a href=https://en.wikipedia.org/wiki/Semantics>word semantics</a>, there is still ample scope for improvement regarding computational approaches. We assume that idiomatic constructions can be characterized by gradual intensities of semantic non-compositionality, formal fixedness, and unusual usage context, and introduce a number of measures for these characteristics, comprising count-based and predictive collocation measures together with measures of context (un)similarity. We evaluate our approach on a manually labelled gold standard, derived from a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of German pop lyrics</a>. To this end, we apply a Random Forest classifier to analyze the individual contribution of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for automatically detecting idioms, and study the trade-off between <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a>. Finally, we evaluate the classifier on an independent dataset of idioms extracted from a list of Wikipedia idioms, achieving state-of-the art accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mwe-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mwe-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mwe-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mwe-1.5/>PIE : A Parallel Idiomatic Expression Corpus for Idiomatic Sentence Generation and Paraphrasing<span class=acl-fixed-case>PIE</span>: A Parallel Idiomatic Expression Corpus for Idiomatic Sentence Generation and Paraphrasing</a></strong><br><a href=/people/j/jianing-zhou/>Jianing Zhou</a>
|
<a href=/people/h/hongyu-gong/>Hongyu Gong</a>
|
<a href=/people/s/suma-bhat/>Suma Bhat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mwe-1--5><div class="card-body p-3 small">Idiomatic expressions (IE) play an important role in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, and have long been a pain in the neck for NLP systems. Despite this, text generation tasks related to IEs remain largely under-explored. In this paper, we propose two new tasks of idiomatic sentence generation and <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> to fill this research gap. We introduce a curated dataset of 823 IEs, and a parallel corpus with sentences containing them and the same sentences where the IEs were replaced by their literal paraphrases as the primary resource for our tasks. We benchmark existing deep learning models, which have state-of-the-art performance on related tasks using automated and manual evaluation with our dataset to inspire further research on our proposed tasks. By establishing baseline models, we pave the way for more comprehensive and accurate <a href=https://en.wikipedia.org/wiki/Computer_simulation>modeling</a> of <a href=https://en.wikipedia.org/wiki/Integrated_circuit>IEs</a>, both for generation and <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mwe-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mwe-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mwe-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.mwe-1.6.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mwe-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mwe-1.6/>Lexical Semantic Recognition</a></strong><br><a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/m/michael-kranzlein/>Michael Kranzlein</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mwe-1--6><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Lexical_semantics>lexical semantics</a>, full-sentence segmentation and segment labeling of various <a href=https://en.wikipedia.org/wiki/Phenomenon>phenomena</a> are generally treated separately, despite their interdependence. We hypothesize that a unified lexical semantic recognition task is an effective way to encapsulate previously disparate styles of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, including multiword expression identification / classification and supersense tagging. Using the STREUSLE corpus, we train a neural CRF sequence tagger and evaluate its performance along various axes of annotation. As the label set generalizes that of previous tasks (PARSEME, DiMSUM), we additionally evaluate how well the model generalizes to those test sets, finding that it approaches or surpasses existing models despite training only on STREUSLE. Our work also establishes <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline models</a> and evaluation metrics for integrated and accurate modeling of <a href=https://en.wikipedia.org/wiki/Lexical_semantics>lexical semantics</a>, facilitating future work in this area.</div></div></div><hr><div id=2021nlp4posimpact-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4posimpact-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.nlp4posimpact-1/>Proceedings of the 1st Workshop on NLP for Positive Impact</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4posimpact-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4posimpact-1.0/>Proceedings of the 1st Workshop on NLP for Positive Impact</a></strong><br><a href=/people/a/anjalie-field/>Anjalie Field</a>
|
<a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/z/zhijing-jin/>Zhijing Jin</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4posimpact-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4posimpact-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4posimpact-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4posimpact-1.2/>The Climate Change Debate and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/m/manfred-stede/>Manfred Stede</a>
|
<a href=/people/r/ronny-patz/>Ronny Patz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4posimpact-1--2><div class="card-body p-3 small">The debate around climate change (CC)its extent, its causes, and the necessary responsesis intense and of global importance. Yet, in the <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) community</a>, this <a href=https://en.wikipedia.org/wiki/Domain_(mathematical_analysis)>domain</a> has so far received little attention. In contrast, it is of enormous prominence in various <a href=https://en.wikipedia.org/wiki/Social_science>social science disciplines</a>, and some of that work follows the text-as-data paradigm, seeking to employ <a href=https://en.wikipedia.org/wiki/Quantitative_research>quantitative methods</a> for analyzing large amounts of CC-related text. Other research is qualitative in nature and studies details, nuances, actors, and motivations within CC discourses. Coming from both <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> and <a href=https://en.wikipedia.org/wiki/Political_science>Political Science</a>, and reviewing key works in both disciplines, we discuss how social science approaches to CC debates can inform advances in text-mining / NLP, and how, in return, <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> can support policy-makers and activists in making sense of large-scale and complex CC discourses across multiple genres, channels, topics, and communities. This is paramount for their ability to make rapid and meaningful impact on the discourse, and for shaping the necessary policy change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4posimpact-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4posimpact-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4posimpact-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4posimpact-1.12/>Dialogue Act Classification for Augmentative and Alternative Communication</a></strong><br><a href=/people/e/e-margaret-perkoff/>E. Margaret Perkoff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4posimpact-1--12><div class="card-body p-3 small">Augmentative and Alternative Communication (AAC) devices and applications are intended to make it easier for individuals with complex communication needs to participate in conversations. However, these <a href=https://en.wikipedia.org/wiki/Peripheral>devices</a> have low adoption and retention rates. We review prior work with text recommendation systems that have not been successful in mitigating these problems. To address these gaps, we propose applying Dialogue Act classification to AAC conversations. We evaluated the performance of a state of the art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a limited AAC dataset that was trained on both AAC and non-AAC datasets. The one trained on <a href=https://en.wikipedia.org/wiki/Advanced_Audio_Coding>AAC</a> (accuracy = 38.6 %) achieved better performance than that trained on a non-AAC corpus (accuracy = 34.1 %). These results reflect the need to incorporate representative datasets in later experiments. We discuss the need to collect more labeled AAC datasets and propose areas of future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4posimpact-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4posimpact-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4posimpact-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4posimpact-1.13/>Improving Policing with <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/a/anthony-dixon/>Anthony Dixon</a>
|
<a href=/people/d/daniel-birks/>Daniel Birks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4posimpact-1--13><div class="card-body p-3 small">This article explores the potential for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a> to enable a more effective, prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale. Problem-Oriented Policing (POP) is a potential replacement, at least in part, for traditional policing which adopts a reactive approach, relying heavily on the <a href=https://en.wikipedia.org/wiki/Criminal_justice>criminal justice system</a>. By contrast, POP seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed. Identifying these underlying conditions requires a detailed understanding of crime events-tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data. One potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration. Yet police agencies do not typically have the skills or resources to analyse these <a href=https://en.wikipedia.org/wiki/Data>data</a> at scale. In this article we argue that <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> offers the potential to unlock these <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a> and by doing so allow police to implement more POP initiatives. However we caution that using NLP models without adequate knowledge may either allow or perpetuate bias within the data potentially leading to unfavourable outcomes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4posimpact-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4posimpact-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4posimpact-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4posimpact-1.15/>A Speech-enabled Fixed-phrase Translator for Healthcare Accessibility</a></strong><br><a href=/people/p/pierrette-bouillon/>Pierrette Bouillon</a>
|
<a href=/people/j/johanna-gerlach/>Johanna Gerlach</a>
|
<a href=/people/j/jonathan-mutal/>Jonathan Mutal</a>
|
<a href=/people/n/nikos-tsourakis/>Nikos Tsourakis</a>
|
<a href=/people/h/herve-spechbach/>Hervé Spechbach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4posimpact-1--15><div class="card-body p-3 small">In this overview article we describe an <a href=https://en.wikipedia.org/wiki/Application_software>application</a> designed to enable communication between health practitioners and patients who do not share a <a href=https://en.wikipedia.org/wiki/Lingua_franca>common language</a>, in situations where professional interpreters are not available. Built on the principle of a fixed phrase translator, the application implements different natural language processing (NLP) technologies, such as <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech</a> to improve usability. Its design allows easy portability to new domains and integration of different types of output for multiple target audiences. Even though BabelDr is far from solving the problem of miscommunication between patients and doctors, it is a clear example of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> in a real world application designed to help minority groups to communicate in a medical context. It also gives some insights into the relevant criteria for the development of such an <a href=https://en.wikipedia.org/wiki/Application_software>application</a>.</div></div></div><hr><div id=2021nlp4prog-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.nlp4prog-1/>Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.0/>Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)</a></strong><br><a href=/people/r/royi-lachmy/>Royi Lachmy</a>
|
<a href=/people/z/ziyu-yao/>Ziyu Yao</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/m/milos-gligoric/>Milos Gligoric</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a>
|
<a href=/people/r/ray-mooney/>Ray Mooney</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/h/huan-sun/>Huan Sun</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.2/>ConTest : A Unit Test Completion Benchmark featuring Context<span class=acl-fixed-case>C</span>on<span class=acl-fixed-case>T</span>est: A Unit Test Completion Benchmark featuring Context</a></strong><br><a href=/people/j/johannes-villmow/>Johannes Villmow</a>
|
<a href=/people/j/jonas-depoix/>Jonas Depoix</a>
|
<a href=/people/a/adrian-ulges/>Adrian Ulges</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--2><div class="card-body p-3 small">We introduce CONTEST, a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> for NLP-based unit test completion, the task of predicting a test&#8217;s assert statements given its setup and focal method, i.e. the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> to be tested. ConTest is large-scale (with 365k datapoints). Besides the test code and tested code, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> also features context code called by either. We found <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> to be crucial for accurately predicting assertions. We also introduce baselines based on transformer encoder-decoders, and study the effects of including <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> and <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>. Overall, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve a BLEU score of 38.2, while only generating unparsable code in 1.92 % of cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4prog-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.3/>CommitBERT : Commit Message Generation Using Pre-Trained Programming Language Model<span class=acl-fixed-case>C</span>ommit<span class=acl-fixed-case>BERT</span>: Commit Message Generation Using Pre-Trained Programming Language Model</a></strong><br><a href=/people/t/tae-hwan-jung/>Tae Hwan Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--3><div class="card-body p-3 small">Commit message is a document that summarizes source code changes in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. A good commit message clearly shows the source code changes, so this enhances collaboration between developers. Therefore, our work is to develop a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> that automatically writes the commit message. To this end, we release 345 K datasets consisting of code modification and commit messages in six programming languages (Python, PHP, <a href=https://en.wikipedia.org/wiki/Go_(programming_language)>Go</a>, <a href=https://en.wikipedia.org/wiki/Java_(programming_language)>Java</a>, <a href=https://en.wikipedia.org/wiki/JavaScript>JavaScript</a>, and Ruby). Similar to the neural machine translation (NMT) model, using our dataset, we feed the code modification to the encoder input and the commit message to the decoder input and measure the result of the generated commit message with BLEU-4. Also, we propose the following two training methods to improve the result of generating the commit message : (1) A method of preprocessing the input to feed the code modification to the encoder input. (2) A method that uses an initial weight suitable for the code domain to reduce the gap in contextual representation between programming language (PL) and natural language (NL).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4prog-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.4/>Time-Efficient Code Completion Model for the <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R Programming Language</a><span class=acl-fixed-case>R</span> Programming Language</a></strong><br><a href=/people/a/artem-popov/>Artem Popov</a>
|
<a href=/people/d/dmitrii-orekhov/>Dmitrii Orekhov</a>
|
<a href=/people/d/denis-litvinov/>Denis Litvinov</a>
|
<a href=/people/n/nikolay-korolev/>Nikolay Korolev</a>
|
<a href=/people/g/gleb-morgachev/>Gleb Morgachev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--4><div class="card-body p-3 small">In this paper we present a deep learning code completion model for the <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R language</a>. We introduce several <a href=https://en.wikipedia.org/wiki/Software_development_process>techniques</a> to utilize <a href=https://en.wikipedia.org/wiki/Language_model>language modeling based architecture</a> in the <a href=https://en.wikipedia.org/wiki/Autocomplete>code completion task</a>. With these techniques, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> requires low resources, but still achieves high quality. We also present an evaluation dataset for the <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R language completion task</a>. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains multiple autocompletion usage contexts that provides robust validation results. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4prog-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.7/>Shellcode_IA32 : A Dataset for Automatic Shellcode Generation<span class=acl-fixed-case>S</span>hellcode_<span class=acl-fixed-case>IA</span>32: A Dataset for Automatic Shellcode Generation</a></strong><br><a href=/people/p/pietro-liguori/>Pietro Liguori</a>
|
<a href=/people/e/erfan-al-hossami/>Erfan Al-Hossami</a>
|
<a href=/people/d/domenico-cotroneo/>Domenico Cotroneo</a>
|
<a href=/people/r/roberto-natella/>Roberto Natella</a>
|
<a href=/people/b/bojan-cukic/>Bojan Cukic</a>
|
<a href=/people/s/samira-shaikh/>Samira Shaikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--7><div class="card-body p-3 small">We take the first step to address the task of <a href=https://en.wikipedia.org/wiki/Shellcode>automatically generating shellcodes</a>, i.e., small pieces of code used as a payload in the exploitation of a <a href=https://en.wikipedia.org/wiki/Vulnerability_(computing)>software vulnerability</a>, starting from <a href=https://en.wikipedia.org/wiki/Comment_(computer_programming)>natural language comments</a>. We assemble and release a novel dataset (Shellcode_IA32), consisting of challenging but common assembly instructions with their natural language descriptions. We experiment with standard methods in neural machine translation (NMT) to establish baseline performance levels on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4prog-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.8/>Reading StackOverflow Encourages Cheating : Adding Question Text Improves Extractive Code Generation<span class=acl-fixed-case>S</span>tack<span class=acl-fixed-case>O</span>verflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation</a></strong><br><a href=/people/g/gabriel-orlanski/>Gabriel Orlanski</a>
|
<a href=/people/a/alex-gittens/>Alex Gittens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--8><div class="card-body p-3 small">Answering a programming question with only its title is difficult as salient contextual information is left out. To address this, we present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of over 40,000 StackOverflow question texts to be used in conjunction with the corresponding intents from the CoNaLa dataset (Yin et al., 2018). Using both the <a href=https://en.wikipedia.org/wiki/Intention>intent</a> and the question body, we use <a href=https://en.wikipedia.org/wiki/Bay_Area_Rapid_Transit>BART</a> to establish a baseline BLEU score of 34.35 for this new task. We then find further improvements of 2.8 % by combining the mined CoNaLa data with the labeled data to achieve a 35.32 BLEU score. We then evaluate the prior state-of-the-art CoNaLa models with this additional <a href=https://en.wikipedia.org/wiki/Data>data</a>. We find that our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> of using the body and mined data beats that of the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by a 71.96 % BLEU score. Finally, we perform ablations that prove that BART is an unsupervised multimodal learner and examine its extractive behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nlp4prog-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.9/>Text-to-SQL in the Wild : A Naturally-Occurring Dataset Based on Stack Exchange Data<span class=acl-fixed-case>SQL</span> in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data</a></strong><br><a href=/people/m/moshe-hazoom/>Moshe Hazoom</a>
|
<a href=/people/v/vibhor-malik/>Vibhor Malik</a>
|
<a href=/people/b/ben-bogin/>Ben Bogin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--9><div class="card-body p-3 small">Most available semantic parsing datasets, comprising of pairs of natural utterances and logical forms, were collected solely for the purpose of training and evaluation of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding systems</a>. As a result, they do not contain any of the richness and variety of natural-occurring utterances, where humans ask about data they need or are curious about. In this work, we release <a href=https://en.wikipedia.org/wiki/Stack_Exchange>SEDE</a>, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with 12,023 pairs of utterances and <a href=https://en.wikipedia.org/wiki/SQL>SQL queries</a> collected from real usage on the <a href=https://en.wikipedia.org/wiki/Stack_Exchange>Stack Exchange website</a>. We show that these pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset, propose an evaluation metric based on comparison of partial query clauses that is more suitable for real-world queries, and conduct experiments with strong baselines, showing a large gap between the performance on SEDE compared to other common datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4prog-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4prog-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4prog-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4prog-1.10/>Bag-of-Words Baselines for Semantic Code Search</a></strong><br><a href=/people/x/xinyu-zhang/>Xinyu Zhang</a>
|
<a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/j/jimmy-lin/>Jimmy Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4prog-1--10><div class="card-body p-3 small">The task of semantic code search is to retrieve <a href=https://en.wikipedia.org/wiki/Snippet_(programming)>code snippets</a> from a <a href=https://en.wikipedia.org/wiki/Text_corpus>source code corpus</a> based on an information need expressed in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. The semantic gap between natural language and programming languages has for long been regarded as one of the most significant obstacles to the effectiveness of keyword-based information retrieval (IR) methods. It is a common assumption that traditional bag-of-words IR methods are poorly suited for semantic code search : our work empirically investigates this assumption. Specifically, we examine the effectiveness of two traditional IR methods, namely <a href=https://en.wikipedia.org/wiki/BM25>BM25</a> and <a href=https://en.wikipedia.org/wiki/RM3>RM3</a>, on the CodeSearchNet Corpus, which consists of natural language queries paired with relevant code snippets. We find that the two keyword-based methods outperform several pre-BERT neural models. We also compare several code-specific data pre-processing strategies and find that specialized tokenization improves effectiveness.</div></div></div><hr><div id=2021repl4nlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.repl4nlp-1/>Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.0/>Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)</a></strong><br><a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/i/iacer-calixto/>Iacer Calixto</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/n/naomi-saphra/>Naomi Saphra</a>
|
<a href=/people/n/nora-kassner/>Nora Kassner</a>
|
<a href=/people/o/oana-maria-camburu/>Oana-Maria Camburu</a>
|
<a href=/people/t/trapit-bansal/>Trapit Bansal</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.1/>Improving Cross-lingual Text Classification with Zero-shot Instance-Weighting</a></strong><br><a href=/people/i/irene-li/>Irene Li</a>
|
<a href=/people/p/prithviraj-sen/>Prithviraj Sen</a>
|
<a href=/people/h/huaiyu-zhu/>Huaiyu Zhu</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--1><div class="card-body p-3 small">Cross-lingual text classification (CLTC) is a challenging task made even harder still due to the lack of labeled data in low-resource languages. In this paper, we propose zero-shot instance-weighting, a general model-agnostic zero-shot learning framework for improving CLTC by leveraging source instance weighting. It adds a module on top of pre-trained language models for similarity computation of instance weights, thus aligning each source instance to the target language. During <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, the framework utilizes <a href=https://en.wikipedia.org/wiki/Gradient_descent>gradient descent</a> that is weighted by instance weights to update parameters. We evaluate this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> over seven target languages on three fundamental tasks and show its effectiveness and extensibility, by improving on F1 score up to 4 % in single-source transfer and 8 % in multi-source transfer. To the best of our knowledge, our method is the first to apply instance weighting in zero-shot CLTC. It is simple yet effective and easily extensible into multi-source transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.3/>Comprehension Based Question Answering using <a href=https://en.wikipedia.org/wiki/Bloom&#8217;s_taxonomy>Bloom’s Taxonomy</a></a></strong><br><a href=/people/p/pritish-sahu/>Pritish Sahu</a>
|
<a href=/people/m/michael-cogswell/>Michael Cogswell</a>
|
<a href=/people/a/ajay-divakaran/>Ajay Divakaran</a>
|
<a href=/people/s/sara-rutherford-quach/>Sara Rutherford-Quach</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--3><div class="card-body p-3 small">Current pre-trained language models have lots of knowledge, but a more limited ability to use that knowledge. Bloom&#8217;s Taxonomy helps educators teach children how to use knowledge by categorizing comprehension skills, so we use it to analyze and improve the <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension skills</a> of large pre-trained language models. Our experiments focus on zero-shot question answering, using the <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a> to provide proximal context that helps the model answer questions by being relevant to those questions. We show targeting context in this manner improves performance across 4 popular common sense question answer datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.repl4nlp-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.5/>Learning Sparse Sentence Encoding without Supervision : An Exploration of Sparsity in Variational Autoencoders</a></strong><br><a href=/people/v/victor-prokhorov/>Victor Prokhorov</a>
|
<a href=/people/y/yingzhen-li/>Yingzhen Li</a>
|
<a href=/people/e/ehsan-shareghi/>Ehsan Shareghi</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--5><div class="card-body p-3 small">It has been long known that sparsity is an effective <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a> for learning efficient representation of data in vectors with fixed dimensionality, and it has been explored in many areas of <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. Of particular interest to this work is the investigation of the sparsity within the VAE framework which has been explored a lot in the image domain, but has been lacking even a basic level of exploration in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Additionally, <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is also lagging behind in terms of learning sparse representations of large units of text e.g., sentences. We use the VAEs that induce sparse latent representations of large units of text to address the aforementioned shortcomings. First, we move in this direction by measuring the success of unsupervised state-of-the-art (SOTA) and other strong VAE-based sparsification baselines for text and propose a hierarchical sparse VAE model to address the stability issue of SOTA. Then, we look at the implications of sparsity on text classification across 3 datasets, and highlight a link between performance of sparse latent representations on downstream tasks and its ability to encode task-related information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.repl4nlp-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.6/>Temporal-aware Language Representation Learning From Crowdsourced Labels</a></strong><br><a href=/people/y/yang-hao/>Yang Hao</a>
|
<a href=/people/x/xiao-zhai/>Xiao Zhai</a>
|
<a href=/people/w/wenbiao-ding/>Wenbiao Ding</a>
|
<a href=/people/z/zitao-liu/>Zitao Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--6><div class="card-body p-3 small">Learning effective language representations from crowdsourced labels is crucial for many real-world machine learning tasks. A challenging aspect of this problem is that the quality of crowdsourced labels suffer high intra- and inter-observer variability. Since the high-capacity deep neural networks can easily memorize all disagreements among crowdsourced labels, directly applying existing supervised language representation learning algorithms may yield suboptimal solutions. In this paper, we propose TACMA, a temporal-aware language representation learning heuristic for crowdsourced labels with multiple annotators. The proposed approach (1) explicitly models the intra-observer variability with attention mechanism ; (2) computes and aggregates per-sample confidence scores from multiple workers to address the inter-observer disagreements. The proposed <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic</a> is extremely easy to implement in around 5 lines of code. The proposed <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>heuristic</a> is evaluated on four synthetic and four real-world data sets. The results show that our approach outperforms a wide range of state-of-the-art <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a> in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>prediction accuracy</a> and <a href=https://en.wikipedia.org/wiki/Analysis_of_covariance>AUC</a>. To encourage the reproducible results, we make our code publicly available at.<i>TACMA</i>, a temporal-aware language representation learning heuristic for crowdsourced labels with multiple annotators. The proposed approach (1) explicitly models the intra-observer variability with attention mechanism; (2) computes and aggregates per-sample confidence scores from multiple workers to address the inter-observer disagreements. The proposed heuristic is extremely easy to implement in around 5 lines of code. The proposed heuristic is evaluated on four synthetic and four real-world data sets. The results show that our approach outperforms a wide range of state-of-the-art baselines in terms of prediction accuracy and AUC. To encourage the reproducible results, we make our code publicly available at <url>https://github.com/CrowdsourcingMining/TACMA</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.repl4nlp-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.12/>Knodle : Modular Weakly Supervised Learning with PyTorch<span class=acl-fixed-case>P</span>y<span class=acl-fixed-case>T</span>orch</a></strong><br><a href=/people/a/anastasiia-sedova/>Anastasiia Sedova</a>
|
<a href=/people/a/andreas-stephan/>Andreas Stephan</a>
|
<a href=/people/m/marina-speranskaya/>Marina Speranskaya</a>
|
<a href=/people/b/benjamin-roth/>Benjamin Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--12><div class="card-body p-3 small">Strategies for improving the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and prediction quality</a> of weakly supervised machine learning models vary in how much they are tailored to a specific task or integrated with a specific model architecture. In this work, we introduce Knodle, a <a href=https://en.wikipedia.org/wiki/Software_framework>software framework</a> that treats weak data annotations, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, and methods for improving weakly supervised training as separate, modular components. This modularization gives the training process access to fine-grained information such as data set characteristics, matches of <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic rules</a>, or elements of the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a> ultimately used for prediction. Hence, our framework can encompass a wide range of training methods for improving weak supervision, ranging from methods that only look at correlations of rules and output classes (independently of the <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> trained with the resulting labels), to those that harness the interplay of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> and weakly labeled data. We illustrate the benchmarking potential of the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> with a performance comparison of several reference implementations on a selection of datasets that are already available in Knodle.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.repl4nlp-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.16/>Probing Cross-Modal Representations in Multi-Step Relational Reasoning</a></strong><br><a href=/people/i/iuliia-parfenova/>Iuliia Parfenova</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a>
|
<a href=/people/s/sandro-pezzelle/>Sandro Pezzelle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--16><div class="card-body p-3 small">We investigate the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> learned by vision and language models in tasks that require relational reasoning. Focusing on the problem of assessing the relative size of objects in abstract visual contexts, we analyse both one-step and two-step reasoning. For the latter, we construct a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of three-image scenes and define a task that requires <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a> at the level of the individual images and across images in a scene. We probe the learned model representations using diagnostic classifiers. Our experiments show that pretrained multimodal transformer-based architectures can perform higher-level relational reasoning, and are able to learn representations for novel tasks and data that are very different from what was seen in pretraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.21/>Predicting the Success of <a href=https://en.wikipedia.org/wiki/Domain_adaptation>Domain Adaptation</a> in Text Similarity</a></strong><br><a href=/people/n/nick-pogrebnyakov/>Nick Pogrebnyakov</a>
|
<a href=/people/s/shohreh-shaghaghian/>Shohreh Shaghaghian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--21><div class="card-body p-3 small">Transfer learning methods, and in particular domain adaptation, help exploit labeled data in one domain to improve the performance of a certain task in another domain. However, it is still not clear what factors affect the success of <a href=https://en.wikipedia.org/wiki/Adaptation_(biology)>domain adaptation</a>. This paper models <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> success and selection of the most suitable source domains among several candidates in <a href=https://en.wikipedia.org/wiki/Similarity_measure>text similarity</a>. We use descriptive domain information and cross-domain similarity metrics as <a href=https://en.wikipedia.org/wiki/Predictive_analytics>predictive features</a>. While mostly positive, the results also point to some domains where <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> success was difficult to predict.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.26/>Deriving Contextualised Semantic Features from BERT (and Other Transformer Model) Embeddings<span class=acl-fixed-case>BERT</span> (and Other Transformer Model) Embeddings</a></strong><br><a href=/people/j/jacob-turton/>Jacob Turton</a>
|
<a href=/people/r/robert-elliott-smith/>Robert Elliott Smith</a>
|
<a href=/people/d/david-vinson/>David Vinson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--26><div class="card-body p-3 small">Models based on the transformer architecture, such as BERT, have marked a crucial step forward in the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. Importantly, they allow the creation of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> that capture important semantic information about words in context. However, as single entities, these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are difficult to interpret and the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> used to create them have been described as opaque. Binder and colleagues proposed an intuitive embedding space where each dimension is based on one of 65 core semantic features. Unfortunately, the <a href=https://en.wikipedia.org/wiki/Space_(mathematics)>space</a> only exists for a small data-set of 535 words, limiting its uses. Previous work (Utsumi, 2018, 2020 ; Turton et al., 2020) has shown that Binder features can be derived from static embeddings and successfully extrapolated to a large new vocabulary. Taking the next step, this paper demonstrates that Binder features can be derived from the BERT embedding space. This provides two things ; (1) semantic feature values derived from contextualised word embeddings and (2) insights into how semantic features are represented across the different layers of the BERT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.29/>An Overview of Uncertainty Calibration for <a href=https://en.wikipedia.org/wiki/Text_classification>Text Classification</a> and the Role of Distillation</a></strong><br><a href=/people/h/han-guo/>Han Guo</a>
|
<a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--29><div class="card-body p-3 small">Recent advances in NLP systems, notably the pretraining-and-finetuning paradigm, have achieved great success in predictive accuracy. However, these <a href=https://en.wikipedia.org/wiki/System>systems</a> are usually not well calibrated for uncertainty out-of-the-box. Many recalibration methods have been proposed in the literature for quantifying predictive uncertainty and calibrating model outputs, with varying degrees of <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>. In this work, we present a systematic study of a few of these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>. Focusing on the text classification task and finetuned large pretrained language models, we first show that many of the finetuned models are not well calibrated out-of-the-box, especially when the data come from out-of-domain settings. Next, we compare the effectiveness of a few widely-used recalibration methods (such as <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensembles</a>, temperature scaling). Then, we empirically illustrate a connection between <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> and <a href=https://en.wikipedia.org/wiki/Calibration>calibration</a>. We view <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> as a regularization term encouraging the student model to output uncertainties that match those of a teacher model. With this insight, we develop simple recalibration methods based on <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> with no additional inference-time cost. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches. Finally, we include <a href=https://en.wikipedia.org/wiki/Ablation>ablations</a> to understand the usefulness of components of our proposed method and examine the transferability of <a href=https://en.wikipedia.org/wiki/Calibration>calibration</a> via <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.repl4nlp-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--repl4nlp-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.repl4nlp-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.repl4nlp-1.32" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.repl4nlp-1.32/>Direction is what you need : Improving Word Embedding Compression in Large Language Models</a></strong><br><a href=/people/k/klaudia-balazy/>Klaudia Bałazy</a>
|
<a href=/people/m/mohammadreza-banaei/>Mohammadreza Banaei</a>
|
<a href=/people/r/remi-lebret/>Rémi Lebret</a>
|
<a href=/people/j/jacek-tabor/>Jacek Tabor</a>
|
<a href=/people/k/karl-aberer/>Karl Aberer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--repl4nlp-1--32><div class="card-body p-3 small">The adoption of Transformer-based models in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> has led to great success using a massive number of parameters. However, due to deployment constraints in <a href=https://en.wikipedia.org/wiki/Edge_device>edge devices</a>, there has been a rising interest in the <a href=https://en.wikipedia.org/wiki/Data_compression>compression</a> of these models to improve their inference time and memory footprint. This paper presents a novel loss objective to compress token embeddings in the Transformer-based models by leveraging an AutoEncoder architecture. More specifically, we emphasize the importance of the direction of compressed embeddings with respect to original uncompressed embeddings. The proposed method is task-agnostic and does not require further language modeling pre-training. Our method significantly outperforms the commonly used SVD-based matrix-factorization approach in terms of initial language model Perplexity. Moreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several downstream tasks from the GLUE benchmark, where we also outperform the baseline in most scenarios. Our code is public.</div></div></div><hr><div id=2021semeval-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.semeval-1/>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.0/>Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)</a></strong><br><a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/n/natalie-schluter/>Natalie Schluter</a>
|
<a href=/people/g/guy-emerson/>Guy Emerson</a>
|
<a href=/people/a/aurelie-herbelot/>Aurelie Herbelot</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.1/>SemEval-2021 Task 1 : Lexical Complexity Prediction<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 1: Lexical Complexity Prediction</a></strong><br><a href=/people/m/matthew-shardlow/>Matthew Shardlow</a>
|
<a href=/people/r/richard-evans/>Richard Evans</a>
|
<a href=/people/g/gustavo-paetzold/>Gustavo Henrique Paetzold</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--1><div class="card-body p-3 small">This paper presents the results and main findings of SemEval-2021 Task 1-Lexical Complexity Prediction. We provided participants with an augmented version of the CompLex Corpus (Shardlow et al. CompLex is an English multi-domain corpus in which words and multi-word expressions (MWEs) were annotated with respect to their complexity using a five point Likert scale. SemEval-2021 Task 1 featured two Sub-tasks : Sub-task 1 focused on single words and Sub-task 2 focused on MWEs. The competition attracted 198 teams in total, of which 54 teams submitted official runs on the test data to <a href=https://en.wikipedia.org/wiki/Task_(computing)>Sub-task 1</a> and 37 to <a href=https://en.wikipedia.org/wiki/Task_(computing)>Sub-task 2</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.7/>SemEval-2021 Task 6 : Detection of Persuasion Techniques in Texts and Images<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: Detection of Persuasion Techniques in Texts and Images</a></strong><br><a href=/people/d/dimitar-dimitrov/>Dimitar Dimitrov</a>
|
<a href=/people/b/bishr-bin-ali/>Bishr Bin Ali</a>
|
<a href=/people/s/shaden-shaar/>Shaden Shaar</a>
|
<a href=/people/f/firoj-alam/>Firoj Alam</a>
|
<a href=/people/f/fabrizio-silvestri/>Fabrizio Silvestri</a>
|
<a href=/people/h/hamed-firooz/>Hamed Firooz</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--7><div class="card-body p-3 small">We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in Texts and Images : the data, the annotation guidelines, the evaluation setup, the results, and the participating systems. The task focused on <a href=https://en.wikipedia.org/wiki/Meme>memes</a> and had three subtasks : (i) detecting the techniques in the <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, (ii) detecting the text spans where the techniques are used, and (iii) detecting techniques in the entire meme, i.e., both in the text and in the image. It was a popular <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, attracting 71 registrations, and 22 teams that eventually made an official submission on the test set. The evaluation results for the third subtask confirmed the importance of both <a href=https://en.wikipedia.org/wiki/Modality_(semiotics)>modalities</a>, the <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> and the image. Moreover, some teams reported benefits when not just combining the two modalities, e.g., by using early or late fusion, but rather modeling the interaction between them in a joint model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.8/>Alpha at SemEval-2021 Task 6 : Transformer Based Propaganda Classification<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: Transformer Based Propaganda Classification</a></strong><br><a href=/people/z/zhida-feng/>Zhida Feng</a>
|
<a href=/people/j/jiji-tang/>Jiji Tang</a>
|
<a href=/people/j/jiaxiang-liu/>Jiaxiang Liu</a>
|
<a href=/people/w/weichong-yin/>Weichong Yin</a>
|
<a href=/people/s/shikun-feng/>Shikun Feng</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a>
|
<a href=/people/l/li-chen/>Li Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--8><div class="card-body p-3 small">This paper describes our system participated in Task 6 of SemEval-2021 : the task focuses on multimodal propaganda technique classification and it aims to classify given image and text into 22 classes. In this paper, we propose to use transformer based architecture to fuse the clues from both image and text. We explore two branches of techniques including fine-tuning the text pretrained transformer with extended visual features, and fine-tuning the multimodal pretrained transformers. For the visual features, we have tested both grid features based on <a href=https://en.wikipedia.org/wiki/ResNet>ResNet</a> and salient region features from pretrained object detector. Among the pretrained multimodal transformers, we choose ERNIE-ViL, a two-steam cross-attended transformers pretrained on large scale image-caption aligned data. Fine-tuing ERNIE-ViL for our task produce a better performance due to general joint multimodal representation for <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> and image learned by ERNIE-ViL. Besides, as the distribution of the classification labels is very unbalanced, we also make a further attempt on the <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> and the experiment result shows that focal loss would perform better than cross entropy loss. Last we have won first for subtask C in the final competition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.9/>SemEval 2021 Task 7 : HaHackathon, Detecting and Rating Humor and Offense<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val 2021 Task 7: <span class=acl-fixed-case>H</span>a<span class=acl-fixed-case>H</span>ackathon, Detecting and Rating Humor and Offense</a></strong><br><a href=/people/j/j-a-meaney/>J. A. Meaney</a>
|
<a href=/people/s/steven-wilson/>Steven Wilson</a>
|
<a href=/people/l/luis-chiruzzo/>Luis Chiruzzo</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--9><div class="card-body p-3 small">SemEval 2021 Task 7, HaHackathon, was the first shared task to combine the previously separate domains of humor detection and offense detection. We collected 10,000 texts from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and the Kaggle Short Jokes dataset, and had each annotated for humor and offense by 20 annotators aged 18-70. Our subtasks were binary humor detection, prediction of humor and offense ratings, and a novel controversy task : to predict if the variance in the humor ratings was higher than a specific threshold. The subtasks attracted 36-58 submissions, with most of the participants choosing to use pre-trained language models. Many of the highest performing teams also implemented additional optimization techniques, including task-adaptive training and adversarial training. The results suggest that the participating <a href=https://en.wikipedia.org/wiki/System>systems</a> are well suited to humor detection, but that humor controversy is a more challenging task. We discuss which <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> excel in this task, which auxiliary techniques boost their performance, and analyze the errors which were not captured by the best <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.11/>Complex words identification using word-level features for SemEval-2020 Task 1<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2020 Task 1</a></strong><br><a href=/people/j/jenny-a-ortiz-zambrano/>Jenny A. Ortiz-Zambrano</a>
|
<a href=/people/a/arturo-montejo-raez/>Arturo Montejo-Ráez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--11><div class="card-body p-3 small">This article describes a system to predict the complexity of words for the Lexical Complexity Prediction (LCP) shared task hosted at SemEval 2021 (Task 1) with a new annotated English dataset with a <a href=https://en.wikipedia.org/wiki/Likert_scale>Likert scale</a>. Located in the Lexical Semantics track, the task consisted of predicting the complexity value of the words in context. A machine learning approach was carried out based on the frequency of the words and several characteristics added at word level. Over these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, a supervised random forest regression algorithm was trained. Several runs were performed with different values to observe the performance of the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>. For the <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>, our best results reported a M.A.E score of 0.07347, M.S.E. of 0.00938, and R.M.S.E. of 0.096871. Our experiments showed that, with a greater number of <a href=https://en.wikipedia.org/wiki/Phenotypic_trait>characteristics</a>, the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> increases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.15/>Uppsala NLP at SemEval-2021 Task 2 : Multilingual Language Models for Fine-tuning and Feature Extraction in Word-in-Context Disambiguation<span class=acl-fixed-case>U</span>ppsala <span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 2: Multilingual Language Models for Fine-tuning and Feature Extraction in Word-in-Context Disambiguation</a></strong><br><a href=/people/h/huiling-you/>Huiling You</a>
|
<a href=/people/x/xingran-zhu/>Xingran Zhu</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--15><div class="card-body p-3 small">We describe the Uppsala NLP submission to SemEval-2021 Task 2 on multilingual and cross-lingual word-in-context disambiguation. We explore the usefulness of three pre-trained multilingual language models, XLM-RoBERTa (XLMR), Multilingual BERT (mBERT) and multilingual distilled BERT (mDistilBERT). We compare these three <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in two setups, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and as <a href=https://en.wikipedia.org/wiki/Software_feature>feature extractors</a>. In the second case we also experiment with using dependency-based information. We find that <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> is better than <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a>. XLMR performs better than mBERT in the cross-lingual setting both with <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a>, whereas these two models give a similar performance in the multilingual setting. mDistilBERT performs poorly with <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> but gives similar results to the other <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> when used as a feature extractor. We submitted our two best <a href=https://en.wikipedia.org/wiki/System>systems</a>, fine-tuned with XLMR and mBERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.16/>SkoltechNLP at SemEval-2021 Task 2 : Generating Cross-Lingual Training Data for the Word-in-Context Task<span class=acl-fixed-case>S</span>koltech<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 2: Generating Cross-Lingual Training Data for the Word-in-Context Task</a></strong><br><a href=/people/a/anton-razzhigaev/>Anton Razzhigaev</a>
|
<a href=/people/n/nikolay-arefyev/>Nikolay Arefyev</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--16><div class="card-body p-3 small">In this paper, we present a <a href=https://en.wikipedia.org/wiki/System>system</a> for the solution of the cross-lingual and multilingual word-in-context disambiguation task. Task organizers provided monolingual data in several languages, but no cross-lingual training data were available. To address the lack of the officially provided cross-lingual training data, we decided to generate such <a href=https://en.wikipedia.org/wiki/Data>data</a> ourselves. We describe a simple yet effective approach based on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and back translation of the lexical units to the original language used in the context of this shared task. In our experiments, we used a neural system based on the XLM-R, a pre-trained transformer-based masked language model, as a baseline. We show the effectiveness of the proposed approach as it allows to substantially improve the performance of this strong neural baseline model. In addition, in this study, we present multiple types of the XLM-R based classifier, experimenting with various ways of mixing information from the first and second occurrences of the target word in two samples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.17/>Zhestyatsky at SemEval-2021 Task 2 : ReLU over Cosine Similarity for BERT Fine-tuning<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 2: <span class=acl-fixed-case>R</span>e<span class=acl-fixed-case>LU</span> over Cosine Similarity for <span class=acl-fixed-case>BERT</span> Fine-tuning</a></strong><br><a href=/people/b/boris-zhestiankin/>Boris Zhestiankin</a>
|
<a href=/people/m/maria-ponomareva/>Maria Ponomareva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--17><div class="card-body p-3 small">This paper presents our contribution to SemEval-2021 Task 2 : Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC). Our experiments cover English (EN-EN) sub-track from the multilingual setting of the task. We experiment with several pre-trained language models and investigate an impact of different top-layers on <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. We find the combination of <a href=https://en.wikipedia.org/wiki/Cosine_similarity>Cosine Similarity</a> and ReLU activation leading to the most effective fine-tuning procedure. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> results in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> 92.7 %, which is the fourth-best score in EN-EN sub-track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.18/>SzegedAI at SemEval-2021 Task 2 : Zero-shot Approach for Multilingual and Cross-lingual Word-in-Context Disambiguation<span class=acl-fixed-case>S</span>zeged<span class=acl-fixed-case>AI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 2: Zero-shot Approach for Multilingual and Cross-lingual Word-in-Context Disambiguation</a></strong><br><a href=/people/g/gabor-berend/>Gábor Berend</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--18><div class="card-body p-3 small">In this paper, we introduce our <a href=https://en.wikipedia.org/wiki/System>system</a> that we participated with at the multilingual and cross-lingual word-in-context disambiguation SemEval 2021 shared task. In our experiments, we investigated the possibility of using an all-words fine-grained word sense disambiguation system trained purely on sense-annotated data in English and draw predictions on the semantic equivalence of words in context based on the similarity of the ranked lists of the (English) WordNet synsets returned for the target words decisions had to be made for. We overcame the multi,-and cross-lingual aspects of the shared task by applying a multilingual transformer for encoding the texts written in either <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. While our results lag behind top scoring submissions, it has the benefit that it not only provides a binary flag whether two words in their context have the same meaning, but also provides a more tangible output in the form of a ranked list of (English) WordNet synsets irrespective of the language of the input texts. As our framework is designed to be as generic as possible, it can be applied as a baseline for basically any language (supported by the multilingual transformed architecture employed) even in the absence of any additional form of language specific training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.20/>ECNU_ICA_1 SemEval-2021 Task 4 : Leveraging Knowledge-enhanced Graph Attention Networks for Reading Comprehension of Abstract Meaning<span class=acl-fixed-case>ECNU</span>_<span class=acl-fixed-case>ICA</span>_1 <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 4: Leveraging Knowledge-enhanced Graph Attention Networks for Reading Comprehension of Abstract Meaning</a></strong><br><a href=/people/p/pingsheng-liu/>Pingsheng Liu</a>
|
<a href=/people/l/linlin-wang/>Linlin Wang</a>
|
<a href=/people/q/qian-zhao/>Qian Zhao</a>
|
<a href=/people/h/hao-chen/>Hao Chen</a>
|
<a href=/people/y/yuxi-feng/>Yuxi Feng</a>
|
<a href=/people/x/xin-lin/>Xin Lin</a>
|
<a href=/people/l/liang-he/>Liang He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--20><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> for SemEval-2021 Task 4 : Reading Comprehension of Abstract Meaning. To accomplish this task, we utilize the Knowledge-Enhanced Graph Attention Network (KEGAT) architecture with a novel semantic space transformation strategy. It leverages heterogeneous knowledge to learn adequate evidences, and seeks for an effective semantic space of abstract concepts to better improve the ability of a machine in understanding the abstract meaning of natural language. Experimental results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves strong performance on this task in terms of both <a href=https://en.wikipedia.org/wiki/Perception>imperceptibility</a> and <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>nonspecificity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.21" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.21/>LRG at SemEval-2021 Task 4 : Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting<span class=acl-fixed-case>LRG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting</a></strong><br><a href=/people/a/abheesht-sharma/>Abheesht Sharma</a>
|
<a href=/people/h/harshit-pandey/>Harshit Pandey</a>
|
<a href=/people/g/gunjan-chhablani/>Gunjan Chhablani</a>
|
<a href=/people/y/yash-bhartia/>Yash Bhartia</a>
|
<a href=/people/t/tirtharaj-dash/>Tirtharaj Dash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--21><div class="card-body p-3 small">We present our approaches and methods for SemEval-2021 Task-4 Reading Comprehension of Abstract Meaning. Given a question with a fill-in-the-blank, and a corresponding context, the task is to predict the most suitable word from a list of 5 options. There are three subtasks : Imperceptibility, Non-Specificity and <a href=https://en.wikipedia.org/wiki/Intersection>Intersection</a>. We use encoders of transformers-based models pretrained on the MLM task to build our Fill-in-the-blank (FitB) models. Moreover, to model imperceptibility, we define certain linguistic features, and to model non-specificity, we leverage information from <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernyms</a> and <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponyms</a> provided by a <a href=https://en.wikipedia.org/wiki/Lexical_database>lexical database</a>. Specifically, for non-specificity, we try out augmentation techniques, and other <a href=https://en.wikipedia.org/wiki/Statistics>statistical techniques</a>. We also propose variants, namely Chunk Voting and Max Context, to take care of input length restrictions for BERT, etc. Additionally, we perform a thorough ablation study, and use Integrated Gradients to explain our predictions on a few samples. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve accuracies of 75.31 % and 77.84 %, on the test sets for subtask-I and subtask-II, respectively. For subtask-III, we achieve accuracies of 65.64 % and 64.27 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.23/>NLP-IIS@UT at SemEval-2021 Task 4 : Machine Reading Comprehension using the Long Document Transformer<span class=acl-fixed-case>NLP</span>-<span class=acl-fixed-case>IIS</span>@<span class=acl-fixed-case>UT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 4: Machine Reading Comprehension using the Long Document Transformer</a></strong><br><a href=/people/h/hossein-basafa/>Hossein Basafa</a>
|
<a href=/people/s/sajad-movahedi/>Sajad Movahedi</a>
|
<a href=/people/a/ali-ebrahimi/>Ali Ebrahimi</a>
|
<a href=/people/a/azadeh-shakery/>Azadeh Shakery</a>
|
<a href=/people/h/heshaam-faili/>Heshaam Faili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--23><div class="card-body p-3 small">This paper presents a technical report of our submission to the 4th task of SemEval-2021, titled : Reading Comprehension of Abstract Meaning. In this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we want to predict the correct answer based on a question given a context. Usually, contexts are very lengthy and require a large <a href=https://en.wikipedia.org/wiki/Receptive_field>receptive field</a> from the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. Thus, common contextualized language models like BERT miss fine representation and performance due to the limited capacity of the input tokens. To tackle this problem, we used the longformer model to better process the sequences. Furthermore, we utilized the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> proposed in the longformer benchmark on wikihop dataset which improved the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on our <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task data</a> from (23.01 % and 22.95 %) achieved by the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for subtask 1 and 2, respectively, to (70.30 % and 64.38 %).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.semeval-1.31.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.semeval-1.31/>HamiltonDinggg at SemEval-2021 Task 5 : Investigating Toxic Span Detection using RoBERTa Pre-training<span class=acl-fixed-case>H</span>amilton<span class=acl-fixed-case>D</span>inggg at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 5: Investigating Toxic Span Detection using <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a Pre-training</a></strong><br><a href=/people/h/huiyang-ding/>Huiyang Ding</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--31><div class="card-body p-3 small">This paper presents our system submission to task 5 : Toxic Spans Detection of the SemEval-2021 competition. The <a href=https://en.wikipedia.org/wiki/Competition>competition</a> aims at detecting the spans that make a toxic span toxic. In this paper, we demonstrate our system for detecting toxic spans, which includes expanding the toxic training set with Local Interpretable Model-Agnostic Explanations (LIME), fine-tuning RoBERTa model for detection, and error analysis. We found that feeding the model with an expanded training set using Reddit comments of polarized-toxicity and labeling with LIME on top of logistic regression classification could help RoBERTa more accurately learn to recognize toxic spans. We achieved a span-level F1 score of 0.6715 on the testing phase. Our quantitative and qualitative results show that the predictions from our <a href=https://en.wikipedia.org/wiki/System>system</a> could be a good supplement to the gold training set&#8217;s annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.32" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.32/>WVOQ at SemEval-2021 Task 6 : BART for Span Detection and Classification<span class=acl-fixed-case>WVOQ</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: <span class=acl-fixed-case>BART</span> for Span Detection and Classification</a></strong><br><a href=/people/c/cees-roele/>Cees Roele</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--32><div class="card-body p-3 small">Simultaneous span detection and classification is a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> not currently addressed in standard NLP frameworks. The present paper describes why and how an EncoderDecoder model was used to combine span detection and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> to address subtask 2 of SemEval-2021 Task 6.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.33/>HumorHunter at SemEval-2021 Task 7 : Humor and Offense Recognition with Disentangled Attention<span class=acl-fixed-case>H</span>umor<span class=acl-fixed-case>H</span>unter at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 7: Humor and Offense Recognition with Disentangled Attention</a></strong><br><a href=/people/y/yubo-xie/>Yubo Xie</a>
|
<a href=/people/j/junze-li/>Junze Li</a>
|
<a href=/people/p/pearl-pu/>Pearl Pu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--33><div class="card-body p-3 small">In this paper, we describe our system submitted to SemEval 2021 Task 7 : HaHackathon : Detecting and Rating Humor and Offense. The task aims at predicting whether the given text is humorous, the average humor rating given by the annotators, and whether the humor rating is controversial. In addition, the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> also involves predicting how offensive the text is. Our approach adopts the DeBERTa architecture with disentangled attention mechanism, where the attention scores between words are calculated based on their content vectors and relative position vectors. We also took advantage of the pre-trained language models and fine-tuned the DeBERTa model on all the four subtasks. We experimented with several BERT-like structures and found that the large DeBERTa model generally performs better. During the evaluation phase, our system achieved an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.9480 on subtask 1a, an RMSE of 0.5510 on subtask 1b, an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.4764 on subtask 1c, and an RMSE of 0.4230 on subtask 2a (rank 3 on the leaderboard).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.34/>Grenzlinie at SemEval-2021 Task 7 : Detecting and Rating Humor and Offense<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 7: Detecting and Rating Humor and Offense</a></strong><br><a href=/people/r/renyuan-liu/>Renyuan Liu</a>
|
<a href=/people/x/xiaobing-zhou/>Xiaobing Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--34><div class="card-body p-3 small">This paper introduces the result of Team Grenzlinie&#8217;s experiment in SemEval-2021 task 7 : HaHackathon : Detecting and Rating Humor and Offense. This <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> has two <a href=https://en.wikipedia.org/wiki/Task_(computing)>subtasks</a>. Subtask1 includes the humor detection task, the humor rating prediction task, and the humor controversy detection task. Subtask2 is an offensive rating prediction task. Detection task is a binary classification task, and the rating prediction task is a regression task between 0 to 5. 0 means the task is not humorous or not offensive, 5 means the task is very humorous or very offensive. For all the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, this paper chooses RoBERTa as the pre-trained model. In classification tasks, Bi-LSTM and <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial training</a> are adopted. In the <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression task</a>, the Bi-LSTM is also adopted. And then we propose a new <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> named compare method. Finally, our system achieves an <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 95.05 % in the humor detection task, <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 61.74 % in the humor controversy detection task, 0.6143 RMSE in humor rating task, 0.4761 RMSE in the offensive rating task on the test datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.36" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.36/>Humor@IITK at SemEval-2021 Task 7 : Large Language Models for Quantifying Humor and Offensiveness<span class=acl-fixed-case>IITK</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 7: Large Language Models for Quantifying Humor and Offensiveness</a></strong><br><a href=/people/a/aishwarya-gupta/>Aishwarya Gupta</a>
|
<a href=/people/a/avik-pal/>Avik Pal</a>
|
<a href=/people/b/bholeshwar-khurana/>Bholeshwar Khurana</a>
|
<a href=/people/l/lakshay-tyagi/>Lakshay Tyagi</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--36><div class="card-body p-3 small">Humor and Offense are highly subjective due to multiple <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a>, <a href=https://en.wikipedia.org/wiki/Cultural_knowledge>cultural knowledge</a>, and pragmatic competence. Hence, accurately detecting humorous and offensive texts has several compelling use cases in <a href=https://en.wikipedia.org/wiki/Recommender_system>Recommendation Systems</a> and Personalized Content Moderation. However, due to the lack of an extensive labeled dataset, most prior works in this domain have n&#8217;t explored large neural models for subjective humor understanding. This paper explores whether large neural models and their ensembles can capture the intricacies associated with humor / offense detection and rating. Our experiments on the SemEval-2021 Task 7 : HaHackathon show that we can develop reasonable humor and offense detection systems with such models. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are ranked 3rd in subtask 1b and consistently ranked around the top 33 % of the leaderboard for the remaining subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.37/>RoMa at SemEval-2021 Task 7 : A Transformer-based Approach for Detecting and Rating Humor and Offense<span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>M</span>a at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 7: A Transformer-based Approach for Detecting and Rating Humor and Offense</a></strong><br><a href=/people/r/roberto-labadie/>Roberto Labadie</a>
|
<a href=/people/m/mariano-jason-rodriguez/>Mariano Jason Rodriguez</a>
|
<a href=/people/r/reynier-ortega/>Reynier Ortega</a>
|
<a href=/people/p/paolo-rosso/>Paolo Rosso</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--37><div class="card-body p-3 small">In this paper we describe the systems used by the RoMa team in the shared task on Detecting and Rating Humor and Offense (HaHackathon) at SemEval 2021. Our systems rely on data representations learned through fine-tuned neural language models. Particularly, we explore two distinct <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>. The first one is based on a Siamese Neural Network (SNN) combined with a graph-based clustering method. The SNN model is used for learning a latent space where instances of <a href=https://en.wikipedia.org/wiki/Humour>humor</a> and non-humor can be distinguished. The clustering method is applied to build prototypes of both classes which are used for training and classifying new messages. The second one combines neural language model representations with a <a href=https://en.wikipedia.org/wiki/Linear_regression>linear regression model</a> which makes the final ratings. Our systems achieved the best results for humor classification using <a href=https://en.wikipedia.org/wiki/Conceptual_model>model one</a>, whereas for offensive and humor rating the second <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> obtained better performance. In the case of the controversial humor prediction, the most significant improvement was achieved by a <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> of the neural language model. In general, the results achieved are encouraging and give us a starting point for further improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.43/>BLCUFIGHT at SemEval-2021 Task 10 : Novel Unsupervised Frameworks For Source-Free Domain Adaptation<span class=acl-fixed-case>BLCUFIGHT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 10: Novel Unsupervised Frameworks For Source-Free Domain Adaptation</a></strong><br><a href=/people/w/weikang-wang/>Weikang Wang</a>
|
<a href=/people/y/yi-wu/>Yi Wu</a>
|
<a href=/people/y/yixiang-liu/>Yixiang Liu</a>
|
<a href=/people/p/pengyuan-liu/>Pengyuan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--43><div class="card-body p-3 small">Domain adaptation assumes that samples from source and target domains are freely accessible during a training phase. However, such assumption is rarely plausible in the real-world and may causes data-privacy issues, especially when the label of the source domain can be a sensitive attribute as an identifier. SemEval-2021 task 10 focuses on these issues. We participate in the task and propose novel <a href=https://en.wikipedia.org/wiki/Conceptual_framework>frameworks</a> based on self-training method. In our <a href=https://en.wikipedia.org/wiki/System>systems</a>, two different <a href=https://en.wikipedia.org/wiki/Software_framework>frameworks</a> are designed to solve <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> and <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>. These approaches are tested to be effective which ranks the third among all system in subtask A, and ranks the first among all system in subtask B.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.52/>BOUN at SemEval-2021 Task 9 : Text Augmentation Techniques for Fact Verification in Tabular Data<span class=acl-fixed-case>BOUN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 9: Text Augmentation Techniques for Fact Verification in Tabular Data</a></strong><br><a href=/people/a/abdullatif-koksal/>Abdullatif Köksal</a>
|
<a href=/people/y/yusuf-yuksel/>Yusuf Yüksel</a>
|
<a href=/people/b/bekir-yildirim/>Bekir Yıldırım</a>
|
<a href=/people/a/arzucan-ozgur/>Arzucan Özgür</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--52><div class="card-body p-3 small">In this paper, we present our text augmentation based approach for the Table Statement Support Subtask (Phase A) of SemEval-2021 Task 9. We experiment with different text augmentation techniques such as <a href=https://en.wikipedia.org/wiki/Back_translation>back translation</a> and synonym swapping using Word2Vec and <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>. We show that text augmentation techniques lead to 2.5 % improvement in F1 on the test set. Further, we investigate the impact of <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> and joint learning on fact verification in tabular data by utilizing the SemTabFacts and TabFact datasets. We observe that joint learning improves the F1 scores on the SemTabFacts and TabFact test sets by 3.31 % and 0.77 %, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.53/>IITK at SemEval-2021 Task 10 : Source-Free Unsupervised Domain Adaptation using Class Prototypes<span class=acl-fixed-case>IITK</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 10: Source-Free Unsupervised Domain Adaptation using Class Prototypes</a></strong><br><a href=/people/h/harshit-kumar-iit/>Harshit Kumar</a>
|
<a href=/people/j/jinang-shah/>Jinang Shah</a>
|
<a href=/people/n/nidhi-hegde/>Nidhi Hegde</a>
|
<a href=/people/p/priyanshu-gupta/>Priyanshu Gupta</a>
|
<a href=/people/v/vaibhav-jindal/>Vaibhav Jindal</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--53><div class="card-body p-3 small">Recent progress in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> has primarily been fueled by the availability of large amounts of annotated data that is obtained from highly expensive manual annotating pro-cesses. To tackle this issue of availability of annotated data, a lot of research has been done on unsupervised domain adaptation that tries to generate systems for an unlabelled target domain data, given labeled source domain data. However, the availability of annotated or labelled source domain dataset ca n&#8217;t always be guaranteed because of <a href=https://en.wikipedia.org/wiki/Data_privacy>data-privacy issues</a>. This is especially the case with medical data, as <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> may contain sensitive information of the patients. Source-free domain adaptation (SFDA) aims to resolve this issue by us-ing <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on the <a href=https://en.wikipedia.org/wiki/Data>source data</a> instead of using the original annotated source data. In this work, we try to build SFDA systems for <a href=https://en.wikipedia.org/wiki/Semantic_processing>semantic processing</a> by specifically focusing on the negation detection subtask of the SemEval2021 Task 10. We propose two approaches -ProtoAUGandAdapt-ProtoAUGthat use the idea of <a href=https://en.wikipedia.org/wiki/Self-entropy>self-entropy</a> to choose reliable and high confidence samples, which are then used for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> and subsequent training of the models. Our methods report an improvement of up to 7 % in <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a> over the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> for the Negation Detection subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.54/>PTST-UoM at SemEval-2021 Task 10 : Parsimonious Transfer for Sequence Tagging<span class=acl-fixed-case>PTST</span>-<span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>M</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 10: Parsimonious Transfer for Sequence Tagging</a></strong><br><a href=/people/k/kemal-kurniawan/>Kemal Kurniawan</a>
|
<a href=/people/l/lea-frermann/>Lea Frermann</a>
|
<a href=/people/p/philip-schulz/>Philip Schulz</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--54><div class="card-body p-3 small">This paper describes PTST, a source-free unsupervised domain adaptation technique for sequence tagging, and its application to the SemEval-2021 Task 10 on time expression recognition. PTST is an extension of the cross-lingual parsimonious parser transfer framework, which uses high-probability predictions of the source model as a supervision signal in self-training. We extend the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> to a sequence prediction setting, and demonstrate its applicability to unsupervised domain adaptation. PTST achieves F1 score of 79.6 % on the official test set, with the <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> of 90.1 %, the highest out of 14 submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.55/>Self-Adapter at SemEval-2021 Task 10 : Entropy-based Pseudo-Labeler for Source-free Domain Adaptation<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 10: Entropy-based Pseudo-Labeler for Source-free Domain Adaptation</a></strong><br><a href=/people/s/sangwon-yoon/>Sangwon Yoon</a>
|
<a href=/people/y/yanghoon-kim/>Yanghoon Kim</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--55><div class="card-body p-3 small">Source-free domain adaptation is an emerging line of work in deep learning research since it is closely related to the real-world environment. We study the domain adaption in the sequence labeling problem where the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on the source domain data is given. We propose two <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> : Self-Adapter and Selective Classifier Training. Self-Adapter is a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training method</a> that uses sentence-level pseudo-labels filtered by the <a href=https://en.wikipedia.org/wiki/Self-entropy>self-entropy threshold</a> to provide supervision to the whole model. Selective Classifier Training uses token-level pseudo-labels and supervises only the classification layer of the model. The proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> are evaluated on data provided by SemEval-2021 task 10 and Self-Adapter achieves 2nd rank performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.56/>The University of Arizona at SemEval-2021 Task 10 : Applying Self-training, Active Learning and Data Augmentation to Source-free Domain Adaptation<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>A</span>rizona at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 10: Applying Self-training, Active Learning and Data Augmentation to Source-free Domain Adaptation</a></strong><br><a href=/people/x/xin-su/>Xin Su</a>
|
<a href=/people/y/yiyun-zhao/>Yiyun Zhao</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--56><div class="card-body p-3 small">This paper describes our systems for negation detection and time expression recognition in SemEval 2021 Task 10, Source-Free Domain Adaptation for Semantic Processing. We show that self-training, <a href=https://en.wikipedia.org/wiki/Active_learning_(machine_learning)>active learning</a> and data augmentation techniques can improve the generalization ability of the model on the unlabeled target domain data without accessing source domain data. We also perform detailed ablation studies and error analyses for our time expression recognition systems to identify the source of the performance improvement and give constructive feedback on the temporal normalization annotation guidelines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.58/>YNU-HPCC at SemEval-2021 Task 11 : Using a BERT Model to Extract Contributions from NLP Scholarly Articles<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 11: Using a <span class=acl-fixed-case>BERT</span> Model to Extract Contributions from <span class=acl-fixed-case>NLP</span> Scholarly Articles</a></strong><br><a href=/people/x/xinge-ma/>Xinge Ma</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--58><div class="card-body p-3 small">This paper describes the system we built as the YNU-HPCC team in the SemEval-2021 Task 11 : NLPContributionGraph. This task involves first identifying sentences in the given natural language processing (NLP) scholarly articles that reflect research contributions through binary classification ; then identifying the core scientific terms and their relation phrases from these contribution sentences by sequence labeling ; and finally, these scientific terms and relation phrases are categorized, identified, and organized into subject-predicate-object triples to form a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> with the help of <a href=https://en.wikipedia.org/wiki/Multiclass_classification>multiclass classification</a> and <a href=https://en.wikipedia.org/wiki/Multi-label_classification>multi-label classification</a>. We developed a system for this task using a pre-trained language representation model called BERT that stands for Bidirectional Encoder Representations from Transformers, and achieved good results. The average F1-score for Evaluation Phase 2, Part 1 was 0.4562 and ranked 7th, and the average F1-score for Evaluation Phase 2, Part 2 was 0.6541, and also ranked 7th.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.59/>ITNLP at SemEval-2021 Task 11 : Boosting BERT with Sampling and Adversarial Training for Knowledge Extraction<span class=acl-fixed-case>ITNLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 11: Boosting <span class=acl-fixed-case>BERT</span> with Sampling and Adversarial Training for Knowledge Extraction</a></strong><br><a href=/people/g/genyu-zhang/>Genyu Zhang</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/c/changhong-he/>Changhong He</a>
|
<a href=/people/l/lei-lin/>Lei Lin</a>
|
<a href=/people/c/cheng-jie-sun/>Chengjie Sun</a>
|
<a href=/people/l/lili-shan/>Lili Shan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--59><div class="card-body p-3 small">This paper describes the winning system in the End-to-end Pipeline phase for the NLPContributionGraph task. The system is composed of three BERT-based models and the three <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are used to extract sentences, entities and triples respectively. Experiments show that sampling and adversarial training can greatly boost the <a href=https://en.wikipedia.org/wiki/System>system</a>. In End-to-end Pipeline phase, our <a href=https://en.wikipedia.org/wiki/System>system</a> got an average F1 of 0.4703, significantly higher than the second-placed system which got an average F1 of 0.3828.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.60/>Duluth at SemEval-2021 Task 11 : Applying DeBERTa to Contributing Sentence Selection and Dependency Parsing for Entity Extraction<span class=acl-fixed-case>D</span>uluth at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 11: Applying <span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>BERT</span>a to Contributing Sentence Selection and Dependency Parsing for Entity Extraction</a></strong><br><a href=/people/a/anna-martin/>Anna Martin</a>
|
<a href=/people/t/ted-pedersen/>Ted Pedersen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--60><div class="card-body p-3 small">This paper describes the Duluth system that participated in SemEval-2021 Task 11, NLP Contribution Graph. It details the extraction of contribution sentences and scientific entities and their relations from scholarly articles in the domain of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. Our solution uses deBERTa for multi-class sentence classification to extract the contributing sentences and their type, and dependency parsing to outline each sentence and extract subject-predicate-object triples. Our system ranked fifth of seven for Phase 1 : end-to-end pipeline, sixth of eight for Phase 2 Part 1 : phrases and triples, and fifth of eight for Phase 2 Part 2 : triples extraction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.61/>INNOVATORS at SemEval-2021 Task-11 : A Dependency Parsing and BERT-based model for Extracting Contribution Knowledge from Scientific Papers<span class=acl-fixed-case>INNOVATORS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task-11: A Dependency Parsing and <span class=acl-fixed-case>BERT</span>-based model for Extracting Contribution Knowledge from Scientific Papers</a></strong><br><a href=/people/h/hardik-arora/>Hardik Arora</a>
|
<a href=/people/t/tirthankar-ghosal/>Tirthankar Ghosal</a>
|
<a href=/people/s/sandeep-kumar/>Sandeep Kumar</a>
|
<a href=/people/s/suraj-patwal/>Suraj Patwal</a>
|
<a href=/people/p/phil-gooch/>Phil Gooch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--61><div class="card-body p-3 small">In this work, we describe our system submission to the SemEval 2021 Task 11 : NLP Contribution Graph Challenge. We attempt all the three sub-tasks in the challenge and report our results. Subtask 1 aims to identify the contributing sentences in a given publication. Subtask 2 follows from Subtask 1 to extract the scientific term and predicate phrases from the identified contributing sentences. The final Subtask 3 entails extracting triples (subject, predicate, object) from the phrases and categorizing them under one or more defined information units. With the NLPContributionGraph Shared Task, the organizers formalized the building of a scholarly contributions-focused graph over NLP scholarly articles as an automated task. Our approaches include a BERT-based classification model for identifying the contributing sentences in a research publication, a rule-based dependency parsing for phrase extraction, followed by a CNN-based model for information units classification, and a set of rules for triples extraction. The quantitative results show that we obtain the 5th, 5th, and 7th rank respectively in three evaluation phases. We make our codes available at https://github.com/HardikArora17/SemEval-2021-INNOVATORS.<i>triples</i> (subject, predicate, object) from the phrases and categorizing them under one or more defined information units. With the NLPContributionGraph Shared Task, the organizers formalized the building of a scholarly contributions-focused graph over NLP scholarly articles as an automated task. Our approaches include a BERT-based classification model for identifying the contributing sentences in a research publication, a rule-based dependency parsing for phrase extraction, followed by a CNN-based model for information units classification, and a set of rules for triples extraction. The quantitative results show that we obtain the 5th, 5th, and 7th rank respectively in three evaluation phases. We make our codes available at https://github.com/HardikArora17/SemEval-2021-INNOVATORS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.63/>HITSZ-HLT at SemEval-2021 Task 5 : Ensemble Sequence Labeling and Span Boundary Detection for Toxic Span Detection<span class=acl-fixed-case>HITSZ</span>-<span class=acl-fixed-case>HLT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 5: Ensemble Sequence Labeling and Span Boundary Detection for Toxic Span Detection</a></strong><br><a href=/people/q/qinglin-zhu/>Qinglin Zhu</a>
|
<a href=/people/z/zijie-lin/>Zijie Lin</a>
|
<a href=/people/y/yice-zhang/>Yice Zhang</a>
|
<a href=/people/j/jingyi-sun/>Jingyi Sun</a>
|
<a href=/people/x/xiang-li/>Xiang Li</a>
|
<a href=/people/q/qihui-lin/>Qihui Lin</a>
|
<a href=/people/y/yixue-dang/>Yixue Dang</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--63><div class="card-body p-3 small">This paper presents the winning <a href=https://en.wikipedia.org/wiki/System>system</a> that participated in SemEval-2021 Task 5 : Toxic Spans Detection. This task aims to locate those spans that attribute to the text&#8217;s toxicity within a text, which is crucial for semi-automated moderation in online discussions. We formalize this task as the Sequence Labeling (SL) problem and the Span Boundary Detection (SBD) problem separately and employ three state-of-the-art models. Next, we integrate predictions of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to produce a more credible and complement result. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves a char-level score of 70.83 %, ranking 1/91. In addition, we also explore the lexicon-based method, which is strongly interpretable and flexible in practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.65/>UPB at SemEval-2021 Task 8 : Extracting Semantic Information on Measurements as Multi-Turn Question Answering<span class=acl-fixed-case>UPB</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 8: Extracting Semantic Information on Measurements as Multi-Turn Question Answering</a></strong><br><a href=/people/a/andrei-marius-avram/>Andrei-Marius Avram</a>
|
<a href=/people/g/george-eduard-zaharia/>George-Eduard Zaharia</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a>
|
<a href=/people/m/mihai-dascalu/>Mihai Dascalu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--65><div class="card-body p-3 small">Extracting semantic information on measurements and counts is an important topic in terms of analyzing scientific discourses. The 8th task of SemEval-2021 : Counts and Measurements (MeasEval) aimed to boost research in this direction by providing a new dataset on which participants train their models to extract meaningful information on measurements from scientific texts. The competition is composed of five subtasks that build on top of each other : (1) quantity span identification, (2) unit extraction from the identified quantities and their value modifier classification, (3) span identification for measured entities and measured properties, (4) qualifier span identification, and (5) relation extraction between the identified quantities, measured entities, measured properties, and qualifiers. We approached these challenges by first identifying the quantities, extracting their units of measurement, classifying them with corresponding modifiers, and afterwards using them to jointly solve the last three subtasks in a multi-turn question answering manner. Our best performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtained an overlapping F1-score of 36.91 % on the test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.66/>IITK@LCP at SemEval-2021 Task 1 : Classification for Lexical Complexity Regression Task<span class=acl-fixed-case>IITK</span>@<span class=acl-fixed-case>LCP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 1: Classification for Lexical Complexity Regression Task</a></strong><br><a href=/people/n/neil-shirude/>Neil Shirude</a>
|
<a href=/people/s/sagnik-mukherjee/>Sagnik Mukherjee</a>
|
<a href=/people/t/tushar-shandhilya/>Tushar Shandhilya</a>
|
<a href=/people/a/ananta-mukherjee/>Ananta Mukherjee</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--66><div class="card-body p-3 small">This paper describes our contribution to SemEval 2021 Task 1 (Shardlow et al., 2021): Lexical Complexity Prediction. In our approach, we leverage the ELECTRA model and attempt to mirror the data annotation scheme. Although the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression task</a>, we show that we can treat it as an aggregation of several <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification and regression models</a>. This somewhat counter-intuitive approach achieved an MAE score of 0.0654 for Sub-Task 1 and MAE of 0.0811 on Sub-Task 2. Additionally, we used the concept of weak supervision signals from Gloss-BERT in our work, and it significantly improved the MAE score in Sub-Task 1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.67/>LCP-RIT at SemEval-2021 Task 1 : Exploring Linguistic Features for Lexical Complexity Prediction<span class=acl-fixed-case>LCP</span>-<span class=acl-fixed-case>RIT</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 1: Exploring Linguistic Features for Lexical Complexity Prediction</a></strong><br><a href=/people/a/abhinandan-tejalkumar-desai/>Abhinandan Tejalkumar Desai</a>
|
<a href=/people/k/kai-north/>Kai North</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/c/christopher-homan/>Christopher Homan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--67><div class="card-body p-3 small">This paper describes team LCP-RIT&#8217;s submission to the SemEval-2021 Task 1 : Lexical Complexity Prediction (LCP). The task organizers provided participants with an augmented version of CompLex (Shardlow et al., 2020), an English multi-domain dataset in which words in context were annotated with respect to their <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> using a five point Likert scale. Our system uses <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> and a wide range of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> (e.g. psycholinguistic features, <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a>, <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequency</a>, POS tags) to predict the complexity of single words in this dataset. We analyze the impact of different linguistic features on the classification performance and we evaluate the results in terms of <a href=https://en.wikipedia.org/wiki/Mean_absolute_error>mean absolute error</a>, <a href=https://en.wikipedia.org/wiki/Mean_squared_error>mean squared error</a>, <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a>, and <a href=https://en.wikipedia.org/wiki/Spearman_correlation>Spearman correlation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.69/>CompNA at SemEval-2021 Task 1 : Prediction of lexical complexity analyzing heterogeneous features<span class=acl-fixed-case>C</span>omp<span class=acl-fixed-case>NA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 1: Prediction of lexical complexity analyzing heterogeneous features</a></strong><br><a href=/people/g/giuseppe-vettigli/>Giuseppe Vettigli</a>
|
<a href=/people/a/antonio-sorgente/>Antonio Sorgente</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--69><div class="card-body p-3 small">This paper describes the CompNa model that has been submitted to the Lexical Complexity Prediction (LCP) shared task hosted at SemEval 2021 (Task 1). The solution is based on combining features of different nature through an ensambling method based on <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>Decision Trees</a> and trained using <a href=https://en.wikipedia.org/wiki/Gradient_boosting>Gradient Boosting</a>. We discuss the results of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and highlight the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with more predictive capabilities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.73/>CS-UM6P at SemEval-2021 Task 1 : A Deep Learning Model-based Pre-trained Transformer Encoder for Lexical Complexity<span class=acl-fixed-case>CS</span>-<span class=acl-fixed-case>UM</span>6<span class=acl-fixed-case>P</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 1: A Deep Learning Model-based Pre-trained Transformer Encoder for Lexical Complexity</a></strong><br><a href=/people/n/nabil-el-mamoun/>Nabil El Mamoun</a>
|
<a href=/people/a/abdelkader-el-mahdaouy/>Abdelkader El Mahdaouy</a>
|
<a href=/people/a/abdellah-el-mekki/>Abdellah El Mekki</a>
|
<a href=/people/k/kabil-essefar/>Kabil Essefar</a>
|
<a href=/people/i/ismail-berrada/>Ismail Berrada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--73><div class="card-body p-3 small">Lexical Complexity Prediction (LCP) involves assigning a difficulty score to a particular word or expression, in a text intended for a target audience. In this paper, we introduce a new deep learning-based system for this challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The proposed system consists of a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a>, based on pre-trained transformer encoder, for word and Multi-Word Expression (MWE) complexity prediction. First, on top of the encoder&#8217;s contextualized word embedding, our model employs an attention layer on the input context and the complex word or MWE. Then, the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention output</a> is concatenated with the pooled output of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and passed to a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression module</a>. We investigate both single-task and joint training on both Sub-Tasks data using multiple pre-trained transformer-based encoders. The obtained results are very promising and show the effectiveness of fine-tuning pre-trained transformers for LCP task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.79/>RG PA at SemEval-2021 Task 1 : A Contextual Attention-based Model with RoBERTa for Lexical Complexity Prediction<span class=acl-fixed-case>RG</span> <span class=acl-fixed-case>PA</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 1: A Contextual Attention-based Model with <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a for Lexical Complexity Prediction</a></strong><br><a href=/people/g/gang-rao/>Gang Rao</a>
|
<a href=/people/m/maochang-li/>Maochang Li</a>
|
<a href=/people/x/xiaolong-hou/>Xiaolong Hou</a>
|
<a href=/people/l/lianxin-jiang/>Lianxin Jiang</a>
|
<a href=/people/y/yang-mo/>Yang Mo</a>
|
<a href=/people/j/jianping-shen/>Jianping Shen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--79><div class="card-body p-3 small">In this paper we propose a contextual attention based model with two-stage fine-tune training using RoBERTa. First, we perform the first-stage fine-tune on corpus with RoBERTa, so that the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> can learn some prior <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>. Then we get the contextual embedding of context words based on the token-level embedding with the fine-tuned model. And we use Kfold cross-validation to get K models and ensemble them to get the final result. Finally, we attain the 2nd place in the final evaluation phase of sub-task 2 with <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>pearson correlation</a> of 0.8575.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.81/>CLULEX at SemEval-2021 Task 1 : A Simple System Goes a Long Way<span class=acl-fixed-case>CLULEX</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 1: A Simple System Goes a Long Way</a></strong><br><a href=/people/g/greta-smolenska/>Greta Smolenska</a>
|
<a href=/people/p/peter-kolb/>Peter Kolb</a>
|
<a href=/people/s/sinan-tang/>Sinan Tang</a>
|
<a href=/people/m/mironas-bitinis/>Mironas Bitinis</a>
|
<a href=/people/h/hector-hernandez/>Héctor Hernández</a>
|
<a href=/people/e/elin-asklov/>Elin Asklöv</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--81><div class="card-body p-3 small">This paper presents the <a href=https://en.wikipedia.org/wiki/System>system</a> we submitted to the first Lexical Complexity Prediction (LCP) Shared Task 2021. The Shared Task provides participants with a new English dataset that includes context of the target word. We participate in the single-word complexity prediction sub-task and focus on <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. Our best system is trained on <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> (Pearson&#8217;s score of 0.7942). We demonstrate, however, that a simpler <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature set</a> achieves comparable results and submit a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on 36 <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>linguistic features</a> (Pearson&#8217;s score of 0.7925).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.83/>UNBNLP at SemEval-2021 Task 1 : Predicting lexical complexity with masked language models and character-level encoders<span class=acl-fixed-case>UNBNLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 1: Predicting lexical complexity with masked language models and character-level encoders</a></strong><br><a href=/people/m/milton-king/>Milton King</a>
|
<a href=/people/a/ali-hakimi-parizi/>Ali Hakimi Parizi</a>
|
<a href=/people/s/samin-fakharian/>Samin Fakharian</a>
|
<a href=/people/p/paul-cook/>Paul Cook</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--83><div class="card-body p-3 small">In this paper, we present three <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a> for English lexical complexity prediction of single and multiword expressions for SemEval-2021 Task 1. We explore the use of statistical baseline features, masked language models, and character-level encoders to predict the complexity of a target token in context. Our best <a href=https://en.wikipedia.org/wiki/System>system</a> combines information from these three sources. The results indicate that information from masked language models and <a href=https://en.wikipedia.org/wiki/Character_encoding>character-level encoders</a> can be combined to improve lexical complexity prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.92" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.92/>GX at SemEval-2021 Task 2 : BERT with Lemma Information for MCL-WiC Task<span class=acl-fixed-case>GX</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 2: <span class=acl-fixed-case>BERT</span> with Lemma Information for <span class=acl-fixed-case>MCL</span>-<span class=acl-fixed-case>W</span>i<span class=acl-fixed-case>C</span> Task</a></strong><br><a href=/people/w/wanying-xie/>Wanying Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--92><div class="card-body p-3 small">This paper presents the GX system for the Multilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC) task. The purpose of the MCL-WiC task is to tackle the challenge of capturing the polysemous nature of words without relying on a fixed sense inventory in a multilingual and cross-lingual setting. To solve the problems, we use context-specific word embeddings from BERT to eliminate the ambiguity between words in different contexts. For languages without an available training corpus, such as <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, we use neuron machine translation model to translate the English data released by the organizers to obtain available pseudo-data. In this paper, we apply our <a href=https://en.wikipedia.org/wiki/System>system</a> to the English and Chinese multilingual setting and the experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> has certain advantages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.93/>PALI at SemEval-2021 Task 2 : Fine-Tune XLM-RoBERTa for Word in Context Disambiguation<span class=acl-fixed-case>PALI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 2: Fine-Tune <span class=acl-fixed-case>XLM</span>-<span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a for Word in Context Disambiguation</a></strong><br><a href=/people/s/shuyi-xie/>Shuyi Xie</a>
|
<a href=/people/j/jian-ma/>Jian Ma</a>
|
<a href=/people/h/haiqin-yang/>Haiqin Yang</a>
|
<a href=/people/l/lianxin-jiang/>Lianxin Jiang</a>
|
<a href=/people/y/yang-mo/>Yang Mo</a>
|
<a href=/people/j/jianping-shen/>Jianping Shen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--93><div class="card-body p-3 small">This paper presents the PALI team&#8217;s winning system for SemEval-2021 Task 2 : Multilingual and Cross-lingual Word-in-Context Disambiguation. We fine-tune XLM-RoBERTa model to solve the task of word in context disambiguation, i.e., to determine whether the target word in the two contexts contains the same meaning or not. In implementation, we first specifically design an input tag to emphasize the target word in the contexts. Second, we construct a new vector on the fine-tuned embeddings from XLM-RoBERTa and feed it to a fully-connected network to output the probability of whether the target word in the context has the same meaning or not. The new <a href=https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)>vector</a> is attained by concatenating the embedding of the [ CLS ] token and the embeddings of the target word in the contexts. In training, we explore several tricks, such as the Ranger optimizer, <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, and adversarial training, to improve the <a href=https://en.wikipedia.org/wiki/Prediction>model prediction</a>. Consequently, we attain the first place in all four cross-lingual tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.96.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--96 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.96 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.96/>Cambridge at SemEval-2021 Task 2 : Neural WiC-Model with Data Augmentation and Exploration of Representation<span class=acl-fixed-case>C</span>ambridge at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 2: Neural <span class=acl-fixed-case>W</span>i<span class=acl-fixed-case>C</span>-Model with Data Augmentation and Exploration of Representation</a></strong><br><a href=/people/z/zheng-yuan/>Zheng Yuan</a>
|
<a href=/people/d/david-strohmaier/>David Strohmaier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--96><div class="card-body p-3 small">This paper describes the system of the Cambridge team submitted to the SemEval-2021 shared task on Multilingual and Cross-lingual Word-in-Context Disambiguation. Building on top of a pre-trained masked language model, our system is first pre-trained on out-of-domain data, and then fine-tuned on in-domain data. We demonstrate the effectiveness of the proposed two-step training strategy and the benefits of <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> from both existing examples and new resources. We further investigate different <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> and show that the addition of distance-based features is helpful in the word-in-context disambiguation task. Our system yields highly competitive results in the cross-lingual track without training on any cross-lingual data ; and achieves state-of-the-art results in the multilingual track, ranking first in two languages (Arabic and Russian) and second in <a href=https://en.wikipedia.org/wiki/French_language>French</a> out of 171 submitted systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.103/>LIORI at SemEval-2021 Task 2 : Span Prediction and Binary Classification approaches to Word-in-Context Disambiguation<span class=acl-fixed-case>LIORI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 2: Span Prediction and Binary Classification approaches to Word-in-Context Disambiguation</a></strong><br><a href=/people/a/adis-davletov/>Adis Davletov</a>
|
<a href=/people/n/nikolay-arefyev/>Nikolay Arefyev</a>
|
<a href=/people/d/denis-gordeev/>Denis Gordeev</a>
|
<a href=/people/a/alexey-rey/>Alexey Rey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--103><div class="card-body p-3 small">This paper presents our approaches to SemEval-2021 Task 2 : Multilingual and Cross-lingual Word-in-Context Disambiguation task. The first approach attempted to reformulate the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> as a <a href=https://en.wikipedia.org/wiki/Question_answering>question answering problem</a>, while the second one framed it as a binary classification problem. Our best system, which is an ensemble of XLM-R based binary classifiers trained with data augmentation, is among the 3 best-performing systems for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> in the multilingual subtask. In the post-evaluation period, we experimented with <a href=https://en.wikipedia.org/wiki/Batch_normalization>batch normalization</a>, subword pooling and target word occurrence aggregation methods, resulting in further performance improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.104/>FII_CROSS at SemEval-2021 Task 2 : Multilingual and Cross-lingual Word-in-Context Disambiguation<span class=acl-fixed-case>FII</span>_<span class=acl-fixed-case>CROSS</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation</a></strong><br><a href=/people/c/ciprian-bodnar/>Ciprian Bodnar</a>
|
<a href=/people/a/andrada-tapuc/>Andrada Tapuc</a>
|
<a href=/people/c/cosmin-pintilie/>Cosmin Pintilie</a>
|
<a href=/people/d/daniela-gifu/>Daniela Gifu</a>
|
<a href=/people/d/diana-trandabat/>Diana Trandabat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--104><div class="card-body p-3 small">This paper presents a word-in-context disambiguation system. The task focuses on capturing the polysemous nature of words in a multilingual and cross-lingual setting, without considering a strict inventory of word meanings. The system applies Natural Language Processing algorithms on datasets from SemEval 2021 Task 2, being able to identify the meaning of words for the languages <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, without making use of any additional mono- or multilingual resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--106 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.106/>UoR at SemEval-2021 Task 4 : Using Pre-trained BERT Token Embeddings for Question Answering of Abstract Meaning<span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>R</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 4: Using Pre-trained <span class=acl-fixed-case>BERT</span> Token Embeddings for Question Answering of Abstract Meaning</a></strong><br><a href=/people/t/thanet-markchom/>Thanet Markchom</a>
|
<a href=/people/h/huizhi-liang/>Huizhi Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--106><div class="card-body p-3 small">Most <a href=https://en.wikipedia.org/wiki/Question_answering>question answering tasks</a> focuses on <a href=https://en.wikipedia.org/wiki/Prediction>predicting concrete answers</a>, e.g., <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entities</a>. These <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> can be normally achieved by understanding the contexts without additional information required. In Reading Comprehension of Abstract Meaning (ReCAM) task, the abstract answers are introduced. To understand <a href=https://en.wikipedia.org/wiki/Abstract_and_concrete>abstract meanings</a> in the context, additional knowledge is essential. In this paper, we propose an approach that leverages the pre-trained BERT Token embeddings as a prior knowledge resource. According to the results, our approach using the pre-trained BERT outperformed the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. It shows that the pre-trained BERT token embeddings can be used as additional knowledge for understanding abstract meanings in <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--107 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.107/>Noobs at Semeval-2021 Task 4 : Masked Language Modeling for abstract answer prediction<span class=acl-fixed-case>S</span>emeval-2021 Task 4: Masked Language Modeling for abstract answer prediction</a></strong><br><a href=/people/s/shikhar-shukla/>Shikhar Shukla</a>
|
<a href=/people/s/sarthak-sarthak/>Sarthak Sarthak</a>
|
<a href=/people/k/karm-veer-arya/>Karm Veer Arya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--107><div class="card-body p-3 small">This paper presents the <a href=https://en.wikipedia.org/wiki/System>system</a> developed by our team for Semeval 2021 Task 4 : Reading Comprehension of Abstract Meaning. The aim of the task was to benchmark the NLP techniques in understanding the abstract concepts present in a passage, and then predict the missing word in a human written summary of the passage. We trained a Roberta-Large model trained with a masked language modeling objective. In cases where this <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> failed to predict one of the available options, another Roberta-Large model trained as a <a href=https://en.wikipedia.org/wiki/Binary_classifier>binary classifier</a> was used to predict correct and incorrect options. We used passage summary generated by Pegasus model and question as inputs. Our best solution was an ensemble of these 2 systems. We achieved an accuracy of 86.22 % on subtask 1 and 87.10 % on subtask 2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.109/>PINGAN Omini-Sinitic at SemEval-2021 Task 4 : Reading Comprehension of Abstract Meaning<span class=acl-fixed-case>PINGAN</span> Omini-Sinitic at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 4:Reading Comprehension of Abstract Meaning</a></strong><br><a href=/people/y/ye-wang/>Ye Wang</a>
|
<a href=/people/y/yanmeng-wang/>Yanmeng Wang</a>
|
<a href=/people/h/haijun-zhu/>Haijun Zhu</a>
|
<a href=/people/b/bo-zeng/>Bo Zeng</a>
|
<a href=/people/z/zhenghong-hao/>Zhenghong Hao</a>
|
<a href=/people/s/shaojun-wang/>Shaojun Wang</a>
|
<a href=/people/j/jing-xiao/>Jing Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--109><div class="card-body p-3 small">This paper describes the winning system for subtask 2 and the second-placed system for subtask 1 in SemEval 2021 Task 4 : ReadingComprehension of Abstract Meaning. We propose to use pre-trianed Electra discriminator to choose the best abstract word from five candidates. An upper attention and auto denoising mechanism is introduced to process the long sequences. The experiment results demonstrate that this contribution greatly facilitatesthe contextual language modeling in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension task</a>. The ablation study is also conducted to show the validity of our proposed methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.114/>GHOST at SemEval-2021 Task 5 : Is explanation all you need?<span class=acl-fixed-case>GHOST</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 5: Is explanation all you need?</a></strong><br><a href=/people/k/kamil-plucinski/>Kamil Pluciński</a>
|
<a href=/people/h/hanna-klimczak/>Hanna Klimczak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--114><div class="card-body p-3 small">This paper discusses different approaches to the Toxic Spans Detection task. The problem posed by the task was to determine which words contribute mostly to recognising a document as toxic. As opposed to <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a> of entire texts, word-level assessment could be of great use during <a href=https://en.wikipedia.org/wiki/Moderation_system>comment moderation</a>, also allowing for a more in-depth comprehension of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s predictions. As the main goal was to ensure transparency and understanding, this paper focuses on the current state-of-the-art approaches based on the explainable AI concepts and compares them to a supervised learning solution with word-level labels. The work consists of two xAI approaches that automatically provide the explanation for models trained for binary classification of toxic documents : an LSTM model with attention as a model-specific approach and the Shapley values for interpreting BERT predictions as a model-agnostic method. The competing approach considers this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as supervised token classification, where models like BERT and its modifications were tested. The paper aims to explore, compare and assess the quality of predictions for different <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The advantages of each <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> and further research direction are also discussed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.115/>GoldenWind at SemEval-2021 Task 5 : Orthrus-An Ensemble Approach to Identify Toxicity<span class=acl-fixed-case>G</span>olden<span class=acl-fixed-case>W</span>ind at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 5: Orthrus - An Ensemble Approach to Identify Toxicity</a></strong><br><a href=/people/m/marco-palomino/>Marco Palomino</a>
|
<a href=/people/d/dawid-grad/>Dawid Grad</a>
|
<a href=/people/j/james-bedwell/>James Bedwell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--115><div class="card-body p-3 small">Many new developments to detect and mitigate <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> are currently being evaluated. We are particularly interested in the correlation between <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> and the <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> expressed in online posts. While <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> may be disguised by amending the wording of posts, <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> will not. Therefore, we describe here an ensemble method to identify toxicity and classify the emotions expressed on a corpus of annotated posts published by Task 5 of SemEval 2021our analysis shows that the majority of such posts express <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, sadness and <a href=https://en.wikipedia.org/wiki/Fear>fear</a>. Our method to identify <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> combines a lexicon-based approach, which on its own achieves an F1 score of 61.07 %, with a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning approach</a>, which on its own achieves an F1 score of 60 %. When both <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> are combined, the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> achieves an F1 score of 66.37 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--119 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.119/>NLP_UIOWA at Semeval-2021 Task 5 : Transferring Toxic Sets to Tag Toxic Spans<span class=acl-fixed-case>NLP</span>_<span class=acl-fixed-case>UIOWA</span> at <span class=acl-fixed-case>S</span>emeval-2021 Task 5: Transferring Toxic Sets to Tag Toxic Spans</a></strong><br><a href=/people/j/jonathan-rusert/>Jonathan Rusert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--119><div class="card-body p-3 small">We leverage a BLSTM with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> to identify toxic spans in texts. We explore different <a href=https://en.wikipedia.org/wiki/Dimension>dimensions</a> which affect the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance. The first dimension explored is the toxic set the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained on. Besides the provided <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we explore the transferability of 5 different toxic related sets, including offensive, toxic, abusive, and hate sets. We find that the solely offensive set shows the highest promise of transferability. The second dimension we explore is <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a>, including leveraging <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, employing a greedy remove method, using a <a href=https://en.wikipedia.org/wiki/Frequency_ratio>frequency ratio</a>, and examining hybrid combinations of multiple methods. We conduct an error analysis to examine which types of toxic spans were missed and which were wrongly inferred as toxic along with the main reasons why they occurred. Finally, we extend our method via ensembles, which achieves our highest F1 score of 55.1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.120/>S-NLP at SemEval-2021 Task 5 : An Analysis of Dual Networks for Sequence Tagging<span class=acl-fixed-case>S</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 5: An Analysis of Dual Networks for Sequence Tagging</a></strong><br><a href=/people/v/viet-anh-nguyen/>Viet Anh Nguyen</a>
|
<a href=/people/t/tam-minh-nguyen/>Tam Minh Nguyen</a>
|
<a href=/people/h/huy-quang-dao/>Huy Quang Dao</a>
|
<a href=/people/q/quang-huu-pham/>Quang Huu Pham</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--120><div class="card-body p-3 small">The SemEval 2021 task 5 : Toxic Spans Detection is a task of identifying considered-toxic spans in text, which provides a valuable, automatic tool for moderating online contents. This paper represents the second-place method for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, an ensemble of two <a href=https://en.wikipedia.org/wiki/Methodology>approaches</a>. While one approach relies on combining different embedding methods to extract diverse semantic and syntactic representations of words in context ; the other utilizes extra data with a slightly customized Self-training, a semi-supervised learning technique, for sequence tagging problems. Both of our <a href=https://en.wikipedia.org/wiki/Software_architecture>architectures</a> take advantage of a strong <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>, which was fine-tuned on a toxic classification task. Although experimental evidence indicates higher effectiveness of the first approach than the second one, combining them leads to our best results of 70.77 F1-score on the test dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.124" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.124/>MIPT-NSU-UTMN at SemEval-2021 Task 5 : Ensembling Learning with Pre-trained Language Models for Toxic Spans Detection<span class=acl-fixed-case>MIPT</span>-<span class=acl-fixed-case>NSU</span>-<span class=acl-fixed-case>UTMN</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 5: Ensembling Learning with Pre-trained Language Models for Toxic Spans Detection</a></strong><br><a href=/people/m/mikhail-kotyushev/>Mikhail Kotyushev</a>
|
<a href=/people/a/anna-glazkova/>Anna Glazkova</a>
|
<a href=/people/d/dmitry-morozov/>Dmitry Morozov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--124><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> for SemEval-2021 Task 5 on Toxic Spans Detection. We developed ensemble models using BERT-based neural architectures and post-processing to combine tokens into spans. We evaluated several pre-trained language models using various ensemble techniques for toxic span identification and achieved sizable improvements over our baseline fine-tuned BERT models. Finally, our <a href=https://en.wikipedia.org/wiki/System>system</a> obtained a F1-score of 67.55 % on test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.125/>UIT-E10dot3 at SemEval-2021 Task 5 : Toxic Spans Detection with Named Entity Recognition and Question-Answering Approaches<span class=acl-fixed-case>UIT</span>-E10dot3 at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 5: Toxic Spans Detection with Named Entity Recognition and Question-Answering Approaches</a></strong><br><a href=/people/p/phu-gia-hoang/>Phu Gia Hoang</a>
|
<a href=/people/l/luan-thanh-nguyen/>Luan Thanh Nguyen</a>
|
<a href=/people/k/kiet-nguyen/>Kiet Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--125><div class="card-body p-3 small">The increment of toxic comments on online space is causing tremendous effects on other vulnerable users. For this reason, considerable efforts are made to deal with this, and SemEval-2021 Task 5 : Toxic Spans Detection is one of those. This task asks competitors to extract spans that have <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a> from the given texts, and we have done several analyses to understand its structure before doing experiments. We solve this task by two approaches, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> with spaCy&#8217;s library and <a href=https://en.wikipedia.org/wiki/Question_answering>Question-Answering</a> with RoBERTa combining with ToxicBERT, and the former gains the highest <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 66.99 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.130" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.130/>YoungSheldon at SemEval-2021 Task 5 : Fine-tuning Pre-trained Language Models for Toxic Spans Detection using Token classification Objective<span class=acl-fixed-case>Y</span>oung<span class=acl-fixed-case>S</span>heldon at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 5: Fine-tuning Pre-trained Language Models for Toxic Spans Detection using Token classification Objective</a></strong><br><a href=/people/m/mayukh-sharma/>Mayukh Sharma</a>
|
<a href=/people/i/ilanthenral-kandasamy/>Ilanthenral Kandasamy</a>
|
<a href=/people/w/w-b-vasantha/>W.b. Vasantha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--130><div class="card-body p-3 small">In this paper, we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> used for SemEval 2021 Task 5 : Toxic Spans Detection. Our proposed <a href=https://en.wikipedia.org/wiki/System>system</a> approaches the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a token classification task. We trained our model to find toxic words and concatenate their spans to predict the toxic spans within a sentence. We fine-tuned Pre-trained Language Models (PLMs) for identifying the toxic words. For fine-tuning, we stacked the classification layer on top of the PLM features of each word to classify if it is toxic or not. PLMs are pre-trained using different objectives and their performance may differ on downstream tasks. We, therefore, compare the performance of BERT, ELECTRA, RoBERTa, XLM-RoBERTa, T5, XLNet, and MPNet for identifying toxic spans within a sentence. Our best performing <a href=https://en.wikipedia.org/wiki/System>system</a> used RoBERTa. It performed well, achieving an F1 score of 0.6841 and secured a rank of 16 on the official leaderboard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.134/>SINAI at SemEval-2021 Task 5 : Combining Embeddings in a BiLSTM-CRF model for Toxic Spans Detection<span class=acl-fixed-case>SINAI</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 5: Combining Embeddings in a <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> model for Toxic Spans Detection</a></strong><br><a href=/people/f/flor-miriam-plaza-del-arco/>Flor Miriam Plaza-del-Arco</a>
|
<a href=/people/p/pilar-lopez-ubeda/>Pilar López-Úbeda</a>
|
<a href=/people/l/l-alfonso-urena-lopez/>L. Alfonso Ureña-López</a>
|
<a href=/people/m/m-teresa-martin-valdivia/>M. Teresa Martín-Valdivia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--134><div class="card-body p-3 small">This paper describes the participation of SINAI team at Task 5 : Toxic Spans Detection which consists of identifying spans that make a text toxic. Although several resources and systems have been developed so far in the context of <a href=https://en.wikipedia.org/wiki/Profanity>offensive language</a>, both <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> and tasks have mainly focused on classifying whether a text is offensive or not. However, detecting toxic spans is crucial to identify why a text is toxic and can assist human moderators to locate this type of content on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In order to accomplish the task, we follow a deep learning-based approach using a Bidirectional variant of a Long Short Term Memory network along with a stacked Conditional Random Field decoding layer (BiLSTM-CRF). Specifically, we test the performance of the combination of different pre-trained word embeddings for recognizing toxic entities in text. The results show that the combination of word embeddings helps in detecting offensive content. Our team ranks 29th out of 91 participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--135 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.135/>CSECU-DSG at SemEval-2021 Task 5 : Leveraging Ensemble of Sequence Tagging Models for Toxic Spans Detection<span class=acl-fixed-case>CSECU</span>-<span class=acl-fixed-case>DSG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 5: Leveraging Ensemble of Sequence Tagging Models for Toxic Spans Detection</a></strong><br><a href=/people/t/tashin-hossain/>Tashin Hossain</a>
|
<a href=/people/j/jannatun-naim/>Jannatun Naim</a>
|
<a href=/people/f/fareen-tasneem/>Fareen Tasneem</a>
|
<a href=/people/r/radiathun-tasnia/>Radiathun Tasnia</a>
|
<a href=/people/a/abu-nowshed-chy/>Abu Nowshed Chy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--135><div class="card-body p-3 small">The upsurge of prolific <a href=https://en.wikipedia.org/wiki/Blog>blogging</a> and <a href=https://en.wikipedia.org/wiki/Microblogging>microblogging platforms</a> enabled the abusers to spread negativity and <a href=https://en.wikipedia.org/wiki/Threat>threats</a> greater than ever. Detecting the toxic portions substantially aids to moderate or exclude the abusive parts for maintaining sound online platforms. This paper describes our participation in the SemEval 2021 toxic span detection task. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> requires detecting spans that convey toxic remarks from the given text. We explore an ensemble of sequence labeling models including the BiLSTM-CRF, spaCy NER model with custom toxic tags, and fine-tuned BERT model to identify the toxic spans. Finally, a majority voting ensemble method is used to determine the unified toxic spans. Experimental results depict the competitive performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> among the participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.140/>AIMH at SemEval-2021 Task 6 : Multimodal Classification Using an Ensemble of Transformer Models<span class=acl-fixed-case>AIMH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: Multimodal Classification Using an Ensemble of Transformer Models</a></strong><br><a href=/people/n/nicola-messina/>Nicola Messina</a>
|
<a href=/people/f/fabrizio-falchi/>Fabrizio Falchi</a>
|
<a href=/people/c/claudio-gennaro/>Claudio Gennaro</a>
|
<a href=/people/g/giuseppe-amato/>Giuseppe Amato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--140><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>system</a> used by the AIMH Team to approach the SemEval Task 6. We propose an approach that relies on an architecture based on the transformer model to process multimodal content (text and images) in memes. Our architecture, called DVTT (Double Visual Textual Transformer), approaches Subtasks 1 and 3 of Task 6 as multi-label classification problems, where the text and/or images of the meme are processed, and the probabilities of the presence of each possible persuasion technique are returned as a result. DVTT uses two complete networks of transformers that work on text and images that are mutually conditioned. One of the two <a href=https://en.wikipedia.org/wiki/Methodology>modalities</a> acts as the main one and the second one intervenes to enrich the first one, thus obtaining two distinct ways of operation. The two transformers outputs are merged by averaging the inferred probabilities for each possible label, and the overall <a href=https://en.wikipedia.org/wiki/Neural_network>network</a> is trained end-to-end with a binary cross-entropy loss.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--142 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.142/>1213Li at SemEval-2021 Task 6 : Detection of Propaganda with Multi-modal Attention and Pre-trained Models<span class=acl-fixed-case>L</span>i at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: Detection of Propaganda with Multi-modal Attention and Pre-trained Models</a></strong><br><a href=/people/p/peiguang-li/>Peiguang Li</a>
|
<a href=/people/x/xuan-li/>Xuan Li</a>
|
<a href=/people/x/xian-sun/>Xian Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--142><div class="card-body p-3 small">This paper presents the solution proposed by the 1213Li team for subtask 3 in SemEval-2021 Task 6 : identifying the multiple persuasion techniques used in the multi-modal content of the meme. We explored various approaches in <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a> and the detection of persuasion labels. Our final model employs pre-trained models including RoBERTa and ResNet-50 as a feature extractor for texts and images, respectively, and adopts a label embedding layer with multi-modal attention mechanism to measure the similarity of labels with the multi-modal information and fuse features for label prediction. Our proposed method outperforms the provided baseline method and achieves 3rd out of 16 participants with 0.54860/0.22830 for Micro / Macro F1 scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.143/>NLyticsFKIE at SemEval-2021 Task 6 : Detection of Persuasion Techniques In Texts And Images<span class=acl-fixed-case>NL</span>ytics<span class=acl-fixed-case>FKIE</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: Detection of Persuasion Techniques In Texts And Images</a></strong><br><a href=/people/a/albert-pritzkau/>Albert Pritzkau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--143><div class="card-body p-3 small">The following system description presents our approach to the detection of persuasion techniques in <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> and <a href=https://en.wikipedia.org/wiki/Digital_image>images</a>. The given <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> has been framed as a multi-label classification problem with the different techniques serving as class labels. The multi-label classification problem is one in which a list of target variables such as our class labels is associated with every input chunk and assumes that a document can simultaneously and independently be assigned to multiple labels or classes. In order to assign class labels to the given memes, we opted for RoBERTa (A Robustly Optimized BERT Pretraining Approach) as a neural network architecture for token and sequence classification. Starting off with a pre-trained model for language representation we fine-tuned this <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on the given classification task with the provided annotated data in supervised training steps. To incorporate image features in the multi-modal setting, we rely on the pre-trained VGG-16 model architecture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--144 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.144/>YNU-HPCC at SemEval-2021 Task 6 : Combining ALBERT and Text-CNN for Persuasion Detection in Texts and Images<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: Combining <span class=acl-fixed-case>ALBERT</span> and Text-<span class=acl-fixed-case>CNN</span> for Persuasion Detection in Texts and Images</a></strong><br><a href=/people/x/xingyu-zhu/>Xingyu Zhu</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--144><div class="card-body p-3 small">In recent years, <a href=https://en.wikipedia.org/wiki/Meme>memes</a> combining image and text have been widely used in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, and <a href=https://en.wikipedia.org/wiki/Meme>memes</a> are one of the most popular types of content used in online disinformation campaigns. In this paper, our study on the detection of persuasion techniques in <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> and <a href=https://en.wikipedia.org/wiki/Image>images</a> in SemEval-2021 Task 6 is summarized. For propaganda technology detection in text, we propose a combination model of both ALBERT and Text CNN for text classification, as well as a BERT-based multi-task sequence labeling model for propaganda technology coverage span detection. For the meme classification task involved in text understanding and visual feature extraction, we designed a parallel channel model divided into text and image channels. Our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieved a good performance on subtasks 1 and 3. The micro F1-scores of 0.492, 0.091, and 0.446 achieved on the test sets of the three subtasks ranked 12th, 7th, and 11th, respectively, and all are higher than the baseline model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.147/>NLPIITR at SemEval-2021 Task 6 : RoBERTa Model with Data Augmentation for Persuasion Techniques Detection<span class=acl-fixed-case>NLPIITR</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a Model with Data Augmentation for Persuasion Techniques Detection</a></strong><br><a href=/people/v/vansh-gupta/>Vansh Gupta</a>
|
<a href=/people/r/raksha-sharma/>Raksha Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--147><div class="card-body p-3 small">This paper describes and examines different systems to address Task 6 of SemEval-2021 : Detection of Persuasion Techniques In Texts And Images, Subtask 1. The task aims to build a model for identifying rhetorical and psycho- logical techniques (such as causal oversimplification, name-calling, smear) in the textual content of a meme which is often used in a disinformation campaign to influence the users. The paper provides an extensive comparison among various <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning systems</a> as a solution to the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We elaborate on the pre-processing of the <a href=https://en.wikipedia.org/wiki/Text_file>text data</a> in favor of the task and present ways to overcome the class imbalance. The results show that fine-tuning a RoBERTa model gave the best results with an <a href=https://en.wikipedia.org/wiki/F-number>F1-Micro score</a> of 0.51 on the development set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.150.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--150 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.150 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.150/>MinD at SemEval-2021 Task 6 : Propaganda Detection using Transfer Learning and Multimodal Fusion<span class=acl-fixed-case>M</span>in<span class=acl-fixed-case>D</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: Propaganda Detection using Transfer Learning and Multimodal Fusion</a></strong><br><a href=/people/j/junfeng-tian/>Junfeng Tian</a>
|
<a href=/people/m/min-gui/>Min Gui</a>
|
<a href=/people/c/chenliang-li/>Chenliang Li</a>
|
<a href=/people/m/ming-yan/>Ming Yan</a>
|
<a href=/people/w/wenming-xiao/>Wenming Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--150><div class="card-body p-3 small">We describe our systems of subtask1 and subtask3 for SemEval-2021 Task 6 on Detection of Persuasion Techniques in Texts and Images. The purpose of subtask1 is to identify <a href=https://en.wikipedia.org/wiki/Propaganda_techniques>propaganda techniques</a> given textual content, and the goal of subtask3 is to detect them given both textual and visual content. For subtask1, we investigate <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> based on pre-trained language models (PLMs) such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, RoBERTa to solve data sparsity problems. For subtask3, we extract <a href=https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity>heterogeneous visual representations</a> (i.e., face features, <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>OCR features</a>, and multimodal representations) and explore various multimodal fusion strategies to combine the textual and visual representations. The official evaluation shows our ensemble model ranks 1st for subtask1 and 2nd for subtask3.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.151/>CSECU-DSG at SemEval-2021 Task 6 : Orchestrating Multimodal Neural Architectures for Identifying Persuasion Techniques in Texts and Images<span class=acl-fixed-case>CSECU</span>-<span class=acl-fixed-case>DSG</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 6: Orchestrating Multimodal Neural Architectures for Identifying Persuasion Techniques in Texts and Images</a></strong><br><a href=/people/t/tashin-hossain/>Tashin Hossain</a>
|
<a href=/people/j/jannatun-naim/>Jannatun Naim</a>
|
<a href=/people/f/fareen-tasneem/>Fareen Tasneem</a>
|
<a href=/people/r/radiathun-tasnia/>Radiathun Tasnia</a>
|
<a href=/people/a/abu-nowshed-chy/>Abu Nowshed Chy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--151><div class="card-body p-3 small">Inscribing persuasion techniques in <a href=https://en.wikipedia.org/wiki/Meme>memes</a> is the most impactful way to influence peoples&#8217; mindsets. People are more inclined to memes as they are more stimulating and convincing and hence <a href=https://en.wikipedia.org/wiki/Meme>memes</a> are often exploited by tactfully engraving propaganda in its context with the intent of attaining specific agenda. This paper describes our participation in the three subtasks featured by SemEval 2021 task 6 on the detection of persuasion techniques in <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> and <a href=https://en.wikipedia.org/wiki/Image>images</a>. We utilize a fusion of <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>, <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>decision tree</a>, and fine-tuned DistilBERT for tackling subtask 1. As for subtask 2, we propose a system that consolidates a span identification model and a multi-label classification model based on pre-trained BERT. We address the multi-modal multi-label classification of memes defined in subtask 3 by utilizing a ResNet50 based image model, DistilBERT based text model, and a multi-modal architecture based on multikernel CNN+LSTM and MLP model. The outcomes illustrated the competitive performance of our <a href=https://en.wikipedia.org/wiki/System>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.152" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.152/>UMUTeam at SemEval-2021 Task 7 : Detecting and Rating Humor and Offense with Linguistic Features and Word Embeddings<span class=acl-fixed-case>UMUT</span>eam at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 7: Detecting and Rating Humor and Offense with Linguistic Features and Word Embeddings</a></strong><br><a href=/people/j/jose-antonio-garcia-diaz/>José Antonio García-Díaz</a>
|
<a href=/people/r/rafael-valencia-garcia/>Rafael Valencia-García</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--152><div class="card-body p-3 small">In writing, <a href=https://en.wikipedia.org/wiki/Humour>humor</a> is mainly based on <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a> in which words and expressions change their conventional meaning to refer to something without saying it directly. This flip in the meaning of the words prevents <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> from revealing the real intention of a communication and, therefore, reduces the effectiveness of tasks such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> or <a href=https://en.wikipedia.org/wiki/Emotion_detection>Emotion Detection</a>. In this manuscript we describe the participation of the UMUTeam in HaHackathon 2021, whose objective is to detect and rate humorous and controversial content. Our proposal is based on the combination of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> with contextual and non-contextual word embeddings. We participate in all the proposed subtasks achieving our best result in the controversial humor subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--153 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.153/>ES-JUST at SemEval-2021 Task 7 : Detecting and Rating Humor and Offensive Text Using <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a><span class=acl-fixed-case>ES</span>-<span class=acl-fixed-case>JUST</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 7: Detecting and Rating Humor and Offensive Text Using Deep Learning</a></strong><br><a href=/people/e/emran-al-bashabsheh/>Emran Al Bashabsheh</a>
|
<a href=/people/s/sanaa-abu-alasal/>Sanaa Abu Alasal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--153><div class="card-body p-3 small">This research presents the work of the team&#8217;s ES-JUST at semEval-2021 task 7 for detecting and rating humor and offensive text using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. The team evaluates several approaches (i.e. Bert, Roberta, XLM-Roberta, and Bert embedding + Bi-LSTM) that employ in four sub-tasks. The first sub-task deal with whether the text is humorous or not. The second sub-task is the degree of <a href=https://en.wikipedia.org/wiki/Humour>humor</a> in the text if the first <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-task</a> is humorous. The third sub-task represents the text is controversial or not if it is humorous. While in the last task is the degree of an offensive in the text. However, Roberta pre-trained model outperforms other approaches and score the highest in all sub-tasks. We rank on the leader board at the evaluation phase are 14, 15, 20, and 5 through 0.9564 <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>, 0.5709 <a href=https://en.wikipedia.org/wiki/Randomized_controlled_trial>RMSE</a>, 0.4888 <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>, and 0.4467 RMSE results, respectively, for each of the first, second, third, and fourth sub-task, respectively.<i>i.e.Bert, Roberta, XLM-Roberta, and Bert embedding + Bi-LSTM</i>) that employ in four sub-tasks. The first sub-task deal with whether the text is humorous or not. The second sub-task is the degree of humor in the text if the first sub-task is humorous. The third sub-task represents the text is controversial or not if it is humorous. While in the last task is the degree of an offensive in the text. However, Roberta pre-trained model outperforms other approaches and score the highest in all sub-tasks. We rank on the leader board at the evaluation phase are 14, 15, 20, and 5 through 0.9564 F-score, 0.5709 RMSE, 0.4888 F-score, and 0.4467 RMSE results, respectively, for each of the first, second, third, and fourth sub-task, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--154 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.154/>Tsia at SemEval-2021 Task 7 : Detecting and Rating Humor and Offense<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 7: Detecting and Rating Humor and Offense</a></strong><br><a href=/people/z/zhengyi-guan/>Zhengyi Guan</a>
|
<a href=/people/x/xiaobing-zxb-zhou/>Xiaobing ZXB Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--154><div class="card-body p-3 small">This paper describes our contribution to SemEval-2021 Task 7 : Detecting and Rating Humor and Of-fense. This task contains two sub-tasks, sub-task 1and sub-task 2. Among them, sub-task 1 containsthree <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-tasks</a>, sub-task 1a, sub-task 1b and sub-task 1c. Sub-task 1a is to predict if the text would beconsidered humorous. Sub-task 1c is described asfollows : if the text is classed as humorous, predictif the humor rating would be considered controver-sial, i.e. the variance of the rating between annota-tors is higher than the median.we combined threepre-trained model with CNN to complete these twoclassification sub-tasks. Sub-task 1b is to judge thedegree of <a href=https://en.wikipedia.org/wiki/Humour>humor</a>. Sub-task 2 aims to predict how of-fensive a text would be with values between 0 and5.We use the idea of <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression</a> to deal with thesetwo sub-tasks. We analyze the performance of ourmethod and demonstrate the contribution of eachcomponent of our architecture. We have achievedgood results under the combination of multiple pre-training models and optimization methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.169" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.169/>DuluthNLP at SemEval-2021 Task 7 : Fine-Tuning RoBERTa Model for Humor Detection and Offense Rating<span class=acl-fixed-case>D</span>uluth<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 7: Fine-Tuning <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a Model for Humor Detection and Offense Rating</a></strong><br><a href=/people/s/samuel-akrah/>Samuel Akrah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--169><div class="card-body p-3 small">This paper presents the DuluthNLP submission to Task 7 of the SemEval 2021 competition on Detecting and Rating Humor and Offense. In it, we explain the approach used to train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> together with the process of fine-tuning our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in getting the results. We focus on humor detection, rating, and of-fense rating, representing three out of the four subtasks that were provided. We show that optimizing hyper-parameters for learning rate, batch size and number of epochs can increase the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> for humor detection</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.172.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--172 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.172 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.172/>EndTimes at SemEval-2021 Task 7 : Detecting and Rating Humor and Offense with BERT and Ensembles<span class=acl-fixed-case>E</span>nd<span class=acl-fixed-case>T</span>imes at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 7: Detecting and Rating Humor and Offense with <span class=acl-fixed-case>BERT</span> and Ensembles</a></strong><br><a href=/people/c/chandan-kumar-pandey/>Chandan Kumar Pandey</a>
|
<a href=/people/c/chirag-singh/>Chirag Singh</a>
|
<a href=/people/k/karan-mangla/>Karan Mangla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--172><div class="card-body p-3 small">This paper describes Humor-BERT, a set of BERT Large based models that we used in the SemEval-2021 Task 7 : Detecting and Rating Humor and Offense. It presents pre and post processing techniques, variable threshold learning, meta learning and Ensemble approach to solve various sub-tasks that were part of the challenge. We also present a comparative analysis of various <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> we tried. Our method was ranked 4th in Humor Controversy Detection, 8th in Humor Detection, 19th in Average Offense Score prediction and 40th in Average Humor Score prediction globally. F1 score obtained for <a href=https://en.wikipedia.org/wiki/Humorism>Humor classification</a> was 0.9655 and for Controversy detection it was 0.6261. Our user name on the leader board is ThisIstheEnd and team name is EndTimes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--173 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.173/>IIITH at SemEval-2021 Task 7 : Leveraging transformer-based humourous and offensive text detection architectures using lexical and hurtlex features and task adaptive pretraining<span class=acl-fixed-case>IIITH</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 7: Leveraging transformer-based humourous and offensive text detection architectures using lexical and hurtlex features and task adaptive pretraining</a></strong><br><a href=/people/t/tathagata-raha/>Tathagata Raha</a>
|
<a href=/people/i/ishan-sanjeev-upadhyay/>Ishan Sanjeev Upadhyay</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--173><div class="card-body p-3 small">This paper describes our approach (IIITH) for SemEval-2021 Task 5 : HaHackathon : Detecting and Rating Humor and Offense. Our results focus on two major objectives : (i) Effect of task adaptive pretraining on the performance of transformer based models (ii) How does lexical and hurtlex features help in quantifying humour and offense. In this paper, we provide a detailed description of our approach along with comparisions mentioned above.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--180 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.semeval-1.180" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.180/>Volta at SemEval-2021 Task 9 : Statement Verification and Evidence Finding with Tables using TAPAS and Transfer Learning<span class=acl-fixed-case>V</span>olta at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 9: Statement Verification and Evidence Finding with Tables using <span class=acl-fixed-case>TAPAS</span> and Transfer Learning</a></strong><br><a href=/people/d/devansh-gautam/>Devansh Gautam</a>
|
<a href=/people/k/kshitij-gupta/>Kshitij Gupta</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--180><div class="card-body p-3 small">Tables are widely used in various kinds of documents to present information concisely. Understanding tables is a challenging problem that requires an understanding of language and table structure, along with numerical and logical reasoning. In this paper, we present our systems to solve Task 9 of SemEval-2021 : Statement Verification and Evidence Finding with Tables (SEM-TAB-FACTS). The task consists of two subtasks : (A) Given a table and a statement, predicting whether the table supports the statement and (B) Predicting which cells in the table provide evidence for / against the statement. We fine-tune TAPAS (a model which extends BERT&#8217;s architecture to capture tabular structure) for both the subtasks as it has shown state-of-the-art performance in various table understanding tasks. In subtask A, we evaluate how <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and standardizing tables to have a single header row improves TAPAS&#8217; performance. In subtask B, we evaluate how different fine-tuning strategies can improve TAPAS&#8217; performance. Our systems achieve an <a href=https://en.wikipedia.org/wiki/International_Federation_of_the_Phonographic_Industry>F1 score</a> of 67.34 in subtask A three-way classification, 72.89 in subtask A <a href=https://en.wikipedia.org/wiki/International_Federation_of_the_Phonographic_Industry>two-way classification</a>, and 62.95 in subtask B.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.184/>YNU-HPCC at SemEval-2021 Task 10 : Using a Transformer-based Source-Free Domain Adaptation Model for Semantic Processing<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 10: Using a Transformer-based Source-Free Domain Adaptation Model for Semantic Processing</a></strong><br><a href=/people/z/zhewen-yu/>Zhewen Yu</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--184><div class="card-body p-3 small">Data sharing restrictions are common in NLP datasets. The purpose of this task is to develop a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained in a source domain to make predictions for a target domain with related domain data. To address the issue, the organizers provided the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that fine-tuned a large number of source domain data on pre-trained models and the dev data for participants. But the source domain data was not distributed. This paper describes the provided <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to the NER (Name entity recognition) task and the ways to develop the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. As a little data provided, pre-trained models are suitable to solve the cross-domain tasks. The models fine-tuned by large number of another domain could be effective in new domain because the task had no change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.semeval-1.186.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--semeval-1--186 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.semeval-1.186 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.semeval-1.186/>UOR at SemEval-2021 Task 12 : On Crowd Annotations ; Learning with Disagreements to optimise crowd truth<span class=acl-fixed-case>UOR</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2021 Task 12: On Crowd Annotations; Learning with Disagreements to optimise crowd truth</a></strong><br><a href=/people/e/emmanuel-osei-brefo/>Emmanuel Osei-Brefo</a>
|
<a href=/people/t/thanet-markchom/>Thanet Markchom</a>
|
<a href=/people/h/huizhi-liang/>Huizhi Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--semeval-1--186><div class="card-body p-3 small">Crowdsourcing has been ubiquitously used for annotating enormous collections of data. However, the major obstacles to using crowd-sourced labels are <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and errors from non-expert annotations. In this work, two approaches dealing with the <a href=https://en.wikipedia.org/wiki/Noise>noise</a> and errors in crowd-sourced labels are proposed. The first approach uses Sharpness-Aware Minimization (SAM), an optimization technique robust to noisy labels. The other approach leverages a neural network layer called softmax-Crowdlayer specifically designed to learn from crowd-sourced annotations. According to the results, the proposed approaches can improve the performance of the Wide Residual Network model and Multi-layer Perception model applied on crowd-sourced datasets in the image processing domain. It also has similar and comparable results with the majority voting technique when applied to the sequential data domain whereby the Bidirectional Encoder Representations from Transformers (BERT) is used as the base model in both instances.</div></div></div><hr><div id=2021sigmorphon-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.sigmorphon-1/>Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.0/>Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/k/kyle-gorman/>Kyle Gorman</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigmorphon-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigmorphon-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.2/>Recursive prosody is not finite-state</a></strong><br><a href=/people/h/hossep-dolatian/>Hossep Dolatian</a>
|
<a href=/people/a/aniello-de-santo/>Aniello De Santo</a>
|
<a href=/people/t/thomas-graf/>Thomas Graf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigmorphon-1--2><div class="card-body p-3 small">This paper investigates bounds on the generative capacity of <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosodic processes</a>, by focusing on the complexity of recursive prosody in coordination contexts in <a href=https://en.wikipedia.org/wiki/English_language>English</a> (Wagner, 2010). Although all <a href=https://en.wikipedia.org/wiki/Phonology>phonological processes</a> and most <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosodic processes</a> are computationally regular string languages, we show that recursive prosody is not. The output string language is instead parallel multiple context-free (Seki et al., 1991). We evaluate the complexity of the pattern over strings, and then move on to a characterization over <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a> that requires the expressivity of multi bottom-up tree transducers. In doing so, we provide a foundation for future mathematically grounded investigations of the syntax-prosody interface.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigmorphon-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigmorphon-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.sigmorphon-1.3.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.3/>The Match-Extend serialization algorithm in Multiprecedence</a></strong><br><a href=/people/m/maxime-papillon/>Maxime Papillon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigmorphon-1--3><div class="card-body p-3 small">Raimy (1999 ; 2000a ; 2000b) proposed a graphical formalism for modeling <a href=https://en.wikipedia.org/wiki/Reduplication>reduplication</a>, originallymostly focused on phonological overapplication in a derivational framework. This <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> is now known as Precedence-based phonology or Multiprecedence phonology. Raimy&#8217;s idea is that the segments at the input to the <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a> are not totally ordered by precedence. This paper tackles a challenge that arose with Raimy&#8217;s work, the development of a deterministic serialization algorithm as part of the derivation of surface forms. The Match-Extend algorithm introduced here requires fewer assumptions and sticks tighter to the attested typology. The <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> also contains no parameter or <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraint</a> specific to individual <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> or <a href=https://en.wikipedia.org/wiki/Topological_space>topologies</a>, unlike previous proposals. Match-Extend requires nothing except knowing the last added set of links.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigmorphon-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigmorphon-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigmorphon-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.6/>A Study of Morphological Robustness of Neural Machine Translation</a></strong><br><a href=/people/s/sai-muralidhar-jayanthi/>Sai Muralidhar Jayanthi</a>
|
<a href=/people/a/adithya-pratapa/>Adithya Pratapa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigmorphon-1--6><div class="card-body p-3 small">In this work, we analyze the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of neural machine translation systems towards grammatical perturbations in the source. In particular, we focus on morphological inflection related perturbations. While this has been recently studied for EnglishFrench (MORPHEUS) (Tan et al., 2020), it is unclear how this extends to AnyEnglish translation systems. We propose MORPHEUS-MULTILINGUAL that utilizes UniMorph dictionaries to identify morphological perturbations to source that adversely affect the translation models. Along with an analysis of state-of-the-art pretrained MT systems, we train and analyze systems for 11 language pairs using the multilingual TED corpus (Qi et al., 2018). We also compare this to actual errors of non-native speakers using Grammatical Error Correction datasets. Finally, we present a qualitative and quantitative analysis of the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of AnyEnglish translation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigmorphon-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigmorphon-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigmorphon-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.7/>Sample-efficient Linguistic Generalizations through <a href=https://en.wikipedia.org/wiki/Program_synthesis>Program Synthesis</a> : Experiments with Phonology Problems</a></strong><br><a href=/people/s/saujas-vaduguru/>Saujas Vaduguru</a>
|
<a href=/people/a/aalok-sathe/>Aalok Sathe</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigmorphon-1--7><div class="card-body p-3 small">Neural models excel at extracting statistical patterns from large amounts of data, but struggle to learn <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> or reason about <a href=https://en.wikipedia.org/wiki/Language>language</a> from only a few examples. In this paper, we ask : Can we learn explicit <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> that generalize well from only a few examples? We explore this question using <a href=https://en.wikipedia.org/wiki/Program_synthesis>program synthesis</a>. We develop a synthesis model to learn <a href=https://en.wikipedia.org/wiki/Phonology>phonology rules</a> as <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a> in a <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific language</a>. We test the ability of our models to generalize from few training examples using our new dataset of problems from the <a href=https://en.wikipedia.org/wiki/International_Linguistic_Olympiad>Linguistics Olympiad</a>, a challenging set of tasks that require strong linguistic reasoning ability. In addition to being highly sample-efficient, our approach generates <a href=https://en.wikipedia.org/wiki/Human-readable_medium>human-readable programs</a>, and allows control over the generalizability of the learnt programs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigmorphon-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigmorphon-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.9/>Adaptor Grammars for Unsupervised Paradigm Clustering<span class=acl-fixed-case>A</span>daptor <span class=acl-fixed-case>G</span>rammars for Unsupervised Paradigm Clustering</a></strong><br><a href=/people/k/kate-mccurdy/>Kate McCurdy</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigmorphon-1--9><div class="card-body p-3 small">This work describes the Edinburgh submission to the SIGMORPHON 2021 Shared Task 2 on unsupervised morphological paradigm clustering. Given raw text input, the task was to assign each token to a cluster with other tokens from the same paradigm. We use Adaptor Grammar segmentations combined with frequency-based heuristics to predict paradigm clusters. Our system achieved the highest average F1 score across 9 test languages, placing first out of 15 submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigmorphon-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigmorphon-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.17/>CLUZH at SIGMORPHON 2021 Shared Task on Multilingual Grapheme-to-Phoneme Conversion : Variations on a Baseline<span class=acl-fixed-case>CLUZH</span> at <span class=acl-fixed-case>SIGMORPHON</span> 2021 Shared Task on Multilingual Grapheme-to-Phoneme Conversion: Variations on a Baseline</a></strong><br><a href=/people/s/simon-clematide/>Simon Clematide</a>
|
<a href=/people/p/peter-makarov/>Peter Makarov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigmorphon-1--17><div class="card-body p-3 small">This paper describes the submission by the team from the Department of Computational Linguistics, Zurich University, to the Multilingual Grapheme-to-Phoneme Conversion (G2P) Task 1 of the SIGMORPHON 2021 challenge in the low and medium settings. The submission is a variation of our 2020 G2P system, which serves as the baseline for this year&#8217;s challenge. The system is a neural transducer that operates over explicit edit actions and is trained with imitation learning. For this challenge, we experimented with the following changes : a) emitting phoneme segments instead of single character phonemes, b) input character dropout, c) a mogrifier LSTM decoder (Melis et al., 2019), d) enriching the decoder input with the currently attended input character, e) parallel BiLSTM encoders, and f) an adaptive batch size scheduler. In the low setting, our best <a href=https://en.wikipedia.org/wiki/Ensemble_cast>ensemble</a> improved over the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a>, however, in the medium setting, the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> was stronger on average, although for certain languages improvements could be observed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigmorphon-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigmorphon-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.20/>Recognizing Reduplicated Forms : Finite-State Buffered Machines</a></strong><br><a href=/people/y/yang-wang/>Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigmorphon-1--20><div class="card-body p-3 small">Total reduplication is common in <a href=https://en.wikipedia.org/wiki/Phonology>natural language phonology</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>. However, formally as copying on reduplicants of unbounded size, unrestricted total reduplication requires computational power beyond context-free, while other phonological and morphological patterns are regular, or even sub-regular. Thus, existing <a href=https://en.wikipedia.org/wiki/Class_(computer_programming)>language classes</a> characterizing reduplicated strings inevitably include typologically unattested context-free patterns, such as reversals. This paper extends <a href=https://en.wikipedia.org/wiki/Regular_language>regular languages</a> to incorporate <a href=https://en.wikipedia.org/wiki/Reduplication>reduplication</a> by introducing a new computational device : finite state buffered machine (FSBMs). We give its mathematical definitions and discuss some <a href=https://en.wikipedia.org/wiki/Closure_(mathematics)>closure properties</a> of the corresponding set of languages. As a result, the class of <a href=https://en.wikipedia.org/wiki/Regular_language>regular languages</a> and <a href=https://en.wikipedia.org/wiki/Formal_language>languages</a> derived from them through a copying mechanism is characterized. Suggested by previous literature, this <a href=https://en.wikipedia.org/wiki/Class_(set_theory)>class of languages</a> should approach the characterization of natural language word sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigmorphon-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigmorphon-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigmorphon-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigmorphon-1.24/>Improved pronunciation prediction accuracy using morphology</a></strong><br><a href=/people/d/dravyansh-sharma/>Dravyansh Sharma</a>
|
<a href=/people/s/saumya-sahai/>Saumya Sahai</a>
|
<a href=/people/n/neha-chaudhari/>Neha Chaudhari</a>
|
<a href=/people/a/antoine-bruguier/>Antoine Bruguier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigmorphon-1--24><div class="card-body p-3 small">Pronunciation lexicons and <a href=https://en.wikipedia.org/wiki/Speech_recognition>prediction models</a> are a key component in several <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech synthesis and recognition systems</a>. We know that morphologically related words typically follow a fixed pattern of <a href=https://en.wikipedia.org/wiki/Pronunciation>pronunciation</a> which can be described by language-specific paradigms. In this work we explore how deep recurrent neural networks can be used to automatically learn and exploit this pattern to improve the pronunciation prediction quality of words related by morphological inflection. We propose two novel approaches for supplying morphological information, using the word&#8217;s morphological class and its lemma, which are typically annotated in standard lexicons. We report improvements across a number of European languages with varying degrees of phonological and morphological complexity, and two <a href=https://en.wikipedia.org/wiki/Language_family>language families</a>, with greater improvements for languages where the pronunciation prediction task is inherently more challenging. We also observe that combining bidirectional LSTM networks with attention mechanisms is an effective neural approach for the computational problem considered, across languages. Our approach seems particularly beneficial in the low resource setting, both by itself and in conjunction with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>.</div></div></div><hr><div id=2021splurobonlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.splurobonlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.splurobonlp-1/>Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.splurobonlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.splurobonlp-1.0/>Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics</a></strong><br><a href=/people/m/malihe-alikhani/>Malihe Alikhani</a>
|
<a href=/people/v/valts-blukis/>Valts Blukis</a>
|
<a href=/people/p/parisa-kordjamshidi/>Parisa Kordjamshidi</a>
|
<a href=/people/a/aishwarya-padmakumar/>Aishwarya Padmakumar</a>
|
<a href=/people/h/hao-tan/>Hao Tan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.splurobonlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--splurobonlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.splurobonlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.splurobonlp-1.4/>Modeling Semantics and Pragmatics of Spatial Prepositions via Hierarchical Common-Sense Primitives</a></strong><br><a href=/people/g/georgiy-platonov/>Georgiy Platonov</a>
|
<a href=/people/y/yifei-yang/>Yifei Yang</a>
|
<a href=/people/h/haoyu-wu/>Haoyu Wu</a>
|
<a href=/people/j/jonathan-waxman/>Jonathan Waxman</a>
|
<a href=/people/m/marcus-hill/>Marcus Hill</a>
|
<a href=/people/l/lenhart-schubert/>Lenhart Schubert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--splurobonlp-1--4><div class="card-body p-3 small">Understanding spatial expressions and using them appropriately is necessary for seamless and natural human-machine interaction. However, capturing the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and appropriate usage of spatial prepositions is notoriously difficult, because of their vagueness and <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a>. Although modern data-driven approaches are good at capturing statistical regularities in the usage, they usually require substantial sample sizes, often do not generalize well to unseen instances and, most importantly, their structure is essentially opaque to analysis, which makes diagnosing problems and understanding their reasoning process difficult. In this work, we discuss our attempt at modeling spatial senses of prepositions in <a href=https://en.wikipedia.org/wiki/English_language>English</a> using a combination of rule-based and statistical learning approaches. Each preposition model is implemented as a <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree</a> where each node computes certain intuitive relations associated with the preposition, with the root computing the final value of the <a href=https://en.wikipedia.org/wiki/Preposition_and_postposition>prepositional relation</a> itself. The <a href=https://en.wikipedia.org/wiki/3D_modeling>models</a> operate on a set of artificial 3D room world environments, designed in <a href=https://en.wikipedia.org/wiki/Blender_(software)>Blender</a>, taking the scene itself as an input. We also discuss our annotation framework used to collect <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> employed in the model training. Both our <a href=https://en.wikipedia.org/wiki/Factor_analysis>factored models</a> and black-box baseline models perform quite well, but the <a href=https://en.wikipedia.org/wiki/Factor_analysis>factored models</a> will enable reasoned explanations of spatial relation judgements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.splurobonlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--splurobonlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.splurobonlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.splurobonlp-1.8/>Interactive Reinforcement Learning for Table Balancing Robot</a></strong><br><a href=/people/h/haein-jeon/>Haein Jeon</a>
|
<a href=/people/y/yewon-kim/>Yewon Kim</a>
|
<a href=/people/b/bo-yeong-kang/>Bo-Yeong Kang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--splurobonlp-1--8><div class="card-body p-3 small">With the development of <a href=https://en.wikipedia.org/wiki/Robotics>robotics</a>, the use of <a href=https://en.wikipedia.org/wiki/Robot>robots</a> in daily life is increasing, which has led to the need for anyone to easily train robots to improve robot use. Interactive reinforcement learning(IARL) is a method for robot training based on <a href=https://en.wikipedia.org/wiki/Human&#8211;robot_interaction>humanrobot interaction</a> ; prior studies on IARL provide only limited types of <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> or require appropriately designed shaping rewards, which is known to be difficult and time-consuming. Therefore, in this study, we propose interactive deep reinforcement learning models based on voice feedback. In the proposed system, a robot learns the task of cooperative table balancing through <a href=https://en.wikipedia.org/wiki/Deep_learning>deep Q-network</a> using voice feedback provided by humans in real-time, with automatic speech recognition(ASR) and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> to understand human voice feedback. As a result, an optimal policy convergence rate of up to 96 % was realized, and performance was improved in all voice feedback-based models</div></div></div><hr><div id=2021spnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.spnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.spnlp-1/>Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.spnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.spnlp-1.0/>Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)</a></strong><br><a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a>
|
<a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/p/priyanka-agrawal/>Priyanka Agrawal</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.spnlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--spnlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.spnlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.spnlp-1.1/>RewardsOfSum : Exploring Reinforcement Learning Rewards for Summarisation<span class=acl-fixed-case>R</span>ewards<span class=acl-fixed-case>O</span>f<span class=acl-fixed-case>S</span>um: Exploring Reinforcement Learning Rewards for Summarisation</a></strong><br><a href=/people/j/jacob-parnell/>Jacob Parnell</a>
|
<a href=/people/i/inigo-jauregi-unanue/>Inigo Jauregi Unanue</a>
|
<a href=/people/m/massimo-piccardi/>Massimo Piccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--spnlp-1--1><div class="card-body p-3 small">To date, most abstractive summarisation models have relied on variants of the negative log-likelihood (NLL) as their training objective. In some cases, <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> has been added to train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with an objective that is closer to their evaluation measures (e.g. ROUGE). However, the <a href=https://en.wikipedia.org/wiki/Reward_system>reward function</a> to be used within the reinforcement learning approach can play a key role for performance and is still partially unexplored. For this reason, in this paper, we propose two reward functions for the task of abstractive summarisation : the first function, referred to as RwB-Hinge, dynamically selects the samples for the gradient update. The second <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>function</a>, nicknamed RISK, leverages a small pool of strong candidates to inform the reward. In the experiments, we probe the proposed approach by fine-tuning an NLL pre-trained model over nine summarisation datasets of diverse size and nature. The experimental results show a consistent improvement over the negative log-likelihood baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.spnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--spnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.spnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.spnlp-1.2.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.spnlp-1.2/>SmBoP : Semi-autoregressive Bottom-up Semantic Parsing<span class=acl-fixed-case>S</span>m<span class=acl-fixed-case>B</span>o<span class=acl-fixed-case>P</span>: Semi-autoregressive Bottom-up Semantic Parsing</a></strong><br><a href=/people/o/ohad-rubin/>Ohad Rubin</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--spnlp-1--2><div class="card-body p-3 small">The de-facto standard decoding method for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> in recent years has been to autoregressively decode the <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>abstract syntax tree</a> of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach : a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step t the top-K sub-trees of height t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, <a href=https://en.wikipedia.org/wiki/Bottom-up_parsing>bottom-up parsing</a> allows to decode all <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>sub-trees</a> of a certain height in parallel, leading to <a href=https://en.wikipedia.org/wiki/Time_complexity>logarithmic runtime complexity</a> rather than <a href=https://en.wikipedia.org/wiki/Time_complexity>linear</a>. From a modeling perspective, a <a href=https://en.wikipedia.org/wiki/Bottom-up_parsing>bottom-up parser</a> learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP leads to a 2.2x speed-up in decoding time and a ~5x speed-up in training time, compared to a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> that uses autoregressive decoding. SmBoP obtains 71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+Grappa.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.spnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--spnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.spnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.spnlp-1.5.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.spnlp-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.spnlp-1.5/>Mode recovery in neural autoregressive sequence modeling</a></strong><br><a href=/people/i/ilia-kulikov/>Ilia Kulikov</a>
|
<a href=/people/s/sean-welleck/>Sean Welleck</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--spnlp-1--5><div class="card-body p-3 small">Despite its wide use, recent studies have revealed unexpected and undesirable properties of neural autoregressive sequence models trained with <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood</a>, such as an unreasonably high affinity to short sequences after training and to infinitely long sequences at decoding time. We propose to study these phenomena by investigating how the modes, or local maxima, of a distribution are maintained throughout the full learning chain of the ground-truth, empirical, learned and decoding-induced distributions, via the newly proposed mode recovery cost. We design a tractable testbed where we build three types of ground-truth distributions : (1) an LSTM based structured distribution, (2) an unstructured distribution where probability of a sequence does not depend on its content, and (3) a product of these two which we call a semi-structured distribution. Our study reveals both expected and unexpected findings. First, starting with <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a>, mode recovery cost strongly relies on the ground-truth distribution and is most costly with the semi-structured distribution. Second, after learning, mode recovery cost from the ground-truth distribution may increase or decrease compared to <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a>, with the largest cost degradation occurring with the semi-structured ground-truth distribution. Finally, the ability of the decoding-induced distribution to recover modes from the learned distribution is highly impacted by the choices made earlier in the learning chain. We conclude that future research must consider the entire learning chain in order to fully understand the potentials and perils and to further improve neural autoregressive sequence models.<i>learning chain</i> of the ground-truth, empirical, learned and decoding-induced distributions, via the newly proposed <i>mode recovery cost</i>. We design a tractable testbed where we build three types of ground-truth distributions: (1) an LSTM based structured distribution, (2) an unstructured distribution where probability of a sequence does not depend on its content, and (3) a product of these two which we call a semi-structured distribution. Our study reveals both expected and unexpected findings. First, starting with data collection, mode recovery cost strongly relies on the ground-truth distribution and is most costly with the semi-structured distribution. Second, after learning, mode recovery cost from the ground-truth distribution may increase or decrease compared to data collection, with the largest cost degradation occurring with the semi-structured ground-truth distribution. Finally, the ability of the decoding-induced distribution to recover modes from the learned distribution is highly impacted by the choices made earlier in the learning chain. We conclude that future research must consider the entire learning chain in order to fully understand the potentials and perils and to further improve neural autoregressive sequence models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.spnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--spnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.spnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.spnlp-1.6/>Using Hierarchical Class Structure to Improve Fine-Grained Claim Classification</a></strong><br><a href=/people/e/erenay-dayanik/>Erenay Dayanik</a>
|
<a href=/people/a/andre-blessing/>Andre Blessing</a>
|
<a href=/people/n/nico-blokker/>Nico Blokker</a>
|
<a href=/people/s/sebastian-haunss/>Sebastian Haunss</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a>
|
<a href=/people/g/gabriella-lapesa/>Gabriella Lapesa</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--spnlp-1--6><div class="card-body p-3 small">The analysis of public debates crucially requires the classification of political demands according to hierarchical claim ontologies (e.g. for immigration, a supercategory Controlling Migration might have subcategories Asylum limit or Border installations). A major challenge for automatic claim classification is the large number and low frequency of such <a href=https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)>subclasses</a>. We address it by jointly predicting pairs of matching super- and subcategories. We operationalize this idea by (a) encoding <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>soft constraints</a> in the claim classifier and (b) imposing <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>hard constraints</a> via <a href=https://en.wikipedia.org/wiki/Integer_linear_programming>Integer Linear Programming</a>. Our experiments with different claim classifiers on a German immigration newspaper corpus show consistent performance increases for joint prediction, in particular for infrequent categories and discuss the complementarity of the two approaches.<i>claim ontologies</i> (e.g. for immigration, a supercategory &#8220;Controlling Migration&#8221; might have subcategories &#8220;Asylum limit&#8221; or &#8220;Border installations&#8221;). A major challenge for automatic claim classification is the large number and low frequency of such subclasses. We address it by jointly predicting pairs of matching super- and subcategories. We operationalize this idea by (a) encoding soft constraints in the claim classifier and (b) imposing hard constraints via Integer Linear Programming. Our experiments with different claim classifiers on a German immigration newspaper corpus show consistent performance increases for joint prediction, in particular for infrequent categories and discuss the complementarity of the two approaches.</div></div></div><hr><div id=2021starsem-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.starsem-1/>Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.0/>Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</a></strong><br><a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a>
|
<a href=/people/v/vivi-nastase/>Vivi Nastase</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.starsem-1.2" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.2/>Can Transformer Language Models Predict Psychometric Properties?</a></strong><br><a href=/people/a/antonio-laverghetta-jr/>Antonio Laverghetta Jr.</a>
|
<a href=/people/a/animesh-nighojkar/>Animesh Nighojkar</a>
|
<a href=/people/j/jamshidbek-mirzakhalov/>Jamshidbek Mirzakhalov</a>
|
<a href=/people/j/john-licato/>John Licato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--2><div class="card-body p-3 small">Transformer-based language models (LMs) continue to advance state-of-the-art performance on NLP benchmark tasks, including tasks designed to mimic human-inspired commonsense competencies. To better understand the degree to which LMs can be said to have certain linguistic reasoning skills, researchers are beginning to adapt the tools and concepts of the field of <a href=https://en.wikipedia.org/wiki/Psychometrics>psychometrics</a>. But to what extent can the benefits flow in the other direction? I.e., can <a href=https://en.wikipedia.org/wiki/Longitudinal_study>LMs</a> be of use in predicting what the <a href=https://en.wikipedia.org/wiki/Psychometrics>psychometric properties</a> of test items will be when those items are given to human participants? We gather responses from numerous human participants and LMs (transformer- and non-transformer-based) on a broad diagnostic test of linguistic competencies. We then use the responses to calculate standard psychometric properties of the items in the <a href=https://en.wikipedia.org/wiki/Medical_test>diagnostic test</a>, using the human responses and the LM responses separately. We then determine how well these two sets of predictions match. We find cases in which transformer-based LMs predict psychometric properties consistently well in certain categories but consistently poorly in others, thus providing new insights into fundamental similarities and differences between human and LM reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.starsem-1.4.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.starsem-1.4/>A Study on Using Semantic Word Associations to Predict the Success of a Novel</a></strong><br><a href=/people/s/syeda-jannatus-saba/>Syeda Jannatus Saba</a>
|
<a href=/people/b/biddut-sarker-bijoy/>Biddut Sarker Bijoy</a>
|
<a href=/people/h/henry-gorelick/>Henry Gorelick</a>
|
<a href=/people/s/sabir-ismail/>Sabir Ismail</a>
|
<a href=/people/m/md-saiful-islam/>Md Saiful Islam</a>
|
<a href=/people/m/mohammad-ruhul-amin/>Mohammad Ruhul Amin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--4><div class="card-body p-3 small">Many new books get published every year, and only a fraction of them become popular among the readers. So the prediction of a book success can be a very useful parameter for publishers to make a reliable decision. This article presents the study of semantic word associations using the word embedding of book content for a set of Roget&#8217;s thesaurus concepts for book success prediction. In this work, we discuss the method to represent a book as a spectrum of concepts based on the association score between its content embedding and a global embedding (i.e. fastText) for a set of semantically linked word clusters. We show that the semantic word associations outperform the previous methods for book success prediction. In addition, we present that semantic word associations also provide better results than using features like the frequency of word groups in Roget&#8217;s thesaurus, LIWC (a popular tool for linguistic inquiry and word count), NRC (word association emotion lexicon), and part of speech (PoS). Our study reports that concept associations based on <a href=https://en.wikipedia.org/wiki/Roget&#8217;s_Thesaurus>Roget&#8217;s Thesaurus</a> using word embedding of individual novel resulted in the state-of-the-art performance of 0.89 average weighted F1-score for book success prediction. Finally, we present a set of dominant themes that contribute towards the popularity of a book for a specific genre.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.starsem-1.9.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.starsem-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.9/>ParsFEVER : a Dataset for Farsi Fact Extraction and Verification<span class=acl-fixed-case>P</span>ars<span class=acl-fixed-case>FEVER</span>: a Dataset for <span class=acl-fixed-case>F</span>arsi Fact Extraction and Verification</a></strong><br><a href=/people/m/majid-zarharan/>Majid Zarharan</a>
|
<a href=/people/m/mahsa-ghaderan/>Mahsa Ghaderan</a>
|
<a href=/people/a/amin-pourdabiri/>Amin Pourdabiri</a>
|
<a href=/people/z/zahra-sayedi/>Zahra Sayedi</a>
|
<a href=/people/b/behrouz-minaei-bidgoli/>Behrouz Minaei-Bidgoli</a>
|
<a href=/people/s/sauleh-eetemadi/>Sauleh Eetemadi</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--9><div class="card-body p-3 small">Training and evaluation of automatic fact extraction and verification techniques require large amounts of annotated data which might not be available for low-resource languages. This paper presents ParsFEVER : the first publicly available Farsi dataset for fact extraction and verification. We adopt the <a href=https://en.wikipedia.org/wiki/Formal_grammar>construction procedure</a> of the standard English dataset for the task, i.e., <a href=https://en.wikipedia.org/wiki/Fever>FEVER</a>, and improve it for the case of low-resource languages. Specifically, claims are extracted from sentences that are carefully selected to be more informative. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> comprises nearly 23 K manually-annotated claims. Over 65 % of the claims in ParsFEVER are many-hop (require evidence from multiple sources), making the dataset a challenging benchmark (only 13 % of the claims in <a href=https://en.wikipedia.org/wiki/FEVER>FEVER</a> are many-hop). Also, despite having a smaller training set (around one-ninth of that in Fever), a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on ParsFEVER attains similar downstream performance, indicating the quality of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We release the dataset and the annotation guidelines at https://github.com/Zarharan/ParsFEVER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.10/>BiQuAD : Towards <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> based on deeper text understanding<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>Q</span>u<span class=acl-fixed-case>AD</span>: Towards <span class=acl-fixed-case>QA</span> based on deeper text understanding</a></strong><br><a href=/people/f/frank-grimm/>Frank Grimm</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--10><div class="card-body p-3 small">Recent question answering and machine reading benchmarks frequently reduce the task to one of pinpointing spans within a certain text passage that answers the given question. Typically, these <a href=https://en.wikipedia.org/wiki/System>systems</a> are not required to actually understand the text on a deeper level that allows for more complex reasoning on the information contained. We introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> called BiQuAD that requires deeper comprehension in order to answer questions in both extractive and deductive fashion. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consist of 4,190 closed-domain texts and a total of 99,149 <a href=https://en.wikipedia.org/wiki/Question_answering>question-answer pairs</a>. The texts are synthetically generated soccer match reports that verbalize the main events of each match. All texts are accompanied by a structured Datalog program that represents a (logical) model of its information. We show that state-of-the-art QA models do not perform well on the challenging long form contexts and reasoning requirements posed by the dataset. In particular, transformer based state-of-the-art models achieve F1-scores of only 39.0. We demonstrate how these synthetic datasets align structured knowledge with <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural text</a> and aid model introspection when approaching complex text understanding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.11/>Evaluating Universal Dependency Parser Recovery of Predicate Argument Structure via CompChain Analysis<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Parser Recovery of Predicate Argument Structure via <span class=acl-fixed-case>C</span>omp<span class=acl-fixed-case>C</span>hain Analysis</a></strong><br><a href=/people/s/sagar-indurkhya/>Sagar Indurkhya</a>
|
<a href=/people/b/beracah-yankama/>Beracah Yankama</a>
|
<a href=/people/r/robert-c-berwick/>Robert C. Berwick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--11><div class="card-body p-3 small">Accurate recovery of predicate-argument structure from a Universal Dependency (UD) parse is central to downstream tasks such as extraction of semantic roles or event representations. This study introduces compchains, a categorization of the hierarchy of predicate dependency relations present within a UD parse. Accuracy of compchain classification serves as a proxy for measuring accurate recovery of predicate-argument structure from sentences with embedding. We analyzed the distribution of compchains in three UD English treebanks, EWT, GUM and LinES, revealing that these treebanks are sparse with respect to sentences with predicate-argument structure that includes predicate-argument embedding. We evaluated the CoNLL 2018 Shared Task UDPipe (v1.2) baseline (dependency parsing) models as compchain classifiers for the EWT, GUMS and LinES UD treebanks. Our results indicate that these three baseline models exhibit poorer performance on sentences with predicate-argument structure with more than one level of embedding ; we used compchains to characterize the errors made by these parsers and present examples of erroneous parses produced by the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> that were identified using compchains. We also analyzed the distribution of compchains in 58 non-English UD treebanks and then used compchains to evaluate the CoNLL&#8217;18 Shared Task baseline model for each of these <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>. Our analysis shows that performance with respect to compchain classification is only weakly correlated with the official evaluation metrics (LAS, MLAS and BLEX). We identify gaps in the distribution of compchains in several of the UD treebanks, thus providing a roadmap for how these <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> may be supplemented.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.14/>Disentangling Online Chats with DAG-structured LSTMs<span class=acl-fixed-case>DAG</span>-structured <span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/d/duccio-pappadopulo/>Duccio Pappadopulo</a>
|
<a href=/people/l/lisa-bauer/>Lisa Bauer</a>
|
<a href=/people/m/marco-farina/>Marco Farina</a>
|
<a href=/people/o/ozan-irsoy/>Ozan İrsoy</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--14><div class="card-body p-3 small">Many modern <a href=https://en.wikipedia.org/wiki/Message-oriented_middleware>messaging systems</a> allow fast and synchronous textual communication among many users. The resulting sequence of messages hides a more complicated structure in which independent sub-conversations are interwoven with one another. This poses a challenge for any task aiming to understand the content of the <a href=https://en.wikipedia.org/wiki/Chat_log>chat logs</a> or gather information from them. The ability to disentangle these <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> is then tantamount to the success of many downstream <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. Structured information accompanying the text such as user turn, user mentions, timestamps, is used as a cue by the participants themselves who need to follow the conversation and has been shown to be important for disentanglement. DAG-LSTMs, a generalization of Tree-LSTMs that can handle directed acyclic dependencies, are a natural way to incorporate such information and its non-sequential nature. In this paper, we apply DAG-LSTMs to the conversation disentanglement task. We perform our experiments on the Ubuntu IRC dataset. We show that the novel model we propose achieves state of the art status on the task of recovering reply-to relations and it is competitive on other disentanglement metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.15/>Toward Diverse Precondition Generation</a></strong><br><a href=/people/h/heeyoung-kwon/>Heeyoung Kwon</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--15><div class="card-body p-3 small">A typical goal for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a> is to logically connect the events of a discourse, but often connective events are not described due to their commonsense nature. In order to address this deficit, we focus here on generating precondition events. Precondition generation can be framed as a sequence-to-sequence problem : given a target event, generate a possible precondition. However, in most real-world scenarios, an event can have several preconditions, which is not always suitable for standard seq2seq frameworks. We propose DiP, the Diverse Precondition generation system that can generate unique and diverse preconditions. DiP consists of three stages of the generative process an event sampler, a candidate generator, and a post-processor. The event sampler provides control codes (precondition triggers) which the candidate generator uses to focus its generation. Post-processing further improves the results through <a href=https://en.wikipedia.org/wiki/Ranking>re-ranking</a> and <a href=https://en.wikipedia.org/wiki/Filter_(software)>filtering</a>. Unlike other conditional generation systems, DiP automatically generates <a href=https://en.wikipedia.org/wiki/Control_code>control codes</a> without training on diverse examples. Analysis reveals that DiP improves the diversity of preconditions significantly compared to a beam search baseline. Also, manual evaluation shows that DiP generates more <a href=https://en.wikipedia.org/wiki/Precursor_(chemistry)>preconditions</a> than a strong nucleus sampling baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.19/>Incorporating EDS Graph for AMR Parsing<span class=acl-fixed-case>EDS</span> Graph for <span class=acl-fixed-case>AMR</span> Parsing</a></strong><br><a href=/people/z/ziyi-shou/>Ziyi Shou</a>
|
<a href=/people/f/fangzhen-lin/>Fangzhen Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--19><div class="card-body p-3 small">AMR (Abstract Meaning Representation) and EDS (Elementary Dependency Structures) are two popular meaning representations in NLP / NLU. AMR is more abstract and conceptual, while EDS is more low level, closer to the lexical structures of the given sentences. It is thus not surprising that EDS parsing is easier than AMR parsing. In this work, we consider using information from EDS parsing to help improve the performance of AMR parsing. We adopt a transition-based parser and propose to add EDS graphs as additional semantic features using a graph encoder composed of LSTM layer and GCN layer. Our experimental results show that the additional information from EDS parsing indeed gives a boost to the performance of the base AMR parser used in our experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.21/>Neural Metaphor Detection with Visibility Embeddings</a></strong><br><a href=/people/g/gitit-kehat/>Gitit Kehat</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--21><div class="card-body p-3 small">We present new results for the problem of sequence metaphor labeling, using the recently developed Visibility Embeddings. We show that concatenating such <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> to the input of a BiLSTM obtains consistent and significant improvements at almost no cost, and we present further improved results when visibility embeddings are combined with BERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.starsem-1.22.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.starsem-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.22/>Inducing Language-Agnostic Multilingual Representations</a></strong><br><a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--22><div class="card-body p-3 small">Cross-lingual representations have the potential to make NLP techniques available to the vast majority of languages in the world. However, they currently require large pretraining corpora or access to typologically similar languages. In this work, we address these obstacles by removing language identity signals from multilingual embeddings. We examine three approaches for this : (i) re-aligning the vector spaces of target languages (all together) to a pivot source language ; (ii) removing language-specific means and variances, which yields better discriminativeness of embeddings as a by-product ; and (iii) increasing input similarity across languages by removing morphological contractions and sentence reordering. We evaluate on XNLI and reference-free MT evaluation across 19 typologically diverse languages. Our findings expose the limitations of these approachesunlike <a href=https://en.wikipedia.org/wiki/Vector_normalization>vector normalization</a>, vector space re-alignment and <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a> do not achieve consistent gains across encoders and languages. Due to the approaches&#8217; additive effects, their combination decreases the cross-lingual transfer gap by 8.9 points (m-BERT) and 18.2 points (XLM-R) on average across all tasks and languages, however.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.starsem-1.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.25/>Spurious Correlations in Cross-Topic Argument Mining</a></strong><br><a href=/people/t/terne-sasha-thorn-jakobsen/>Terne Sasha Thorn Jakobsen</a>
|
<a href=/people/m/maria-barrett/>Maria Barrett</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--25><div class="card-body p-3 small">Recent work in cross-topic argument mining attempts to learn <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that generalise across topics rather than merely relying on within-topic spurious correlations. We examine the effectiveness of this approach by analysing the output of single-task and multi-task models for cross-topic argument mining, through a combination of linear approximations of their decision boundaries, manual feature grouping, challenge examples, and ablations across the input vocabulary. Surprisingly, we show that cross-topic models still rely mostly on spurious correlations and only generalise within closely related topics, e.g., a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained only on closed-class words and a few common open-class words outperforms a state-of-the-art cross-topic model on distant target topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.starsem-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--starsem-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.starsem-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.starsem-1.27/>Overcoming Poor Word Embeddings with Word Definitions</a></strong><br><a href=/people/c/christopher-malon/>Christopher Malon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--starsem-1--27><div class="card-body p-3 small">Modern natural language understanding models depend on pretrained subword embeddings, but applications may need to reason about words that were never or rarely seen during pretraining. We show that examples that depend critically on a rarer word are more challenging for natural language inference models. Then we explore how a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> could learn to use <a href=https://en.wikipedia.org/wiki/Definition>definitions</a>, provided in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural text</a>, to overcome this handicap. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s understanding of a <a href=https://en.wikipedia.org/wiki/Definition>definition</a> is usually weaker than a well-modeled word embedding, but it recovers most of the performance gap from using a completely untrained word.</div></div></div><hr><div id=2021unimplicit-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.unimplicit-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.unimplicit-1/>Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.unimplicit-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.unimplicit-1.0/>Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language</a></strong><br><a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.unimplicit-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--unimplicit-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.unimplicit-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.unimplicit-1.3/>Evaluation Guidelines to Deal with Implicit Phenomena to Assess Factuality in Data-to-Text Generation</a></strong><br><a href=/people/r/roy-eisenstadt/>Roy Eisenstadt</a>
|
<a href=/people/m/michael-elhadad/>Michael Elhadad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--unimplicit-1--3><div class="card-body p-3 small">Data-to-text generation systems are trained on large datasets, such as WebNLG, Ro-toWire, E2E or DART. Beyond traditional token-overlap evaluation metrics (BLEU or METEOR), a key concern faced by recent generators is to control the factuality of the generated text with respect to the input data specification. We report on our experience when developing an automatic factuality evaluation system for data-to-text generation that we are testing on WebNLG and E2E data. We aim to prepare gold data annotated manually to identify cases where the text communicates more information than is warranted based on the in-put data (extra) or fails to communicate data that is part of the input (missing). While analyzing reference (data, text) samples, we encountered a range of systematic uncertainties that are related to cases on implicit phenomena in text, and the nature of non-linguistic knowledge we expect to be involved when assessing factuality. We derive from our experience a set of evaluation guidelines to reach high inter-annotator agreement on such cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.unimplicit-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--unimplicit-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.unimplicit-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.unimplicit-1.4/>UnImplicit Shared Task Report : Detecting Clarification Requirements in Instructional Text<span class=acl-fixed-case>U</span>n<span class=acl-fixed-case>I</span>mplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text</a></strong><br><a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/t/talita-anthonio/>Talita Anthonio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--unimplicit-1--4><div class="card-body p-3 small">This paper describes the data, task setup, and results of the shared task at the First Workshop on Understanding Implicit and Underspecified Language (UnImplicit). The task requires <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> to predict whether a sentence contains aspects of meaning that are contextually unspecified and thus require clarification. Two teams participated and the best <a href=https://en.wikipedia.org/wiki/Scoring_system>scoring system</a> achieved an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 68 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.unimplicit-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--unimplicit-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.unimplicit-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.unimplicit-1.6/>Human-Model Divergence in the Handling of Vagueness</a></strong><br><a href=/people/e/elias-stengel-eskin/>Elias Stengel-Eskin</a>
|
<a href=/people/j/jimena-guallar-blasco/>Jimena Guallar-Blasco</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--unimplicit-1--6><div class="card-body p-3 small">While aggregate performance metrics can generate valuable insights at a large scale, their dominance means more complex and nuanced language phenomena, such as <a href=https://en.wikipedia.org/wiki/Vagueness>vagueness</a>, may be overlooked. Focusing on vague terms (e.g. sunny, cloudy, young, etc.) we inspect the behavior of visually grounded and text-only models, finding systematic divergences from human judgments even when a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s overall performance is high. To help explain this disparity, we identify two assumptions made by the datasets and <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> examined and, guided by the philosophy of <a href=https://en.wikipedia.org/wiki/Vagueness>vagueness</a>, isolate cases where they do not hold.</div></div></div><hr><div id=2021wat-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.wat-1/>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.0/>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></strong><br><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a>
|
<a href=/people/i/isao-goto/>Isao Goto</a>
|
<a href=/people/h/hideya-mino/>Hideya Mino</a>
|
<a href=/people/c/chenchen-ding/>Chenchen Ding</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/s/shohei-higashiyama/>Shohei Higashiyama</a>
|
<a href=/people/h/hiroshi-manabe/>Hiroshi Manabe</a>
|
<a href=/people/w/win-pa-pa/>Win Pa Pa</a>
|
<a href=/people/s/shantipriya-parida/>Shantipriya Parida</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a>
|
<a href=/people/a/akiko-eriguchi/>Akiko Eriguchi</a>
|
<a href=/people/k/kaori-abe/>Kaori Abe</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.4/>NICT’s Neural Machine Translation Systems for the WAT21 Restricted Translation Task<span class=acl-fixed-case>NICT</span>’s Neural Machine Translation Systems for the <span class=acl-fixed-case>WAT</span>21 Restricted Translation Task</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--4><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> (Team ID : nictrb) for participating in the WAT&#8217;21 restricted machine translation task. In our submitted <a href=https://en.wikipedia.org/wiki/System>system</a>, we designed a new <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training approach</a> for restricted machine translation. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary. With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, as well as <a href=https://en.wikipedia.org/wiki/Mathematical_model>model ensembling</a>, which further improved the final translation performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.6/>NECTEC’s Participation in WAT-2021<span class=acl-fixed-case>NECTEC</span>’s Participation in <span class=acl-fixed-case>WAT</span>-2021</a></strong><br><a href=/people/z/zar-zar-hlaing/>Zar Zar Hlaing</a>
|
<a href=/people/y/ye-kyaw-thu/>Ye Kyaw Thu</a>
|
<a href=/people/t/thazin-myint-oo/>Thazin Myint Oo</a>
|
<a href=/people/m/mya-ei-san/>Mya Ei San</a>
|
<a href=/people/s/sasiporn-usanavasin/>Sasiporn Usanavasin</a>
|
<a href=/people/p/ponrudee-netisopakul/>Ponrudee Netisopakul</a>
|
<a href=/people/t/thepchai-supnithi/>Thepchai Supnithi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--6><div class="card-body p-3 small">In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021. Basically, our models are based on <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural methods</a> for both directions of English-Myanmar and Myanmar-English language pairs. Most of the existing Neural Machine Translation (NMT) models mainly focus on the conversion of sequential data and do not directly use syntactic information. However, we conduct multi-source neural machine translation (NMT) models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus. The multi-source translation is an approach to exploit multiple inputs (e.g. in two different formats) to increase translation accuracy. The RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment. The experimental results showed that the proposed models of RNN-based architecture outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a> for English-to-Myanmar translation task, and the multi-source and shared-multi-source transformer models yield better translation results than the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.11/>Zero-pronoun Data Augmentation for Japanese-to-English Translation<span class=acl-fixed-case>J</span>apanese-to-<span class=acl-fixed-case>E</span>nglish Translation</a></strong><br><a href=/people/r/ryokan-ri/>Ryokan Ri</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--11><div class="card-body p-3 small">For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding <a href=https://en.wikipedia.org/wiki/Pronoun>pronoun</a> in the target side of the English sentence. However, although fully resolving zero pronouns often needs <a href=https://en.wikipedia.org/wiki/Context_(language_use)>discourse context</a>, in some cases, the local context within a sentence gives clues to the inference of the zero pronoun. In this study, we propose a data augmentation method that provides additional training signals for the translation model to learn correlations between <a href=https://en.wikipedia.org/wiki/Context_(language_use)>local context</a> and zero pronouns. We show that the proposed method significantly improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of zero pronoun translation with <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> experiments in the conversational domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.13/>TMU NMT System with Japanese BART for the Patent task of WAT 2021<span class=acl-fixed-case>TMU</span> <span class=acl-fixed-case>NMT</span> System with <span class=acl-fixed-case>J</span>apanese <span class=acl-fixed-case>BART</span> for the Patent task of <span class=acl-fixed-case>WAT</span> 2021</a></strong><br><a href=/people/h/hwichan-kim/>Hwichan Kim</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--13><div class="card-body p-3 small">In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. One of the pre-trained models, BART (Lewis et al., 2020), was shown to improve translation accuracy via fine-tuning with bilingual data. However, they experimented only Romanian!English translation using <a href=https://en.wikipedia.org/wiki/BART>English BART</a>. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.18/>IITP at WAT 2021 : System description for English-Hindi Multimodal Translation Task<span class=acl-fixed-case>IITP</span> at <span class=acl-fixed-case>WAT</span> 2021: System description for <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>H</span>indi Multimodal Translation Task</a></strong><br><a href=/people/b/baban-gain/>Baban Gain</a>
|
<a href=/people/d/dibyanayan-bandyopadhyay/>Dibyanayan Bandyopadhyay</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--18><div class="card-body p-3 small">Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> by removing <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT-2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU points</a> for Evaluation and Challenge subset, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.20/>TMEKU System for the WAT2021 Multimodal Translation Task<span class=acl-fixed-case>TMEKU</span> System for the <span class=acl-fixed-case>WAT</span>2021 Multimodal Translation Task</a></strong><br><a href=/people/y/yuting-zhao/>Yuting Zhao</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--20><div class="card-body p-3 small">We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT). The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.21/>Optimal Word Segmentation for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> into Dravidian Languages<span class=acl-fixed-case>D</span>ravidian Languages</a></strong><br><a href=/people/p/prajit-dhar/>Prajit Dhar</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--21><div class="card-body p-3 small">Dravidian languages, such as <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a> and <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, are notoriously difficult to translate by state-of-the-art neural models. This stems from the fact that these <a href=https://en.wikipedia.org/wiki/Language>languages</a> are morphologically very rich as well as being low-resourced. In this paper, we focus on subword segmentation and evaluate Linguistically Motivated Vocabulary Reduction (LMVR) against the more commonly used SentencePiece (SP) for the task of translating from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into four different <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a>. Additionally we investigate the optimal subword vocabulary size for each language. We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.22/>Itihasa : A <a href=https://en.wikipedia.org/wiki/Text_corpus>large-scale corpus</a> for Sanskrit to English translation<span class=acl-fixed-case>S</span>anskrit to <span class=acl-fixed-case>E</span>nglish translation</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--22><div class="card-body p-3 small">This work introduces <a href=https://en.wikipedia.org/wiki/Itihasa>Itihasa</a>, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The <a href=https://en.wikipedia.org/wiki/Shloka>shlokas</a> are extracted from two <a href=https://en.wikipedia.org/wiki/Indian_epic_poetry>Indian epics</a> viz., The <a href=https://en.wikipedia.org/wiki/Ramayana>Ramayana</a> and The <a href=https://en.wikipedia.org/wiki/Mahabharata>Mahabharata</a>. We first describe the motivation behind the curation of such a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.23/>NICT-5’s Submission To WAT 2021 : MBART Pre-training And In-Domain Fine Tuning For Indic Languages<span class=acl-fixed-case>NICT</span>-5’s Submission To <span class=acl-fixed-case>WAT</span> 2021: <span class=acl-fixed-case>MBART</span> Pre-training And In-Domain Fine Tuning For Indic Languages</a></strong><br><a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/abhisek-chakrabarty/>Abhisek Chakrabarty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--23><div class="card-body p-3 small">In this paper we describe our submission to the multilingual Indic language translation wtask MultiIndicMT under the team name NICT-5. This task involves <a href=https://en.wikipedia.org/wiki/Translation>translation</a> from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.24/>How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task<span class=acl-fixed-case>GPU</span> in 100 hours? <span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>AS</span>ta<span class=acl-fixed-case>L</span> at <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>I</span>ndic<span class=acl-fixed-case>MT</span> Shared Task</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/h/hector-ricardo-murrieta-bello/>Héctor Ricardo Murrieta Bello</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/m/marcel-bollmann/>Marcel Bollmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--24><div class="card-body p-3 small">This work shows that competitive <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translation</a> results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a> for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.26/>Language Relatedness and Lexical Closeness can help Improve Multilingual NMT : IITBombay@MultiIndicNMT WAT2021<span class=acl-fixed-case>NMT</span>: <span class=acl-fixed-case>IITB</span>ombay@<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>I</span>ndic<span class=acl-fixed-case>NMT</span> <span class=acl-fixed-case>WAT</span>2021</a></strong><br><a href=/people/j/jyotsana-khatri/>Jyotsana Khatri</a>
|
<a href=/people/n/nikhil-saini/>Nikhil Saini</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--26><div class="card-body p-3 small">Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for <a href=https://en.wikipedia.org/wiki/Multilingualism>multiple languages</a>. This paper describes our submission (Team ID : CFILT-IITB) for the MultiIndicMT : An Indic Language Multilingual Task at WAT 2021. We train multilingual NMT systems by sharing <a href=https://en.wikipedia.org/wiki/Encoder>encoder and decoder parameters</a> with language embedding associated with each token in both <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and decoder. Furthermore, we demonstrate the use of transliteration (script conversion) for <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indic languages</a> in reducing the lexical gap for training a multilingual NMT system. Further, we show improvement in performance by training a multilingual NMT system using languages of the same family, i.e., <a href=https://en.wikipedia.org/wiki/Lingua_franca>related languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.27/>Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task<span class=acl-fixed-case>S</span>amsung <span class=acl-fixed-case>R</span>&<span class=acl-fixed-case>D</span> Institute <span class=acl-fixed-case>P</span>oland submission to <span class=acl-fixed-case>WAT</span> 2021 Indic Language Multilingual Task</a></strong><br><a href=/people/a/adam-dobrowolski/>Adam Dobrowolski</a>
|
<a href=/people/m/marcin-szymanski/>Marcin Szymański</a>
|
<a href=/people/m/marcin-chochowski/>Marcin Chochowski</a>
|
<a href=/people/p/pawel-przybysz/>Paweł Przybysz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--27><div class="card-body p-3 small">This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&D Institute Poland. The task covered translation between 10 Indic Languages (Bengali, <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a>, <a href=https://en.wikipedia.org/wiki/Odia_language>Oriya</a>, <a href=https://en.wikipedia.org/wiki/Punjabi_language>Punjabi</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a> and <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>) and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We combined a variety of techniques : <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a>, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models. We applied an effective approach to low-resource training that consist of pretraining on backtranslations and tuning on parallel corpora. We experimented with two different domain-adaptation techniques which significantly improved translation quality when applied to monolingual corpora. We researched and applied a novel approach for finding the best <a href=https://en.wikipedia.org/wiki/Hyperparameter>hyperparameters</a> for ensembling a number of <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation models</a>. All techniques combined gave significant improvement-up to +8 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> over baseline results. The quality of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> has been confirmed by the human evaluation where SRPOL models scored best for all 5 manually evaluated languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.28/>Multilingual Machine Translation Systems at WAT 2021 : One-to-Many and Many-to-One Transformer based NMT<span class=acl-fixed-case>WAT</span> 2021: One-to-Many and Many-to-One Transformer based <span class=acl-fixed-case>NMT</span></a></strong><br><a href=/people/s/shivam-mhaskar/>Shivam Mhaskar</a>
|
<a href=/people/a/aditya-jain/>Aditya Jain</a>
|
<a href=/people/a/aakash-banerjee/>Aakash Banerjee</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--28><div class="card-body p-3 small">In this paper, we present the details of the systems that we have submitted for the WAT 2021 MultiIndicMT : An Indic Language Multilingual Task. We have submitted two separate multilingual NMT models : one for <a href=https://en.wikipedia.org/wiki/English_language>English</a> to 10 Indic languages and another for 10 Indic languages to English. We discuss the implementation details of two separate multilingual NMT approaches, namely one-to-many and many-to-one, that makes use of a shared decoder and a shared encoder, respectively. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.30/>ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task<span class=acl-fixed-case>ANVITA</span> Machine Translation System for <span class=acl-fixed-case>WAT</span> 2021 <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>I</span>ndic<span class=acl-fixed-case>MT</span> Shared Task</a></strong><br><a href=/people/p/pavanpankaj-vegi/>Pavanpankaj Vegi</a>
|
<a href=/people/s/sivabhavani-j/>Sivabhavani J</a>
|
<a href=/people/b/biswajit-paul/>Biswajit Paul</a>
|
<a href=/people/c/chitra-viswanathan/>Chitra Viswanathan</a>
|
<a href=/people/p/prasanna-kumar-k-r/>Prasanna Kumar K R</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--30><div class="card-body p-3 small">This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions : EnglishIndic and IndicEnglish ; Indic set comprised of 10 Indian languages. ANVITA-1.0 MT system comprised of two multi-lingual NMT models one for the EnglishIndic directions and other for the IndicEnglish directions with shared encoder-decoder, catering 10 language pairs and twenty translation directions. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization. Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training. ANVITA-1.0 achieved highest AM-FM score for <a href=https://en.wikipedia.org/wiki/Bengali_language>EnglishBengali</a>, 2nd for <a href=https://en.wikipedia.org/wiki/Tamil_language>EnglishTamil</a> and 3rd for <a href=https://en.wikipedia.org/wiki/Indian_English>EnglishHindi</a>, BengaliEnglish directions on official test set. In general, performance achieved by ANVITA for the IndicEnglish directions are relatively better than that of EnglishIndic directions for all the 10 language pairs when evaluated using BLEU and RIBES, although the same trend is not observed consistently when AM-FM based evaluation was carried out. As compared to <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, RIBES and AM-FM based scoring placed ANVITA relatively better among all the task participants.</div></div></div><hr><div id=2021woah-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.woah-1/>Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.woah-1.0/>Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)</a></strong><br><a href=/people/a/aida-mostafazadeh-davani/>Aida Mostafazadeh Davani</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/m/mathias-lambert/>Mathias Lambert</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--woah-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.woah-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.woah-1.10/>Improving Counterfactual Generation for Fair Hate Speech Detection</a></strong><br><a href=/people/a/aida-mostafazadeh-davani/>Aida Mostafazadeh Davani</a>
|
<a href=/people/a/ali-omrani/>Ali Omrani</a>
|
<a href=/people/b/brendan-kennedy/>Brendan Kennedy</a>
|
<a href=/people/m/mohammad-atari/>Mohammad Atari</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/m/morteza-dehghani/>Morteza Dehghani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--woah-1--10><div class="card-body p-3 small">Bias mitigation approaches reduce models&#8217; dependence on sensitive features of data, such as social group tokens (SGTs), resulting in equal predictions across the sensitive features. In <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech detection</a>, however, equalizing model predictions may ignore important differences among targeted social groups, as <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> can contain stereotypical language specific to each SGT. Here, to take the specific language about each SGT into account, we rely on counterfactual fairness and equalize predictions among counterfactuals, generated by changing the SGTs. Our method evaluates the similarity in sentence likelihoods (via pre-trained language models) among <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a>, to treat SGTs equally only within interchangeable contexts. By applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance, we improve fairness metrics while preserving model performance on hate speech detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--woah-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.woah-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.woah-1.15/>Context Sensitivity Estimation in Toxicity Detection</a></strong><br><a href=/people/a/alexandros-xenos/>Alexandros Xenos</a>
|
<a href=/people/j/john-pavlopoulos/>John Pavlopoulos</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--woah-1--15><div class="card-body p-3 small">User posts whose perceived toxicity depends on the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversational context</a> are rare in current toxicity detection datasets. Hence, toxicity detectors trained on current datasets will also disregard <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>, making the detection of context-sensitive toxicity a lot harder when it occurs. We constructed and publicly release a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 10k posts with two kinds of toxicity labels per post, obtained from annotators who considered (i) both the current post and the previous one as context, or (ii) only the current post. We introduce a new task, context-sensitivity estimation, which aims to identify posts whose perceived toxicity changes if the context (previous post) is also considered. Using the new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we show that <a href=https://en.wikipedia.org/wiki/System>systems</a> can be developed for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Such systems could be used to enhance toxicity detection datasets with more context-dependent posts or to suggest when moderators should consider the parent posts, which may not always be necessary and may introduce additional costs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--woah-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.woah-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.woah-1.18.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.woah-1.18/>When the Echo Chamber Shatters : Examining the Use of Community-Specific Language Post-Subreddit Ban</a></strong><br><a href=/people/m/milo-trujillo/>Milo Trujillo</a>
|
<a href=/people/s/sam-rosenblatt/>Sam Rosenblatt</a>
|
<a href=/people/g/guillermo-de-anda-jauregui/>Guillermo de Anda Jáuregui</a>
|
<a href=/people/e/emily-moog/>Emily Moog</a>
|
<a href=/people/b/briane-paul-v-samson/>Briane Paul V. Samson</a>
|
<a href=/people/l/laurent-hebert-dufresne/>Laurent Hébert-Dufresne</a>
|
<a href=/people/a/allison-m-roth/>Allison M. Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--woah-1--18><div class="card-body p-3 small">Community-level bans are a common tool against groups that enable <a href=https://en.wikipedia.org/wiki/Cyberbullying>online harassment</a> and <a href=https://en.wikipedia.org/wiki/Cyberbullying>harmful speech</a>. Unfortunately, the efficacy of community bans has only been partially studied and with mixed results. Here, we provide a flexible <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methodology</a> to identify in-group language and track user activity on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> both before and after the ban of a community (subreddit). We use a simple word frequency divergence to identify uncommon words overrepresented in a given <a href=https://en.wikipedia.org/wiki/Community>community</a>, not as a proxy for harmful speech but as a linguistic signature of the community. We apply our method to 15 banned subreddits, and find that community response is heterogeneous between <a href=https://en.wikipedia.org/wiki/Reddit>subreddits</a> and between users of a <a href=https://en.wikipedia.org/wiki/Reddit>subreddit</a>. Top users were more likely to become less active overall, while random users often reduced use of in-group language without decreasing activity. Finally, we find some evidence that the effectiveness of <a href=https://en.wikipedia.org/wiki/Ban_(law)>bans</a> aligns with the content of a community. Users of dark humor communities were largely unaffected by bans while users of communities organized around <a href=https://en.wikipedia.org/wiki/White_supremacy>white supremacy</a> and <a href=https://en.wikipedia.org/wiki/Fascism>fascism</a> were the most affected. Altogether, our results show that <a href=https://en.wikipedia.org/wiki/Ban_(law)>bans</a> do not affect all groups or users equally, and pave the way to understanding the effect of <a href=https://en.wikipedia.org/wiki/Ban_(law)>bans</a> across communities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--woah-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.woah-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.woah-1.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.woah-1.19/>Targets and Aspects in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> Hate Speech</a></strong><br><a href=/people/a/alexander-shvets/>Alexander Shvets</a>
|
<a href=/people/p/paula-fortuna/>Paula Fortuna</a>
|
<a href=/people/j/juan-soler/>Juan Soler</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--woah-1--19><div class="card-body p-3 small">Mainstream research on <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> focused so far predominantly on the task of classifying mainly social media posts with respect to predefined typologies of rather coarse-grained hate speech categories. This may be sufficient if the goal is to detect and delete abusive language posts. However, removal is not always possible due to the legislation of a country. Also, there is evidence that <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> can not be successfully combated by merely removing <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech posts</a> ; they should be countered by <a href=https://en.wikipedia.org/wiki/Education>education</a> and counter-narratives. For this purpose, we need to identify (i) who is the target in a given hate speech post, and (ii) what aspects (or characteristics) of the target are attributed to the target in the post. As the first approximation, we propose to adapt a generic state-of-the-art concept extraction model to the hate speech domain. The outcome of the experiments is promising and can serve as inspiration for further work on the task</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.woah-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--woah-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.woah-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.woah-1.23/>Racist or Sexist Meme? Classifying Memes beyond Hateful</a></strong><br><a href=/people/h/haris-bin-zia/>Haris Bin Zia</a>
|
<a href=/people/i/ignacio-castro/>Ignacio Castro</a>
|
<a href=/people/g/gareth-tyson/>Gareth Tyson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--woah-1--23><div class="card-body p-3 small">Memes are the combinations of text and images that are often humorous in nature. But, that may not always be the case, and certain combinations of <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> and <a href=https://en.wikipedia.org/wiki/Image>images</a> may depict <a href=https://en.wikipedia.org/wiki/Hatred>hate</a>, referred to as hateful memes. This work presents a multimodal pipeline that takes both visual and textual features from <a href=https://en.wikipedia.org/wiki/Meme>memes</a> into account to (1) identify the protected category (e.g. race, sex etc.) that has been attacked ; and (2) detect the type of attack (e.g. contempt, <a href=https://en.wikipedia.org/wiki/Pejorative>slurs</a> etc.). Our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> uses state-of-the-art pre-trained visual and textual representations, followed by a simple <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression classifier</a>. We employ our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> on the Hateful Memes Challenge dataset with additional newly created fine-grained labels for protected category and type of attack. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an AUROC of 0.96 for identifying the protected category, and 0.97 for detecting the type of attack. We release our code at https://github.com/harisbinzia/HatefulMemes</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>