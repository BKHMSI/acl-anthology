<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>The Workshop on Evaluation and Comparison of NLP Systems (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>The Workshop on Evaluation and Comparison of NLP Systems (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021eval4nlp-1>Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li></ul></div></div><div id=2021eval4nlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.eval4nlp-1/>Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.0/>Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</a></strong><br><a href=/people/y/yang-gao/>Yang Gao</a>
|
<a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/p/piyawat-lertvittayakumjorn/>Piyawat Lertvittayakumjorn</a>
|
<a href=/people/m/marina-fomicheva/>Marina Fomicheva</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.1/>Differential Evaluation : a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing</a></strong><br><a href=/people/l/lucie-gianola/>Lucie Gianola</a>
|
<a href=/people/h/hicham-el-boukkouri/>Hicham El Boukkouri</a>
|
<a href=/people/c/cyril-grouin/>Cyril Grouin</a>
|
<a href=/people/t/thomas-lavergne/>Thomas Lavergne</a>
|
<a href=/people/p/patrick-paroubek/>Patrick Paroubek</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--1><div class="card-body p-3 small">Most of the time, when dealing with a particular Natural Language Processing task, systems are compared on the basis of global statistics such as <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, F1-score, etc. While such scores provide a general idea of the behavior of these <a href=https://en.wikipedia.org/wiki/System>systems</a>, they ignore a key piece of information that can be useful for assessing progress and discerning remaining challenges : the relative difficulty of test instances. To address this shortcoming, we introduce the notion of differential evaluation which effectively defines a pragmatic partition of instances into gradually more difficult bins by leveraging the predictions made by a set of systems. Comparing systems along these difficulty bins enables us to produce a finer-grained analysis of their relative merits, which we illustrate on two use-cases : a comparison of systems participating in a multi-label text classification task (CLEF eHealth 2018 ICD-10 coding), and a comparison of neural models trained for biomedical entity detection (BioCreative V chemical-disease relations dataset).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.2/>Validating Label Consistency in NER Data Annotation<span class=acl-fixed-case>NER</span> Data Annotation</a></strong><br><a href=/people/q/qingkai-zeng/>Qingkai Zeng</a>
|
<a href=/people/m/mengxia-yu/>Mengxia Yu</a>
|
<a href=/people/w/wenhao-yu/>Wenhao Yu</a>
|
<a href=/people/t/tianwen-jiang/>Tianwen Jiang</a>
|
<a href=/people/m/meng-jiang/>Meng Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--2><div class="card-body p-3 small">Data annotation plays a crucial role in ensuring your named entity recognition (NER) projects are trained with the right information to learn from. Producing the most accurate labels is a challenge due to the complexity involved with <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. Label inconsistency between multiple subsets of data annotation (e.g., training set and test set, or multiple training subsets) is an indicator of label mistakes. In this work, we present an <a href=https://en.wikipedia.org/wiki/Empirical_research>empirical method</a> to explore the relationship between label (in-)consistency and <a href=https://en.wikipedia.org/wiki/NER_model>NER model</a> performance. It can be used to validate the label consistency (or catches the inconsistency) in multiple sets of NER data annotation. In experiments, our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets (with 26.7 % and 5.4 % label mistakes). It validated the consistency in the corrected version of both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.4/>StoryDB : Broad Multi-language Narrative Dataset<span class=acl-fixed-case>S</span>tory<span class=acl-fixed-case>DB</span>: Broad Multi-language Narrative Dataset</a></strong><br><a href=/people/a/alexey-tikhonov/>Alexey Tikhonov</a>
|
<a href=/people/i/igor-samenko/>Igor Samenko</a>
|
<a href=/people/i/ivan-yamshchikov/>Ivan Yamshchikov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--4><div class="card-body p-3 small">This paper presents StoryDB a broad multi-language dataset of <a href=https://en.wikipedia.org/wiki/Narrative>narratives</a>. StoryDB is a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of texts</a> that includes stories in 42 different languages. Every language includes 500 + stories. Some of the languages include more than 20 000 stories. Every story is indexed across languages and labeled with tags such as a genre or a topic. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> shows rich topical and language variation and can serve as a resource for the study of the role of <a href=https://en.wikipedia.org/wiki/Narrative>narrative</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> across various languages including low resource ones. We also demonstrate how the dataset could be used to benchmark three modern multilanguage models, namely, mDistillBERT, mBERT, and XLM-RoBERTa.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eval4nlp-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.5/>SeqScore : Addressing Barriers to Reproducible Named Entity Recognition Evaluation<span class=acl-fixed-case>S</span>eq<span class=acl-fixed-case>S</span>core: Addressing Barriers to Reproducible Named Entity Recognition Evaluation</a></strong><br><a href=/people/c/chester-palen-michel/>Chester Palen-Michel</a>
|
<a href=/people/n/nolan-holley/>Nolan Holley</a>
|
<a href=/people/c/constantine-lignos/>Constantine Lignos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--5><div class="card-body p-3 small">To address a looming crisis of unreproducible evaluation for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, we propose guidelines and introduce SeqScore, a software package to improve <a href=https://en.wikipedia.org/wiki/Reproducibility>reproducibility</a>. The guidelines we propose are extremely simple and center around transparency regarding how chunks are encoded and scored. We demonstrate that despite the apparent simplicity of NER evaluation, unreported differences in the scoring procedure can result in changes to scores that are both of noticeable magnitude and statistically significant. We describe SeqScore, which addresses many of the issues that cause <a href=https://en.wikipedia.org/wiki/Replication_(computing)>replication failures</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.6/>Trainable Ranking Models to Evaluate the Semantic Accuracy of Data-to-Text Neural Generator</a></strong><br><a href=/people/n/nicolas-garneau/>Nicolas Garneau</a>
|
<a href=/people/l/luc-lamontagne/>Luc Lamontagne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--6><div class="card-body p-3 small">In this paper, we introduce a new embedding-based metric relying on trainable ranking models to evaluate the semantic accuracy of neural data-to-text generators. This <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> is especially well suited to semantically and factually assess the performance of a <a href=https://en.wikipedia.org/wiki/Text_generator>text generator</a> when tables can be associated with multiple references and table values contain textual utterances. We first present how one can implement and further specialize the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> by training the underlying ranking models on a legal Data-to-Text dataset. We show how it may provide a more robust evaluation than other evaluation schemes in challenging settings using a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> comprising <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> between the table values and their respective references. Finally, we evaluate its generalization capabilities on a well-known dataset, WebNLG, by comparing it with human evaluation and a recently introduced <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> based on natural language inference. We then illustrate how it naturally characterizes, both quantitatively and qualitatively, omissions and <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.7/>Evaluation of Unsupervised Automatic Readability Assessors Using Rank Correlations</a></strong><br><a href=/people/y/yo-ehara/>Yo Ehara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--7><div class="card-body p-3 small">Automatic readability assessment (ARA) is the task of automatically assessing readability with little or no <a href=https://en.wikipedia.org/wiki/Supervisor>human supervision</a>. ARA is essential for many second language acquisition applications to reduce the workload of annotators, who are usually language teachers. Previous <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approaches</a> manually searched textual features that correlated well with <a href=https://en.wikipedia.org/wiki/Readability>readability labels</a>, such as perplexity scores of large language models. This paper argues that, to evaluate an assessors&#8217; performance, rank-correlation coefficients should be used instead of Pearson&#8217;s correlation coefficient (). In the experiments, we show that its performance can be easily underestimated using Pearson&#8217;s, which is significantly affected by the <a href=https://en.wikipedia.org/wiki/Linearity>linearity</a> of the output readability scores. We also propose a lightweight unsupervised readability assessor that achieved the best performance in both the rank correlations and Pearson&#8217;s among all unsupervised assessors compared.<tex-math>\\rho</tex-math>). In the experiments, we show that its performance can be easily underestimated using Pearson&#8217;s <tex-math>\\rho</tex-math>, which is significantly affected by the linearity of the output readability scores. We also propose a lightweight unsupervised readability assessor that achieved the best performance in both the rank correlations and Pearson&#8217;s <tex-math>\\rho</tex-math> among all unsupervised assessors compared.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eval4nlp-1.8" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.8/>Testing Cross-Database Semantic Parsers With Canonical Utterances</a></strong><br><a href=/people/h/heather-lent/>Heather Lent</a>
|
<a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/t/tong-niu/>Tong Niu</a>
|
<a href=/people/y/yingbo-zhou/>Yingbo Zhou</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--8><div class="card-body p-3 small">The benchmark performance of cross-database semantic parsing has climbed steadily in recent years, catalyzed by the wide adoption of pre-trained language models. Yet existing work have shown that state-of-the-art cross-database semantic parsers struggle to generalize to novel user utterances, databases and query structures. To obtain transparent details on the strengths and limitation of these models, we propose a diagnostic testing approach based on controlled synthesis of canonical natural language and SQL pairs. Inspired by the CheckList, we characterize a set of essential capabilities for cross-database semantic parsing models, and detailed the method for synthesizing the corresponding test data. We evaluated a variety of high performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> using the proposed approach, and identified several non-obvious weaknesses across <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> (e.g. unable to correctly select many columns). Our dataset and code are released as a test suite at http://github.com/hclent/BehaviorCheckingSemPar.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.eval4nlp-1.9.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eval4nlp-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.9/>Writing Style Author Embedding Evaluation</a></strong><br><a href=/people/e/enzo-terreau/>Enzo Terreau</a>
|
<a href=/people/a/antoine-gourru/>Antoine Gourru</a>
|
<a href=/people/j/julien-velcin/>Julien Velcin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--9><div class="card-body p-3 small">Learning authors representations from their textual productions is now widely used to solve multiple downstream tasks, such as <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, link prediction or <a href=https://en.wikipedia.org/wiki/Recommender_system>user recommendation</a>. Author embedding methods are often built on top of either Doc2Vec (Mikolov et al. 2014) or the Transformer architecture (Devlin et al. Evaluating the quality of these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> and what they capture is a difficult task. Most articles use either <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> or <a href=https://en.wikipedia.org/wiki/Attribution_(copyright)>authorship attribution</a>, which does not clearly measure the quality of the <a href=https://en.wikipedia.org/wiki/Representation_space>representation space</a>, if it really captures what it has been built for. In this paper, we propose a novel evaluation framework of author embedding methods based on the <a href=https://en.wikipedia.org/wiki/Writing_style>writing style</a>. It allows to quantify if the embedding space effectively captures a set of <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>stylistic features</a>, chosen to be the best proxy of an author writing style. This approach gives less importance to the topics conveyed by the documents. It turns out that recent <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are mostly driven by the inner semantic of authors&#8217; production. They are outperformed by simple baselines, based on state-of-the-art pretrained sentence embedding models, on several linguistic axes. These baselines can grasp complex linguistic phenomena and <a href=https://en.wikipedia.org/wiki/Writing_style>writing style</a> more efficiently, paving the way for designing new style-driven author embedding models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.11/>Statistically Significant Detection of Semantic Shifts using Contextual Word Embeddings</a></strong><br><a href=/people/y/yang-liu-Helsinki/>Yang Liu</a>
|
<a href=/people/a/alan-medlar/>Alan Medlar</a>
|
<a href=/people/d/dorota-glowacka/>Dorota Glowacka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--11><div class="card-body p-3 small">Detecting lexical semantic change in smaller data sets, e.g. in <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a> and <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities</a>, is challenging due to a lack of <a href=https://en.wikipedia.org/wiki/Statistical_power>statistical power</a>. This issue is exacerbated by non-contextual embedding models that produce one embedding per word and, therefore, mask the variability present in the data. In this article, we propose an approach to estimate <a href=https://en.wikipedia.org/wiki/Semantic_shift>semantic shift</a> by combining contextual word embeddings with permutation-based statistical tests. We use the false discovery rate procedure to address the large number of <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>hypothesis tests</a> being conducted simultaneously. We demonstrate the performance of this approach in simulation where it achieves consistently high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> by suppressing false positives. We additionally analyze real-world data from SemEval-2020 Task 1 and the Liverpool FC subreddit corpus. We show that by taking sample variation into account, we can improve the robustness of individual semantic shift estimates without degrading overall performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.12/>Referenceless Parsing-Based Evaluation of AMR-to-English Generation<span class=acl-fixed-case>AMR</span>-to-<span class=acl-fixed-case>E</span>nglish Generation</a></strong><br><a href=/people/e/emma-manning/>Emma Manning</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--12><div class="card-body p-3 small">Reference-based automatic evaluation metrics are notoriously limited for NLG due to their inability to fully capture the range of possible outputs. We examine a referenceless alternative : evaluating the adequacy of English sentences generated from Abstract Meaning Representation (AMR) graphs by parsing into AMR and comparing the parse directly to the input. We find that the errors introduced by automatic AMR parsing substantially limit the effectiveness of this approach, but a manual editing study indicates that as <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> improves, parsing-based evaluation has the potential to outperform most reference-based metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.eval4nlp-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.14/>IST-Unbabel 2021 Submission for the Explainable Quality Estimation Shared Task<span class=acl-fixed-case>IST</span>-Unbabel 2021 Submission for the Explainable Quality Estimation Shared Task</a></strong><br><a href=/people/m/marcos-treviso/>Marcos Treviso</a>
|
<a href=/people/n/nuno-m-guerreiro/>Nuno M. Guerreiro</a>
|
<a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--14><div class="card-body p-3 small">We present the joint contribution of Instituto Superior Tcnico (IST) and Unbabel to the Explainable Quality Estimation (QE) shared task, where systems were submitted to two tracks : constrained (without word-level supervision) and unconstrained (with word-level supervision). For the constrained track, we experimented with several explainability methods to extract the relevance of input tokens from sentence-level QE models built on top of multilingual pre-trained transformers. Among the different tested methods, composing explanations in the form of attention weights scaled by the <a href=https://en.wikipedia.org/wiki/Norm_(mathematics)>norm of value vectors</a> yielded the best results. When word-level labels are used during training, our best results were obtained by using word-level predicted probabilities. We further improve the performance of our methods on the two tracks by ensembling explanation scores extracted from models trained with different pre-trained transformers, achieving strong results for in-domain and zero-shot language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.21/>What is SemEval evaluating? A Systematic Analysis of Evaluation Campaigns in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a><span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val evaluating? A Systematic Analysis of Evaluation Campaigns in <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/o/oskar-wysocki/>Oskar Wysocki</a>
|
<a href=/people/m/malina-florea/>Malina Florea</a>
|
<a href=/people/d/donal-landers/>Dónal Landers</a>
|
<a href=/people/a/andre-freitas/>André Freitas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--21><div class="card-body p-3 small">SemEval is the primary venue in the NLP community for the proposal of new challenges and for the systematic empirical evaluation of NLP systems. This paper provides a systematic quantitative analysis of <a href=https://en.wikipedia.org/wiki/SemEval>SemEval</a> aiming to evidence the patterns of the contributions behind <a href=https://en.wikipedia.org/wiki/SemEval>SemEval</a>. By understanding the distribution of task types, <a href=https://en.wikipedia.org/wiki/Performance_metric>metrics</a>, <a href=https://en.wikipedia.org/wiki/Software_architecture>architectures</a>, participation and <a href=https://en.wikipedia.org/wiki/Citation>citations</a> over time we aim to answer the question on what is being evaluated by <a href=https://en.wikipedia.org/wiki/SemEval>SemEval</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.eval4nlp-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--eval4nlp-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.eval4nlp-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.eval4nlp-1.22/>The UMD Submission to the Explainable MT Quality Estimation Shared Task : Combining Explanation Models with Sequence Labeling<span class=acl-fixed-case>UMD</span> Submission to the Explainable <span class=acl-fixed-case>MT</span> Quality Estimation Shared Task: Combining Explanation Models with Sequence Labeling</a></strong><br><a href=/people/t/tasnim-kabir/>Tasnim Kabir</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--eval4nlp-1--22><div class="card-body p-3 small">This paper describes the UMD submission to the Explainable Quality Estimation Shared Task at the EMNLP 2021 Workshop on Evaluation & Comparison of NLP Systems. We participated in the word-level and sentence-level MT Quality Estimation (QE) constrained tasks for all language pairs : Estonian-English, Romanian-English, German-Chinese, and Russian-German. Our approach combines the predictions of a word-level explainer model on top of a sentence-level QE model and a sequence labeler trained on synthetic data. These <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are based on pre-trained multilingual language models and do not require any word-level annotations for training, making them well suited to zero-shot settings. Our best-performing system improves over the best baseline across all metrics and language pairs, with an average gain of 0.1 in AUC, <a href=https://en.wikipedia.org/wiki/Average_precision>Average Precision</a>, and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a> at Top-K score.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>