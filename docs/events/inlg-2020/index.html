<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>International Natural Language Generation Conference (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>International Natural Language Generation Conference (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020inlg-1>Proceedings of the 13th International Conference on Natural Language Generation</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#2020evalnlgeval-1>Proceedings of the 1st Workshop on Evaluating NLG Evaluation</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020nl4xai-1>2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li></ul></div></div><div id=2020inlg-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.inlg-1/>Proceedings of the 13th International Conference on Natural Language Generation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.0/>Proceedings of the 13th International Conference on Natural Language Generation</a></strong><br><a href=/people/b/brian-davis/>Brian Davis</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/j/john-kelleher/>John Kelleher</a>
|
<a href=/people/y/yaji-sripada/>Yaji Sripada</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.8/>Assessing Discourse Relations in <a href=https://en.wikipedia.org/wiki/Language_generation>Language Generation</a> from GPT-2<span class=acl-fixed-case>GPT</span>-2</a></strong><br><a href=/people/w/wei-jen-ko/>Wei-Jen Ko</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--8><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> have been attributed to the emergence of large-scale pre-trained language models. GPT-2, in particular, is suited for generation tasks given its left-to-right language modeling objective, yet the linguistic quality of its generated text has largely remain unexplored. Our work takes a step in understanding GPT-2&#8217;s outputs in terms of discourse coherence. We perform a comprehensive study on the validity of explicit discourse relations in GPT-2&#8217;s outputs under both organic generation and fine-tuned scenarios. Results show GPT-2 does not always generate text containing valid discourse relations ; nevertheless, its text is more aligned with <a href=https://en.wikipedia.org/wiki/Expectation_(epistemic)>human expectation</a> in the fine-tuned scenario. We propose a decoupled strategy to mitigate these problems and highlight the importance of explicitly modeling discourse information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.inlg-1.10.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.inlg-1.10/>The CACAPO Dataset : A Multilingual, Multi-Domain Dataset for Neural Pipeline and End-to-End Data-to-Text Generation<span class=acl-fixed-case>CACAPO</span> Dataset: A Multilingual, Multi-Domain Dataset for Neural Pipeline and End-to-End Data-to-Text Generation</a></strong><br><a href=/people/c/chris-van-der-lee/>Chris van der Lee</a>
|
<a href=/people/c/chris-emmery/>Chris Emmery</a>
|
<a href=/people/s/sander-wubben/>Sander Wubben</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--10><div class="card-body p-3 small">This paper describes the CACAPO dataset, built for training both neural pipeline and end-to-end data-to-text language generation systems. The dataset is multilingual (Dutch and English), and contains almost 10,000 sentences from human-written news texts in the sports, weather, stocks, and incidents domain, together with aligned attribute-value paired data. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is unique in that the <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variation</a> and indirect ways of expressing data in these texts reflect the challenges of real world NLG tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.inlg-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.15/>DaMata : A Robot-Journalist Covering the Brazilian Amazon Deforestation<span class=acl-fixed-case>D</span>a<span class=acl-fixed-case>M</span>ata: A Robot-Journalist Covering the <span class=acl-fixed-case>B</span>razilian <span class=acl-fixed-case>A</span>mazon Deforestation</a></strong><br><a href=/people/a/andre-luiz-rosa-teixeira/>André Luiz Rosa Teixeira</a>
|
<a href=/people/j/joao-campos/>João Campos</a>
|
<a href=/people/r/rossana-cunha/>Rossana Cunha</a>
|
<a href=/people/t/thiago-castro-ferreira/>Thiago Castro Ferreira</a>
|
<a href=/people/a/adriana-pagano/>Adriana Pagano</a>
|
<a href=/people/f/fabio-cozman/>Fabio Cozman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--15><div class="card-body p-3 small">This demo paper introduces DaMata, a robot-journalist covering <a href=https://en.wikipedia.org/wiki/Deforestation>deforestation</a> in the Brazilian Amazon. The robot-journalist is based on a pipeline architecture of Natural Language Generation, which yields multilingual daily and monthly reports based on the public data provided by DETER, a real-time deforestation satellite monitor developed and maintained by the Brazilian National Institute for Space Research (INPE). DaMata automatically generates reports in <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> and publishes them on the <a href=https://en.wikipedia.org/wiki/Twitter>Twitter platform</a>. Corpus and code are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.inlg-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.18/>PARENTing via Model-Agnostic Reinforcement Learning to Correct Pathological Behaviors in Data-to-Text Generation<span class=acl-fixed-case>PARENT</span>ing via Model-Agnostic Reinforcement Learning to Correct Pathological Behaviors in Data-to-Text Generation</a></strong><br><a href=/people/c/clement-rebuffel/>Clement Rebuffel</a>
|
<a href=/people/l/laure-soulier/>Laure Soulier</a>
|
<a href=/people/g/geoffrey-scoutheeten/>Geoffrey Scoutheeten</a>
|
<a href=/people/p/patrick-gallinari/>Patrick Gallinari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--18><div class="card-body p-3 small">In language generation models conditioned by <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a>, the classical training via <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood</a> almost always leads models to pick up on <a href=https://en.wikipedia.org/wiki/Divergence_(statistics)>dataset divergence</a> (i.e., <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a> or omissions), and to incorporate them erroneously in their own generations at inference. In this work, we build on top of previous Reinforcement Learning based approaches and show that a model-agnostic framework relying on the recently introduced PARENT metric is efficient at reducing both <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a> and omissions. Evaluations on the widely used WikiBIO and WebNLG benchmarks demonstrate the effectiveness of this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> compared to state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.inlg-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.20/>Chart-to-Text : Generating Natural Language Descriptions for Charts by Adapting the Transformer Model</a></strong><br><a href=/people/j/jason-obeid/>Jason Obeid</a>
|
<a href=/people/e/enamul-hoque/>Enamul Hoque</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--20><div class="card-body p-3 small">Information visualizations such as <a href=https://en.wikipedia.org/wiki/Bar_chart>bar charts</a> and <a href=https://en.wikipedia.org/wiki/Line_chart>line charts</a> are very popular for exploring data and communicating insights. Interpreting and making sense of such <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualizations</a> can be challenging for some people, such as those who are visually impaired or have low visualization literacy. In this work, we introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and present a neural model for automatically generating <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language summaries</a> for <a href=https://en.wikipedia.org/wiki/Chart>charts</a>. The generated summaries provide an interpretation of the chart and convey the key insights found within that chart. Our neural model is developed by extending the state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for the data-to-text generation task, which utilizes a transformer-based encoder-decoder architecture. We found that our approach outperforms the base model on a content selection metric by a wide margin (55.42 % vs. 8.49 %) and generates more informative, concise, and coherent summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.24/>Disentangling the Properties of Human Evaluation Methods : A Classification System to Support Comparability, Meta-Evaluation and Reproducibility Testing</a></strong><br><a href=/people/a/anja-belz/>Anya Belz</a>
|
<a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/d/david-m-howcroft/>David M. Howcroft</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--24><div class="card-body p-3 small">Current standards for designing and reporting human evaluations in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> mean it is generally unclear which evaluations are comparable and can be expected to yield similar results when applied to the same system outputs. This has serious implications for reproducibility testing and meta-evaluation, in particular given that human evaluation is considered the gold standard against which the trustworthiness of automatic metrics is gauged. % and merging others, as well as deciding which evaluations should be able to reproduce each other&#8217;s results. Using examples from NLG, we propose a classification system for evaluations based on disentangling (i) what is being evaluated (which aspect of quality), and (ii) how it is evaluated in specific (a) evaluation modes and (b) experimental designs. We show that this approach provides a basis for determining <a href=https://en.wikipedia.org/wiki/Comparability>comparability</a>, hence for comparison of evaluations across papers, <a href=https://en.wikipedia.org/wiki/Meta-analysis>meta-evaluation experiments</a>, <a href=https://en.wikipedia.org/wiki/Reproducibility>reproducibility testing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.26/>Listener’s Social Identity Matters in Personalised Response Generation</a></strong><br><a href=/people/g/guanyi-chen/>Guanyi Chen</a>
|
<a href=/people/y/yinhe-zheng/>Yinhe Zheng</a>
|
<a href=/people/y/yupei-du/>Yupei Du</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--26><div class="card-body p-3 small">Personalised response generation enables generating human-like responses by means of assigning the generator a <a href=https://en.wikipedia.org/wiki/Identity_(social_science)>social identity</a>. However, <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatics theory</a> suggests that human beings adjust the way of speaking based on not only who they are but also whom they are talking to. In other words, when modelling personalised dialogues, it might be favourable if we also take the listener&#8217;s social identity into consideration. To validate this idea, we use <a href=https://en.wikipedia.org/wiki/Gender>gender</a> as a typical example of a social variable to investigate how the listener&#8217;s identity influences the language used in Chinese dialogues on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Also, we build <a href=https://en.wikipedia.org/wiki/Electric_generator>personalised generators</a>. The experiment results demonstrate that the listener&#8217;s identity indeed matters in the language use of responses and that the response generator can capture such differences in language use. More interestingly, by additionally modelling the listener&#8217;s identity, the personalised response generator performs better in its own identity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.inlg-1.35" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.35/>Schema-Guided Natural Language Generation</a></strong><br><a href=/people/y/yuheng-du/>Yuheng Du</a>
|
<a href=/people/s/shereen-oraby/>Shereen Oraby</a>
|
<a href=/people/v/vittorio-perera/>Vittorio Perera</a>
|
<a href=/people/m/minmin-shen/>Minmin Shen</a>
|
<a href=/people/a/anjali-narayan-chen/>Anjali Narayan-Chen</a>
|
<a href=/people/t/tagyoung-chung/>Tagyoung Chung</a>
|
<a href=/people/a/anushree-venkatesh/>Anushree Venkatesh</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--35><div class="card-body p-3 small">Neural network based approaches to data-to-text natural language generation (NLG) have gained popularity in recent years, with the goal of generating a natural language prompt that accurately realizes an input meaning representation. To facilitate the training of <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a>, researchers created large datasets of paired utterances and their meaning representations. However, the creation of such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> is an arduous task and they mostly consist of simple meaning representations composed of slot and value tokens to be realized. These representations do not include any contextual information that an NLG system can use when trying to generalize, such as domain information and descriptions of slots and values. In this paper, we present the novel task of Schema-Guided Natural Language Generation (SG-NLG). Here, the goal is still to generate a <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language prompt</a>, but in SG-NLG, the input MRs are paired with rich schemata providing contextual information. To generate a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for SG-NLG we re-purpose an existing <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for another task : dialog state tracking, which includes a large and rich schema spanning multiple different attributes, including information about the domain, user intent, and slot descriptions. We train different state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for neural natural language generation on this dataset and show that in many cases, including rich schema information allows our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to produce higher quality outputs both in terms of <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and diversity. We also conduct experiments comparing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance on seen versus unseen domains, and present a human evaluation demonstrating high ratings for overall output quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.inlg-1.37.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.inlg-1.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.37/>Neural NLG for Methodius : From RST Meaning Representations to Texts<span class=acl-fixed-case>NLG</span> for Methodius: From <span class=acl-fixed-case>RST</span> Meaning Representations to Texts</a></strong><br><a href=/people/s/symon-stevens-guille/>Symon Stevens-Guille</a>
|
<a href=/people/a/aleksandre-maskharashvili/>Aleksandre Maskharashvili</a>
|
<a href=/people/a/amy-isard/>Amy Isard</a>
|
<a href=/people/x/xintong-li/>Xintong Li</a>
|
<a href=/people/m/michael-white/>Michael White</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--37><div class="card-body p-3 small">While classic NLG systems typically made use of hierarchically structured content plans that included <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> as central components, more recent neural approaches have mostly mapped simple, flat inputs to texts without representing <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> explicitly. In this paper, we investigate whether it is beneficial to include <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> in the input to neural data-to-text generators for texts where <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a> play an important role. To do so, we reimplement the sentence planning and realization components of a classic NLG system, Methodius, using LSTM sequence-to-sequence (seq2seq) models. We find that although seq2seq models can learn to generate fluent and grammatical texts remarkably well with sufficiently representative Methodius training data, they can not learn to correctly express Methodius&#8217;s similarity and contrast comparisons unless the corresponding RST relations are included in the inputs. Additionally, we experiment with using self-training and reverse model reranking to better handle train / test data mismatches, and find that while these methods help reduce content errors, it remains essential to include discourse relations in the input to obtain optimal performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.inlg-1.38.Supplementary_Attachment.pdf data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.inlg-1.38/>From Before to After : Generating Natural Language Instructions from Image Pairs in a Simple Visual Domain</a></strong><br><a href=/people/r/robin-rojowiec/>Robin Rojowiec</a>
|
<a href=/people/j/jana-gotze/>Jana Götze</a>
|
<a href=/people/p/philipp-sadler/>Philipp Sadler</a>
|
<a href=/people/h/henrik-voigt/>Henrik Voigt</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--38><div class="card-body p-3 small">While certain types of <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>instructions</a> can be com-pactly expressed via <a href=https://en.wikipedia.org/wiki/Digital_image>images</a>, there are situations where one might want to verbalise them, for example when directing someone. We investigate the task of Instruction Generation from Before / After Image Pairs which is to derive from images an instruction for effecting the implied change. For this, we make use of prior work on instruction following in a visual environment. We take an existing <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, the BLOCKS data collected by Bisk et al. (2016) and investigate whether it is suitable for training an <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>instruction generator</a> as well. We find that it is, and investigate several simple baselines, taking these from the related task of image captioning. Through a series of experiments that simplify the task (by making <a href=https://en.wikipedia.org/wiki/Digital_image_processing>image processing</a> easier or completely side-stepping it ; and by creating template-based targeted instructions), we investigate areas for improvement. We find that captioning models get some way towards solving the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, but have some difficulty with it, and future improvements must lie in the way the change is detected in the instruction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.inlg-1.42.Supplementary_Attachment.zip data-toggle=tooltip data-placement=top title="Supplementary attachment"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.inlg-1.42/>Rapformer : Conditional Rap Lyrics Generation with Denoising Autoencoders</a></strong><br><a href=/people/n/nikola-i-nikolov/>Nikola I. Nikolov</a>
|
<a href=/people/e/eric-malmi/>Eric Malmi</a>
|
<a href=/people/c/curtis-northcutt/>Curtis Northcutt</a>
|
<a href=/people/l/loreto-parisi/>Loreto Parisi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--42><div class="card-body p-3 small">The ability to combine <a href=https://en.wikipedia.org/wiki/Symbol>symbols</a> to generate <a href=https://en.wikipedia.org/wiki/Language>language</a> is a defining characteristic of <a href=https://en.wikipedia.org/wiki/Human_intelligence>human intelligence</a>, particularly in the context of artistic story-telling through lyrics. We develop a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> for synthesizing a <a href=https://en.wikipedia.org/wiki/Rapping>rap verse</a> based on the content of any text (e.g., a news article), or for augmenting pre-existing <a href=https://en.wikipedia.org/wiki/Rapping>rap lyrics</a>. Our method, called Rapformer, is based on training a Transformer-based denoising autoencoder to reconstruct rap lyrics from content words extracted from the <a href=https://en.wikipedia.org/wiki/Lyrics>lyrics</a>, trying to preserve the essential meaning, while matching the target style. Rapformer features a novel BERT-based paraphrasing scheme for rhyme enhancement which increases the average rhyme density of output lyrics by 10 %. Experimental results on three diverse input domains show that Rapformer is capable of generating technically fluent verses that offer a good trade-off between content preservation and style transfer. Furthermore, a Turing-test-like experiment reveals that Rapformer fools human lyrics experts 25 % of the time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.inlg-1.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--inlg-1--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.inlg-1.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.inlg-1.46/>Policy-Driven Neural Response Generation for Knowledge-Grounded Dialog Systems</a></strong><br><a href=/people/b/behnam-hedayatnia/>Behnam Hedayatnia</a>
|
<a href=/people/k/karthik-gopalakrishnan/>Karthik Gopalakrishnan</a>
|
<a href=/people/s/seokhwan-kim/>Seokhwan Kim</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a>
|
<a href=/people/m/mihail-eric/>Mihail Eric</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--inlg-1--46><div class="card-body p-3 small">Open-domain dialog systems aim to generate relevant, informative and engaging responses. In this paper, we propose using a dialog policy to plan the content and style of target, open domain responses in the form of an action plan, which includes knowledge sentences related to the dialog context, targeted dialog acts, topic information, etc. For <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, the attributes within the <a href=https://en.wikipedia.org/wiki/Action_plan>action plan</a> are obtained by automatically annotating the publicly released Topical-Chat dataset. We condition neural response generators on the <a href=https://en.wikipedia.org/wiki/Action_plan>action plan</a> which is then realized as target utterances at the turn and sentence levels. We also investigate different dialog policy models to predict an <a href=https://en.wikipedia.org/wiki/Action_plan>action plan</a> given the dialog context. Through automated and human evaluation, we measure the appropriateness of the generated responses and check if the generation models indeed learn to realize the given <a href=https://en.wikipedia.org/wiki/Action_plan>action plans</a>. We demonstrate that a basic dialog policy that operates at the sentence level generates better responses in comparison to turn level generation as well as baseline models with no <a href=https://en.wikipedia.org/wiki/Action_plan>action plan</a>. Additionally the basic dialog policy has the added benefit of <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a>.</div></div></div><hr><div id=2020evalnlgeval-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.evalnlgeval-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2020.evalnlgeval-1/>Proceedings of the 1st Workshop on Evaluating NLG Evaluation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.evalnlgeval-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.evalnlgeval-1.0/>Proceedings of the 1st Workshop on Evaluating NLG Evaluation</a></strong><br><a href=/people/s/shubham-agarwal/>Shubham Agarwal</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/d/dimitra-gkatzia/>Dimitra Gkatzia</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/e/emiel-van-miltenburg/>Emiel Van Miltenburg</a>
|
<a href=/people/s/sashank-santhanam/>Sashank Santhanam</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.evalnlgeval-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--evalnlgeval-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.evalnlgeval-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.evalnlgeval-1.4/>NUBIA : NeUral Based Interchangeability Assessor for Text Generation<span class=acl-fixed-case>NUBIA</span>: <span class=acl-fixed-case>N</span>e<span class=acl-fixed-case>U</span>ral Based Interchangeability Assessor for Text Generation</a></strong><br><a href=/people/h/hassan-kane/>Hassan Kane</a>
|
<a href=/people/m/muhammed-yusuf-kocyigit/>Muhammed Yusuf Kocyigit</a>
|
<a href=/people/a/ali-abdalla/>Ali Abdalla</a>
|
<a href=/people/p/pelkins-ajanoh/>Pelkins Ajanoh</a>
|
<a href=/people/m/mohamed-coulibali/>Mohamed Coulibali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--evalnlgeval-1--4><div class="card-body p-3 small">We present NUBIA, a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to build automatic evaluation metrics for <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a> using only <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> as core components. A typical NUBIA model is composed of three modules : a neural feature extractor, an aggregator and a calibrator. We demonstrate an implementation of NUBIA showing competitive performance with stateof-the art metrics used to evaluate <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and state-of-the art results for image captions quality evaluation. In addition to strong performance, NUBIA models have the advantage of being modular and improve in synergy with advances in text generation models.</div></div></div><hr><div id=2020nl4xai-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2020.nl4xai-1/>2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.0/>2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence</a></strong><br><a href=/people/j/jose-m-alonso/>Jose M. Alonso</a>
|
<a href=/people/a/alejandro-catala/>Alejandro Catala</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.2/>Bias in <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI-systems</a> : A multi-step approach<span class=acl-fixed-case>AI</span>-systems: A multi-step approach</a></strong><br><a href=/people/e/eirini-ntoutsi/>Eirini Ntoutsi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--2><div class="card-body p-3 small">Algorithmic-based decision making powered via AI and (big) data has already penetrated into almost all spheres of human life, from <a href=https://en.wikipedia.org/wiki/Recommender_system>content recommendation</a> and <a href=https://en.wikipedia.org/wiki/Health_care>healthcare</a> to <a href=https://en.wikipedia.org/wiki/Predictive_policing>predictive policing</a> and <a href=https://en.wikipedia.org/wiki/Self-driving_car>autonomous driving</a>, deeply affecting everyone, anywhere, anytime. While <a href=https://en.wikipedia.org/wiki/Technology>technology</a> allows previously unthinkable optimizations in the automation of expensive human decision making, the risks that the <a href=https://en.wikipedia.org/wiki/Technology>technology</a> can pose are also high, leading to an ever increasing public concern about the impact of the <a href=https://en.wikipedia.org/wiki/Technology>technology</a> in our lives. The area of responsible AI has recently emerged in an attempt to put humans at the center of AI-based systems by considering aspects, such as <a href=https://en.wikipedia.org/wiki/Social_justice>fairness</a>, <a href=https://en.wikipedia.org/wiki/Reliability_engineering>reliability</a> and <a href=https://en.wikipedia.org/wiki/Privacy>privacy of decision-making systems</a>. In this talk, we will focus on the fairness aspect. We will start with understanding the many sources of <a href=https://en.wikipedia.org/wiki/Bias>bias</a> and how <a href=https://en.wikipedia.org/wiki/Bias>biases</a> can enter at each step of the learning process and even get propagated / amplified from previous steps. We will continue with methods for mitigating <a href=https://en.wikipedia.org/wiki/Bias>bias</a> which typically focus on some step of the pipeline (data, algorithms or results) and why it is important to target bias in each step and collectively, in the whole (machine) learning pipeline. We will conclude this talk by discussing accountability issues in connection to <a href=https://en.wikipedia.org/wiki/Bias>bias</a> and in particular, proactive consideration via bias-aware data collection, processing and algorithmic selection and retroactive consideration via explanations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.3/>Content Selection for Explanation Requests in Customer-Care Domain</a></strong><br><a href=/people/l/luca-anselma/>Luca Anselma</a>
|
<a href=/people/m/mirko-di-lascio/>Mirko Di Lascio</a>
|
<a href=/people/d/dario-mana/>Dario Mana</a>
|
<a href=/people/a/alessandro-mazzei/>Alessandro Mazzei</a>
|
<a href=/people/m/manuela-sanguinetti/>Manuela Sanguinetti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--3><div class="card-body p-3 small">This paper describes a content selection module for the generation of explanations in a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> designed for customer care domain. First we describe the construction of a corpus of a dialogues containing explanation requests from customers to a virtual agent of a telco, and second we study and formalize the importance of a specific information content for the generated message. In particular, we adapt the notions of <a href=https://en.wikipedia.org/wiki/Importance>importance</a> and <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> in the case of schematic knowledge bases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.5/>The Natural Language Pipeline, Neural Text Generation and Explainability</a></strong><br><a href=/people/j/juliette-faille/>Juliette Faille</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--5><div class="card-body p-3 small">End-to-end encoder-decoder approaches to data-to-text generation are often black boxes whose predictions are difficult to explain. Breaking up the end-to-end model into sub-modules is a natural way to address this problem. The traditional pre-neural Natural Language Generation (NLG) pipeline provides a framework for breaking up the end-to-end encoder-decoder. We survey recent papers that integrate traditional NLG submodules in neural approaches and analyse their explainability. Our survey is a first step towards building explainable neural NLG models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.11/>Toward Natural Language Mitigation Strategies for Cognitive Biases in Recommender Systems</a></strong><br><a href=/people/a/alisa-rieger/>Alisa Rieger</a>
|
<a href=/people/m/mariet-theune/>Mariët Theune</a>
|
<a href=/people/n/nava-tintarev/>Nava Tintarev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--11><div class="card-body p-3 small">Cognitive biases in the context of consuming online information filtered by <a href=https://en.wikipedia.org/wiki/Recommender_system>recommender systems</a> may lead to sub-optimal choices. One approach to mitigate such <a href=https://en.wikipedia.org/wiki/Bias>biases</a> is through interface and interaction design. This survey reviews studies focused on cognitive bias mitigation of recommender system users during two processes : 1) item selection and 2) preference elicitation. It highlights a number of promising directions for Natural Language Generation research for mitigating <a href=https://en.wikipedia.org/wiki/Cognitive_bias>cognitive bias</a> including : the need for <a href=https://en.wikipedia.org/wiki/Personalization>personalization</a>, as well as for transparency and control.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.13/>Learning from Explanations and Demonstrations : A Pilot Study</a></strong><br><a href=/people/s/silvia-tulli/>Silvia Tulli</a>
|
<a href=/people/s/sebastian-wallkotter/>Sebastian Wallkötter</a>
|
<a href=/people/a/ana-paiva/>Ana Paiva</a>
|
<a href=/people/f/francisco-s-melo/>Francisco S. Melo</a>
|
<a href=/people/m/mohamed-chetouani/>Mohamed Chetouani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--13><div class="card-body p-3 small">AI has become prominent in a growing number of <a href=https://en.wikipedia.org/wiki/System>systems</a>, and, as a direct consequence, the desire for explainability in such <a href=https://en.wikipedia.org/wiki/System>systems</a> has become prominent as well. To build explainable systems, a large portion of existing research uses various kinds of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language technologies</a>, e.g., <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech mechanisms</a>, or string visualizations. Here, we provide an overview of the challenges associated with <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language explanations</a> by reviewing existing literature. Additionally, we discuss the relationship between explainability and <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> in <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. We argue that explainability methods, in particular <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> that model the recipient of an explanation, might help increasing sample efficiency. For this, we present a computational approach to optimize the learner&#8217;s performance using explanations of another agent and discuss our results in light of effective natural language explanations for humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.nl4xai-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--nl4xai-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.nl4xai-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.nl4xai-1.14/>Generating Explanations of Action Failures in a <a href=https://en.wikipedia.org/wiki/Cognitive_robotics>Cognitive Robotic Architecture</a></a></strong><br><a href=/people/r/ravenna-thielstrom/>Ravenna Thielstrom</a>
|
<a href=/people/a/antonio-roque/>Antonio Roque</a>
|
<a href=/people/m/meia-chita-tegmark/>Meia Chita-Tegmark</a>
|
<a href=/people/m/matthias-scheutz/>Matthias Scheutz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--nl4xai-1--14><div class="card-body p-3 small">We describe an approach to generating explanations about why robot actions fail, focusing on the considerations of robots that are run by cognitive robotic architectures. We define a set of Failure Types and Explanation Templates, motivating them by the needs and constraints of cognitive architectures that use action scripts and interpretable belief states, and describe content realization and surface realization in this context. We then describe an <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> that can be extended to further study the effects of varying the explanation templates.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>