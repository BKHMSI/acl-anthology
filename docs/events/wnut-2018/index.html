<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Noisy User-generated Text (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Noisy User-generated Text (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w18-61>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</a>
<span class="badge badge-info align-middle ml-1">21&nbsp;papers</span></li></ul></div></div><div id=w18-61><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-61.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-61/>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6100/>Proceedings of the 2018 <span class=acl-fixed-case>EMNLP</span> Workshop W-<span class=acl-fixed-case>NUT</span>: The 4th Workshop on Noisy User-generated Text</a></strong><br><a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/t/timothy-baldwin/>Tim Baldwin</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6101/>Inducing a lexicon of sociolinguistic variables from code-mixed text</a></strong><br><a href=/people/p/philippa-shoemark/>Philippa Shoemark</a>
|
<a href=/people/j/james-kirby/>James Kirby</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6101><div class="card-body p-3 small">Sociolinguistics is often concerned with how variants of a linguistic item (e.g., nothing vs. nothin&#8217;) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text : that is, identifying equivalence pairs such as (football, fitba) along with their linguistic code (footballBritish, fitbaScottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of <a href=https://en.wikipedia.org/wiki/List_of_dialects_of_English>English dialects</a>, using <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> as the code-mixed text. Our system achieves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of over 70 % for two of these three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, and produces useful results even without extensive parameter tuning. Our success in adapting this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> from <a href=https://en.wikipedia.org/wiki/Gender>gender</a> to <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language variety</a> suggests that it could be used to discover other types of analogous pairs as well.<i>nothing</i> vs. <i>nothin&#8217;</i>) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text: that is, identifying equivalence pairs such as (<i>football</i>, <i>fitba</i>) along with their linguistic code (<i>football</i>&#8594;British, <i>fitba</i>&#8594;Scottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of English dialects, using tweets as the code-mixed text. Our system achieves precision of over 70% for two of these three datasets, and produces useful results even without extensive parameter tuning. Our success in adapting this framework from gender to language variety suggests that it could be used to discover other types of analogous pairs as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6102/>Twitter Geolocation using Knowledge-Based Methods<span class=acl-fixed-case>T</span>witter Geolocation using Knowledge-Based Methods</a></strong><br><a href=/people/t/taro-miyazaki/>Taro Miyazaki</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6102><div class="card-body p-3 small">Automatic geolocation of microblog posts from their text content is particularly difficult because many location-indicative terms are rare terms, notably entity names such as locations, people or local organisations. Their low frequency means that key terms observed in testing are often unseen in training, such that standard <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> are unable to learn weights for them. We propose a method for reasoning over such terms using a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, through exploiting their relations with other entities. Our technique uses a <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embedding</a> over the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, which we couple with a text representation to learn a geolocation classifier, trained end-to-end. We show that our method improves over purely text-based methods, which we ascribe to more robust treatment of low-count and out-of-vocabulary entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6104/>Assigning people to tasks identified in email : The EPA dataset for addressee tagging for detected task intent<span class=acl-fixed-case>EPA</span> dataset for addressee tagging for detected task intent</a></strong><br><a href=/people/r/revanth-rameshkumar/>Revanth Rameshkumar</a>
|
<a href=/people/p/peter-bailey/>Peter Bailey</a>
|
<a href=/people/a/abhishek-jha/>Abhishek Jha</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6104><div class="card-body p-3 small">We describe the Enron People Assignment (EPA) dataset, in which <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that are described in emails are associated with the person(s) responsible for carrying out these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We identify <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and the responsible people in the Enron email dataset. We define evaluation methods for this challenge and report scores for our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and nave baselines. The resulting model enables a <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a> operating within a commercial email service : given a person and a task, it determines if the person should be notified of the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6109/>Paraphrase Detection on Noisy Subtitles in Six Languages</a></strong><br><a href=/people/e/eetu-sjoblom/>Eetu Sjöblom</a>
|
<a href=/people/m/mathias-creutz/>Mathias Creutz</a>
|
<a href=/people/m/mikko-aulamo/>Mikko Aulamo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6109><div class="card-body p-3 small">We perform automatic paraphrase detection on subtitle data from the Opusparcus corpus comprising six European languages : <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. We train two types of supervised sentence embedding models : a word-averaging (WA) model and a gated recurrent averaging network (GRAN) model. We find out that <a href=https://en.wikipedia.org/wiki/GRAN>GRAN</a> outperforms WA and is more robust to noisy training data. Better results are obtained with more and noisier data than less and cleaner data. Additionally, we experiment on other <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, without reaching the same level of performance, because of domain mismatch between training and test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6110/>Distantly Supervised Attribute Detection from Reviews</a></strong><br><a href=/people/l/lisheng-fu/>Lisheng Fu</a>
|
<a href=/people/p/pablo-barrio/>Pablo Barrio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6110><div class="card-body p-3 small">This work aims to detect specific <a href=https://en.wikipedia.org/wiki/Attribute_(philosophy)>attributes</a> of a place (e.g., if it has a romantic atmosphere, or if it offers outdoor seating) from its user reviews via distant supervision : without direct annotation of the review text, we use the crowdsourced attribute labels of the place as labels of the review text. We then use review-level attention to pay more attention to those reviews related to the <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a>. The experimental results show that our attention-based model predicts attributes for places from reviews with over 98 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. The attention weights assigned to each review provide explanation of capturing relevant reviews.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6111 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6111/>Using Wikipedia Edits in Low Resource Grammatical Error Correction<span class=acl-fixed-case>W</span>ikipedia Edits in Low Resource Grammatical Error Correction</a></strong><br><a href=/people/a/adriane-boyd/>Adriane Boyd</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6111><div class="card-body p-3 small">We develop a grammatical error correction (GEC) system for <a href=https://en.wikipedia.org/wiki/German_language>German</a> using a small gold GEC corpus augmented with edits extracted from Wikipedia revision history. We extend the automatic error annotation tool ERRANT (Bryant et al., 2017) for German and use it to analyze both gold GEC corrections and Wikipedia edits (Grundkiewicz and Junczys-Dowmunt, 2014) in order to select as additional training data Wikipedia edits containing grammatical corrections similar to those in the gold corpus. Using a multilayer convolutional encoder-decoder neural network GEC approach (Chollampatt and Ng, 2018), we evaluate the contribution of Wikipedia edits and find that carefully selected Wikipedia edits increase performance by over 5 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6112/>Empirical Evaluation of Character-Based Model on Neural Named-Entity Recognition in Indonesian Conversational Texts<span class=acl-fixed-case>I</span>ndonesian Conversational Texts</a></strong><br><a href=/people/k/kemal-kurniawan/>Kemal Kurniawan</a>
|
<a href=/people/s/samuel-louvan/>Samuel Louvan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6112><div class="card-body p-3 small">Despite the long history of named-entity recognition (NER) task in the natural language processing community, previous work rarely studied the task on conversational texts. Such <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> are challenging because they contain a lot of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>word variations</a> which increase the number of out-of-vocabulary (OOV) words. The high number of OOV words poses a difficulty for word-based neural models. Meanwhile, there is plenty of evidence to the effectiveness of character-based neural models in mitigating this OOV problem. We report an empirical evaluation of neural sequence labeling models with character embedding to tackle NER task in Indonesian conversational texts. Our experiments show that (1) character models outperform word embedding-only models by up to 4 F1 points, (2) character models perform better in OOV cases with an improvement of as high as 15 F1 points, and (3) character models are robust against a very high OOV rate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6113 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6113" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6113/>Orthogonal Matching Pursuit for Text Classification</a></strong><br><a href=/people/k/konstantinos-skianis/>Konstantinos Skianis</a>
|
<a href=/people/n/nikolaos-tziortziotis/>Nikolaos Tziortziotis</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6113><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>, the problem of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> arises due to the high dimensionality, making <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> essential. Although classic <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizers</a> provide sparsity, they fail to return highly accurate models. On the contrary, state-of-the-art group-lasso regularizers provide better results at the expense of low sparsity. In this paper, we apply a greedy variable selection algorithm, called Orthogonal Matching Pursuit, for the text classification task. We also extend standard group OMP by introducing overlapping Group OMP to handle overlapping groups of features. Empirical analysis verifies that both OMP and overlapping GOMP constitute powerful <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizers</a>, able to produce effective and very sparse models. Code and data are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6114 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6114/>Training and Prediction Data Discrepancies : Challenges of Text Classification with Noisy, Historical Data</a></strong><br><a href=/people/r/r-andrew-kreek/>R. Andrew Kreek</a>
|
<a href=/people/e/emilia-apostolova/>Emilia Apostolova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6114><div class="card-body p-3 small">Industry datasets used for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> are rarely created for that purpose. In most cases, the data and target predictions are a by-product of accumulated historical data, typically fraught with <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>, present in both the text-based document, as well as in the targeted labels. In this work, we address the question of how well performance metrics computed on noisy, historical data reflect the performance on the intended future machine learning model input. The results demonstrate the utility of dirty training datasets used to build prediction models for cleaner (and different) prediction inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6115/>Detecting Code-Switching between Turkish-English Language Pair<span class=acl-fixed-case>T</span>urkish-<span class=acl-fixed-case>E</span>nglish Language Pair</a></strong><br><a href=/people/z/zeynep-yirmibesoglu/>Zeynep Yirmibeşoğlu</a>
|
<a href=/people/g/gulsen-eryigit/>Gülşen Eryiğit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6115><div class="card-body p-3 small">Code-switching (usage of different languages within a single conversation context in an alternative manner) is a highly increasing phenomenon in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and <a href=https://en.wikipedia.org/wiki/Colloquialism>colloquial usage</a> which poses different challenges for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. This paper introduces the first study for the detection of Turkish-English code-switching and also a small test data collected from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in order to smooth the way for further studies. The proposed system using character level n-grams and conditional random fields (CRFs) obtains 95.6 % micro-averaged F1-score on the introduced test data set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6116 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6116/>Language Identification in Code-Mixed Data using Multichannel Neural Networks and Context Capture</a></strong><br><a href=/people/s/soumil-mandal/>Soumil Mandal</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6116><div class="card-body p-3 small">An accurate language identification tool is an absolute necessity for building complex NLP systems to be used on code-mixed data. Lot of work has been recently done on the same, but there&#8217;s still room for improvement. Inspired from the recent advancements in neural network architectures for computer vision tasks, we have implemented multichannel neural networks combining CNN and LSTM for word level language identification of code-mixed data. Combining this with a Bi-LSTM-CRF context capture module, accuracies of 93.28 % and 93.32 % is achieved on our two testing sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6117 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6117/>Modeling Student Response Times : Towards Efficient One-on-one Tutoring Dialogues</a></strong><br><a href=/people/l/luciana-benotti/>Luciana Benotti</a>
|
<a href=/people/j/jayadev-bhaskaran/>Jayadev Bhaskaran</a>
|
<a href=/people/s/sigtryggur-kjartansson/>Sigtryggur Kjartansson</a>
|
<a href=/people/d/david-lang/>David Lang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6117><div class="card-body p-3 small">In this paper we investigate the task of modeling how long it would take a student to respond to a tutor question during a tutoring dialogue. Solving such a task has applications in educational settings such as <a href=https://en.wikipedia.org/wiki/Intelligent_tutoring_system>intelligent tutoring systems</a>, as well as in platforms that help busy human tutors to keep students engaged. Knowing how long it would normally take a student to respond to different types of questions could help tutors optimize their own time while answering multiple dialogues concurrently, as well as deciding when to prompt a student again. We study this problem using data from a service that offers tutor support for <a href=https://en.wikipedia.org/wiki/Mathematics>math</a>, <a href=https://en.wikipedia.org/wiki/Chemistry>chemistry</a> and <a href=https://en.wikipedia.org/wiki/Physics>physics</a> through an instant messaging platform. We create a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 240 K questions. We explore several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and compare them with human performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6118 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6118/>Content Extraction and <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Analysis</a> from Customer-Agent Interactions</a></strong><br><a href=/people/s/sergiu-nisioi/>Sergiu Nisioi</a>
|
<a href=/people/a/anca-bucur/>Anca Bucur</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6118><div class="card-body p-3 small">In this paper, we provide a lexical comparative analysis of the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> used by customers and agents in an Enterprise Resource Planning (ERP) environment and a potential solution to clean the data and extract relevant content for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. As a result, we demonstrate that the actual vocabulary for the <a href=https://en.wikipedia.org/wiki/Language>language</a> that prevails in the ERP conversations is highly divergent from the standardized dictionary and further different from general language usage as extracted from the Common Crawl corpus. Moreover, in specific business communication circumstances, where it is expected to observe a high usage of <a href=https://en.wikipedia.org/wiki/Standard_language>standardized language</a>, <a href=https://en.wikipedia.org/wiki/Code_switching>code switching</a> and non-standard expression are predominant, emphasizing once more the discrepancy between the day-to-day use of language and the standardized one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6120 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6120" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6120/>Word-like character n-gram embedding</a></strong><br><a href=/people/g/geewook-kim/>Geewook Kim</a>
|
<a href=/people/k/kazuki-fukui/>Kazuki Fukui</a>
|
<a href=/people/h/hidetoshi-shimodaira/>Hidetoshi Shimodaira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6120><div class="card-body p-3 small">We propose a new word embedding method called word-like character n-gram embedding, which learns distributed representations of words by embedding word-like character n-grams. Our method is an extension of recently proposed segmentation-free word embedding, which directly embeds frequent character n-grams from a raw corpus. However, its <a href=https://en.wikipedia.org/wiki/N-gram>n-gram vocabulary</a> tends to contain too many non-word n-grams. We solved this problem by introducing an idea of expected word frequency. Compared to the previously proposed methods, our method can embed more words, along with the words that are not included in a given basic word dictionary. Since our method does not rely on <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> with rich word dictionaries, it is especially effective when the text in the corpus is in unsegmented language and contains many <a href=https://en.wikipedia.org/wiki/Neologism>neologisms</a> and informal words (e.g., Chinese SNS dataset). Our experimental results on <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Sina Weibo</a> (a Chinese microblog service) and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> show that the proposed method can embed more words and improve the performance of downstream tasks.<i>word-like character</i> n<i>-gram embedding</i>, which learns distributed representations of words by embedding word-like character n-grams. Our method is an extension of recently proposed <i>segmentation-free word embedding</i>, which directly embeds frequent character n-grams from a raw corpus. However, its n-gram vocabulary tends to contain too many non-word n-grams. We solved this problem by introducing an idea of <i>expected word frequency</i>. Compared to the previously proposed methods, our method can embed more words, along with the words that are not included in a given basic word dictionary. Since our method does not rely on word segmentation with rich word dictionaries, it is especially effective when the text in the corpus is in unsegmented language and contains many neologisms and informal words (e.g., Chinese SNS dataset). Our experimental results on Sina Weibo (a Chinese microblog service) and Twitter show that the proposed method can embed more words and improve the performance of downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6121 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6121/>Classification of Tweets about Reported Events using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a></a></strong><br><a href=/people/k/kiminobu-makino/>Kiminobu Makino</a>
|
<a href=/people/y/yuka-takei/>Yuka Takei</a>
|
<a href=/people/t/taro-miyazaki/>Taro Miyazaki</a>
|
<a href=/people/j/jun-goto/>Jun Goto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6121><div class="card-body p-3 small">We developed a system that automatically extracts Event-describing Tweets which include incidents or accidents information for creating news reports. Event-describing Tweets can be classified into Reported-event Tweets and New-information Tweets. Reported-event Tweets cite <a href=https://en.wikipedia.org/wiki/News_agency>news agencies</a> or user generated content sites, and New-information Tweets are other Event-describing Tweets. A <a href=https://en.wikipedia.org/wiki/System>system</a> is needed to classify them so that creators of factual TV programs can use <a href=https://en.wikipedia.org/wiki/Them_(band)>them</a> in their productions. Proposing this Tweet classification task is one of the contributions of this paper, because no prior papers have used the same task even though program creators and other events information collectors have to do it to extract required information from <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking sites</a>. To classify Tweets in this task, this paper proposes a method to input and concatenate character and word sequences in Japanese Tweets by using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. This proposed method is another contribution of this paper. For comparison, character or word input methods and other <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> are also used. Results show that a <a href=https://en.wikipedia.org/wiki/System>system</a> using the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> and <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> can classify Tweets with an F1 score of 88 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6122 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6122/>Learning to Define Terms in the Software Domain</a></strong><br><a href=/people/v/vidhisha-balachandran/>Vidhisha Balachandran</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a>
|
<a href=/people/r/rose-catherine-kanjirathinkal/>Rose Catherine Kanjirathinkal</a>
|
<a href=/people/w/william-cohen/>William Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6122><div class="card-body p-3 small">One way to test a person&#8217;s knowledge of a domain is to ask them to define domain-specific terms. Here, we investigate the task of automatically generating definitions of technical terms by reading text from the technical domain. Specifically, we learn definitions of software entities from a large corpus built from the user forum Stack Overflow. To model definitions, we train a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and incorporate additional domain-specific information like <a href=https://en.wikipedia.org/wiki/Co-occurrence>word co-occurrence</a>, and <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological category information</a>. Our approach improves previous <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by 2 <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>BLEU points</a> for the definition generation task. Our experiments also show the additional challenges associated with the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and the short-comings of language-model based architectures for definition generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6123 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6123/>FrameIt : <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>Ontology Discovery</a> for Noisy User-Generated Text<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>I</span>t: Ontology Discovery for Noisy User-Generated Text</a></strong><br><a href=/people/d/dan-iter/>Dan Iter</a>
|
<a href=/people/a/alon-halevy/>Alon Halevy</a>
|
<a href=/people/w/wang-chiew-tan/>Wang-Chiew Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6123><div class="card-body p-3 small">A common need of NLP applications is to extract <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> in order to perform <a href=https://en.wikipedia.org/wiki/Analytics>analytics</a> or trigger an appropriate action. The <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> defining the structure is typically application dependent and in many cases it is not known a priori. We describe the FrameIt System that provides a workflow for (1) quickly discovering an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> to model a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> and (2) learning an SRL model that extracts the instances of the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> from sentences in the corpus. FrameIt exploits data that is obtained in the ontology discovery phase as weak supervision data to bootstrap the SRL model and then enables the user to refine the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with active learning. We present empirical results and qualitative analysis of the performance of FrameIt on three corpora of noisy user-generated text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6125 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6125/>Low-resource named entity recognition via multi-source projection : Not quite there yet?</a></strong><br><a href=/people/j/jan-vium-enghoff/>Jan Vium Enghoff</a>
|
<a href=/people/s/soren-harrison/>Søren Harrison</a>
|
<a href=/people/z/zeljko-agic/>Željko Agić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6125><div class="card-body p-3 small">Projecting linguistic annotations through word alignments is one of the most prevalent approaches to cross-lingual transfer learning. Conventional wisdom suggests that annotation projection just works regardless of the task at hand. We carefully consider multi-source projection for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Our experiment with 17 languages shows that to detect <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> in true low-resource languages, annotation projection may not be the right way to move forward. On a more positive note, we also uncover the conditions that do favor named entity projection from multiple sources. We argue these are infeasible under noisy low-resource constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6126 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6126/>A Case Study on Learning a Unified Encoder of Relations</a></strong><br><a href=/people/l/lisheng-fu/>Lisheng Fu</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a>
|
<a href=/people/r/ralph-grishman/>Ralph Grishman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6126><div class="card-body p-3 small">Typical relation extraction models are trained on a single corpus annotated with a pre-defined relation schema. An individual corpus is often small, and the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> may often be biased or overfitted to the corpus. We hypothesize that we can learn a better <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> by combining multiple relation datasets. We attempt to use a shared encoder to learn the unified feature representation and to augment it with <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> by <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a>. The additional corpora feeding the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> can help to learn a better feature representation layer even though the relation schemas are different. We use ACE05 and ERE datasets as our case study for experiments. The multi-task model obtains significant improvement on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6129 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6129/>Combining Human and Machine Transcriptions on the Zooniverse Platform</a></strong><br><a href=/people/d/daniel-hanson/>Daniel Hanson</a>
|
<a href=/people/a/andrea-simenstad/>Andrea Simenstad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6129><div class="card-body p-3 small">Transcribing handwritten documents to create fully searchable texts is an essential part of the <a href=https://en.wikipedia.org/wiki/Archival_science>archival process</a>. Traditional text recognition methods, such as <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>optical character recognition (OCR)</a>, do not work on handwritten documents due to their frequent noisiness and OCR&#8217;s need for individually segmented letters. Crowdsourcing and improved machine models are two modern methods for transcribing handwritten documents.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>