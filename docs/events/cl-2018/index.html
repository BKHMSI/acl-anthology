<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Computational Linguistics Journal (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Computational Linguistics Journal (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#j18-1>Computational Linguistics, Volume 44, Issue 1 - April 2018</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#j18-2>Computational Linguistics, Volume 44, Issue 2 - June 2018</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#j18-3>Computational Linguistics, Volume 44, Issue 3 - September 2018</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#j18-4>Computational Linguistics, Volume 44, Issue 4 - December 2018</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li></ul></div></div><div id=j18-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/J18-1/>Computational Linguistics, Volume 44, Issue 1 - April 2018</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-1000/>Computational Linguistics, Volume 44, Issue 1 - <span class=acl-fixed-case>A</span>pril 2018</a></strong><br></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-1003/>A Notion of Semantic Coherence for Underspecified Semantic Representation</a></strong><br><a href=/people/m/mehdi-manshadi/>Mehdi Manshadi</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a>
|
<a href=/people/j/james-allen/>James F. Allen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-1003><div class="card-body p-3 small">The general problem of finding satisfying solutions to constraint-based underspecified representations of quantifier scope is NP-complete. Existing frameworks, including Dominance Graphs, <a href=https://en.wikipedia.org/wiki/Minimal_recursion_semantics>Minimal Recursion Semantics</a>, and Hole Semantics, have struggled to balance expressivity and tractability in order to cover real natural language sentences with efficient algorithms. We address this trade-off with a general principle of <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a>, which requires that every variable introduced in the <a href=https://en.wikipedia.org/wiki/Domain_of_discourse>domain of discourse</a> must contribute to the overall <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of the sentence. We show that every underspecified representation meeting this criterion can be efficiently processed, and that our set of <a href=https://en.wikipedia.org/wiki/Representation_theory>representations</a> subsumes all previously identified tractable sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-1004/>Cache Transition Systems for <a href=https://en.wikipedia.org/wiki/Graph_traversal>Graph Parsing</a></a></strong><br><a href=/people/d/daniel-gildea/>Daniel Gildea</a>
|
<a href=/people/g/giorgio-satta/>Giorgio Satta</a>
|
<a href=/people/x/xiaochang-peng/>Xiaochang Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-1004><div class="card-body p-3 small">Motivated by the task of <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, we describe a transition system that generalizes standard transition-based dependency parsing techniques to generate a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> rather than a <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree</a>. Our system includes a <a href=https://en.wikipedia.org/wiki/Cache_(computing)>cache</a> with fixed size m, and we characterize the relationship between the parameter m and the class of <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> that can be produced through the graph-theoretic concept of tree decomposition. We find empirically that small cache sizes cover a high percentage of sentences in existing semantic corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-1005/>Weighted DAG Automata for Semantic Graphs<span class=acl-fixed-case>DAG</span> Automata for Semantic Graphs</a></strong><br><a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/f/frank-drewes/>Frank Drewes</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/g/giorgio-satta/>Giorgio Satta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-1005><div class="card-body p-3 small">Graphs have a variety of uses in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, particularly as representations of linguistic meaning. A deficit in this area of research is a formal framework for creating, combining, and using models involving graphs that parallels the frameworks of <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite automata</a> for <a href=https://en.wikipedia.org/wiki/String_(computer_science)>strings</a> and <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite tree automata</a> for <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a>. A possible starting point for such a framework is the formalism of directed acyclic graph (DAG) automata, defined by Kamimura and Slutzki and extended by Quernheim and Knight. In this article, we study the latter in depth, demonstrating several new results, including a practical recognition algorithm that can be used for <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> with <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> defined on DAG automata. We also propose an extension to <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> with unbounded node degree and show that our results carry over to the extended <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalism</a>.</div></div></div><hr><div id=j18-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/J18-2/>Computational Linguistics, Volume 44, Issue 2 - June 2018</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-2000/>Computational Linguistics, Volume 44, Issue 2 - June 2018</a></strong><br></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-2001/>A Dependency Perspective on RST Discourse Parsing and Evaluation<span class=acl-fixed-case>RST</span> Discourse Parsing and Evaluation</a></strong><br><a href=/people/m/mathieu-morey/>Mathieu Morey</a>
|
<a href=/people/p/philippe-muller/>Philippe Muller</a>
|
<a href=/people/n/nicholas-asher/>Nicholas Asher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-2001><div class="card-body p-3 small">Computational text-level discourse analysis mostly happens within Rhetorical Structure Theory (RST), whose structures have classically been presented as constituency trees, and relies on data from the RST Discourse Treebank (RST-DT) ; as a result, the RST discourse parsing community has largely borrowed from the syntactic constituency parsing community. The standard evaluation procedure for RST discourse parsers is thus a simplified variant of PARSEVAL, and most RST discourse parsers use techniques that originated in syntactic constituency parsing. In this article, we isolate a number of conceptual and computational problems with the constituency hypothesis. We then examine the consequences, for the implementation and evaluation of RST discourse parsers, of adopting a dependency perspective on RST structures, a view advocated so far only by a few approaches to discourse parsing. While doing that, we show the importance of the notion of headedness of RST structures. We analyze RST discourse parsing as dependency parsing by adapting to RST a recent proposal in syntactic parsing that relies on head-ordered dependency trees, a representation isomorphic to headed constituency trees. We show how to convert the original trees from the RST corpus, RST-DT, and their binarized versions used by all existing RST parsers to head-ordered dependency trees. We also propose a way to convert existing simple dependency parser output to <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>constituent trees</a>. This allows us to evaluate and to compare approaches from both constituent-based and dependency-based perspectives in a unified framework, using constituency and dependency metrics. We thus propose an evaluation framework to compare extant approaches easily and uniformly, something the RST parsing community has lacked up to now. We can also compare parsers&#8217; predictions to each other across frameworks. This allows us to characterize families of parsing strategies across the different frameworks, in particular with respect to the notion of <a href=https://en.wikipedia.org/wiki/Headedness>headedness</a>. Our experiments provide evidence for the conceptual similarities between dependency parsers and shift-reduce constituency parsers, and confirm that dependency parsing constitutes a viable approach to RST discourse parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-2002/>Unrestricted Bridging Resolution</a></strong><br><a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/k/katja-markert/>Katja Markert</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-2002><div class="card-body p-3 small">In contrast to identity anaphors, which indicate <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> between a noun phrase and its antecedent, bridging anaphors link to their antecedent(s) via lexico-semantic, frame, or encyclopedic relations. Bridging resolution involves recognizing bridging anaphors and finding links to antecedents. In contrast to most prior work, we tackle both <a href=https://en.wikipedia.org/wiki/Problem_solving>problems</a>. Our work also follows a more wide-ranging definition of bridging than most previous work and does not impose any restrictions on the type of bridging anaphora or relations between <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphor</a> and antecedent. We create a corpus (ISNotes) annotated for information status (IS), bridging being one of the IS subcategories. The annotations reach high <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> for all categories and marginal reliability for the bridging subcategory. We use a two-stage statistical global inference method for bridging resolution. Given all mentions in a document, the first stage, bridging anaphora recognition, recognizes bridging anaphors as a subtask of learning fine-grained IS. We use a cascading collective classification method where (i) collective classification allows us to investigate relations among several mentions and autocorrelation among IS classes and (ii) cascaded classification allows us to tackle class imbalance, important for minority classes such as bridging. We show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms current methods both for <a href=https://en.wikipedia.org/wiki/Computer_vision>IS recognition</a> overall as well as for bridging, specifically. The second stage, bridging antecedent selection, finds the antecedents for all predicted bridging anaphors. We investigate the phenomenon of semantically or syntactically related bridging anaphors that share the same antecedent, a phenomenon we call sibling anaphors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-2003/>Spurious Ambiguity and Focalization</a></strong><br><a href=/people/g/glyn-morrill/>Glyn Morrill</a>
|
<a href=/people/o/oriol-valentin/>Oriol Valentín</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-2003><div class="card-body p-3 small">Spurious ambiguity is the phenomenon whereby distinct derivations in <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> may assign the same structural reading, resulting in redundancy in the parse search space and inefficiency in <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Understanding the problem depends on identifying the essential <a href=https://en.wikipedia.org/wiki/Derivation_(differential_algebra)>mathematical structure of derivations</a>. This is trivial in the case of <a href=https://en.wikipedia.org/wiki/Context-free_grammar>context free grammar</a>, where the parse structures are <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>ordered trees</a> ; in the case of type logical categorial grammar, the parse structures are <a href=https://en.wikipedia.org/wiki/Proof_net>proof nets</a>. However, with respect to multiplicatives, intrinsic proof nets have not yet been given for displacement calculus, and <a href=https://en.wikipedia.org/wiki/Proof_net>proof nets</a> for additives, which have applications to <a href=https://en.wikipedia.org/wiki/Polymorphism_(computer_science)>polymorphism</a>, are not easy to characterize. In this context we approach here multiplicative-additive spurious ambiguity by means of the proof-theoretic technique of focalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-2004/>The Influence of Context on the Learning of Metrical Stress Systems Using <a href=https://en.wikipedia.org/wiki/Finite-state_machine>Finite-State Machines</a></a></strong><br><a href=/people/c/cesko-voeten/>Cesko Voeten</a>
|
<a href=/people/m/menno-van-zaanen/>Menno van Zaanen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-2004><div class="card-body p-3 small">Languages vary in the way stress is assigned to syllables within words. This article investigates the learnability of <a href=https://en.wikipedia.org/wiki/Stress_(linguistics)>stress systems</a> in a wide range of <a href=https://en.wikipedia.org/wiki/Language>languages</a>. The stress systems can be described using <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite-state automata</a> with symbols indicating levels of stress (primary, secondary, or no stress). Finite-state automata have been the focus of research in the area of <a href=https://en.wikipedia.org/wiki/Grammatical_inference>grammatical inference</a> for some time now. It has been shown that <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite-state machines</a> are learnable from examples using <a href=https://en.wikipedia.org/wiki/Finite-state_machine>state-merging</a>. One such approach, which aims to learn k-testable languages, has been applied to <a href=https://en.wikipedia.org/wiki/Stress_(linguistics)>stress systems</a> with some success. The family of k-testable languages has been shown to be efficiently learnable (in polynomial time). Here, we extend this approach to k, l-local languages by taking not only left context, but also right context, into account. We consider empirical results testing the performance of our learner using various amounts of context (corresponding to varying definitions of phonological locality). Our results show that our approach of learning stress patterns using <a href=https://en.wikipedia.org/wiki/State-merging>state-merging</a> is more reliant on left context than on right context. Additionally, some stress systems fail to be learned by our learner using either the left-context k-testable or the left-and-right-context k, l-local learning system. A more complex merging strategy, and hence <a href=https://en.wikipedia.org/wiki/Grammar>grammar representation</a>, is required for these <a href=https://en.wikipedia.org/wiki/Stress_(linguistics)>stress systems</a>.</div></div></div><hr><div id=j18-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/J18-3/>Computational Linguistics, Volume 44, Issue 3 - September 2018</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3000/>Computational Linguistics, Volume 44, Issue 3 - September 2018</a></strong><br></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3002/>A Structured Review of the Validity of BLEU<span class=acl-fixed-case>BLEU</span></a></strong><br><a href=/people/e/ehud-reiter/>Ehud Reiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-3002><div class="card-body p-3 small">The BLEU metric has been widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for over 15 years to evaluate <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>, especially in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. I present a structured review of the evidence on whether <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> is a valid evaluation techniquein other words, whether <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> correlate with real-world utility and user-satisfaction of NLP systems ; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3003/>Native Language Identification With Classifier Stacking and Ensembles</a></strong><br><a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/m/mark-dras/>Mark Dras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-3003><div class="card-body p-3 small">Ensemble methods using multiple <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> have proven to be among the most successful approaches for the task of Native Language Identification (NLI), achieving the current state of the art. However, a systematic examination of ensemble methods for NLI has yet to be conducted. Additionally, deeper <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble architectures</a> such as classifier stacking have not been closely evaluated. We present a set of experiments using three ensemble-based models, testing each with multiple configurations and algorithms. This includes a rigorous application of meta-classification models for NLI, achieving state-of-the-art results on several large data sets, evaluated in both intra-corpus and cross-corpus modes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3005/>Using <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> for Granularities of Tokenization</a></strong><br><a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-3005><div class="card-body p-3 small">Depending on downstream applications, it is advisable to extend the notion of <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenization</a> from low-level character-based token boundary detection to identification of meaningful and useful language units. This entails both identifying units composed of several single words that form a several single words that form a, as well as splitting single-word compounds into their meaningful parts. In this article, we introduce unsupervised and knowledge-free methods for these two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. The main novelty of our research is based on the fact that methods are primarily based on distributional similarity, of which we use two flavors : a sparse count-based and a dense neural-based distributional semantic model. First, we introduce DRUID, which is a method for detecting MWEs. The evaluation on MWE-annotated data sets in two languages and newly extracted evaluation data sets for 32 languages shows that DRUID compares favorably over previous methods not utilizing distributional information. Second, we present SECOS, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for decompounding close compounds. In an evaluation of four dedicated decompounding data sets across four languages and on data sets extracted from <a href=https://en.wikipedia.org/wiki/Wiktionary>Wiktionary</a> for 14 languages, we demonstrate the superiority of our approach over unsupervised baselines, sometimes even matching the performance of previous language-specific and supervised methods. In a final experiment, we show how both decompounding and MWE information can be used in <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>. Here, we obtain the best results when combining <a href=https://en.wikipedia.org/wiki/Word>word information</a> with MWEs and the <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound parts</a> in a bag-of-words retrieval set-up.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3006/>Feature-Based Decipherment for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/i/iftekhar-naim/>Iftekhar Naim</a>
|
<a href=/people/p/parker-riley/>Parker Riley</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-3006><div class="card-body p-3 small">Orthographic similarities across languages provide a strong signal for unsupervised probabilistic transduction (decipherment) for closely related language pairs. The existing decipherment models, however, are not well suited for exploiting these orthographic similarities. We propose a <a href=https://en.wikipedia.org/wiki/Log-linear_model>log-linear model</a> with <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive for the proposed <a href=https://en.wikipedia.org/wiki/Log-linear_model>log-linear model</a>. To address this challenge, we perform <a href=https://en.wikipedia.org/wiki/Approximate_inference>approximate inference</a> via Markov chain Monte Carlo sampling and <a href=https://en.wikipedia.org/wiki/Contrastive_divergence>contrastive divergence</a>. Our results show that the proposed <a href=https://en.wikipedia.org/wiki/Log-linear_model>log-linear model</a> with <a href=https://en.wikipedia.org/wiki/Contrastive_divergence>contrastive divergence</a> outperforms the existing generative decipherment models by exploiting the orthographic features. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> both scales to large vocabularies and preserves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in low- and no-resource contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-3007/>Survey : <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>Anaphora</a> With Non-nominal Antecedents in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>Computational Linguistics</a> : a Survey<span class=acl-fixed-case>S</span>urvey: Anaphora With Non-nominal Antecedents in Computational Linguistics: a <span class=acl-fixed-case>S</span>urvey</a></strong><br><a href=/people/v/varada-kolhatkar/>Varada Kolhatkar</a>
|
<a href=/people/a/adam-roussel/>Adam Roussel</a>
|
<a href=/people/s/stefanie-dipper/>Stefanie Dipper</a>
|
<a href=/people/h/heike-zinsmeister/>Heike Zinsmeister</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-3007><div class="card-body p-3 small">This article provides an extensive overview of the literature related to the phenomenon of non-nominal-antecedent anaphora (also known as abstract anaphora or discourse deixis), a type of <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a> in which an <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphor</a> like that refers to an antecedent (marked in boldface) that is syntactically non-nominal, such as the first sentence in It&#8217;s way too hot here. That&#8217;s why I&#8217;m moving to Alaska. Annotating and automatically resolving these cases of <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a> is interesting in its own right because of the complexities involved in identifying non-nominal antecedents, which typically represent abstract objects such as <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a>, facts, and propositions. There is also practical value in the resolution of non-nominal-antecedent anaphora, as this would help computational systems in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, as well as, conceivably, any other task dependent on some measure of text understanding. Most of the existing approaches to anaphora annotation and resolution focus on nominal-antecedent anaphora, classifying many of the cases where the antecedents are syntactically non-nominal as non-anaphoric. There has been some work done on this topic, but it remains scattered and difficult to collect and assess. With this article, we hope to bring together and synthesize work done in disparate contexts up to now in order to identify fundamental problems and draw conclusions from an overarching perspective. Having a good picture of the current state of the art in this field can help researchers direct their efforts to where they are most necessary. Because of the great variety of theoretical approaches that have been brought to bear on the problem, there is an equally diverse array of terminologies that are used to describe it, so we will provide an overview and discussion of these <a href=https://en.wikipedia.org/wiki/Terminology>terminologies</a>. We also describe the linguistic properties of non-nominal-antecedent anaphora, examine previous annotation efforts that have addressed this topic, and present the computational approaches that aim at resolving non-nominal-antecedent anaphora automatically. We close with a review of the remaining open questions in this area and some of our recommendations for future research.</div></div></div><hr><div id=j18-4><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/J18-4/>Computational Linguistics, Volume 44, Issue 4 - December 2018</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-4000/>Computational Linguistics, Volume 44, Issue 4 - <span class=acl-fixed-case>D</span>ecember 2018</a></strong><br></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-4002/>Squib : The Language Resource Switchboard<span class=acl-fixed-case>S</span>quib: The Language Resource Switchboard</a></strong><br><a href=/people/c/claus-zinn/>Claus Zinn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-4002><div class="card-body p-3 small">The CLARIN research infrastructure gives users access to an increasingly rich and diverse set of language-related resources and tools. Whereas there is ample support for searching resources using metadata-based search, or <a href=https://en.wikipedia.org/wiki/Full-text_search>full-text search</a>, or for aggregating resources into virtual collections, there is little support for users to help them process resources in one way or another. In spite of the large number of tools that process texts in many different languages, there is no single point of access where users can find tools to fit their needs and the resources they have. In this squib, we present the Language Resource Switchboard (LRS), which helps users to discover tools that can process their resources. For this, the LRS identifies all applicable tools for a given resource, lists the tasks the tools can achieve, and invokes the selected tool in such a way so that processing can start immediately with little or no prior tool parameterization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-4003/>Squib : Reproducibility in Computational Linguistics : Are We Willing to Share?<span class=acl-fixed-case>S</span>quib: Reproducibility in Computational Linguistics: Are We Willing to Share?</a></strong><br><a href=/people/m/martijn-wieling/>Martijn Wieling</a>
|
<a href=/people/j/josine-rawee/>Josine Rawee</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-4003><div class="card-body p-3 small">This study focuses on an essential precondition for reproducibility in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> : the willingness of authors to share relevant source code and data. Ten years after Ted Pedersen&#8217;s influential Last Words contribution in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>Computational Linguistics</a>, we investigate to what extent researchers in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> are willing and able to share their data and code. We surveyed all 395 full papers presented at the 2011 and 2016 ACL Annual Meetings, and identified whether links to data and code were provided. If working links were not provided, authors were requested to provide this information. Although data were often available, code was shared less often. When working links to code or data were not provided in the paper, authors provided the <a href=https://en.wikipedia.org/wiki/Source_code>code</a> in about one third of cases. For a selection of ten papers, we attempted to reproduce the results using the provided data and code. We were able to reproduce the results approximately for six papers. For only a single paper did we obtain the exact same results. Our findings show that even though the situation appears to have improved comparing 2016 to 2011, <a href=https://en.wikipedia.org/wiki/Empiricism>empiricism</a> in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> still largely remains a matter of faith. Nevertheless, we are somewhat optimistic about the future. Ensuring reproducibility is not only important for the field as a whole, but also seems worthwhile for individual researchers : The median citation count for studies with working links to the source code is higher.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-4007/>Interactional Stancetaking in Online Forums</a></strong><br><a href=/people/s/scott-f-kiesling/>Scott F. Kiesling</a>
|
<a href=/people/u/umashanthi-pavalanathan/>Umashanthi Pavalanathan</a>
|
<a href=/people/j/jim-fitzpatrick/>Jim Fitzpatrick</a>
|
<a href=/people/x/xiaochuang-han/>Xiaochuang Han</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-4007><div class="card-body p-3 small">Language is shaped by the relationships between the speaker / writer and the audience, the object of discussion, and the talk itself. In turn, <a href=https://en.wikipedia.org/wiki/Language>language</a> is used to reshape these relationships over the course of an interaction. Computational researchers have succeeded in operationalizing <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, <a href=https://en.wikipedia.org/wiki/Formality>formality</a>, and <a href=https://en.wikipedia.org/wiki/Politeness>politeness</a>, but each of these constructs captures only some aspects of social and relational meaning. Theories of interactional stancetaking have been put forward as holistic accounts, but until now, these <a href=https://en.wikipedia.org/wiki/Theory>theories</a> have been applied only through detailed qualitative analysis of (portions of) a few individual conversations. In this article, we propose a new computational operationalization of interpersonal stancetaking. We begin with annotations of three linked stance dimensionsaffect, investment, and alignmenton 68 conversation threads from the online platform Reddit. Using these annotations, we investigate thread structure and linguistic properties of stancetaking in online conversations. We identify lexical features that characterize the extremes along each stancetaking dimension, and show that these stancetaking properties can be predicted with moderate accuracy from bag-of-words features, even with a relatively small labeled training set. These <a href=https://en.wikipedia.org/wiki/Quantitative_research>quantitative analyses</a> are supplemented by extensive <a href=https://en.wikipedia.org/wiki/Qualitative_research>qualitative analysis</a>, highlighting the compatibility of computational and qualitative methods in synthesizing evidence about the creation of interactional meaning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-4008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-4008 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-4008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-4008/>A Joint Model of Conversational Discourse Latent Topics on Microblogs</a></strong><br><a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-4008><div class="card-body p-3 small">Conventional <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> are ineffective for topic extraction from <a href=https://en.wikipedia.org/wiki/Microblogging>microblog messages</a>, because the data sparseness exhibited in short messages lacking structure and contexts results in poor message-level word co-occurrence patterns. To address this issue, we organize microblog messages as conversation trees based on their reposting and replying relations, and propose an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised model</a> that jointly learns word distributions to represent : (1) different roles of conversational discourse, and (2) various latent topics in reflecting content information. By explicitly distinguishing the probabilities of messages with varying discourse roles in containing topical words, our model is able to discover clusters of discourse words that are indicative of topical content. In an automatic evaluation on large-scale microblog corpora, our joint model yields topics with better <a href=https://en.wikipedia.org/wiki/Coherence_(statistics)>coherence scores</a> than competitive topic models from previous studies. Qualitative analysis on model outputs indicates that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> induces meaningful <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> for both discourse and topics. We further present an empirical study on microblog summarization based on the outputs of our joint model. The results show that the jointly modeled discourse and topic representations can effectively indicate summary-worthy content in microblog conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-4009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-4009 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-4009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-4009/>Sarcasm Analysis Using Conversation Context</a></strong><br><a href=/people/d/debanjan-ghosh/>Debanjan Ghosh</a>
|
<a href=/people/a/alexander-richard-fabbri/>Alexander R. Fabbri</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-4009><div class="card-body p-3 small">Computational models for sarcasm detection have often relied on the content of utterances in isolation. However, the speaker&#8217;s sarcastic intent is not always apparent without additional context. Focusing on social media discussions, we investigate three issues : (1) does modeling conversation context help in sarcasm detection? (2) can we identify what part of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversation context</a> triggered the <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcastic reply</a>? and (3) given a sarcastic post that contains multiple sentences, can we identify the specific sentence that is sarcastic? To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the current turn. We show that LSTM networks with sentence-level attention on context and current turn, as well as the conditional LSTM network, outperform the LSTM model that reads only the current turn. As <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversation context</a>, we consider the prior turn, the succeeding turn, or both. Our <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> are tested on two types of <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> : <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Internet_forum>discussion forums</a>. We discuss several differences between these <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a>, ranging from their size to the nature of the gold-label annotations. To address the latter two issues, we present a qualitative analysis of the attention weights produced by the LSTM models (with attention) and discuss the results compared with human performance on the two tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-4010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-4010 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-4010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-4010/>We Usually Do n’t Like Going to the Dentist : Using Common Sense to Detect Irony on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/c/cynthia-van-hee/>Cynthia Van Hee</a>
|
<a href=/people/e/els-lefever/>Els Lefever</a>
|
<a href=/people/v/veronique-hoste/>Véronique Hoste</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-4010><div class="card-body p-3 small">Although common sense and connotative knowledge come naturally to most people, computers still struggle to perform well on tasks for which such extratextual information is required. Automatic approaches to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and irony detection have revealed that the lack of such <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> undermines <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance. In this article, we therefore address the challenge of modeling implicit or prototypical sentiment in the framework of automatic irony detection. Starting from <a href=https://en.wikipedia.org/wiki/Lexical_analysis>manually annotated connoted situation phrases</a> (e.g., flight delays, sitting the whole day at the doctor&#8217;s office), we defined the <a href=https://en.wikipedia.org/wiki/Implicit_stereotype>implicit sentiment</a> held towards such situations automatically by using both a <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexico-semantic knowledge base</a> and a data-driven method. We further investigate how such implicit sentiment information affects irony detection by assessing a state-of-the-art irony classifier before and after it is informed with implicit sentiment information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J18-4011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J18-4011 data-toggle=collapse aria-expanded=false aria-controls=abstract-J18-4011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J18-4011/>Combining <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Argumentative Reasoning for the Analysis of Social Media Textual Content Using Small Data Sets</a></strong><br><a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/f/francesca-toni/>Francesca Toni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J18-4011><div class="card-body p-3 small">The use of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has become a regular habit for many and has changed the way people interact with each other. In this article, we focus on analyzing whether news headlines support tweets and whether reviews are deceptive by analyzing the interaction or the influence that these texts have on the others, thus exploiting contextual information. Concretely, we define a deep learning method for relationbased argument mining to extract argumentative relations of attack and support. We then use this method for determining whether news articles support <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, a useful task in fact-checking settings, where determining agreement toward a statement is a useful step toward determining its truthfulness. Furthermore, we use our method for extracting bipolar argumentation frameworks from reviews to help detect whether they are deceptive. We show experimentally that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> performs well in both settings. In particular, in the case of deception detection, our method contributes a novel argumentative feature that, when used in combination with other <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> in standard <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifiers</a>, outperforms the latter even on small data sets.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>