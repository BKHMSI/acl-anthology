<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Computational Linguistics Journal (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Computational Linguistics Journal (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020cl-1>Computational Linguistics, Volume 46, Issue 1 - March 2020</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2020cl-2>Computational Linguistics, Volume 46, Issue 2 - June 2020</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2020cl-3>Computational Linguistics, Volume 46, Issue 3 - September 2020</a>
<span class="badge badge-info align-middle ml-1">0&nbsp;paper</span></li><li><a class=align-middle href=#2020cl-4>Computational Linguistics, Volume 46, Issue 4 - December 2020</a>
<span class="badge badge-info align-middle ml-1">0&nbsp;paper</span></li></ul></div></div><div id=2020cl-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.cl-1/>Computational Linguistics, Volume 46, Issue 1 - March 2020</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cl-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cl-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cl-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cl-1.3/>An Empirical Study on Crosslingual Transfer in Probabilistic Topic Models</a></strong><br><a href=/people/s/shudong-hao/>Shudong Hao</a>
|
<a href=/people/m/michael-paul/>Michael J. Paul</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cl-1--3><div class="card-body p-3 small">Probabilistic topic modeling is a common first step in crosslingual tasks to enable <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> and extract <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual features</a>. Although many multilingual topic models have been developed, their assumptions about the training corpus are quite varied, and it is not clear how well the different <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> can be utilized under various training conditions. In this article, the knowledge transfer mechanisms behind different multilingual topic models are systematically studied, and through a broad set of experiments with four <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> on ten languages, we provide empirical insights that can inform the selection and future development of multilingual topic models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cl-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cl-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cl-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cl-1.4/>Data-Driven Sentence Simplification : Survey and Benchmark</a></strong><br><a href=/people/f/fernando-alva-manchego/>Fernando Alva-Manchego</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cl-1--4><div class="card-body p-3 small">Sentence Simplification (SS) aims to modify a sentence in order to make it easier to read and understand. In order to do so, several <a href=https://en.wikipedia.org/wiki/Rewriting>rewriting transformations</a> can be performed such as <a href=https://en.wikipedia.org/wiki/Rewriting>replacement</a>, <a href=https://en.wikipedia.org/wiki/Rewriting>reordering</a>, and <a href=https://en.wikipedia.org/wiki/Rewriting>splitting</a>. Executing these transformations while keeping sentences grammatical, preserving their main idea, and generating simpler output, is a challenging and still far from solved problem. In this article, we survey research on SS, focusing on approaches that attempt to learn how to simplify using corpora of aligned original-simplified sentence pairs in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, which is the dominant paradigm nowadays. We also include a benchmark of different approaches on common data sets so as to compare them and highlight their strengths and limitations. We expect that this survey will serve as a starting point for researchers interested in the task and help spark new ideas for future developments.</div></div></div><hr><div id=2020cl-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.cl-2/>Computational Linguistics, Volume 46, Issue 2 - June 2020</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cl-2.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cl-2--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cl-2.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cl-2.2/>Unsupervised Word Translation with Adversarial Autoencoder</a></strong><br><a href=/people/m/muhammad-tasnim-mohiuddin/>Tasnim Mohiuddin</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cl-2--2><div class="card-body p-3 small">Crosslingual word embeddings learned from monolingual embeddings have a crucial role in many downstream tasks, ranging from <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. Adversarial training has shown impressive success in learning crosslingual embeddings and the associated word translation task without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this article, we investigate adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. We use two types of <a href=https://en.wikipedia.org/wiki/Refinement_(computing)>refinement procedures</a> sequentially after obtaining the trained encoders and mappings from the adversarial training, namely, <a href=https://en.wikipedia.org/wiki/Refinement_(computing)>refinement</a> with Procrustes solution and <a href=https://en.wikipedia.org/wiki/Refinement_(computing)>refinement</a> with symmetric re-weighting. Extensive experimentations with high- and low-resource languages from two different data sets show that our method achieves better performance than existing adversarial and non-adversarial approaches and is also competitive with the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised system</a>. Along with performing comprehensive ablation studies to understand the contribution of different components of our <a href=https://en.wikipedia.org/wiki/Adversarial_model>adversarial model</a>, we also conduct a thorough analysis of the refinement procedures to understand their effects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cl-2.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cl-2--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cl-2.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.cl-2.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.cl-2.4/>LINSPECTOR : Multilingual Probing Tasks for Word Representations<span class=acl-fixed-case>LINSPECTOR</span>: Multilingual Probing Tasks for Word Representations</a></strong><br><a href=/people/g/gozde-gul-sahin/>Gözde Gül Şahin</a>
|
<a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/i/ilia-kuznetsov/>Ilia Kuznetsov</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cl-2--4><div class="card-body p-3 small">Despite an ever-growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation that requires substantial computational resources not all researchers have access to. A recent development in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is to use simple classification tasks, also called probing tasks, that test for a single <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic feature</a> such as <a href=https://en.wikipedia.org/wiki/Part_of_speech>part-of-speech</a>. Existing studies mostly focus on exploring the <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a> encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier : The information encoded by the <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> and function words in <a href=https://en.wikipedia.org/wiki/English_language>English</a> is often stored on a subword, morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as <a href=https://en.wikipedia.org/wiki/Grammatical_case>case marking</a>, <a href=https://en.wikipedia.org/wiki/Possession_(linguistics)>possession</a>, <a href=https://en.wikipedia.org/wiki/Word_length>word length</a>, morphological tag count, and pseudoword identification for 24 languages. We present a reusable <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for creation and evaluation of such <a href=https://en.wikipedia.org/wiki/Test_(assessment)>tests</a> in a multilingual setting, which is challenging because of a lack of resources, lower quality of tools, and differences among languages. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks : POS-tagging, dependency parsing, semantic role labeling, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> or black-box neural models for linguistic cues in a multilingual setting. We release the probing data sets and the evaluation suite LINSPECTOR with https://github.com/UKPLab/linspector.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.cl-2.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--cl-2--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.cl-2.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.cl-2.8/>The Limitations of <a href=https://en.wikipedia.org/wiki/Stylometry>Stylometry</a> for Detecting Machine-Generated Fake News</a></strong><br><a href=/people/t/tal-schuster/>Tal Schuster</a>
|
<a href=/people/r/roei-schuster/>Roei Schuster</a>
|
<a href=/people/d/darsh-j-shah/>Darsh J. Shah</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--cl-2--8><div class="card-body p-3 small">Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed <a href=https://en.wikipedia.org/wiki/Stylometry>stylometry</a>, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that <a href=https://en.wikipedia.org/wiki/Stylometry>stylometry</a> is limited against machine-generated misinformation. Whereas humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though <a href=https://en.wikipedia.org/wiki/Stylometry>stylometry</a> can successfully prevent <a href=https://en.wikipedia.org/wiki/Impersonator>impersonation</a> by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> demonstrating the stylistic similarity between malicious and legitimate uses of LMs, utilized in auto-completion and editing-assistance settings.1 Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.</div></div></div><hr><div id=2020cl-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.cl-3/>Computational Linguistics, Volume 46, Issue 3 - September 2020</a></h4></div><hr><div id=2020cl-4><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.cl-4/>Computational Linguistics, Volume 46, Issue 4 - December 2020</a></h4></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>