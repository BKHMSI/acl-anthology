<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Asian Translation (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Asian Translation (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021wat-1>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li></ul></div></div><div id=2021wat-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.wat-1/>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.0/>Proceedings of the 8th Workshop on Asian Translation (WAT2021)</a></strong><br><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a>
|
<a href=/people/i/isao-goto/>Isao Goto</a>
|
<a href=/people/h/hideya-mino/>Hideya Mino</a>
|
<a href=/people/c/chenchen-ding/>Chenchen Ding</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/s/shohei-higashiyama/>Shohei Higashiyama</a>
|
<a href=/people/h/hiroshi-manabe/>Hiroshi Manabe</a>
|
<a href=/people/w/win-pa-pa/>Win Pa Pa</a>
|
<a href=/people/s/shantipriya-parida/>Shantipriya Parida</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a>
|
<a href=/people/a/akiko-eriguchi/>Akiko Eriguchi</a>
|
<a href=/people/k/kaori-abe/>Kaori Abe</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.4/>NICT’s Neural Machine Translation Systems for the WAT21 Restricted Translation Task<span class=acl-fixed-case>NICT</span>’s Neural Machine Translation Systems for the <span class=acl-fixed-case>WAT</span>21 Restricted Translation Task</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--4><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> (Team ID : nictrb) for participating in the WAT&#8217;21 restricted machine translation task. In our submitted <a href=https://en.wikipedia.org/wiki/System>system</a>, we designed a new <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training approach</a> for restricted machine translation. By sampling from the translation target, we can solve the problem that ordinary training data does not have a restricted vocabulary. With the further help of constrained decoding in the inference phase, we achieved better results than the baseline, confirming the effectiveness of our solution. In addition, we also tried the vanilla and sparse Transformer as the backbone network of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, as well as <a href=https://en.wikipedia.org/wiki/Mathematical_model>model ensembling</a>, which further improved the final translation performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.6/>NECTEC’s Participation in WAT-2021<span class=acl-fixed-case>NECTEC</span>’s Participation in <span class=acl-fixed-case>WAT</span>-2021</a></strong><br><a href=/people/z/zar-zar-hlaing/>Zar Zar Hlaing</a>
|
<a href=/people/y/ye-kyaw-thu/>Ye Kyaw Thu</a>
|
<a href=/people/t/thazin-myint-oo/>Thazin Myint Oo</a>
|
<a href=/people/m/mya-ei-san/>Mya Ei San</a>
|
<a href=/people/s/sasiporn-usanavasin/>Sasiporn Usanavasin</a>
|
<a href=/people/p/ponrudee-netisopakul/>Ponrudee Netisopakul</a>
|
<a href=/people/t/thepchai-supnithi/>Thepchai Supnithi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--6><div class="card-body p-3 small">In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team for the translation tasks of WAT-2021. Basically, our models are based on <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural methods</a> for both directions of English-Myanmar and Myanmar-English language pairs. Most of the existing Neural Machine Translation (NMT) models mainly focus on the conversion of sequential data and do not directly use syntactic information. However, we conduct multi-source neural machine translation (NMT) models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus. The multi-source translation is an approach to exploit multiple inputs (e.g. in two different formats) to increase translation accuracy. The RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment. The experimental results showed that the proposed models of RNN-based architecture outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a> for English-to-Myanmar translation task, and the multi-source and shared-multi-source transformer models yield better translation results than the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.11/>Zero-pronoun Data Augmentation for Japanese-to-English Translation<span class=acl-fixed-case>J</span>apanese-to-<span class=acl-fixed-case>E</span>nglish Translation</a></strong><br><a href=/people/r/ryokan-ri/>Ryokan Ri</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--11><div class="card-body p-3 small">For Japanese-to-English translation, zero pronouns in Japanese pose a challenge, since the model needs to infer and produce the corresponding <a href=https://en.wikipedia.org/wiki/Pronoun>pronoun</a> in the target side of the English sentence. However, although fully resolving zero pronouns often needs <a href=https://en.wikipedia.org/wiki/Context_(language_use)>discourse context</a>, in some cases, the local context within a sentence gives clues to the inference of the zero pronoun. In this study, we propose a data augmentation method that provides additional training signals for the translation model to learn correlations between <a href=https://en.wikipedia.org/wiki/Context_(language_use)>local context</a> and zero pronouns. We show that the proposed method significantly improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of zero pronoun translation with <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> experiments in the conversational domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.13/>TMU NMT System with Japanese BART for the Patent task of WAT 2021<span class=acl-fixed-case>TMU</span> <span class=acl-fixed-case>NMT</span> System with <span class=acl-fixed-case>J</span>apanese <span class=acl-fixed-case>BART</span> for the Patent task of <span class=acl-fixed-case>WAT</span> 2021</a></strong><br><a href=/people/h/hwichan-kim/>Hwichan Kim</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--13><div class="card-body p-3 small">In this paper, we introduce our TMU Neural Machine Translation (NMT) system submitted for the Patent task (Korean Japanese and English Japanese) of 8th Workshop on Asian Translation (Nakazawa et al., 2021). Recently, several studies proposed pre-trained encoder-decoder models using monolingual data. One of the pre-trained models, BART (Lewis et al., 2020), was shown to improve translation accuracy via fine-tuning with bilingual data. However, they experimented only Romanian!English translation using <a href=https://en.wikipedia.org/wiki/BART>English BART</a>. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both Korean Japanese and English Japanese translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.18/>IITP at WAT 2021 : System description for English-Hindi Multimodal Translation Task<span class=acl-fixed-case>IITP</span> at <span class=acl-fixed-case>WAT</span> 2021: System description for <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>H</span>indi Multimodal Translation Task</a></strong><br><a href=/people/b/baban-gain/>Baban Gain</a>
|
<a href=/people/d/dibyanayan-bandyopadhyay/>Dibyanayan Bandyopadhyay</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--18><div class="card-body p-3 small">Neural Machine Translation (NMT) is a predominant machine translation technology nowadays because of its end-to-end trainable flexibility. However, NMT still struggles to translate properly in low-resource settings specifically on distant language pairs. One way to overcome this is to use the information from other modalities if available. The idea is that despite differences in languages, both the source and target language speakers see the same thing and the visual representation of both the source and target is the same, which can positively assist the system. Multimodal information can help the NMT system to improve the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> by removing <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> on some phrases or words. We participate in the 8th Workshop on Asian Translation (WAT-2021) for English-Hindi multimodal translation task and achieve 42.47 and 37.50 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU points</a> for Evaluation and Challenge subset, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.20/>TMEKU System for the WAT2021 Multimodal Translation Task<span class=acl-fixed-case>TMEKU</span> System for the <span class=acl-fixed-case>WAT</span>2021 Multimodal Translation Task</a></strong><br><a href=/people/y/yuting-zhao/>Yuting Zhao</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/c/chenhui-chu/>Chenhui Chu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--20><div class="card-body p-3 small">We introduce our TMEKU system submitted to the English-Japanese Multimodal Translation Task for WAT 2021. We participated in the Flickr30kEnt-JP task and Ambiguous MSCOCO Multimodal task under the constrained condition using only the officially provided datasets. Our proposed system employs soft alignment of word-region for multimodal neural machine translation (MNMT). The experimental results evaluated on the BLEU metric provided by the WAT 2021 evaluation site show that the TMEKU system has achieved the best performance among all the participated systems. Further analysis of the case study demonstrates that leveraging word-region alignment between the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.21/>Optimal Word Segmentation for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> into Dravidian Languages<span class=acl-fixed-case>D</span>ravidian Languages</a></strong><br><a href=/people/p/prajit-dhar/>Prajit Dhar</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--21><div class="card-body p-3 small">Dravidian languages, such as <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a> and <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, are notoriously difficult to translate by state-of-the-art neural models. This stems from the fact that these <a href=https://en.wikipedia.org/wiki/Language>languages</a> are morphologically very rich as well as being low-resourced. In this paper, we focus on subword segmentation and evaluate Linguistically Motivated Vocabulary Reduction (LMVR) against the more commonly used SentencePiece (SP) for the task of translating from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into four different <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a>. Additionally we investigate the optimal subword vocabulary size for each language. We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.22/>Itihasa : A <a href=https://en.wikipedia.org/wiki/Text_corpus>large-scale corpus</a> for Sanskrit to English translation<span class=acl-fixed-case>S</span>anskrit to <span class=acl-fixed-case>E</span>nglish translation</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--22><div class="card-body p-3 small">This work introduces <a href=https://en.wikipedia.org/wiki/Itihasa>Itihasa</a>, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The <a href=https://en.wikipedia.org/wiki/Shloka>shlokas</a> are extracted from two <a href=https://en.wikipedia.org/wiki/Indian_epic_poetry>Indian epics</a> viz., The <a href=https://en.wikipedia.org/wiki/Ramayana>Ramayana</a> and The <a href=https://en.wikipedia.org/wiki/Mahabharata>Mahabharata</a>. We first describe the motivation behind the curation of such a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.23/>NICT-5’s Submission To WAT 2021 : MBART Pre-training And In-Domain Fine Tuning For Indic Languages<span class=acl-fixed-case>NICT</span>-5’s Submission To <span class=acl-fixed-case>WAT</span> 2021: <span class=acl-fixed-case>MBART</span> Pre-training And In-Domain Fine Tuning For Indic Languages</a></strong><br><a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/abhisek-chakrabarty/>Abhisek Chakrabarty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--23><div class="card-body p-3 small">In this paper we describe our submission to the multilingual Indic language translation wtask MultiIndicMT under the team name NICT-5. This task involves <a href=https://en.wikipedia.org/wiki/Translation>translation</a> from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.24/>How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task<span class=acl-fixed-case>GPU</span> in 100 hours? <span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>AS</span>ta<span class=acl-fixed-case>L</span> at <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>I</span>ndic<span class=acl-fixed-case>MT</span> Shared Task</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/h/hector-ricardo-murrieta-bello/>Héctor Ricardo Murrieta Bello</a>
|
<a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/m/marcel-bollmann/>Marcel Bollmann</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--24><div class="card-body p-3 small">This work shows that competitive <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translation</a> results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a> for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of translation systems and <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.26/>Language Relatedness and Lexical Closeness can help Improve Multilingual NMT : IITBombay@MultiIndicNMT WAT2021<span class=acl-fixed-case>NMT</span>: <span class=acl-fixed-case>IITB</span>ombay@<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>I</span>ndic<span class=acl-fixed-case>NMT</span> <span class=acl-fixed-case>WAT</span>2021</a></strong><br><a href=/people/j/jyotsana-khatri/>Jyotsana Khatri</a>
|
<a href=/people/n/nikhil-saini/>Nikhil Saini</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--26><div class="card-body p-3 small">Multilingual Neural Machine Translation has achieved remarkable performance by training a single translation model for <a href=https://en.wikipedia.org/wiki/Multilingualism>multiple languages</a>. This paper describes our submission (Team ID : CFILT-IITB) for the MultiIndicMT : An Indic Language Multilingual Task at WAT 2021. We train multilingual NMT systems by sharing <a href=https://en.wikipedia.org/wiki/Encoder>encoder and decoder parameters</a> with language embedding associated with each token in both <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and decoder. Furthermore, we demonstrate the use of transliteration (script conversion) for <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indic languages</a> in reducing the lexical gap for training a multilingual NMT system. Further, we show improvement in performance by training a multilingual NMT system using languages of the same family, i.e., <a href=https://en.wikipedia.org/wiki/Lingua_franca>related languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.27/>Samsung R&D Institute Poland submission to WAT 2021 Indic Language Multilingual Task<span class=acl-fixed-case>S</span>amsung <span class=acl-fixed-case>R</span>&<span class=acl-fixed-case>D</span> Institute <span class=acl-fixed-case>P</span>oland submission to <span class=acl-fixed-case>WAT</span> 2021 Indic Language Multilingual Task</a></strong><br><a href=/people/a/adam-dobrowolski/>Adam Dobrowolski</a>
|
<a href=/people/m/marcin-szymanski/>Marcin Szymański</a>
|
<a href=/people/m/marcin-chochowski/>Marcin Chochowski</a>
|
<a href=/people/p/pawel-przybysz/>Paweł Przybysz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--27><div class="card-body p-3 small">This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R&D Institute Poland. The task covered translation between 10 Indic Languages (Bengali, <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a>, <a href=https://en.wikipedia.org/wiki/Odia_language>Oriya</a>, <a href=https://en.wikipedia.org/wiki/Punjabi_language>Punjabi</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a> and <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>) and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We combined a variety of techniques : <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a>, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models. We applied an effective approach to low-resource training that consist of pretraining on backtranslations and tuning on parallel corpora. We experimented with two different domain-adaptation techniques which significantly improved translation quality when applied to monolingual corpora. We researched and applied a novel approach for finding the best <a href=https://en.wikipedia.org/wiki/Hyperparameter>hyperparameters</a> for ensembling a number of <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation models</a>. All techniques combined gave significant improvement-up to +8 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> over baseline results. The quality of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> has been confirmed by the human evaluation where SRPOL models scored best for all 5 manually evaluated languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.28/>Multilingual Machine Translation Systems at WAT 2021 : One-to-Many and Many-to-One Transformer based NMT<span class=acl-fixed-case>WAT</span> 2021: One-to-Many and Many-to-One Transformer based <span class=acl-fixed-case>NMT</span></a></strong><br><a href=/people/s/shivam-mhaskar/>Shivam Mhaskar</a>
|
<a href=/people/a/aditya-jain/>Aditya Jain</a>
|
<a href=/people/a/aakash-banerjee/>Aakash Banerjee</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--28><div class="card-body p-3 small">In this paper, we present the details of the systems that we have submitted for the WAT 2021 MultiIndicMT : An Indic Language Multilingual Task. We have submitted two separate multilingual NMT models : one for <a href=https://en.wikipedia.org/wiki/English_language>English</a> to 10 Indic languages and another for 10 Indic languages to English. We discuss the implementation details of two separate multilingual NMT approaches, namely one-to-many and many-to-one, that makes use of a shared decoder and a shared encoder, respectively. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wat-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wat-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wat-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wat-1.30/>ANVITA Machine Translation System for WAT 2021 MultiIndicMT Shared Task<span class=acl-fixed-case>ANVITA</span> Machine Translation System for <span class=acl-fixed-case>WAT</span> 2021 <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>I</span>ndic<span class=acl-fixed-case>MT</span> Shared Task</a></strong><br><a href=/people/p/pavanpankaj-vegi/>Pavanpankaj Vegi</a>
|
<a href=/people/s/sivabhavani-j/>Sivabhavani J</a>
|
<a href=/people/b/biswajit-paul/>Biswajit Paul</a>
|
<a href=/people/c/chitra-viswanathan/>Chitra Viswanathan</a>
|
<a href=/people/p/prasanna-kumar-k-r/>Prasanna Kumar K R</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wat-1--30><div class="card-body p-3 small">This paper describes ANVITA-1.0 MT system, architected for submission to WAT2021 MultiIndicMT shared task by mcairt team, where the team participated in 20 translation directions : EnglishIndic and IndicEnglish ; Indic set comprised of 10 Indian languages. ANVITA-1.0 MT system comprised of two multi-lingual NMT models one for the EnglishIndic directions and other for the IndicEnglish directions with shared encoder-decoder, catering 10 language pairs and twenty translation directions. The base models were built based on Transformer architecture and trained over MultiIndicMT WAT 2021 corpora and further employed back translation and transliteration for selective data augmentation, and model ensemble for better generalization. Additionally, MultiIndicMT WAT 2021 corpora was distilled using a series of filtering operations before putting up for training. ANVITA-1.0 achieved highest AM-FM score for <a href=https://en.wikipedia.org/wiki/Bengali_language>EnglishBengali</a>, 2nd for <a href=https://en.wikipedia.org/wiki/Tamil_language>EnglishTamil</a> and 3rd for <a href=https://en.wikipedia.org/wiki/Indian_English>EnglishHindi</a>, BengaliEnglish directions on official test set. In general, performance achieved by ANVITA for the IndicEnglish directions are relatively better than that of EnglishIndic directions for all the 10 language pairs when evaluated using BLEU and RIBES, although the same trend is not observed consistently when AM-FM based evaluation was carried out. As compared to <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, RIBES and AM-FM based scoring placed ANVITA relatively better among all the task participants.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>