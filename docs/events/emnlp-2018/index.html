<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Conference on Empirical Methods in Natural Language Processing (and forerunners) (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Conference on Empirical Methods in Natural Language Processing (and forerunners) (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#d18-1>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">319&nbsp;papers</span></li><li><a class=align-middle href=#d18-2>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a>
<span class="badge badge-info align-middle ml-1">23&nbsp;papers</span></li><li><a class=align-middle href=#d18-3>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#w18-51>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w18-52>Proceedings of the 5th Workshop on Argument Mining</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li><li><a class=align-middle href=#w18-53>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w18-54>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a>
<span class="badge badge-info align-middle ml-1">30&nbsp;papers</span></li><li><a class=align-middle href=#w18-55>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</a>
<span class="badge badge-info align-middle ml-1">19&nbsp;papers</span></li><li><a class=align-middle href=#w18-56>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#w18-57>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w18-58>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</a>
<span class="badge badge-info align-middle ml-1">17&nbsp;papers</span></li><li><a class=align-middle href=#w18-59>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop & Shared Task</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w18-60>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w18-61>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</a>
<span class="badge badge-info align-middle ml-1">21&nbsp;papers</span></li><li><a class=align-middle href=#w18-62>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a>
<span class="badge badge-info align-middle ml-1">34&nbsp;papers</span></li><li><a class=align-middle href=#w18-63>Proceedings of the Third Conference on Machine Translation: Research Papers</a>
<span class="badge badge-info align-middle ml-1">18&nbsp;papers</span></li><li><a class=align-middle href=#w18-64>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a>
<span class="badge badge-info align-middle ml-1">51&nbsp;papers</span></li><li><a class=align-middle href=#2018iwslt-1>Proceedings of the 15th International Conference on Spoken Language Translation</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li></ul></div></div><div id=d18-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D18-1/>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1000/>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</a></strong><br><a href=/people/e/ellen-riloff/>Ellen Riloff</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/j/julia-hockenmaier/>Julia Hockenmaier</a>
|
<a href=/people/j/junichi-tsujii/>Jun’ichi Tsujii</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1002.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305203150 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1002/>Adversarial Removal of Demographic Attributes from Text Data</a></strong><br><a href=/people/y/yanai-elazar/>Yanai Elazar</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1002><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Representation_learning>Representation Learning</a> and Adversarial Training seem to succeed in removing unwanted features from the learned <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a>. We show that <a href=https://en.wikipedia.org/wiki/Demography>demographic information</a> of authors is encoded inand can be recovered fromthe <a href=https://en.wikipedia.org/wiki/Intermediate_representation>intermediate representations</a> learned by text-based neural classifiers. The implication is that decisions of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on textual data are not agnostic toand likely condition ondemographic attributes. When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data. This behavior is consistent across several <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, <a href=https://en.wikipedia.org/wiki/Demography>demographic properties</a> and datasets. We explore several techniques to improve the effectiveness of the adversarial component. Our main conclusion is a cautionary one : do not rely on the adversarial training to achieve invariant representation to sensitive features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305203523 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1003/>DeClarE : Debunking Fake News and False Claims using Evidence-Aware Deep Learning<span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>C</span>lar<span class=acl-fixed-case>E</span>: Debunking Fake News and False Claims using Evidence-Aware Deep Learning</a></strong><br><a href=/people/k/kashyap-popat/>Kashyap Popat</a>
|
<a href=/people/s/subhabrata-mukherjee/>Subhabrata Mukherjee</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/g/gerhard-weikum/>Gerhard Weikum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1003><div class="card-body p-3 small">Misinformation such as <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> is one of the big challenges of our society. Research on automated fact-checking has proposed methods based on <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>, but these approaches do not consider external evidence apart from labeled training instances. Recent approaches counter this deficit by considering <a href=https://en.wikipedia.org/wiki/Source_text>external sources</a> related to a claim. However, these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> require substantial <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature modeling</a> and rich lexicons. This paper overcomes these limitations of prior work with an end-to-end model for evidence-aware credibility assessment of arbitrary textual claims, without any human intervention. It presents a neural network model that judiciously aggregates signals from external evidence articles, the language of these articles and the trustworthiness of their sources. It also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user. Experiments with four datasets and ablation studies show the strength of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1005.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305204297 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1005" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1005/>Detecting Gang-Involved Escalation on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> Using Context</a></strong><br><a href=/people/s/serina-chang/>Serina Chang</a>
|
<a href=/people/r/ruiqi-zhong/>Ruiqi Zhong</a>
|
<a href=/people/e/ethan-adams/>Ethan Adams</a>
|
<a href=/people/f/fei-tzin-lee/>Fei-Tzin Lee</a>
|
<a href=/people/s/siddharth-varia/>Siddharth Varia</a>
|
<a href=/people/d/desmond-patton/>Desmond Patton</a>
|
<a href=/people/w/william-frey/>William Frey</a>
|
<a href=/people/c/chris-kedzie/>Chris Kedzie</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1005><div class="card-body p-3 small">Gang-involved youth in cities such as Chicago have increasingly turned to <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> to post about their experiences and intents online. In some situations, when they experience the loss of a loved one, their online expression of emotion may evolve into aggression towards rival gangs and ultimately into real-world violence. In this paper, we present a novel <a href=https://en.wikipedia.org/wiki/System>system</a> for detecting Aggression and Loss in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Our system features the use of domain-specific resources automatically derived from a large unlabeled corpus, and contextual representations of the emotional and semantic content of the user&#8217;s recent tweets as well as their interactions with other users. Incorporating <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> in our Convolutional Neural Network (CNN) leads to a significant improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305193585 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1006" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1006/>Reasoning about Actions and State Changes by Injecting Commonsense Knowledge</a></strong><br><a href=/people/n/niket-tandon/>Niket Tandon</a>
|
<a href=/people/b/bhavana-dalvi/>Bhavana Dalvi</a>
|
<a href=/people/j/joel-grus/>Joel Grus</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1006><div class="card-body p-3 small">Comprehending procedural text, e.g., a paragraph describing photosynthesis, requires modeling actions and the state changes they produce, so that questions about entities at different timepoints can be answered. Although several recent systems have shown impressive progress in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, their predictions can be globally inconsistent or highly improbable. In this paper, we show how the predicted effects of actions in the context of a paragraph can be improved in two ways : (1) by incorporating global, commonsense constraints (e.g., a non-existent entity can not be destroyed), and (2) by biasing reading with preferences from large-scale corpora (e.g., trees rarely move). Unlike earlier methods, we treat the problem as a neural structured prediction task, allowing <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>hard and soft constraints</a> to steer the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> away from unlikely predictions. We show that the new model significantly outperforms earlier <a href=https://en.wikipedia.org/wiki/System>systems</a> on a benchmark dataset for procedural text comprehension (+8 % relative gain), and that it also avoids some of the nonsensical predictions that earlier systems make.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305194062 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1007/>Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation</a></strong><br><a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/a/aparajita-haldar/>Aparajita Haldar</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/j/j-edward-hu/>J. Edward Hu</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a>
|
<a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1007><div class="card-body p-3 small">We present a large-scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a sentence representation captures distinct types of <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. We refer to our <a href=https://en.wikipedia.org/wiki/Collection_(artwork)>collection</a> as the DNC : Diverse Natural Language Inference Collection. The DNC is available online at, and will grow over time as additional resources are recast and added from novel sources.<url>https://www.decomp.net</url>, and will grow over time as additional resources are recast and added from novel sources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1009.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305195438 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1009/>SWAG : A Large-Scale Adversarial Dataset for Grounded Commonsense Inference<span class=acl-fixed-case>SWAG</span>: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference</a></strong><br><a href=/people/r/rowan-zellers/>Rowan Zellers</a>
|
<a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1009><div class="card-body p-3 small">Given a partial description like she opened the hood of the car, humans can reason about the situation and anticipate what might come next (then, she examined the engine). In this paper, we introduce the task of grounded commonsense inference, unifying <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a> and <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>. We present SWAG, a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a>. Empirical results demonstrate that while humans can solve the resulting inference problems with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> (88 %), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305209813 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1011/>Associative Multichannel Autoencoder for Multimodal Word Representation</a></strong><br><a href=/people/s/shaonan-wang/>Shaonan Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1011><div class="card-body p-3 small">In this paper we address the problem of learning multimodal word representations by integrating textual, visual and auditory inputs. Inspired by the re-constructive and associative nature of human memory, we propose a novel associative multichannel autoencoder (AMA). Our model first learns the associations between textual and perceptual modalities, so as to predict the missing perceptual information of concepts. Then the textual and predicted perceptual representations are fused through reconstructing their original and associated embeddings. Using a gating mechanism our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> assigns different weights to each modality according to the different concepts. Results on six benchmark concept similarity tests show that the proposed method significantly outperforms strong unimodal baselines and state-of-the-art multimodal models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1014.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305210831 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1014/>Multimodal Language Analysis with Recurrent Multistage Fusion</a></strong><br><a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/z/ziyin-liu/>Ziyin Liu</a>
|
<a href=/people/a/amirali-bagher-zadeh/>AmirAli Bagher Zadeh</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1014><div class="card-body p-3 small">Computational modeling of human multimodal language is an emerging research area in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> spanning the <a href=https://en.wikipedia.org/wiki/Language>language</a>, visual and acoustic modalities. Comprehending multimodal language requires modeling not only the interactions within each modality (intra-modal interactions) but more importantly the interactions between modalities (cross-modal interactions). In this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple stages, each of them focused on a subset of multimodal signals for specialized, effective fusion. Cross-modal interactions are modeled using this multistage fusion approach which builds upon intermediate representations of previous stages. Temporal and intra-modal interactions are modeled by integrating our proposed fusion approach with a system of recurrent neural networks. The RMFN displays state-of-the-art performance in modeling human multimodal language across three public datasets relating to multimodal sentiment analysis, <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a>, and speaker traits recognition. We provide visualizations to show that each stage of fusion focuses on a different subset of multimodal signals, learning increasingly discriminative multimodal representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306353798 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1016/>PreCo : A Large-scale Dataset in Preschool Vocabulary for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a><span class=acl-fixed-case>P</span>re<span class=acl-fixed-case>C</span>o: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution</a></strong><br><a href=/people/h/hong-chen/>Hong Chen</a>
|
<a href=/people/z/zhenhua-fan/>Zhenhua Fan</a>
|
<a href=/people/h/hao-lu/>Hao Lu</a>
|
<a href=/people/a/alan-yuille/>Alan Yuille</a>
|
<a href=/people/s/shu-rong/>Shu Rong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1016><div class="card-body p-3 small">We introduce PreCo, a large-scale English dataset for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. The dataset is designed to embody the core challenges in <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>, such as <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity representation</a>, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering. To strengthen the training-test overlap, we collect a large corpus of 38 K documents and 12.5 M words which are mostly from the vocabulary of English-speaking preschoolers. Experiments show that with higher training-test overlap, error analysis on PreCo is more efficient than the one on OntoNotes, a popular existing dataset. Furthermore, we annotate singleton mentions making it possible for the first time to quantify the influence that a mention detector makes on <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> performance. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is freely available at.<url>https://preschool-lab.github.io/PreCo/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306354811 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1017/>Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism<span class=acl-fixed-case>C</span>hinese Named Entity Recognition with Self-Attention Mechanism</a></strong><br><a href=/people/p/pengfei-cao/>Pengfei Cao</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/s/shengping-liu/>Shengping Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1017><div class="card-body p-3 small">Named entity recognition (NER) is an important task in natural language processing area, which needs to determine entities boundaries and classify them into pre-defined categories. For Chinese NER task, there is only a very small amount of annotated data available. Chinese NER task and Chinese word segmentation (CWS) task have many similar word boundaries. There are also specificities in each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, existing methods for Chinese NER either do not exploit word boundary information from CWS or can not filter the specific information of CWS. In this paper, we propose a novel adversarial transfer learning framework to make full use of task-shared boundaries information and prevent the task-specific features of CWS. Besides, since arbitrary character can provide important cues when predicting entity type, we exploit self-attention to explicitly capture long range dependencies between two tokens. Experimental results on two different widely used datasets show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly and consistently outperforms other state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1018.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306355512 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1018/>Using <a href=https://en.wikipedia.org/wiki/Linguistic_feature>Linguistic Features</a> to Improve the Generalization Capability of Neural Coreference Resolvers</a></strong><br><a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1018><div class="card-body p-3 small">Coreference resolution is an intermediate step for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>text understanding</a>. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> is of special importance for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. However, while recent coreference resolvers have notable improvements on the CoNLL dataset, they struggle to generalize properly to new domains or datasets. In this paper, we investigate the role of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> in building more generalizable coreference resolvers. We show that <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> improves only slightly by merely using a set of additional <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. However, employing <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and subsets of their values that are informative for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, considerably improves <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>. Thanks to better <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our <a href=https://en.wikipedia.org/wiki/System>system</a>, which is trained on CoNLL, achieves on-par performance with a <a href=https://en.wikipedia.org/wiki/System>system</a> designed for this dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1021/>Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision</a></strong><br><a href=/people/y/yixin-cao/>Yixin Cao</a>
|
<a href=/people/l/lei-hou/>Lei Hou</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/c/chengjiang-li/>Chengjiang Li</a>
|
<a href=/people/x/xu-chen/>Xu Chen</a>
|
<a href=/people/t/tiansi-dong/>Tiansi Dong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1021><div class="card-body p-3 small">Jointly representation learning of words and entities benefits many NLP tasks, but has not been well explored in cross-lingual settings. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. It captures mutually complementary knowledge, and enables cross-lingual inferences among <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> and texts. Our method does not require <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a>, and automatically generates comparable data via distant supervision using <a href=https://en.wikipedia.org/wiki/Multilingualism>multi-lingual knowledge bases</a>. We utilize two types of regularizers to align cross-lingual words and entities, and design knowledge attention and cross-lingual attention to further reduce noises. We conducted a series of experiments on three tasks : <a href=https://en.wikipedia.org/wiki/Translation>word translation</a>, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity relatedness</a>, and cross-lingual entity linking. The results, both qualitative and quantitative, demonstrate the significance of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1023/>Multi-lingual Common Semantic Space Construction via Cluster-consistent Word Embedding</a></strong><br><a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1023><div class="card-body p-3 small">We construct a multilingual common semantic space based on <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>, where words from multiple languages are projected into a shared space via which all available resources and knowledge can be shared across multiple languages. Beyond <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a>, we introduce multiple cluster-level alignments and enforce the word clusters to be consistently distributed across multiple languages. We exploit three signals for clustering : (1) neighbor words in the monolingual word embedding space ; (2) character-level information ; and (3) <a href=https://en.wikipedia.org/wiki/Semantic_property>linguistic properties</a> (e.g., <a href=https://en.wikipedia.org/wiki/Apposition>apposition</a>, locative suffix) derived from linguistic structure knowledge bases available for thousands of languages. We introduce a new cluster-consistent correlational neural network to construct the common semantic space by aligning words as well as <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a>. Intrinsic evaluation on monolingual and multilingual QVEC tasks shows our approach achieves significantly higher correlation with linguistic features which are extracted from manually crafted lexical resources than state-of-the-art multi-lingual embedding learning methods do. Using low-resource language name tagging as a case study for extrinsic evaluation, our <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> achieves up to 14.6 % absolute F-score gain over the state of the art on cross-lingual direct transfer. Our approach is also shown to be robust even when the size of <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a> is small.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1024" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1024/>Unsupervised Multilingual Word Embeddings</a></strong><br><a href=/people/x/xilun-chen/>Xilun Chen</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1024><div class="card-body p-3 small">Multilingual Word Embeddings (MWEs) represent words from multiple languages in a single distributional vector space. Unsupervised MWE (UMWE) methods acquire multilingual embeddings without cross-lingual supervision, which is a significant advantage over traditional supervised approaches and opens many new possibilities for low-resource languages. Prior art for learning UMWEs, however, merely relies on a number of independently trained Unsupervised Bilingual Word Embeddings (UBWEs) to obtain multilingual embeddings. These <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> fail to leverage the interdependencies that exist among many languages. To address this shortcoming, we propose a fully unsupervised framework for learning MWEs that directly exploits the relations between all language pairs. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> substantially outperforms previous approaches in the experiments on multilingual word translation and cross-lingual word similarity. In addition, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> even beats <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised approaches</a> trained with cross-lingual resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1026/>Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization</a></strong><br><a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1026><div class="card-body p-3 small">Semantic specialization is a process of fine-tuning pre-trained <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional word vectors</a> using <a href=https://en.wikipedia.org/wiki/Lexical_analysis>external lexical knowledge</a> (e.g., WordNet) to accentuate a particular <a href=https://en.wikipedia.org/wiki/Semantic_relation>semantic relation</a> in the specialized vector space. While post-processing specialization methods are applicable to arbitrary distributional vectors, they are limited to updating only the vectors of words occurring in external lexicons (i.e., seen words), leaving the vectors of all other words unchanged. We propose a novel <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> to specializing the full distributional vocabulary. Our adversarial post-specialization method propagates the external lexical knowledge to the full distributional space. We exploit words seen in the resources as training examples for learning a global specialization function. This function is learned by combining a standard L2-distance loss with a adversarial loss : the adversarial component produces more realistic output vectors. We show the effectiveness and robustness of the proposed method across three languages and on three tasks : <a href=https://en.wikipedia.org/wiki/Similarity_measure>word similarity</a>, dialog state tracking, and lexical simplification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1031/>Personalized Microblog Sentiment Classification via Adversarial Cross-lingual Multi-task Learning</a></strong><br><a href=/people/w/weichao-wang/>Weichao Wang</a>
|
<a href=/people/s/shi-feng/>Shi Feng</a>
|
<a href=/people/w/wei-gao/>Wei Gao</a>
|
<a href=/people/d/daling-wang/>Daling Wang</a>
|
<a href=/people/y/yifei-zhang/>Yifei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1031><div class="card-body p-3 small">Sentiment expression in microblog posts can be affected by user&#8217;s personal character, <a href=https://en.wikipedia.org/wiki/Opinion>opinion bias</a>, <a href=https://en.wikipedia.org/wiki/Politics>political stance</a> and so on. Most of existing personalized microblog sentiment classification methods suffer from the insufficiency of discriminative tweets for personalization learning. We observed that <a href=https://en.wikipedia.org/wiki/Microblogging>microblog users</a> have consistent individuality and opinion bias in different languages. Based on this observation, in this paper we propose a novel user-attention-based Convolutional Neural Network (CNN) model with adversarial cross-lingual learning framework. The user attention mechanism is leveraged in CNN model to capture user&#8217;s language-specific individuality from the posts. Then the attention-based CNN model is incorporated into a novel adversarial cross-lingual learning framework, in which with the help of user properties as bridge between languages, we can extract the language-specific features and language-independent features to enrich the user post representation so as to alleviate the data insufficiency problem. Results on English and Chinese microblog datasets confirm that our method outperforms state-of-the-art baseline algorithms with large margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1033" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1033/>Cross-lingual Lexical Sememe Prediction</a></strong><br><a href=/people/f/fanchao-qi/>Fanchao Qi</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/h/hao-zhu/>Hao Zhu</a>
|
<a href=/people/r/ruobing-xie/>Ruobing Xie</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1033><div class="card-body p-3 small">Sememes are defined as the minimum semantic units of human languages. As important knowledge sources, sememe-based linguistic knowledge bases have been widely used in many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. However, most languages still do not have sememe-based linguistic knowledge bases. Thus we present a task of cross-lingual lexical sememe prediction, aiming to automatically predict <a href=https://en.wikipedia.org/wiki/Sememe>sememes</a> for words in other languages. We propose a novel framework to model correlations between <a href=https://en.wikipedia.org/wiki/Sememe>sememes</a> and multi-lingual words in low-dimensional semantic space for sememe prediction. Experimental results on real-world datasets show that our proposed model achieves consistent and significant improvements as compared to baseline methods in cross-lingual sememe prediction. The codes and data of this paper are available at.<url>https://github.com/thunlp/CL-SP</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1035/>A Stable and Effective Learning Strategy for Trainable Greedy Decoding</a></strong><br><a href=/people/y/yun-chen/>Yun Chen</a>
|
<a href=/people/v/victor-o-k-li/>Victor O.K. Li</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1035><div class="card-body p-3 small">Beam search is a widely used approximate search strategy for neural network decoders, and it generally outperforms simple <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy decoding</a> on tasks like <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. However, this improvement comes at substantial computational cost. In this paper, we propose a flexible new method that allows us to reap nearly the full benefits of <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> with nearly no additional computational cost. The method revolves around a small neural network actor that is trained to observe and manipulate the hidden state of a previously-trained decoder. To train this actor network, we introduce the use of a pseudo-parallel corpus built using the output of <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> on a base model, ranked by a target quality metric like BLEU. Our method is inspired by earlier work on this problem, but requires no <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, and can be trained reliably on a range of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Experiments on three parallel corpora and three architectures show that the method yields substantial improvements in translation quality and speed over each base system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1036/>Addressing Troublesome Words in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1036><div class="card-body p-3 small">One of the weaknesses of Neural Machine Translation (NMT) is in handling lowfrequency and ambiguous words, which we refer as troublesome words. To address this problem, we propose a novel memoryenhanced NMT method. First, we investigate different <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to define and detect the troublesome words. Then, a contextual memory is constructed to memorize which target words should be produced in what situations. Finally, we design a hybrid model to dynamically access the contextual memory so as to correctly translate the troublesome words. The extensive experiments on Chinese-to-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1038.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1038/>XL-NBT : A Cross-lingual Neural Belief Tracking Framework<span class=acl-fixed-case>XL</span>-<span class=acl-fixed-case>NBT</span>: A Cross-lingual Neural Belief Tracking Framework</a></strong><br><a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/j/jianshu-chen/>Jianshu Chen</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/d/dong-yu/>Dong Yu</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1038><div class="card-body p-3 small">Task-oriented dialog systems are becoming pervasive, and many companies heavily rely on them to complement <a href=https://en.wikipedia.org/wiki/Intelligent_agent>human agents</a> for customer service in call centers. With globalization, the need for providing cross-lingual customer support becomes more urgent than ever. However, cross-lingual support poses great challengesit requires a large amount of additional annotated data from <a href=https://en.wikipedia.org/wiki/First_language>native speakers</a>. In order to bypass the expensive human annotation and achieve the first step towards the ultimate goal of building a universal dialog system, we set out to build a cross-lingual state tracking framework. Specifically, we assume that there exists a source language with dialog belief tracking annotations while the target languages have no annotated dialog data of any form. Then, we pre-train a state tracker for the source language as a teacher, which is able to exploit easy-to-access parallel data. We then distill and transfer its own knowledge to the student state tracker in target languages. We specifically discuss two types of common parallel resources : <a href=https://en.wikipedia.org/wiki/Bilingual_corpus>bilingual corpus</a> and <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual dictionary</a>, and design different transfer learning strategies accordingly. Experimentally, we successfully use English state tracker as the teacher to transfer its knowledge to both Italian and German trackers and achieve promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1040/>Back-Translation Sampling by Targeting Difficult Words in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/marzieh-fadaee/>Marzieh Fadaee</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1040><div class="card-body p-3 small">Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, and show that words with high prediction loss during training benefit most from the addition of <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a>. We introduce several variations of <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling strategies</a> targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> using <a href=https://en.wikipedia.org/wiki/Simple_random_sample>random sampling</a> for German-English and English-German, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1042" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1042/>A Discriminative Latent-Variable Model for Bilingual Lexicon Induction</a></strong><br><a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/y/yova-kementchedjhieva/>Yova Kementchedjhieva</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1042><div class="card-body p-3 small">We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1043 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1043" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1043/>Non-Adversarial Unsupervised Word Translation</a></strong><br><a href=/people/y/yedid-hoshen/>Yedid Hoshen</a>
|
<a href=/people/l/lior-wolf/>Lior Wolf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1043><div class="card-body p-3 small">Unsupervised word translation from non-parallel inter-lingual corpora has attracted much research interest. Very recently, <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural network methods</a> trained with adversarial loss functions achieved high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Despite the impressive success of the recent techniques, they suffer from the typical drawbacks of generative adversarial models : sensitivity to <a href=https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)>hyper-parameters</a>, long training time and lack of <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. In this paper, we make the observation that two sufficiently similar distributions can be aligned correctly with iterative matching methods. We present a novel method that first aligns the second moment of the word distributions of the two languages and then iteratively refines the alignment. Extensive experiments on word translation of European and Non-European languages show that our method achieves better performance than recent state-of-the-art deep adversarial approaches and is competitive with the supervised baseline. It is also efficient, easy to parallelize on CPU and interpretable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1044" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1044/>Semi-Autoregressive Neural Machine Translation</a></strong><br><a href=/people/c/chunqi-wang/>Chunqi Wang</a>
|
<a href=/people/j/ji-zhang/>Ji Zhang</a>
|
<a href=/people/h/haiqing-chen/>Haiqing Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1044><div class="card-body p-3 small">Existing approaches to <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> are typically <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a>. While these models attain state-of-the-art translation quality, they are suffering from low <a href=https://en.wikipedia.org/wiki/Parallelizability>parallelizability</a> and thus slow at decoding long sequences. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for fast sequence generation the semi-autoregressive Transformer (SAT). The SAT keeps the autoregressive property in global but relieves in local and thus are able to produce multiple successive words in parallel at each time step. Experiments conducted on English-German and Chinese-English translation tasks show that the SAT achieves a good balance between translation quality and decoding speed. On WMT&#8217;14 English-German translation, the SAT achieves 5.58 speedup while maintaining 88 % translation quality, significantly better than the previous non-autoregressive methods. When produces two words at each time step, the SAT is almost lossless (only 1 % degeneration in BLEU score).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1045" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1045/>Understanding Back-Translation at Scale</a></strong><br><a href=/people/s/sergey-edunov/>Sergey Edunov</a>
|
<a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a>
|
<a href=/people/d/david-grangier/>David Grangier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1045><div class="card-body p-3 small">An effective method to improve <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings <a href=https://en.wikipedia.org/wiki/Back_translation>back-translations</a> obtained via <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling</a> or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than <a href=https://en.wikipedia.org/wiki/Data>data</a> generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT&#8217;14 English-German test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1046/>Bootstrapping Transliteration with Constrained Discovery for Low-Resource Languages</a></strong><br><a href=/people/s/shyam-upadhyay/>Shyam Upadhyay</a>
|
<a href=/people/j/jordan-kodner/>Jordan Kodner</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1046><div class="card-body p-3 small">Generating the English transliteration of a name written in a foreign script is an important and challenging step in multilingual knowledge acquisition and <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>. Existing approaches to transliteration generation require a large (5000) number of training examples. This difficulty contrasts with <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration discovery</a>, a somewhat easier task that involves picking a plausible <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> from a given list. In this work, we present a <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>bootstrapping algorithm</a> that uses constrained discovery to improve generation, and can be used with as few as 500 training examples, which we show can be sourced from annotators in a matter of hours. This opens the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> to languages for which large number of training examples are unavailable. We evaluate transliteration generation performance itself, as well the improvement it brings to cross-lingual candidate generation for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a>, a typical downstream task. We present a comprehensive evaluation of our approach on nine languages, each written in a unique script.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1048/>Adaptive Multi-pass Decoder for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/x/xinwei-geng/>Xinwei Geng</a>
|
<a href=/people/x/xiaocheng-feng/>Xiaocheng Feng</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1048><div class="card-body p-3 small">Although end-to-end neural machine translation (NMT) has achieved remarkable progress in the recent years, the idea of adopting multi-pass decoding mechanism into conventional NMT is not well explored. In this paper, we propose a novel architecture called adaptive multi-pass decoder, which introduces a flexible multi-pass polishing mechanism to extend the capacity of NMT via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. More specifically, we adopt an extra policy network to automatically choose a suitable and effective number of decoding passes, according to the <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of source sentences and the <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> of the generated translations. Extensive experiments on Chinese-English translation demonstrate the effectiveness of our proposed adaptive multi-pass decoder upon the conventional <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> with a significant improvement about 1.55 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305204813 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1051/>SimpleQuestions Nearly Solved : A New Upperbound and Baseline Approach<span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>Q</span>uestions Nearly Solved: A New Upperbound and Baseline Approach</a></strong><br><a href=/people/m/michael-petrochuk/>Michael Petrochuk</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1051><div class="card-body p-3 small">The SimpleQuestions dataset is one of the most commonly used benchmarks for studying single-relation factoid questions. In this paper, we present new evidence that this <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> can be nearly solved by standard methods. First, we show that ambiguity in the data bounds performance at 83.4 % ; many questions have more than one equally plausible interpretation. Second, we introduce a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> that sets a new state-of-the-art performance level at 78.1 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, despite using standard <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a>. Finally, we report an empirical analysis showing that the <a href=https://en.wikipedia.org/wiki/Upper_and_lower_bounds>upperbound</a> is loose ; roughly a quarter of the remaining errors are also not resolvable from the <a href=https://en.wikipedia.org/wiki/Signal_(IPC)>linguistic signal</a>. Together, these results suggest that the SimpleQuestions dataset is nearly solved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305205055 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1052" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1052/>Phrase-Indexed Question Answering : A New Challenge for Scalable Document Comprehension</a></strong><br><a href=/people/m/minjoon-seo/>Minjoon Seo</a>
|
<a href=/people/t/tom-kwiatkowski/>Tom Kwiatkowski</a>
|
<a href=/people/a/ankur-parikh/>Ankur Parikh</a>
|
<a href=/people/a/ali-farhadi/>Ali Farhadi</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1052><div class="card-body p-3 small">We formalize a new modular variant of current question answering tasks by enforcing complete independence of the document encoder from the question encoder. This formulation addresses a key challenge in machine comprehension by building a standalone representation of the document discourse. It additionally leads to a significant scalability advantage since the encoding of the answer candidate phrases in the document can be pre-computed and indexed offline for efficient retrieval. We experiment with baseline models for the new task, which achieve a reasonable accuracy but significantly underperform unconstrained QA models. We invite the QA research community to engage in Phrase-Indexed Question Answering (PIQA, pika) for closing the gap. The leaderboard is at :<url>nlp.cs.washington.edu/piqa</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1054.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305205548 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1054/>Cut to the Chase : A Context Zoom-in Network for Reading Comprehension</a></strong><br><a href=/people/s/sathish-reddy-indurthi/>Sathish Reddy Indurthi</a>
|
<a href=/people/s/seunghak-yu/>Seunghak Yu</a>
|
<a href=/people/s/seohyun-back/>Seohyun Back</a>
|
<a href=/people/h/heriberto-cuayahuitl/>Heriberto Cuayáhuitl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1054><div class="card-body p-3 small">In recent years many <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> have been proposed to solve Reading Comprehension (RC) tasks. Most of these models suffer from reasoning over long documents and do not trivially generalize to cases where the answer is not present as a span in a given document. We present a novel neural-based architecture that is capable of extracting relevant regions based on a given question-document pair and generating a well-formed answer. To show the effectiveness of our architecture, we conducted several experiments on the recently proposed and challenging RC dataset &#8216;NarrativeQA&#8217;. The proposed <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> outperforms state-of-the-art results by 12.62 % (ROUGE-L) relative improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305205847 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1055" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1055/>Adaptive Document Retrieval for Deep Question Answering</a></strong><br><a href=/people/b/bernhard-kratzwald/>Bernhard Kratzwald</a>
|
<a href=/people/s/stefan-feuerriegel/>Stefan Feuerriegel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1055><div class="card-body p-3 small">State-of-the-art systems in deep question answering proceed as follows : (1)an initial <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a> selects relevant documents, which (2) are then processed by a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> in order to extract the final answer. Yet the exact interplay between both components is poorly understood, especially concerning the number of candidate documents that should be retrieved. We show that choosing a static number of documents-as used in prior research-suffers from a noise-information trade-off and yields suboptimal results. As a remedy, we propose an adaptive document retrieval model. This learns the optimal candidate number for <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, conditional on the size of the corpus and the query. We report extensive experimental results showing that our adaptive approach outperforms state-of-the-art methods on multiple benchmark datasets, as well as in the context of corpora with variable sizes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305196498 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1056/>Why is unsupervised alignment of English embeddings from different algorithms so hard?<span class=acl-fixed-case>E</span>nglish embeddings from different algorithms so hard?</a></strong><br><a href=/people/m/mareike-hartmann/>Mareike Hartmann</a>
|
<a href=/people/y/yova-kementchedjhieva/>Yova Kementchedjhieva</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1056><div class="card-body p-3 small">This paper presents a challenge to the community : Generative adversarial networks (GANs) can perfectly align independent English word embeddings induced using the same <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, based on distributional information alone ; but fails to do so, for two different embeddings algorithms. Why is that? We believe understanding why, is key to understand both modern word embedding algorithms and the limitations and instability dynamics of GANs. This paper shows that (a) in all these cases, where alignment fails, there exists a linear transform between the two embeddings (so algorithm biases do not lead to non-linear differences), and (b) similar effects can not easily be obtained by varying hyper-parameters. One plausible suggestion based on our initial experiments is that the differences in the inductive biases of the embedding algorithms lead to an optimization landscape that is riddled with <a href=https://en.wikipedia.org/wiki/Local_optimum>local optima</a>, leading to a very small basin of convergence, but we present this more as a challenge paper than a technical contribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305196755 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1057/>Quantifying Context Overlap for Training Word Embeddings</a></strong><br><a href=/people/y/yimeng-zhuang/>Yimeng Zhuang</a>
|
<a href=/people/j/jinghui-xie/>Jinghui Xie</a>
|
<a href=/people/y/yinhe-zheng/>Yinhe Zheng</a>
|
<a href=/people/x/xuan-zhu/>Xuan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1057><div class="card-body p-3 small">Most <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for learning <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> are trained based on the context information of words, more precisely first order co-occurrence relations. In this paper, a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> is designed to estimate second order co-occurrence relations based on context overlap. The estimated values are further used as the augmented data to enhance the learning of word embeddings by joint training with existing neural word embedding models. Experimental results show that better word vectors can be obtained for word similarity tasks and some downstream NLP tasks by the enhanced approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305197006 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1058/>Neural Latent Relational Analysis to Capture Lexical Semantic Relations in a Vector Space</a></strong><br><a href=/people/k/koki-washio/>Koki Washio</a>
|
<a href=/people/t/tsuneaki-kato/>Tsuneaki Kato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1058><div class="card-body p-3 small">Capturing the semantic relations of words in a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> contributes to many natural language processing tasks. One promising approach exploits lexico-syntactic patterns as features of word pairs. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> of this pattern-based approach, neural latent relational analysis (NLRA). NLRA can generalize co-occurrences of word pairs and lexico-syntactic patterns, and obtain embeddings of the word pairs that do not co-occur. This overcomes the critical data sparseness problem encountered in previous pattern-based models. Our experimental results on measuring relational similarity demonstrate that NLRA outperforms the previous pattern-based models. In addition, when combined with a vector offset model, NLRA achieves a performance comparable to that of the state-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> that exploits additional semantic relational data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305197257 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1059" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1059/>Generalizing Word Embeddings using Bag of Subwords</a></strong><br><a href=/people/j/jinman-zhao/>Jinman Zhao</a>
|
<a href=/people/s/sidharth-mudgal/>Sidharth Mudgal</a>
|
<a href=/people/y/yingyu-liang/>Yingyu Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1059><div class="card-body p-3 small">We approach the problem of generalizing pre-trained word embeddings beyond fixed-size vocabularies without using additional <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>. We propose a subword-level word vector generation model that views <a href=https://en.wikipedia.org/wiki/Word_(computer_science)>words</a> as bags of character n-grams. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is simple, fast to train and provides good vectors for rare or unseen words. Experiments show that our model achieves state-of-the-art performances in English word similarity task and in joint prediction of part-of-speech tag and morphosyntactic attributes in 23 languages, suggesting our model&#8217;s ability in capturing the relationship between words&#8217; textual representations and their embeddings.<tex-math>n</tex-math>-grams. The model is simple, fast to train and provides good vectors for rare or unseen words. Experiments show that our model achieves state-of-the-art performances in English word similarity task and in joint prediction of part-of-speech tag and morphosyntactic attributes in 23 languages, suggesting our model&#8217;s ability in capturing the relationship between words&#8217; textual representations and their embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305197464 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1060" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1060/>Neural Metaphor Detection in Context</a></strong><br><a href=/people/g/ge-gao/>Ge Gao</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1060><div class="card-body p-3 small">We present end-to-end neural models for detecting metaphorical word use in context. We show that relatively standard BiLSTM models which operate on complete sentences work well in this setting, in comparison to previous work that used more restricted forms of linguistic context. These models establish a new state-of-the-art on existing verb metaphor detection benchmarks, and show strong performance on jointly predicting the <a href=https://en.wikipedia.org/wiki/Metaphor>metaphoricity</a> of all words in a running text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305211701 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1061" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1061/>Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging</a></strong><br><a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/z/zeljko-agic/>Željko Agić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1061><div class="card-body p-3 small">a cross-lingual neural part-of-speech tagger that learns from disparate sources of distant supervision, and realistically scales to hundreds of low-resource languages. The model exploits annotation projection, <a href=https://en.wikipedia.org/wiki/Instance_selection>instance selection</a>, tag dictionaries, morphological lexicons, and <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representations</a>, all in a uniform framework. The approach is simple, yet surprisingly effective, resulting in a new state of the art without access to any gold annotated data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305212477 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1064/>Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance Intent Classification</a></strong><br><a href=/people/r/ryo-masumura/>Ryo Masumura</a>
|
<a href=/people/y/yusuke-shinohara/>Yusuke Shinohara</a>
|
<a href=/people/r/ryuichiro-higashinaka/>Ryuichiro Higashinaka</a>
|
<a href=/people/y/yushi-aono/>Yushi Aono</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1064><div class="card-body p-3 small">This paper proposes an adversarial training method for the multi-task and multi-lingual joint modeling needed for utterance intent classification. In joint modeling, <a href=https://en.wikipedia.org/wiki/Common_knowledge_(logic)>common knowledge</a> can be efficiently utilized among multiple tasks or multiple languages. This is achieved by introducing both language-specific networks shared among different tasks and task-specific networks shared among different languages. However, the shared networks are often specialized in majority tasks or languages, so performance degradation must be expected for some minor data sets. In order to improve the invariance of shared networks, the proposed method introduces both language-specific task adversarial networks and task-specific language adversarial networks ; both are leveraged for purging the task or language dependencies of the shared networks. The effectiveness of the adversarial training proposal is demonstrated using Japanese and English data sets for three different utterance intent classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1069 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306360792 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1069/>Hybrid Neural Attention for Agreement / Disagreement Inference in Online Debates</a></strong><br><a href=/people/d/di-chen/>Di Chen</a>
|
<a href=/people/j/jiachen-du/>Jiachen Du</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/r/ruifeng-xu/>Ruifeng Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1069><div class="card-body p-3 small">Inferring the agreement / disagreement relation in <a href=https://en.wikipedia.org/wiki/Debate>debates</a>, especially in online debates, is one of the fundamental tasks in argumentation mining. The expressions of agreement / disagreement usually rely on argumentative expressions in text as well as interactions between participants in debates. Previous works usually lack the capability of jointly modeling these two factors. To alleviate this problem, this paper proposes a hybrid neural attention model which combines self and cross attention mechanism to locate salient part from textual context and interaction between users. Experimental results on three (dis)agreement inference datasets show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1071/>A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional Conversation Generation</a></strong><br><a href=/people/j/jingyuan-li/>Jingyuan Li</a>
|
<a href=/people/x/xiao-sun/>Xiao Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1071><div class="card-body p-3 small">Traditional neural language models tend to generate generic replies with poor logic and no <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>. In this paper, a syntactically constrained bidirectional-asynchronous approach for emotional conversation generation (E-SCBA) is proposed to address this issue. In our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, pre-generated emotion keywords and topic keywords are asynchronously introduced into the process of decoding. It is much different from most existing <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> which generate replies from the first word to the last. Through experiments, the results indicate that our approach not only improves the diversity of replies, but gains a boost on both <a href=https://en.wikipedia.org/wiki/Logic>logic</a> and emotion compared with <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1072/>Auto-Dialabel : Labeling Dialogue Data with <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>Unsupervised Learning</a></a></strong><br><a href=/people/c/chen-shi/>Chen Shi</a>
|
<a href=/people/q/qi-chen/>Qi Chen</a>
|
<a href=/people/l/lei-sha/>Lei Sha</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a>
|
<a href=/people/l/lintao-zhang/>Lintao Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1072><div class="card-body p-3 small">The lack of <a href=https://en.wikipedia.org/wiki/Data_(computing)>labeled data</a> is one of the main challenges when building a task-oriented dialogue system. Existing dialogue datasets usually rely on human labeling, which is expensive, limited in size, and in low coverage. In this paper, we instead propose our framework auto-dialabel to automatically cluster the dialogue intents and slots. In this framework, we collect a set of context features, leverage an <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoder</a> for feature assembly, and adapt a dynamic hierarchical clustering method for intent and slot labeling. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> can promote human labeling cost to a great extent, achieve good intent clustering accuracy (84.1 %), and provide reasonable and instructive slot labeling results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1073/>Extending Neural Generative Conversational Model using External Knowledge Sources</a></strong><br><a href=/people/p/prasanna-parthasarathi/>Prasanna Parthasarathi</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1073><div class="card-body p-3 small">The use of connectionist approaches in conversational agents has been progressing rapidly due to the availability of large corpora. However current generative dialogue models often lack <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> and are content poor. This work proposes an architecture to incorporate unstructured knowledge sources to enhance the next utterance prediction in chit-chat type of generative dialogue models. We focus on Sequence-to-Sequence (Seq2Seq) conversational agents trained with the Reddit News dataset, and consider incorporating external knowledge from Wikipedia summaries as well as from the NELL knowledge base. Our experiments show faster training time and improved <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> when leveraging <a href=https://en.wikipedia.org/wiki/Knowledge>external knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1075.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1075 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1075 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1075" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1075/>An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation</a></strong><br><a href=/people/l/liangchen-luo/>Liangchen Luo</a>
|
<a href=/people/j/jingjing-xu/>Jingjing Xu</a>
|
<a href=/people/j/junyang-lin/>Junyang Lin</a>
|
<a href=/people/q/qi-zeng/>Qi Zeng</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1075><div class="card-body p-3 small">Generating semantically coherent responses is still a major challenge in dialogue generation. Different from conventional text generation tasks, the mapping between inputs and responses in conversations is more complicated, which highly demands the understanding of utterance-level semantic dependency, a relation between the whole meanings of inputs and outputs. To address this problem, we propose an Auto-Encoder Matching (AEM) model to learn such dependency. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> contains two <a href=https://en.wikipedia.org/wiki/Auto-encoder>auto-encoders</a> and one mapping module. The auto-encoders learn the semantic representations of inputs and responses, and the mapping module learns to connect the utterance-level representations. Experimental results from automatic and human evaluations demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is capable of generating responses of high coherence and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> compared to baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1076.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1076" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1076/>A Dataset for Document Grounded Conversations</a></strong><br><a href=/people/k/kangyan-zhou/>Kangyan Zhou</a>
|
<a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1076><div class="card-body p-3 small">This paper introduces a document grounded dataset for <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a>. We define Document Grounded Conversations as conversations that are about the contents of a specified document. In this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> the specified documents were Wikipedia articles about popular movies. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> contains 4112 conversations with an average of 21.43 turns per conversation. This positions this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to not only provide a relevant chat history while generating responses but also provide a source of information that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> could use. We describe two neural architectures that provide benchmark performance on the task of generating the next response. We also evaluate our models for <a href=https://en.wikipedia.org/wiki/Engagement>engagement</a> and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, and find that the information from the document helps in generating more engaging and fluent responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1079" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1079/>Using active learning to expand training data for implicit discourse relation recognition</a></strong><br><a href=/people/y/yang-xu/>Yang Xu</a>
|
<a href=/people/y/yu-hong/>Yu Hong</a>
|
<a href=/people/h/huibin-ruan/>Huibin Ruan</a>
|
<a href=/people/j/jianmin-yao/>Jianmin Yao</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1079><div class="card-body p-3 small">We tackle discourse-level relation recognition, a problem of determining semantic relations between text spans. Implicit relation recognition is challenging due to the lack of explicit relational clues. The increasingly popular <a href=https://en.wikipedia.org/wiki/Neural_network>neural network techniques</a> have been proven effective for semantic encoding, whereby widely employed to boost semantic relation discrimination. However, learning to predict semantic relations at a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep level</a> heavily relies on a great deal of training data, but the scale of the publicly available data in this field is limited. In this paper, we follow Rutherford and Xue (2015) to expand the training data set using the corpus of explicitly-related arguments, by arbitrarily dropping the overtly presented discourse connectives. On the basis, we carry out an experiment of <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling</a>, in which a simple active learning approach is used, so as to take the informative instances for data expansion. The goal is to verify whether the selective use of external data not only reduces the time consumption of <a href=https://en.wikipedia.org/wiki/Retraining>retraining</a> but also ensures a better <a href=https://en.wikipedia.org/wiki/System>system</a> performance. Using the expanded training data, we retrain a convolutional neural network (CNN) based classifer which is a simplified version of Qin et al. (2016)&#8217;s stacking gated relation recognizer. Experimental results show that expanding the training set with small-scale carefully-selected external data yields substantial performance gain, with the improvements of about 4 % for <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and 3.6 % for <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>. This allows a <a href=https://en.wikipedia.org/wiki/Weak_classifier>weak classifier</a> to achieve a comparable performance against the state-of-the-art systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1081.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1081" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1081/>BLEU is Not Suitable for the Evaluation of Text Simplification<span class=acl-fixed-case>BLEU</span> is Not Suitable for the Evaluation of Text Simplification</a></strong><br><a href=/people/e/elior-sulem/>Elior Sulem</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/a/ari-rappoport/>Ari Rappoport</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1081><div class="card-body p-3 small">BLEU is widely considered to be an informative metric for text-to-text generation, including Text Simplification (TS). TS includes both lexical and structural aspects. In this paper we show that <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> is not suitable for the evaluation of sentence splitting, the major structural simplification operation. We manually compiled a sentence splitting gold standard corpus containing multiple structural paraphrases, and performed a correlation analysis with human judgments. We find low or no correlation between <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> often negatively correlates with <a href=https://en.wikipedia.org/wiki/Simplicity>simplicity</a>, essentially penalizing simpler sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1082 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1082/>S2SPMN : A Simple and Effective Framework for Response Generation with Relevant Information<span class=acl-fixed-case>S</span>2<span class=acl-fixed-case>SPMN</span>: A Simple and Effective Framework for Response Generation with Relevant Information</a></strong><br><a href=/people/j/jiaxin-pei/>Jiaxin Pei</a>
|
<a href=/people/c/chenliang-li/>Chenliang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1082><div class="card-body p-3 small">How to generate relevant and informative responses is one of the core topics in response generation area. Following the task formulation of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, previous works mainly consider response generation task as a mapping from a source sentence to a target sentence. To realize this mapping, existing works tend to design intuitive but complex models. However, the relevant information existed in large dialogue corpus is mainly overlooked. In this paper, we propose Sequence to Sequence with Prototype Memory Network (S2SPMN) to exploit the relevant information provided by the large dialogue corpus to enhance response generation. Specifically, we devise two simple approaches in S2SPMN to select the relevant information (named prototypes) from the dialogue corpus. These <a href=https://en.wikipedia.org/wiki/Prototype>prototypes</a> are then saved into prototype memory network (PMN). Furthermore, a hierarchical attention mechanism is devised to extract the semantic information from the PMN to assist the response generation process. Empirical studies reveal the advantage of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> over several classical and strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1083.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1083" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1083/>Improving Reinforcement Learning Based Image Captioning with Natural Language Prior</a></strong><br><a href=/people/t/tszhang-guo/>Tszhang Guo</a>
|
<a href=/people/s/shiyu-chang/>Shiyu Chang</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/k/kun-bai/>Kun Bai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1083><div class="card-body p-3 small">Recently, Reinforcement Learning (RL) approaches have demonstrated advanced performance in image captioning by directly optimizing the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> used for testing. However, this shaped reward introduces <a href=https://en.wikipedia.org/wiki/Learning_bias>learning biases</a>, which reduces the readability of generated text. In addition, the large sample space makes training unstable and slow. To alleviate these issues, we propose a simple coherent solution that constrains the action space using an n-gram language prior. Quantitative and qualitative evaluations on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> show that RL with the simple add-on module performs favorably against its counterpart in terms of both <a href=https://en.wikipedia.org/wiki/Readability>readability</a> and <a href=https://en.wikipedia.org/wiki/Speed_of_convergence>speed of convergence</a>. Human evaluation results show that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is more human readable and graceful. The <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> will become publicly available upon the acceptance of the paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1084/>Training for Diversity in Image Paragraph Captioning</a></strong><br><a href=/people/l/luke-melas-kyriazi/>Luke Melas-Kyriazi</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a>
|
<a href=/people/g/george-han/>George Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1084><div class="card-body p-3 small">Image paragraph captioning models aim to produce detailed descriptions of a source image. These <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> use similar techniques as standard image captioning models, but they have encountered issues in text generation, notably a lack of diversity between sentences, that have limited their effectiveness. In this work, we consider applying sequence-level training for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We find that standard self-critical training produces poor results, but when combined with an integrated penalty on trigram repetition produces much more diverse paragraphs. This simple training approach improves on the best result on the Visual Genome paragraph captioning dataset from 16.9 to 30.6 CIDEr, with gains on <a href=https://en.wikipedia.org/wiki/METEOR>METEOR</a> and <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> as well, without requiring any architectural changes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1088/>Neural Latent Extractive Document Summarization</a></strong><br><a href=/people/x/xingxing-zhang/>Xingxing Zhang</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1088><div class="card-body p-3 small">Extractive summarization models need sentence level labels, which are usually created with rule-based methods since most summarization datasets only have document summary pairs. These labels might be suboptimal. We propose a latent variable extractive model, where sentences are viewed as <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variables</a> and sentences with activated variables are used to infer gold summaries. During <a href=https://en.wikipedia.org/wiki/Training>training</a>, the loss can come directly from gold summaries. Experiments on CNN / Dailymail dataset show our latent extractive model outperforms a strong extractive baseline trained on rule-based labels and also performs competitively with several recent models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1089/>On the Abstractiveness of Neural Document Summarization</a></strong><br><a href=/people/f/fang-fang-zhang/>Fangfang Zhang</a>
|
<a href=/people/j/jin-ge-yao/>Jin-ge Yao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1089><div class="card-body p-3 small">Many modern neural document summarization systems based on encoder-decoder networks are designed to produce abstractive summaries. We attempted to verify the degree of abstractiveness of modern neural abstractive summarization systems by calculating overlaps in terms of various types of units. Upon the observation that many abstractive systems tend to be near-extractive in practice, we also implemented a pure copy system, which achieved comparable results as abstractive summarizers while being far more computationally efficient. These findings suggest the possibility for future efforts towards more efficient <a href=https://en.wikipedia.org/wiki/System>systems</a> that could better utilize the vocabulary in the original document.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1090 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1090/>Automatic Essay Scoring Incorporating Rating Schema via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/y/yucheng-wang/>Yucheng Wang</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/y/yaqian-zhou/>Yaqian Zhou</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1090><div class="card-body p-3 small">Automatic essay scoring (AES) is the task of assigning grades to essays without <a href=https://en.wikipedia.org/wiki/Interference_(communication)>human interference</a>. Existing systems for AES are typically trained to predict the score of each single essay at a time without considering the rating schema. In order to address this issue, we propose a reinforcement learning framework for essay scoring that incorporates quadratic weighted kappa as guidance to optimize the scoring system. Experiment results on benchmark datasets show the effectiveness of our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1091 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1091/>Identifying Well-formed Natural Language Questions</a></strong><br><a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/d/dipanjan-das/>Dipanjan Das</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1091><div class="card-body p-3 small">Understanding search queries is a hard problem as it involves dealing with word salad text ubiquitously issued by users. However, if a query resembles a well-formed question, a natural language processing pipeline is able to perform more accurate interpretation, thus reducing downstream compounding errors. Hence, identifying whether or not a query is well formed can enhance <a href=https://en.wikipedia.org/wiki/Query_understanding>query understanding</a>. Here, we introduce a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> of identifying a well-formed natural language question. We construct and release a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 25,100 publicly available questions classified into <a href=https://en.wikipedia.org/wiki/Well-formedness>well-formed and non-wellformed categories</a> and report an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 70.7 % on the test set. We also show that our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> can be used to improve the performance of neural sequence-to-sequence models for generating questions for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1092 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305197775 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1092/>Self-Governing Neural Networks for On-Device Short Text Classification</a></strong><br><a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1092><div class="card-body p-3 small">Deep neural networks reach state-of-the-art performance for wide range of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> and speech applications. Yet, one of the biggest challenges is running these complex networks on devices such as <a href=https://en.wikipedia.org/wiki/Mobile_phone>mobile phones</a> or <a href=https://en.wikipedia.org/wiki/Smartwatch>smart watches</a> with tiny memory footprint and low computational capacity. We propose on-device Self-Governing Neural Networks (SGNNs), which learn compact projection vectors with local sensitive hashing. The key advantage of SGNNs over existing work is that they surmount the need for pre-trained word embeddings and <a href=https://en.wikipedia.org/wiki/Complex_network>complex networks</a> with huge parameters. We conduct extensive evaluation on dialog act classification and show significant improvement over state-of-the-art results. Our findings show that SGNNs are effective at capturing low-dimensional semantic text representations, while maintaining high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1093 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1093.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1093" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1093/>HFT-CNN : Learning Hierarchical Category Structure for Multi-label Short Text Categorization<span class=acl-fixed-case>HFT</span>-<span class=acl-fixed-case>CNN</span>: Learning Hierarchical Category Structure for Multi-label Short Text Categorization</a></strong><br><a href=/people/k/kazuya-shimura/>Kazuya Shimura</a>
|
<a href=/people/j/jiyi-li/>Jiyi Li</a>
|
<a href=/people/f/fumiyo-fukumoto/>Fumiyo Fukumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1093><div class="card-body p-3 small">We focus on the multi-label categorization task for short texts and explore the use of a hierarchical structure (HS) of categories. In contrast to the existing work using non-hierarchical flat model, the method leverages the <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchical relations</a> between the pre-defined categories to tackle the data sparsity problem. The lower the HS level, the less the <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> performance. Because the number of training data per category in a lower level is much smaller than that in an upper level. We propose an approach which can effectively utilize the data in the upper levels to contribute the <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> in the lower levels by applying the Convolutional Neural Network (CNN) with a fine-tuning technique. The results using two benchmark datasets show that proposed method, Hierarchical Fine-Tuning based CNN (HFT-CNN) is competitive with the state-of-the-art CNN based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1094 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1094.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1094" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1094/>A Hierarchical Neural Attention-based Text Classifier</a></strong><br><a href=/people/k/koustuv-sinha/>Koustuv Sinha</a>
|
<a href=/people/y/yue-dong/>Yue Dong</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/d/derek-ruths/>Derek Ruths</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1094><div class="card-body p-3 small">Deep neural networks have been displaying superior performance over traditional <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifiers</a> in <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. They learn to extract useful <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> automatically when sufficient amount of data is presented. However, along with the growth in the number of documents comes the increase in the number of categories, which often results in poor performance of the multiclass classifiers. In this work, we use external knowledge in the form of topic category taxonomies to aide the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> by introducing a deep hierarchical neural attention-based classifier. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better than or comparable to state-of-the-art hierarchical models at significantly lower <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> while maintaining high <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1096" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1096/>Coherence-Aware Neural Topic Modeling</a></strong><br><a href=/people/r/ran-ding/>Ran Ding</a>
|
<a href=/people/r/ramesh-nallapati/>Ramesh Nallapati</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1096><div class="card-body p-3 small">Topic models are evaluated based on their ability to describe documents well (i.e. low perplexity) and to produce topics that carry coherent semantic meaning. In <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a> so far, <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> is a direct optimization target. However, topic coherence, owing to its challenging computation, is not optimized for and is only evaluated after training. In this work, under a neural variational inference framework, we propose methods to incorporate a topic coherence objective into the training process. We demonstrate that such a coherence-aware topic model exhibits a similar level of <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> as baseline models but achieves substantially higher topic coherence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1097/>Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models</a></strong><br><a href=/people/d/daniel-watson/>Daniel Watson</a>
|
<a href=/people/n/nasser-zalmout/>Nasser Zalmout</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1097><div class="card-body p-3 small">Text normalization is an important enabling technology for several NLP tasks. Recently, neural-network-based approaches have outperformed well-established <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in this task. However, in languages other than English, there has been little exploration in this direction. Both the scarcity of annotated data and the complexity of the language increase the difficulty of the problem. To address these challenges, we use a sequence-to-sequence model with character-based attention, which in addition to its self-learned character embeddings, uses word embeddings pre-trained with an approach that also models subword information. This provides the neural model with access to more linguistic information especially suitable for <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a>, without large parallel corpora. We show that providing the model with word-level features bridges the gap for the neural network approach to achieve a state-of-the-art F1 score on a standard Arabic language correction shared task dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1099/>Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents</a></strong><br><a href=/people/a/abhijith-athreya-mysore-gopinath/>Abhijith Athreya Mysore Gopinath</a>
|
<a href=/people/s/shomir-wilson/>Shomir Wilson</a>
|
<a href=/people/n/norman-sadeh/>Norman Sadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1099><div class="card-body p-3 small">The text in many web documents is organized into a hierarchy of section titles and corresponding prose content, a structure which provides potentially exploitable information on discourse structure and <a href=https://en.wikipedia.org/wiki/Topicality>topicality</a>. However, this <a href=https://en.wikipedia.org/wiki/Organization>organization</a> is generally discarded during text collection, and collecting it is not straightforward : the same visual organization can be implemented in a myriad of different ways in the underlying <a href=https://en.wikipedia.org/wiki/HTML>HTML</a>. To remedy this, we present a flexible system for automatically extracting the hierarchical section titles and prose organization of web documents irrespective of differences in HTML representation. This system uses features from <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> and <a href=https://en.wikipedia.org/wiki/Markup_language>markup</a> to build two models which classify HTML text into section titles and prose text. When tested on three different domains of web text, our domain-independent system achieves an overall <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> of 0.82 and a recall of 0.98. The domain-dependent variation produces very high <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> (0.99) at the expense of <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>recall</a> (0.75). These results exhibit a robust level of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> suitable for enhancing <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, and <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1100 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1100.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305206127 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1100/>SwitchOut : an Efficient Data Augmentation Algorithm for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a><span class=acl-fixed-case>S</span>witch<span class=acl-fixed-case>O</span>ut: an Efficient Data Augmentation Algorithm for Neural Machine Translation</a></strong><br><a href=/people/x/xinyi-wang/>Xinyi Wang</a>
|
<a href=/people/h/hieu-pham/>Hieu Pham</a>
|
<a href=/people/z/zihang-dai/>Zihang Dai</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1100><div class="card-body p-3 small">In this work, we examine methods for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>text-based tasks</a> such as <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a>. We formulate the design of a data augmentation policy with desirable properties as an <a href=https://en.wikipedia.org/wiki/Optimization_problem>optimization problem</a>, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT : randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> is included in the appendix.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305206655 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1102/>Decipherment of Substitution Ciphers with Neural Language Models</a></strong><br><a href=/people/n/nishant-kambhatla/>Nishant Kambhatla</a>
|
<a href=/people/a/anahita-mansouri-bigvand/>Anahita Mansouri Bigvand</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1102><div class="card-body p-3 small">Decipherment of homophonic substitution ciphers using <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> is a well-studied task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram language models. The most widely used technique is the use of <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> with n-gram language models proposed by Nuhn et al.(2013). We propose a beam search algorithm that scores the entire candidate plaintext at each step of the <a href=https://en.wikipedia.org/wiki/Decipherment>decipherment</a> using a neural language model. We augment <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> with a novel rest cost estimation that exploits the prediction power of a neural language model. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging <a href=https://en.wikipedia.org/wiki/Cipher>ciphers</a> such as the <a href=https://en.wikipedia.org/wiki/Beale_cipher>Beale cipher</a> we provide significantly better error rates with much smaller beam sizes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305207187 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1103" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1103/>Rapid Adaptation of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> to New Languages</a></strong><br><a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/j/junjie-hu/>Junjie Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1103><div class="card-body p-3 small">This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual seed models, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of similar-language regularization, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305207608 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1104/>Compact Personalized Models for Neural Machine Translation</a></strong><br><a href=/people/j/joern-wuebker/>Joern Wuebker</a>
|
<a href=/people/p/patrick-simianer/>Patrick Simianer</a>
|
<a href=/people/j/john-denero/>John DeNero</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1104><div class="card-body p-3 small">We propose and compare methods for gradient-based domain adaptation of self-attentive neural machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a> via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecturecombining a state-of-the-art self-attentive model with compact domain adaptationprovides high quality personalized machine translation that is both space and time efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1105 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305197775 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1105/>Self-Governing Neural Networks for On-Device Short Text Classification</a></strong><br><a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1105><div class="card-body p-3 small">Deep neural networks reach state-of-the-art performance for wide range of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> and speech applications. Yet, one of the biggest challenges is running these complex networks on devices such as <a href=https://en.wikipedia.org/wiki/Mobile_phone>mobile phones</a> or <a href=https://en.wikipedia.org/wiki/Smartwatch>smart watches</a> with tiny memory footprint and low computational capacity. We propose on-device Self-Governing Neural Networks (SGNNs), which learn compact projection vectors with local sensitive hashing. The key advantage of SGNNs over existing work is that they surmount the need for pre-trained word embeddings and <a href=https://en.wikipedia.org/wiki/Complex_network>complex networks</a> with huge parameters. We conduct extensive evaluation on dialog act classification and show significant improvement over state-of-the-art results. Our findings show that SGNNs are effective at capturing low-dimensional semantic text representations, while maintaining high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305198062 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1106/>Supervised Domain Enablement Attention for Personalized Domain Classification</a></strong><br><a href=/people/j/joo-kyung-kim/>Joo-Kyung Kim</a>
|
<a href=/people/y/young-bum-kim/>Young-Bum Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1106><div class="card-body p-3 small">In large-scale domain classification for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, leveraging each user&#8217;s domain enablement information, which refers to the preferred or authenticated domains by the user, with attention mechanism has been shown to improve the overall domain classification performance. In this paper, we propose a supervised enablement attention mechanism, which utilizes sigmoid activation for the <a href=https://en.wikipedia.org/wiki/Attention>attention weighting</a> so that the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> can be computed with more expressive power without the weight sum constraint of softmax attention. The attention weights are explicitly encouraged to be similar to the corresponding elements of the output one-hot vector, and self-distillation is used to leverage the attention information of the other enabled domains. By evaluating on the actual utterances from a large-scale IPDA, we show that our approach significantly improves domain classification performance</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1108.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305198410 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1108/>Towards Dynamic Computation Graphs via Sparse Latent Structure</a></strong><br><a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1108><div class="card-body p-3 small">Deep NLP models benefit from underlying structures in the datae.g., parse treestypically extracted using off-the-shelf <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. Recent attempts to jointly learn the latent structure encounter a tradeoff : either make factorization assumptions that limit expressiveness, or sacrifice end-to-end differentiability. Using the recently proposed SparseMAP inference, which retrieves a sparse distribution over latent structures, we propose a novel approach for end-to-end learning of latent structure predictors jointly with a downstream predictor. To the best of our knowledge, our method is the first to enable unrestricted dynamic computation graph construction from the global latent structure, while maintaining <a href=https://en.wikipedia.org/wiki/Derivative>differentiability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305198501 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1109/>Convolutional Neural Networks with Recurrent Neural Filters</a></strong><br><a href=/people/y/yi-yang/>Yi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1109><div class="card-body p-3 small">We introduce a class of <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks (CNNs)</a> that utilize <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNNs)</a> as convolution filters. A convolution filter is typically implemented as a linear affine transformation followed by a <a href=https://en.wikipedia.org/wiki/Nonlinear_system>non-linear function</a>, which fails to account for language compositionality. As a result, it limits the use of high-order filters that are often warranted for natural language processing tasks. In this work, we model convolution filters with <a href=https://en.wikipedia.org/wiki/Random-access_memory>RNNs</a> that naturally capture compositionality and long-term dependencies in language. We show that simple CNN architectures equipped with recurrent neural filters (RNFs) achieve results that are on par with the best published ones on the Stanford Sentiment Treebank and two answer sentence selection datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305213468 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1111/>Retrieval-Based Neural Code Generation</a></strong><br><a href=/people/s/shirley-anugrah-hayati/>Shirley Anugrah Hayati</a>
|
<a href=/people/r/raphael-olivier/>Raphael Olivier</a>
|
<a href=/people/p/pravalika-avvaru/>Pravalika Avvaru</a>
|
<a href=/people/p/pengcheng-yin/>Pengcheng Yin</a>
|
<a href=/people/a/anthony-tomasic/>Anthony Tomasic</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1111><div class="card-body p-3 small">In models to generate program source code from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>, representing this <a href=https://en.wikipedia.org/wiki/Source_code>code</a> in a <a href=https://en.wikipedia.org/wiki/Tree_structure>tree structure</a> has been a common approach. However, existing <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> often fail to generate complex code correctly due to a lack of ability to memorize large and complex structures. We introduce RECODE, a method based on subtree retrieval that makes it possible to explicitly reference existing code examples within a neural code generation model. First, we retrieve sentences that are similar to input sentences using a dynamic-programming-based sentence similarity scoring method. Next, we extract n-grams of action sequences that build the associated <a href=https://en.wikipedia.org/wiki/Abstract_syntax_tree>abstract syntax tree</a>. Finally, we increase the probability of actions that cause the retrieved n-gram action subtree to be in the predicted code. We show that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>approach</a> improves the performance on two <a href=https://en.wikipedia.org/wiki/Code_generation_(compiler)>code generation tasks</a> by up to +2.6 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1112 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305213739 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1112/>SQL-to-Text Generation with Graph-to-Sequence Model<span class=acl-fixed-case>SQL</span>-to-Text Generation with Graph-to-Sequence Model</a></strong><br><a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/z/zhiguo-wang/>Zhiguo Wang</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/v/vadim-sheinin/>Vadim Sheinin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1112><div class="card-body p-3 small">Previous work approaches the SQL-to-text generation task using vanilla Seq2Seq models, which may not fully capture the inherent <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph-structured information</a> in <a href=https://en.wikipedia.org/wiki/SQL>SQL query</a>. In this paper, we propose a graph-to-sequence model to encode the global structure information into node embeddings. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can effectively learn the correlation between the SQL query pattern and its interpretation. Experimental results on the WikiSQL dataset and Stackoverflow dataset show that our model outperforms the Seq2Seq and Tree2Seq baselines, achieving the state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305214075 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1113/>Generating Syntactic Paraphrases</a></strong><br><a href=/people/e/emilie-colin/>Emilie Colin</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1113><div class="card-body p-3 small">We study the automatic generation of syntactic paraphrases using four different models for generation : data-to-text generation, text-to-text generation, text reduction and text expansion, We derive training data for each of these tasks from the WebNLG dataset and we show (i) that conditioning generation on syntactic constraints effectively permits the generation of syntactically distinct paraphrases for the same input and (ii) that exploiting different types of input (data, text or data+text) further increases the number of distinct paraphrases that can be generated for a given input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306361340 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1116/>Toward Fast and Accurate Neural Discourse Segmentation</a></strong><br><a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a>
|
<a href=/people/j/jingfeng-yang/>Jingfeng Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1116><div class="card-body p-3 small">Discourse segmentation, which segments texts into Elementary Discourse Units, is a fundamental step in <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse analysis</a>. Previous discourse segmenters rely on complicated hand-crafted features and are not practical in actual use. In this paper, we propose an end-to-end neural segmenter based on BiLSTM-CRF framework. To improve its <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, we address the problem of data insufficiency by transferring a word representation model that is trained on a large corpus. We also propose a restricted self-attention mechanism in order to capture useful information within a neighborhood. Experiments on the RST-DT corpus show that our model is significantly faster than previous methods, while achieving new state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1118.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306362249 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1118/>Cascaded Mutual Modulation for Visual Reasoning</a></strong><br><a href=/people/y/yiqun-yao/>Yiqun Yao</a>
|
<a href=/people/j/jiaming-xu/>Jiaming Xu</a>
|
<a href=/people/f/feng-wang/>Feng Wang</a>
|
<a href=/people/b/bo-xu/>Bo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1118><div class="card-body p-3 small">Visual reasoning is a special visual question answering problem that is multi-step and compositional by nature, and also requires intensive text-vision interactions. We propose CMM : Cascaded Mutual Modulation as a novel end-to-end visual reasoning model. CMM includes a <a href=https://en.wikipedia.org/wiki/Comprehension_(logic)>multi-step comprehension process</a> for both question and image. In each step, we use a Feature-wise Linear Modulation (FiLM) technique to enable textual / visual pipeline to mutually control each other. Experiments show that CMM significantly outperforms most related models, and reach state-of-the-arts on two visual reasoning benchmarks : CLEVR and NLVR, collected from both synthetic and natural languages. Ablation studies confirm the effectiveness of CMM to comprehend <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language logics</a> under the guidence of images. Our code is available at.<url>https://github.com/FlamingHorizon/CMM-VR</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306362292 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1119/>How agents see things : On visual representations in an emergent language game</a></strong><br><a href=/people/d/diane-bouchacourt/>Diane Bouchacourt</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1119><div class="card-body p-3 small">There is growing interest in the <a href=https://en.wikipedia.org/wiki/Language>language</a> developed by agents interacting in emergent-communication settings. Earlier studies have focused on the agents&#8217; symbol usage, rather than on their <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representation of visual input</a>. In this paper, we consider the referential games of Lazaridou et al. (2017), and investigate the representations the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> develop during their evolving interaction. We find that the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> establish successful communication by inducing visual representations that almost perfectly align with each other, but, surprisingly, do not capture the conceptual properties of the objects depicted in the input images. We conclude that, if we care about developing language-like communication systems, we must pay more attention to the visual semantics agents associate to the symbols they use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1121 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1121" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1121/>Put It Back : Entity Typing with Language Model Enhancement</a></strong><br><a href=/people/j/ji-xin/>Ji Xin</a>
|
<a href=/people/h/hao-zhu/>Hao Zhu</a>
|
<a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1121><div class="card-body p-3 small">Entity typing aims to classify semantic types of an entity mention in a specific context. Most existing <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> obtain training data using distant supervision, and inevitably suffer from the problem of noisy labels. To address this issue, we propose <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity typing</a> with language model enhancement. It utilizes a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to measure the compatibility between context sentences and labels, and thereby automatically focuses more on context-dependent labels. Experiments on benchmark datasets demonstrate that our method is capable of enhancing the entity typing model with information from the language model, and significantly outperforms the state-of-the-art baseline. Code and data for this paper can be found from.<url>https://github.com/thunlp/LME</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1123 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1123.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1123/>PubSE : A Hierarchical Model for Publication Extraction from Academic Homepages<span class=acl-fixed-case>P</span>ub<span class=acl-fixed-case>SE</span>: A Hierarchical Model for Publication Extraction from Academic Homepages</a></strong><br><a href=/people/y/yiqing-zhang/>Yiqing Zhang</a>
|
<a href=/people/j/jianzhong-qi/>Jianzhong Qi</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/c/chuandong-yin/>Chuandong Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1123><div class="card-body p-3 small">Publication information in a researcher&#8217;s academic homepage provides insights about the researcher&#8217;s expertise, research interests, and collaboration networks. We aim to extract all the <a href=https://en.wikipedia.org/wiki/Article_(publishing)>publication strings</a> from a given academic homepage. This is a challenging task because the publication strings in different academic homepages may be located at different positions with different structures. To capture the positional and structural diversity, we propose an end-to-end hierarchical model named PubSE based on Bi-LSTM-CRF. We further propose an alternating training method for training the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Experiments on real data show that PubSE outperforms the state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> by up to 11.8 % in <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1126 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1126" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1126/>Effective Use of <a href=https://en.wikipedia.org/wiki/Context_(computing)>Context</a> in Noisy Entity Linking</a></strong><br><a href=/people/d/david-mueller/>David Mueller</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1126><div class="card-body p-3 small">To disambiguate between closely related concepts, entity linking systems need to effectively distill cues from their context, which may be quite noisy. We investigate several techniques for using these <a href=https://en.wikipedia.org/wiki/Sensory_cue>cues</a> in the context of noisy entity linking on <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>short texts</a>. Our starting point is a state-of-the-art attention-based model from prior work ; while this model&#8217;s attention typically identifies context that is topically relevant, it fails to identify some of the most indicative surface strings, especially those exhibiting lexical overlap with the true title. Augmenting the model with convolutional networks over characters still leaves it largely unable to pick up on these cues compared to sparse features that target them directly, indicating that automatically learning how to identify relevant character-level context features is a hard problem. Our final system outperforms past work on the WikilinksNED test set by 2.8 % absolute.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1127.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1127 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1127 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1127" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1127/>Exploiting Contextual Information via Dynamic Memory Network for Event Detection</a></strong><br><a href=/people/s/shaobo-liu/>Shaobo Liu</a>
|
<a href=/people/r/rui-cheng/>Rui Cheng</a>
|
<a href=/people/x/xiaoming-yu/>Xiaoming Yu</a>
|
<a href=/people/x/xueqi-cheng/>Xueqi Cheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1127><div class="card-body p-3 small">The task of event detection involves identifying and categorizing event triggers. Contextual information has been shown effective on the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> which utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> only process the context once. We argue that the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> can be better exploited by processing the context multiple times, allowing the model to perform complex reasoning and to generate better <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context representation</a>, thus improving the overall performance. Meanwhile, dynamic memory network (DMN) has demonstrated promising capability in capturing <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> and has been applied successfully to various <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. In light of the multi-hop mechanism of the DMN to model the context, we propose the trigger detection dynamic memory network (TD-DMN) to tackle the event detection problem. We performed a five-fold cross-validation on the ACE-2005 dataset and experimental results show that the multi-hop mechanism does improve the performance and the proposed model achieves best F1 score compared to the state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1128 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1128.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1128/>Do explanations make VQA models more predictable to a human?<span class=acl-fixed-case>VQA</span> models more predictable to a human?</a></strong><br><a href=/people/a/arjun-chandrasekaran/>Arjun Chandrasekaran</a>
|
<a href=/people/v/viraj-prabhu/>Viraj Prabhu</a>
|
<a href=/people/d/deshraj-yadav/>Deshraj Yadav</a>
|
<a href=/people/p/prithvijit-chattopadhyay/>Prithvijit Chattopadhyay</a>
|
<a href=/people/d/devi-parikh/>Devi Parikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1128><div class="card-body p-3 small">A rich line of research attempts to make <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> more transparent by generating human-interpretable &#8216;explanations&#8217; of their <a href=https://en.wikipedia.org/wiki/Decision-making>decision process</a>, especially for interactive tasks like Visual Question Answering (VQA). In this work, we analyze if existing explanations indeed make a VQA model its responses as well as failures more predictable to a human. Surprisingly, we find that they do not. On the other hand, we find that human-in-the-loop approaches that treat the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> as a black-box do.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1132 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1132/>Translating a Math Word Problem to a <a href=https://en.wikipedia.org/wiki/Expression_tree>Expression Tree</a></a></strong><br><a href=/people/l/lei-wang/>Lei Wang</a>
|
<a href=/people/y/yan-wang/>Yan Wang</a>
|
<a href=/people/d/deng-cai/>Deng Cai</a>
|
<a href=/people/d/dongxiang-zhang/>Dongxiang Zhang</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1132><div class="card-body p-3 small">Sequence-to-sequence (SEQ2SEQ) models have been successfully applied to automatic math word problem solving. Despite its simplicity, a drawback still remains : a math word problem can be correctly solved by more than one equations. This non-deterministic transduction harms the performance of <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation</a>. In this paper, by considering the uniqueness of <a href=https://en.wikipedia.org/wiki/Expression_tree>expression tree</a>, we propose an equation normalization method to normalize the duplicated equations. Moreover, we analyze the performance of three popular SEQ2SEQ models on the math word problem solving. We find that each <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has its own specialty in solving problems, consequently an ensemble model is then proposed to combine their advantages. Experiments on dataset Math23 K show that the <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble model</a> with equation normalization significantly outperforms the previous state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1133 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1133.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1133/>Semantic Linking in <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for Answer Sentence Selection</a></strong><br><a href=/people/m/massimo-nicosia/>Massimo Nicosia</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1133><div class="card-body p-3 small">State-of-the-art <a href=https://en.wikipedia.org/wiki/Social_network>networks</a> that model relations between two pieces of text often use complex architectures and <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. In this paper, instead of focusing on architecture engineering, we take advantage of small amounts of labelled data that model semantic phenomena in text to encode matching features directly in the word representations. This greatly boosts the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of our reference network, while keeping the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> simple and fast to train. Our approach also beats a tree kernel model that uses similar input encodings, and neural models which use advanced attention and compare-aggregate mechanisms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1135 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1135/>Improving the results of string kernels in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and Arabic dialect identification by adapting them to your test set<span class=acl-fixed-case>A</span>rabic dialect identification by adapting them to your test set</a></strong><br><a href=/people/r/radu-tudor-ionescu/>Radu Tudor Ionescu</a>
|
<a href=/people/a/andrei-butnaru/>Andrei M. Butnaru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1135><div class="card-body p-3 small">Recently, <a href=https://en.wikipedia.org/wiki/String_kernel>string kernels</a> have obtained state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification. In this paper, we apply two simple yet effective transductive learning approaches to further improve the results of <a href=https://en.wikipedia.org/wiki/String_kernel>string kernels</a>. The first approach is based on interpreting the pairwise string kernel similarities between samples in the training set and samples in the test set as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Our second approach is a simple self-training method based on two learning iterations. In the first <a href=https://en.wikipedia.org/wiki/Iteration>iteration</a>, a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> is trained on the training set and tested on the test set, as usual. In the second iteration, a number of test samples (to which the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> associated higher confidence scores) are added to the training set for another round of training. However, the ground-truth labels of the added test samples are not necessary. Instead, we use the labels predicted by the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> in the first training iteration. By adapting <a href=https://en.wikipedia.org/wiki/String_kernel>string kernels</a> to the <a href=https://en.wikipedia.org/wiki/Test_set>test set</a>, we report significantly better accuracy rates in English polarity classification and Arabic dialect identification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1137 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1137/>Improving Multi-label Emotion Classification via Sentiment Classification with Dual Attention Transfer Network</a></strong><br><a href=/people/j/jianfei-yu/>Jianfei Yu</a>
|
<a href=/people/l/luis-marujo/>Luís Marujo</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a>
|
<a href=/people/p/pradeep-karuturi/>Pradeep Karuturi</a>
|
<a href=/people/w/william-brendel/>William Brendel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1137><div class="card-body p-3 small">In this paper, we target at improving the performance of multi-label emotion classification with the help of sentiment classification. Specifically, we propose a new transfer learning architecture to divide the sentence representation into two different feature spaces, which are expected to respectively capture the general sentiment words and the other important emotion-specific words via a dual attention mechanism. Experimental results on two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> demonstrate the effectiveness of our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1139 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1139/>Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks</a></strong><br><a href=/people/m/martin-schmitt/>Martin Schmitt</a>
|
<a href=/people/s/simon-steinheber/>Simon Steinheber</a>
|
<a href=/people/k/konrad-schreiber/>Konrad Schreiber</a>
|
<a href=/people/b/benjamin-roth/>Benjamin Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1139><div class="card-body p-3 small">In this work, we propose a new <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for aspect-based sentiment analysis. In contrast to previous approaches, we jointly model the <a href=https://en.wikipedia.org/wiki/Detection_theory>detection of aspects</a> and the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification of their polarity</a> in an end-to-end trainable neural network. We conduct experiments with different neural architectures and word representations on the recent GermEval 2017 dataset. We were able to show considerable performance gains by using the joint modeling approach in all settings compared to pipeline approaches. The combination of a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> and fasttext embeddings outperformed the best submission of the shared task in 2017, establishing a new state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1140 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1140.Attachment.tgz data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1140/>Representing Social Media Users for Sarcasm Detection</a></strong><br><a href=/people/y/y-alex-kolchinski/>Y. Alex Kolchinski</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1140><div class="card-body p-3 small">We explore two methods for representing <a href=https://en.wikipedia.org/wiki/Author>authors</a> in the context of textual sarcasm detection : a Bayesian approach that directly represents authors&#8217; propensities to be sarcastic, and a dense embedding approach that can learn interactions between the author and the text. Using the SARC dataset of Reddit comments, we show that augmenting a bidirectional RNN with these representations improves performance ; the Bayesian approach suffices in homogeneous contexts, whereas the added power of the dense embeddings proves valuable in more diverse ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1141 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1141.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1141/>Syntactical Analysis of the Weaknesses of Sentiment Analyzers</a></strong><br><a href=/people/r/rohil-verma/>Rohil Verma</a>
|
<a href=/people/s/samuel-kim/>Samuel Kim</a>
|
<a href=/people/d/david-walter/>David Walter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1141><div class="card-body p-3 small">We carry out a syntactic analysis of two state-of-the-art sentiment analyzers, Google Cloud Natural Language and Stanford CoreNLP, to assess their classification accuracy on sentences with negative polarity items. We were motivated by the absence of studies investigating <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analyzer</a> performance on sentences with polarity items, a common construct in <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>. Our analysis focuses on two sentential structures : <a href=https://en.wikipedia.org/wiki/Downward_entailment>downward entailment</a> and non-monotone quantifiers ; and demonstrates weaknesses of Google Natural Language and CoreNLP in capturing polarity item information. We describe the particular syntactic phenomenon that these analyzers fail to understand that any ideal <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analyzer</a> must. We also provide a set of 150 test sentences that any ideal <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analyzer</a> must be able to understand.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1142.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1142 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1142 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1142.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1142/>Is Nike female? Exploring the role of <a href=https://en.wikipedia.org/wiki/Sound_symbolism>sound symbolism</a> in predicting brand name gender<span class=acl-fixed-case>N</span>ike female? Exploring the role of sound symbolism in predicting brand name gender</a></strong><br><a href=/people/s/sridhar-moorthy/>Sridhar Moorthy</a>
|
<a href=/people/r/ruth-pogacar/>Ruth Pogacar</a>
|
<a href=/people/s/samin-khan/>Samin Khan</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1142><div class="card-body p-3 small">Are <a href=https://en.wikipedia.org/wiki/Brand>brand names</a> such as Nike female or male? Previous research suggests that the sound of a person&#8217;s first name is associated with the person&#8217;s gender, but no research has tried to use this knowledge to assess the gender of brand names. We present a simple computational approach that uses <a href=https://en.wikipedia.org/wiki/Sound_symbolism>sound symbolism</a> to address this open issue. Consistent with previous research, a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on various linguistic features of name endings predicts human gender with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Applying this model to a data set of over a thousand commercially-traded brands in 17 product categories, our results reveal an overall bias toward male names, cutting across both male-oriented product categories as well as female-oriented categories. In addition, we find variation within categories, suggesting that firms might be seeking to imbue their brands with differentiating characteristics as part of their competitive strategy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1143 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1143/>Improving Large-Scale Fact-Checking using Decomposable Attention Models and Lexical Tagging</a></strong><br><a href=/people/n/nayeon-lee/>Nayeon Lee</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1143><div class="card-body p-3 small">Fact-checking of textual sources needs to effectively extract relevant information from large knowledge bases. In this paper, we extend an existing pipeline approach to better tackle this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. We propose a neural ranker using a decomposable attention model that dynamically selects sentences to achieve promising improvement in evidence retrieval F1 by 38.80 %, with (x65) speedup compared to a TF-IDF method. Moreover, we incorporate lexical tagging methods into our pipeline framework to simplify the tasks and render the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> more generalizable. As a result, our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves promising performance on a large-scale fact extraction and verification dataset with speedup.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1146.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1146 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1146 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1146/>Somm : Into the Model<span class=acl-fixed-case>S</span>omm: Into the Model</a></strong><br><a href=/people/s/shengli-hu/>Shengli Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1146><div class="card-body p-3 small">To what extent could the <a href=https://en.wikipedia.org/wiki/Sommelier>sommelier profession</a>, or wine stewardship, be displaced by <a href=https://en.wikipedia.org/wiki/Algorithm>machine leaning algorithms</a>? There are at least three essential skills that make a qualified <a href=https://en.wikipedia.org/wiki/Sommelier>sommelier</a> : wine theory, blind tasting, and beverage service, as exemplified in the rigorous certification processes of certified sommeliers and above (advanced and master) with the most authoritative body in the industry, the Court of Master Sommelier (hereafter CMS). We propose and train corresponding machine learning models that match these skills, and compare algorithmic results with real data collected from a large group of wine professionals. We find that our machine learning models outperform human sommeliers on most tasks most notably in the section of blind tasting, where hierarchically supervised Latent Dirichlet Allocation outperforms <a href=https://en.wikipedia.org/wiki/Sommelier>sommeliers&#8217; judgment calls</a> by over 6 % in terms of F1-score ; and in the section of beverage service, especially wine and food pairing, a modified Siamese neural network based on BiLSTM achieves better results than <a href=https://en.wikipedia.org/wiki/Sommelier>sommeliers</a> by 2 %. This demonstrates, contrary to popular opinion in the industry, that the sommelier profession is at least to some extent automatable, barring economic (Kleinberg et al., 2017) and psychological (Dietvorst et al., 2015) complications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1148 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1148.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1148/>The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level Predictions</a></strong><br><a href=/people/s/salvatore-giorgi/>Salvatore Giorgi</a>
|
<a href=/people/d/daniel-preotiuc-pietro/>Daniel Preoţiuc-Pietro</a>
|
<a href=/people/a/anneke-buffone/>Anneke Buffone</a>
|
<a href=/people/d/daniel-rieman/>Daniel Rieman</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1148><div class="card-body p-3 small">Nowcasting based on social media text promises to provide unobtrusive and near real-time predictions of community-level outcomes. These outcomes are typically regarding people, but the data is often aggregated without regard to users in the Twitter populations of each community. This paper describes a simple yet effective method for building community-level models using Twitter language aggregated by user. Results on four different U.S. county-level tasks, spanning demographic, health, and psychological outcomes show large and consistent improvements in prediction accuracies (e.g. from Pearson r=.73 to.82 for median income prediction or r=.37 to.47 for life satisfaction prediction) over the standard approach of aggregating all tweets. We make our aggregated and anonymized community-level data, derived from 37 billion tweets over 1 billion of which were mapped to counties, available for research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1149.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1149 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1149 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1149.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305207923 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1149" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1149/>Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</a></strong><br><a href=/people/j/jason-lee/>Jason Lee</a>
|
<a href=/people/e/elman-mansimov/>Elman Mansimov</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1149><div class="card-body p-3 small">We propose a conditional non-autoregressive neural sequence model based on <a href=https://en.wikipedia.org/wiki/Iterative_refinement>iterative refinement</a>. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is designed based on the principles of <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable models</a> and denoising autoencoders, and is generally applicable to any sequence generation task. We extensively evaluate the proposed model on machine translation (En-De and En-Ro) and image caption generation, and observe that it significantly speeds up decoding while maintaining the generation quality comparable to the autoregressive counterpart.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1151 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1151.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305208737 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1151" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1151/>Targeted Syntactic Evaluation of <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a></a></strong><br><a href=/people/r/rebecca-marvin/>Rebecca Marvin</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1151><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> for evaluating the <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a> of the predictions of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. We automatically construct a large number of minimally different pairs of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>English sentences</a>, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena : <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>subject-verb agreement</a>, reflexive anaphora and negative polarity items. We expect a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to assign a higher probability to the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>grammatical sentence</a> than the ungrammatical one. In an experiment using this <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM&#8217;s accuracy, but a large gap remained between its performance and the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>syntax</a> in a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1153 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305209444 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1153" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1153/>Efficient Contextualized Representation : Language Model Pruning for Sequence Labeling</a></strong><br><a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/j/jingbo-shang/>Jingbo Shang</a>
|
<a href=/people/x/xiaotao-gu/>Xiaotao Gu</a>
|
<a href=/people/j/jian-peng/>Jian Peng</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1153><div class="card-body p-3 small">Many efforts have been made to facilitate natural language processing tasks with pre-trained language models (LMs), and brought significant improvements to various applications. To fully leverage the nearly unlimited corpora and capture linguistic information of multifarious levels, large-size LMs are required ; but for a specific task, only parts of these information are useful. Such large-sized LMs, even in the inference stage, may cause heavy computation workloads, making them too time-consuming for large-scale applications. Here we propose to compress bulky LMs while preserving useful information with regard to a specific <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. As different layers of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> keep different information, we develop a layer selection method for <a href=https://en.wikipedia.org/wiki/Mathematical_model>model pruning</a> using sparsity-inducing regularization. By introducing the dense connectivity, we can detach any layer without affecting others, and stretch shallow and wide LMs to be deep and narrow. In <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>model training</a>, LMs are learned with layer-wise dropouts for better robustness. Experiments on two benchmark datasets demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1154 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305198570 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1154" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1154/>Automatic Event Salience Identification</a></strong><br><a href=/people/z/zhengzhong-liu/>Zhengzhong Liu</a>
|
<a href=/people/c/chenyan-xiong/>Chenyan Xiong</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1154><div class="card-body p-3 small">Identifying the salience (i.e. importance) of discourse units is an important task in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>. While <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> play important roles in <a href=https://en.wikipedia.org/wiki/Text_corpus>text documents</a>, little research exists on analyzing their saliency status. This paper empirically studies Event Salience and proposes two salience detection models based on <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>. The <a href=https://en.wikipedia.org/wiki/First_law_of_thermodynamics>first</a> is a feature based salience model that incorporates cohesion among <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse units</a>. The second is a neural model that captures more complex interactions between <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse units</a>. In our new large-scale event salience corpus, both methods significantly outperform the strong frequency baseline, while our neural model further improves the feature based one by a large margin. Our analyses demonstrate that our neural model captures interesting connections between <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>salience</a> and discourse unit relations (e.g., <a href=https://en.wikipedia.org/wiki/Scripting_language>scripts</a> and frame structures).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1158 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305199664 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1158" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1158/>Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms</a></strong><br><a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/h/hang-yang/>Hang Yang</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a>
|
<a href=/people/y/yantao-jia/>Yantao Jia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1158><div class="card-body p-3 small">Traditional approaches to the task of ACE event detection primarily regard multiple events in one sentence as independent ones and recognize them separately by using sentence-level information. However, events in one sentence are usually interdependent and sentence-level information is often insufficient to resolve <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguities</a> for some types of events. This paper proposes a novel framework dubbed as Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA) to solve the two problems simultaneously. Firstly, we propose a hierachical and bias tagging networks to detect multiple events in one sentence collectively. Then, we devise a gated multi-level attention to automatically extract and dynamically fuse the sentence-level and document-level information. The experimental results on the widely used ACE 2005 dataset show that our approach significantly outperforms other state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1159.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1159 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1159 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1159.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305214708 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1159" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1159/>Valency-Augmented Dependency Parsing</a></strong><br><a href=/people/t/tianze-shi/>Tianze Shi</a>
|
<a href=/people/l/lillian-lee/>Lillian Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1159><div class="card-body p-3 small">We present a complete, automated, and efficient approach for utilizing valency analysis in making dependency parsing decisions. It includes extraction of valency patterns, a probabilistic model for tagging these patterns, and a joint decoding process that explicitly considers the number and types of each token&#8217;s syntactic dependents. On 53 treebanks representing 41 languages in the Universal Dependencies data, we find that incorporating valency information yields higher precision and F1 scores on the core arguments (subjects and complements) and functional relations (e.g., auxiliaries) that we employ for <a href=https://en.wikipedia.org/wiki/Valency_(linguistics)>valency analysis</a>. Precision on core arguments improves from 80.87 to 85.43. We further show that our approach can be applied to an ostensibly different formalism and dataset, Tree Adjoining Grammar as extracted from the Penn Treebank ; there, we outperform the previous state-of-the-art labeled attachment score by 0.7. Finally, we explore the potential of extending valency patterns beyond their traditional domain by confirming their helpfulness in improving PP attachment decisions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1160.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1160 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1160 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305215139 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1160" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1160/>Unsupervised Learning of Syntactic Structure with Invertible Neural Projections</a></strong><br><a href=/people/j/junxian-he/>Junxian He</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1160><div class="card-body p-3 small">Unsupervised learning of syntactic structure is typically performed using <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> with <a href=https://en.wikipedia.org/wiki/Latent_variable>discrete latent variables</a> and <a href=https://en.wikipedia.org/wiki/Multinomial_distribution>multinomial parameters</a>. In most cases, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have not leveraged continuous word representations. In this work, we propose a novel <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient <a href=https://en.wikipedia.org/wiki/Exact_inference>exact inference</a> and marginal likelihood computation in our model so long as the <a href=https://en.wikipedia.org/wiki/Prior_probability>prior</a> is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks : part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank</a>, our <a href=https://en.wikipedia.org/wiki/Markov_chain>Markov-structured model</a> surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1164 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306362344 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1164/>Tell-and-Answer : Towards Explainable Visual Question Answering using Attributes and Captions</a></strong><br><a href=/people/q/qing-li/>Qing Li</a>
|
<a href=/people/j/jianlong-fu/>Jianlong Fu</a>
|
<a href=/people/d/dongfei-yu/>Dongfei Yu</a>
|
<a href=/people/t/tao-mei/>Tao Mei</a>
|
<a href=/people/j/jiebo-luo/>Jiebo Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1164><div class="card-body p-3 small">In Visual Question Answering, most existing approaches adopt the <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> of representing an image via pre-trained CNNs, and then using the uninterpretable CNN features in conjunction with the question to predict the answer. Although such end-to-end models might report promising performance, they rarely provide any insight, apart from the answer, into the VQA process. In this work, we propose to break up the end-to-end VQA into two steps : explaining and reasoning, in an attempt towards a more explainable VQA by shedding light on the intermediate results between these two steps. To that end, we first extract <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a> and generate descriptions as explanations for an image. Next, a reasoning module utilizes these explanations in place of the image to infer an answer. The advantages of such a breakdown include : (1) the attributes and captions can reflect what the system extracts from the image, thus can provide some insights for the predicted answer ; (2) these intermediate results can help identify the inabilities of the image understanding or the answer inference part when the predicted answer is wrong. We conduct extensive experiments on a popular VQA dataset and our system achieves comparable performance with the baselines, yet with added benefits of explanability and the inherent ability to further improve with higher quality explanations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1165.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1165 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1165 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1165.Attachment.tgz data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306362379 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1165/>Learning a Policy for Opportunistic Active Learning</a></strong><br><a href=/people/a/aishwarya-padmakumar/>Aishwarya Padmakumar</a>
|
<a href=/people/p/peter-stone/>Peter Stone</a>
|
<a href=/people/r/raymond-mooney/>Raymond Mooney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1165><div class="card-body p-3 small">Active learning identifies data points to label that are expected to be the most useful in improving a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a>. Opportunistic active learning incorporates <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> into <a href=https://en.wikipedia.org/wiki/Interactive_learning>interactive tasks</a> that constrain possible queries during interactions. Prior work has shown that opportunistic active learning can be used to improve grounding of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language descriptions</a> in an interactive object retrieval task. In this work, we use <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> for such an object retrieval task, to learn a policy that effectively trades off task completion with model improvement that would benefit future tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1166 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1166.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306363701 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1166/>RecipeQA : A Challenge Dataset for Multimodal Comprehension of Cooking Recipes<span class=acl-fixed-case>R</span>ecipe<span class=acl-fixed-case>QA</span>: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes</a></strong><br><a href=/people/s/semih-yagcioglu/>Semih Yagcioglu</a>
|
<a href=/people/a/aykut-erdem/>Aykut Erdem</a>
|
<a href=/people/e/erkut-erdem/>Erkut Erdem</a>
|
<a href=/people/n/nazli-ikizler-cinbis/>Nazli Ikizler-Cinbis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1166><div class="card-body p-3 small">Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for multimodal comprehension of cooking recipes. It comprises of approximately 20 K instructional recipes with multiple modalities such as titles, descriptions and aligned set of <a href=https://en.wikipedia.org/wiki/Image>images</a>. With over 36 K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of <a href=https://en.wikipedia.org/wiki/Procedural_knowledge>procedural knowledge</a>. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems. The data and leaderboard are available at.<url>http://hucvl.github.io/recipeqa</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1168 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1168.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306365884 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1168/>Localizing Moments in Video with Temporal Language</a></strong><br><a href=/people/l/lisa-anne-hendricks/>Lisa Anne Hendricks</a>
|
<a href=/people/o/oliver-wang/>Oliver Wang</a>
|
<a href=/people/e/eli-shechtman/>Eli Shechtman</a>
|
<a href=/people/j/josef-sivic/>Josef Sivic</a>
|
<a href=/people/t/trevor-darrell/>Trevor Darrell</a>
|
<a href=/people/b/bryan-russell/>Bryan Russell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1168><div class="card-body p-3 small">Localizing moments in a longer video via <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language queries</a> is a new, challenging task at the intersection of language and video understanding. Though moment localization with <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> is similar to other language and vision tasks like <a href=https://en.wikipedia.org/wiki/Natural_language>natural language object retrieval</a> in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text. We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language. To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of two parts : a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with real videos and template sentences (TEMPO-Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans (TEMPO-Human Language).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1171 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1171/>Weeding out Conventionalized Metaphors : A Corpus of Novel Metaphor Annotations</a></strong><br><a href=/people/e/erik-lan-do-dinh/>Erik-Lân Do Dinh</a>
|
<a href=/people/h/hannah-wieland/>Hannah Wieland</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1171><div class="card-body p-3 small">We encounter <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> every day, but only a few jump out on us and make us stumble. However, little effort has been devoted to investigating more novel metaphors in comparison to general metaphor detection efforts. We attribute this gap primarily to the lack of larger datasets that distinguish between conventionalized, i.e., very common, and novel metaphors. The goal of this paper is to alleviate this situation by introducing a crowdsourced novel metaphor annotation layer for an existing metaphor corpus. Further, we analyze our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and investigate correlations between novelty and features that are typically used in metaphor detection, such as concreteness ratings and more semantic features like the Potential for <a href=https://en.wikipedia.org/wiki/Metaphor>Metaphoricity</a>. Finally, we present a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline approach</a> to assess novelty in metaphors based on our <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1174.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1174 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1174 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1174.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1174/>Disambiguated skip-gram model</a></strong><br><a href=/people/k/karol-grzegorczyk/>Karol Grzegorczyk</a>
|
<a href=/people/m/marcin-kurdziel/>Marcin Kurdziel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1174><div class="card-body p-3 small">We present disambiguated skip-gram : a neural-probabilistic model for learning multi-sense distributed representations of words. Disambiguated skip-gram jointly estimates a skip-gram-like context word prediction model and a <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation model</a>. Unlike previous probabilistic models for learning multi-sense word embeddings, disambiguated skip-gram is end-to-end differentiable and can be interpreted as a simple feed-forward neural network. We also introduce an effective pruning strategy for the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> learned by disambiguated skip-gram. This allows us to control the granularity of representations learned by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. In experimental evaluation disambiguated skip-gram improves state-of-the are results in several word sense induction benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1175 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1175/>Picking Apart Story Salads</a></strong><br><a href=/people/s/su-wang/>Su Wang</a>
|
<a href=/people/e/eric-holgate/>Eric Holgate</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/k/katrin-erk/>Katrin Erk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1175><div class="card-body p-3 small">During <a href=https://en.wikipedia.org/wiki/Natural_disaster>natural disasters</a> and conflicts, information about what happened is often confusing and messy, and distributed across many sources. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. To make this task accessible to neural models, we introduce Story Salads, mixtures of multiple documents that can be generated at scale. By exploiting the <a href=https://en.wikipedia.org/wiki/Wikipedia_hierarchy>Wikipedia hierarchy</a>, we can generate <a href=https://en.wikipedia.org/wiki/Salad_(disambiguation)>salads</a> that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task, and that it is necessary to take into account global context and <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a>.<i>Story Salads</i>, mixtures of multiple documents that can be generated at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task, and that it is necessary to take into account global context and coherence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1176 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1176" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1176/>Dynamic Meta-Embeddings for Improved Sentence Representations</a></strong><br><a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/c/changhan-wang/>Changhan Wang</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1176><div class="card-body p-3 small">While one of the first steps in many NLP systems is selecting what pre-trained word embeddings to use, we argue that such a step is better left for <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to figure out by themselves. To that end, we introduce dynamic meta-embeddings, a simple yet effective method for the supervised learning of embedding ensembles, which leads to state-of-the-art performance within the same model class on a variety of tasks. We subsequently show how the technique can be used to shed new light on the usage of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1177 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1177/>A Probabilistic Model for Joint Learning of Word Embeddings from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>Texts</a> and Images</a></strong><br><a href=/people/m/melissa-ailem/>Melissa Ailem</a>
|
<a href=/people/b/bowen-zhang/>Bowen Zhang</a>
|
<a href=/people/a/aurelien-bellet/>Aurelien Bellet</a>
|
<a href=/people/p/pascal-denis/>Pascal Denis</a>
|
<a href=/people/f/fei-sha/>Fei Sha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1177><div class="card-body p-3 small">Several recent studies have shown the benefits of combining language and perception to infer <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. These <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multimodal approaches</a> either simply combine pre-trained textual and visual representations (e.g. features extracted from convolutional neural networks), or use the latter to bias the learning of textual word embeddings. In this work, we propose a novel probabilistic model to formalize how linguistic and perceptual inputs can work in concert to explain the observed word-context pairs in a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a>. Our approach learns textual and visual representations jointly : latent visual factors couple together a skip-gram model for co-occurrence in linguistic data and a generative latent variable model for visual data. Extensive experimental studies validate the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Concretely, on the tasks of assessing pairwise word similarity and image / caption retrieval, our approach attains equally competitive or stronger results when compared to other state-of-the-art multimodal models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1178 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1178/>Transfer and Multi-Task Learning for NounNoun Compound Interpretation</a></strong><br><a href=/people/m/murhaf-fares/>Murhaf Fares</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1178><div class="card-body p-3 small">In this paper, we empirically evaluate the utility of transfer and multi-task learning on a challenging semantic classification task : semantic interpretation of nounnoun compounds. Through a comprehensive series of experiments and in-depth error analysis, we show that <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> via parameter initialization and <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> via parameter sharing can help a neural classification model generalize over a highly skewed distribution of relations. Further, we demonstrate how dual annotation with two distinct sets of relations over the same set of compounds can be exploited to improve the overall accuracy of a neural classifier and its F1 scores on the less frequent, but more difficult relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1180 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1180.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1180" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1180/>Preposition Sense Disambiguation and Representation</a></strong><br><a href=/people/h/hongyu-gong/>Hongyu Gong</a>
|
<a href=/people/j/jiaqi-mu/>Jiaqi Mu</a>
|
<a href=/people/s/suma-bhat/>Suma Bhat</a>
|
<a href=/people/p/pramod-viswanath/>Pramod Viswanath</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1180><div class="card-body p-3 small">Prepositions are highly polysemous, and their variegated senses encode significant semantic information. In this paper we match each preposition&#8217;s left- and right context, and their interplay to the geometry of the word vectors to the left and right of the <a href=https://en.wikipedia.org/wiki/Preposition_and_postposition>preposition</a>. Extracting these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from a large corpus and using them with <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> makes for an efficient preposition sense disambiguation (PSD) algorithm, which is comparable to and better than <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on two benchmark datasets. Our reliance on no linguistic tool allows us to scale the PSD algorithm to a large corpus and learn sense-specific preposition representations. The crucial abstraction of <a href=https://en.wikipedia.org/wiki/Preposition_and_postposition>preposition senses</a> as word representations permits their use in downstream applicationsphrasal verb paraphrasing and preposition selectionwith new state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1181 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1181.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1181" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1181/>Auto-Encoding Dictionary Definitions into Consistent Word Embeddings</a></strong><br><a href=/people/t/tom-bosc/>Tom Bosc</a>
|
<a href=/people/p/pascal-vincent/>Pascal Vincent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1181><div class="card-body p-3 small">Monolingual dictionaries are widespread and semantically rich resources. This paper presents a simple <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that learns to compute <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> by processing <a href=https://en.wikipedia.org/wiki/Dictionary_definition>dictionary definitions</a> and trying to reconstruct them. It exploits the inherent recursivity of dictionaries by encouraging consistency between the representations it uses as inputs and the representations it produces as outputs. The resulting <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are shown to capture <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> better than regular distributional methods and other dictionary-based methods. In addition, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> shows strong performance when trained exclusively on <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary data</a> and generalizes in one shot.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1183.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1183 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1183 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1183/>Neural Multitask Learning for Simile Recognition</a></strong><br><a href=/people/l/lizhen-liu/>Lizhen Liu</a>
|
<a href=/people/x/xiao-hu/>Xiao Hu</a>
|
<a href=/people/w/wei-song/>Wei Song</a>
|
<a href=/people/r/ruiji-fu/>Ruiji Fu</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/g/guoping-hu/>Guoping Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1183><div class="card-body p-3 small">Simile is a special type of <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a>, where comparators such as like and as are used to compare two objects. Simile recognition is to recognize <a href=https://en.wikipedia.org/wiki/Simile>simile sentences</a> and extract simile components, i.e., the tenor and the vehicle. This paper presents a study of simile recognition in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We construct an annotated corpus for this research, which consists of 11.3k sentences that contain a <a href=https://en.wikipedia.org/wiki/Comparator>comparator</a>. We propose a neural network framework for jointly optimizing three tasks : simile sentence classification, simile component extraction and <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. The experimental results show that the neural network based approaches can outperform all rule-based and feature-based baselines. Both simile sentence classification and simile component extraction can benefit from <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a>. The <a href=https://en.wikipedia.org/wiki/Conjecture>former</a> can be solved very well, while the <a href=https://en.wikipedia.org/wiki/Conjecture>latter</a> is more difficult.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1188 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1188/>Question Generation from SQL Queries Improves Neural Semantic Parsing<span class=acl-fixed-case>SQL</span> Queries Improves Neural Semantic Parsing</a></strong><br><a href=/people/d/daya-guo/>Daya Guo</a>
|
<a href=/people/y/yibo-sun/>Yibo Sun</a>
|
<a href=/people/d/duyu-tang/>Duyu Tang</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/j/jian-yin/>Jian Yin</a>
|
<a href=/people/h/hong-chi/>Hong Chi</a>
|
<a href=/people/j/james-cao/>James Cao</a>
|
<a href=/people/p/peng-chen/>Peng Chen</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1188><div class="card-body p-3 small">In this paper, we study how to learn a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> of state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> with less supervised training data. We conduct our study on WikiSQL, the largest hand-annotated semantic parsing dataset to date. First, we demonstrate that question generation is an effective method that empowers us to learn a state-of-the-art neural network based semantic parser with thirty percent of the supervised training data. Second, we show that applying question generation to the full supervised training data further improves the state-of-the-art model. In addition, we observe that there is a logarithmic relationship between the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> and the amount of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1189 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1189/>SemRegex : A Semantics-Based Approach for Generating <a href=https://en.wikipedia.org/wiki/Regular_expression>Regular Expressions</a> from Natural Language Specifications<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>R</span>egex: A Semantics-Based Approach for Generating Regular Expressions from Natural Language Specifications</a></strong><br><a href=/people/z/zexuan-zhong/>Zexuan Zhong</a>
|
<a href=/people/j/jiaqi-guo/>Jiaqi Guo</a>
|
<a href=/people/w/wei-yang/>Wei Yang</a>
|
<a href=/people/j/jian-peng/>Jian Peng</a>
|
<a href=/people/t/tao-xie/>Tao Xie</a>
|
<a href=/people/j/jian-guang-lou/>Jian-Guang Lou</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a>
|
<a href=/people/d/dongmei-zhang/>Dongmei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1189><div class="card-body p-3 small">Recent research proposes syntax-based approaches to address the problem of <a href=https://en.wikipedia.org/wiki/Computer_programming>generating programs</a> from <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language specifications</a>. These approaches typically train a sequence-to-sequence learning model using a syntax-based objective : <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimation (MLE)</a>. Such syntax-based approaches do not effectively address the goal of generating semantically correct programs, because these approaches fail to handle Program Aliasing, i.e., semantically equivalent programs may have many syntactically different forms. To address this issue, in this paper, we propose a semantics-based approach named SemRegex. SemRegex provides solutions for a subtask of the program-synthesis problem : generating <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> from <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Different from the existing syntax-based approaches, SemRegex trains the model by maximizing the expected semantic correctness of the generated <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a>. The semantic correctness is measured using the DFA-equivalence oracle, random test cases, and distinguishing test cases. The experiments on three public datasets demonstrate the superiority of SemRegex over the existing state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1190 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1190" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1190/>Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing</a></strong><br><a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1190><div class="card-body p-3 small">Building a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> quickly in a new domain is a fundamental challenge for conversational interfaces, as current semantic parsers require expensive supervision and lack the ability to generalize to new domains. In this paper, we introduce a zero-shot approach to <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> that can parse utterances in unseen domains while only being trained on examples in other source domains. First, we map an utterance to an abstract, domain independent, logical form that represents the structure of the logical form, but contains slots instead of KB constants. Then, we replace slots with KB constants via lexical alignment scores and global inference. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> reaches an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>average accuracy</a> of 53.4 % on 7 domains in the OVERNIGHT dataset, substantially better than other zero-shot baselines, and performs as good as a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> trained on over 30 % of the target domain examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1192.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1192 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1192 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1192" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1192/>Mapping Language to Code in Programmatic Context</a></strong><br><a href=/people/s/srinivasan-iyer/>Srinivasan Iyer</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/a/alvin-cheung/>Alvin Cheung</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1192><div class="card-body p-3 small">Source code is rarely written in isolation. It depends significantly on the <a href=https://en.wikipedia.org/wiki/Context_(computing)>programmatic context</a>, such as the class that the code would reside in. To study this phenomenon, we introduce the task of generating class member functions given English documentation and the programmatic context provided by the rest of the class. This task is challenging because the desired code can vary greatly depending on the functionality the class provides (e.g., a sort function may or may not be available when we are asked to return the smallest element in a particular member variable list). We introduce CONCODE, a new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment. We also present a detailed error analysis suggesting that there is significant room for future work on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1193 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1193/>SyntaxSQLNet : Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task<span class=acl-fixed-case>S</span>yntax<span class=acl-fixed-case>SQLN</span>et: Syntax Tree Networks for Complex and Cross-Domain Text-to-<span class=acl-fixed-case>SQL</span> Task</a></strong><br><a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/m/michihiro-yasunaga/>Michihiro Yasunaga</a>
|
<a href=/people/k/kai-yang/>Kai Yang</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/d/dongxu-wang/>Dongxu Wang</a>
|
<a href=/people/z/zifan-li/>Zifan Li</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1193><div class="card-body p-3 small">Most existing studies in text-to-SQL tasks do not require generating complex SQL queries with multiple clauses or sub-queries, and generalizing to new, unseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network to address the complex and cross-domain text-to-SQL generation task. SyntaxSQLNet employs a SQL specific syntax tree-based decoder with SQL generation path history and table-aware column attention encoders. We evaluate SyntaxSQLNet on a new large-scale text-to-SQL corpus containing databases with multiple tables and complex SQL queries containing multiple SQL clauses and nested queries. We use a database split setting where databases in the test set are unseen during training. Experimental results show that SyntaxSQLNet can handle a significantly greater number of complex SQL examples than prior work, outperforming the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> by 9.5 % in exact matching accuracy. To our knowledge, we are the first to study this complex text-to-SQL task. Our <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> and models with the latest updates are available at.<url>https://yale-lily.github.io/seq2sql/spider</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1195 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1195/>Learning to Learn Semantic Parsers from Natural Language Supervision</a></strong><br><a href=/people/i/igor-labutov/>Igor Labutov</a>
|
<a href=/people/b/bishan-yang/>Bishan Yang</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1195><div class="card-body p-3 small">As humans, we often rely on <a href=https://en.wikipedia.org/wiki/Language>language</a> to learn <a href=https://en.wikipedia.org/wiki/Language>language</a>. For example, when corrected in a conversation, we may learn from that correction, over time improving our <a href=https://en.wikipedia.org/wiki/Fluency>language fluency</a>. Inspired by this observation, we propose a <a href=https://en.wikipedia.org/wiki/Machine_learning>learning algorithm</a> for training semantic parsers from supervision (feedback) expressed in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> learns a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> from users&#8217; corrections such as no, what I really meant was before his job, not after, by also simultaneously learning to parse this natural language feedback in order to leverage it as a form of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a>. Unlike <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> with gold-standard logical forms, our method does not require the user to be familiar with the underlying logical formalism, and unlike <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> from <a href=https://en.wikipedia.org/wiki/Denotation>denotation</a>, it does not require the user to know the correct answer to their query. This makes our learning algorithm naturally scalable in settings where existing conversational logs are available and can be leveraged as training data. We construct a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language feedback</a> in a conversational setting, and show that our method is effective at learning a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> from such <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language supervision</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1197 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1197/>What It Takes to Achieve 100 % Condition Accuracy on WikiSQL<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>SQL</span></a></strong><br><a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/i/izzeddin-gur/>Izzeddin Gur</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/x/xifeng-yan/>Xifeng Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1197><div class="card-body p-3 small">WikiSQL is a newly released dataset for studying the <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language sequence</a> to SQL translation problem. The SQL queries in WikiSQL are simple : Each involves one relation and does not have any <a href=https://en.wikipedia.org/wiki/Join_(SQL)>join operation</a>. Despite of its simplicity, none of the publicly reported structured query generation models can achieve an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> beyond 62 %, which is still far from enough for practical use. In this paper, we ask two questions, Why is the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> still low for such simple <a href=https://en.wikipedia.org/wiki/Information_retrieval>queries</a>? and What does it take to achieve 100 % accuracy on WikiSQL? To limit the scope of our study, we focus on the WHERE clause in <a href=https://en.wikipedia.org/wiki/SQL>SQL</a>. The answers will help us gain insights about the directions we should explore in order to further improve the translation accuracy. We will then investigate alternative solutions to realize the potential ceiling performance on WikiSQL. Our proposed <a href=https://en.wikipedia.org/wiki/Solution>solution</a> can reach up to 88.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>condition accuracy</a> on the WikiSQL dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1199 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305931117 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1199/>Heuristically Informed Unsupervised Idiom Usage Recognition</a></strong><br><a href=/people/c/changsheng-liu/>Changsheng Liu</a>
|
<a href=/people/r/rebecca-hwa/>Rebecca Hwa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1199><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomatic expressions</a> can be interpreted figuratively or literally depending on their contexts. This paper proposes an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning method</a> for recognizing the intended usages of <a href=https://en.wikipedia.org/wiki/Idiom>idioms</a>. We treat the usages as a <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> in <a href=https://en.wikipedia.org/wiki/Statistical_model>probabilistic models</a> and train them in a linguistically motivated feature space. Crucially, we show that <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a> is a helpful heuristic for distinguishing the literal usage of idioms, giving us a way to formulate a literal usage metric to estimate the likelihood that the <a href=https://en.wikipedia.org/wiki/Idiom>idiom</a> is intended literally. This information then serves as a form of distant supervision to guide the unsupervised training process for the <a href=https://en.wikipedia.org/wiki/Statistical_model>probabilistic models</a>. Experiments show that our overall model performs competitively against <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1204 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305686976 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1204" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1204/>Neural Related Work Summarization with a Joint Context-driven Attention Mechanism</a></strong><br><a href=/people/y/yongzhen-wang/>Yongzhen Wang</a>
|
<a href=/people/x/xiaozhong-liu/>Xiaozhong Liu</a>
|
<a href=/people/z/zheng-gao/>Zheng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1204><div class="card-body p-3 small">Conventional solutions to automatic related work summarization rely heavily on human-engineered features. In this paper, we develop a neural data-driven summarizer by leveraging the seq2seq paradigm, in which a joint context-driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously. Our motivation is to maintain the topic coherency between a related work section and its target document, where both the textual and graphic contexts play a big role in characterizing the relationship among <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publications</a> accurately. Experimental results on a large dataset show that our approach achieves a considerable improvement over a typical seq2seq summarizer and five classical summarization baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1205 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1205.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305885506 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1205/>Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling</a></strong><br><a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/x/xinyan-xiao/>Xinyan Xiao</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/y/yuanzhuo-wang/>Yuanzhuo Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1205><div class="card-body p-3 small">Information selection is the most important component in document summarization task. In this paper, we propose to extend the basic neural encoding-decoding framework with an information selection layer to explicitly model and optimize the information selection process in abstractive document summarization. Specifically, our information selection layer consists of two parts : gated global information filtering and local sentence selection. Unnecessary information in the original document is first globally filtered, then salient sentences are selected locally while generating each summary sentence sequentially. To optimize the information selection process directly, distantly-supervised training guided by the golden summary is also imported. Experimental results demonstrate that the explicit modeling and optimizing of the information selection process improves document summarization performance significantly, which enables our model to generate more informative and concise summaries, and thus significantly outperform state-of-the-art neural abstractive methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1207 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305886179 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1207/>Improving Abstraction in Text Summarization</a></strong><br><a href=/people/w/wojciech-kryscinski/>Wojciech Kryściński</a>
|
<a href=/people/r/romain-paulus/>Romain Paulus</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1207><div class="card-body p-3 small">Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through <a href=https://en.wikipedia.org/wiki/Policy_learning>policy learning</a> to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1208 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1208.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305886331 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1208/>Content Selection in Deep Learning Models of Summarization</a></strong><br><a href=/people/c/chris-kedzie/>Chris Kedzie</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1208><div class="card-body p-3 small">We carry out experiments with deep learning models of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> across the domains of <a href=https://en.wikipedia.org/wiki/News>news</a>, personal stories, meetings, and <a href=https://en.wikipedia.org/wiki/Medicine>medical articles</a> in order to understand how content selection is performed. We find that many sophisticated <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> of state of the art extractive summarizers do not improve performance over simpler <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. These results suggest that it is easier to create a <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarizer</a> for a new domain than previous work suggests and bring into question the benefit of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> for those domains that do have <a href=https://en.wikipedia.org/wiki/Big_data>massive datasets</a> (i.e., news). At the same time, they suggest important questions for new research in summarization ; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the sumarization task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1209 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306030030 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1209/>Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment</a></strong><br><a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/x/xinyuan-zhang/>Xinyuan Zhang</a>
|
<a href=/people/r/ricardo-henao/>Ricardo Henao</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1209><div class="card-body p-3 small">Network embeddings, which learns low-dimensional representations for each <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>vertex</a> in a large-scale network, have received considerable attention in recent years. For a wide range of applications, <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>vertices</a> in a <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>network</a> are typically accompanied by rich textual information such as user profiles, paper abstracts, etc. In this paper, we propose to incorporate semantic features into <a href=https://en.wikipedia.org/wiki/Graph_embedding>network embeddings</a> by matching important words between text sequences for all pairs of <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>vertices</a>. We introduce an word-by-word alignment framework that measures the compatibility of embeddings between word pairs, and then adaptively accumulates these alignment features with a simple yet effective aggregation function. In experiments, we evaluate the proposed framework on three real-world benchmarks for downstream tasks, including link prediction and multi-label vertex classification. The experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art network embedding methods by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306040551 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1210/>Learning Context-Sensitive Convolutional Filters for <a href=https://en.wikipedia.org/wiki/Text_processing>Text Processing</a></a></strong><br><a href=/people/d/dinghan-shen/>Dinghan Shen</a>
|
<a href=/people/m/martin-renqiang-min/>Martin Renqiang Min</a>
|
<a href=/people/y/yitong-li/>Yitong Li</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1210><div class="card-body p-3 small">Convolutional neural networks (CNNs) have recently emerged as a popular building block for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. Despite their success, most existing CNN models employed in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> share the same learned (and static) set of <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filters</a> for all input sentences. In this paper, we consider an approach of using a small meta network to learn context-sensitive convolutional filters for <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a>. The role of meta network is to abstract the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> of a sentence or document into a set of input-sensitive filters. We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations. In our benchmarks on four different tasks, including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-sensitive filters, consistently outperforms the standard CNN and attention-based CNN baselines. By visualizing the learned context-sensitive filters, we further validate and rationalize the effectiveness of proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1211 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1211.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306041612 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1211/>Deep Relevance Ranking Using Enhanced Document-Query Interactions</a></strong><br><a href=/people/r/ryan-mcdonald/>Ryan McDonald</a>
|
<a href=/people/g/george-brokos/>George Brokos</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1211><div class="card-body p-3 small">We explore several new models for document relevance ranking, building upon the Deep Relevance Matching Model (DRMM) of Guo et al. Unlike DRMM, which uses context-insensitive encodings of terms and query-document term interactions, we inject rich context-sensitive encodings throughout our models, inspired by PACRR&#8217;s (Hui et al., 2017) convolutional n-gram matching features, but extended in several ways including multiple views of query and document inputs. We test our models on datasets from the BIOASQ question answering challenge (Tsatsaronis et al., 2015) and TREC ROBUST 2004 (Voorhees, 2005), showing they outperform BM25-based baselines, DRMM, and PACRR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1213 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306043956 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1213" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1213/>AD3 : Attentive Deep Document Dater<span class=acl-fixed-case>AD</span>3: Attentive Deep Document Dater</a></strong><br><a href=/people/s/swayambhu-nath-ray/>Swayambhu Nath Ray</a>
|
<a href=/people/s/shib-sankar-dasgupta/>Shib Sankar Dasgupta</a>
|
<a href=/people/p/partha-talukdar/>Partha Talukdar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1213><div class="card-body p-3 small">Knowledge of the creation date of documents facilitates several tasks such as summarization, event extraction, temporally focused information extraction etc. Unfortunately, for most of the documents on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>, the time-stamp metadata is either missing or ca n&#8217;t be trusted. Thus, predicting creation time from document content itself is an important task. In this paper, we propose Attentive Deep Document Dater (AD3), an attention-based neural document dating system which utilizes both context and temporal information in documents in a flexible and principled manner. We perform extensive experimentation on multiple real-world datasets to demonstrate the effectiveness of AD3 over neural and non-neural baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1216.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305661928 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1216" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1216/>Deriving Machine Attention from Human Rationales</a></strong><br><a href=/people/y/yujia-bao/>Yujia Bao</a>
|
<a href=/people/s/shiyu-chang/>Shiyu Chang</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1216><div class="card-body p-3 small">Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, <a href=https://en.wikipedia.org/wiki/Attention>attention</a> can be learned effectively. To this end, we start with discrete human-annotated rationales and map them into continuous attention. Our central hypothesis is that this <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-of-the-art baselines, yielding over 15 % average error reduction on benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1219 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1219/>A Deterministic Algorithm for Bridging Anaphora Resolution</a></strong><br><a href=/people/y/yufang-hou/>Yufang Hou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1219><div class="card-body p-3 small">Previous work on bridging anaphora resolution (Poesio et al., 2004 ; Hou et al., 2013) use syntactic preposition patterns to calculate word relatedness. However, such patterns only consider <a href=https://en.wikipedia.org/wiki/Noun_phrase>NPs&#8217; head nouns</a> and hence do not fully capture the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of <a href=https://en.wikipedia.org/wiki/Noun_phrase>NPs</a>. Recently, Hou (2018) created word embeddings (embeddings_PP) to capture associative similarity (i.e., relatedness) between <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> by exploring the syntactic structure of noun phrases. But embeddings_PP only contains <a href=https://en.wikipedia.org/wiki/Word_formation>word representations</a> for <a href=https://en.wikipedia.org/wiki/Noun>nouns</a>. In this paper, we create new word vectors by combining embeddings_PP with <a href=https://en.wikipedia.org/wiki/GloVe>GloVe</a>. This new word embeddings (embeddings_bridging) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an NP beyond its head easily. We therefore develop a deterministic approach for bridging anaphora resolution, which represents the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of an <a href=https://en.wikipedia.org/wiki/NP_(complexity)>NP</a> based on its head noun and modifications. We show that this simple approach achieves the competitive results compared to the best <a href=https://en.wikipedia.org/wiki/System>system</a> in Hou et al. (2013) which explores <a href=https://en.wikipedia.org/wiki/Markov_logic_network>Markov Logic Networks</a> to model the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. Additionally, we further improve the results for bridging anaphora resolution reported in Hou (2018) by combining our simple deterministic approach with Hou et al. (2013)&#8217;s best system MLN II.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1221 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1221/>Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs<span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/d/dimitri-kartsaklis/>Dimitri Kartsaklis</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1221><div class="card-body p-3 small">This paper addresses the problem of mapping <a href=https://en.wikipedia.org/wiki/Natural_language>natural language text</a> to knowledge base entities. The mapping process is approached as a composition of a phrase or a sentence into a point in a multi-dimensional entity space obtained from a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>. The compositional model is an LSTM equipped with a dynamic disambiguation mechanism on the input word embeddings (a Multi-Sense LSTM), addressing polysemy issues. Further, the knowledge base space is prepared by collecting random walks from a <a href=https://en.wikipedia.org/wiki/Graph_(abstract_data_type)>graph</a> enhanced with textual features, which act as a set of semantic bridges between text and knowledge base entities. The ideas of this work are demonstrated on large-scale text-to-entity mapping and entity classification tasks, with state of the art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1223 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1223.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1223" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1223/>One-Shot Relational Learning for Knowledge Graphs</a></strong><br><a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/s/shiyu-chang/>Shiyu Chang</a>
|
<a href=/people/x/xiaoxiao-guo/>Xiaoxiao Guo</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1223><div class="card-body p-3 small">Knowledge graphs (KG) are the key components of various <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>. To further expand KGs&#8217; coverage, previous studies on knowledge graph completion usually require a large number of positive examples for each relation. However, we observe long-tail relations are actually more common in KGs and those newly added relations often do not have many known triples for training. In this work, we aim at predicting new facts under a challenging setting where only one training instance is available. We propose a one-shot relational learning framework, which utilizes the knowledge distilled by embedding models and learns a matching metric by considering both the learned embeddings and one-hop graph structures. Empirically, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> yields considerable performance improvements over existing <a href=https://en.wikipedia.org/wiki/Embedding>embedding models</a>, and also eliminates the need of re-training the <a href=https://en.wikipedia.org/wiki/Embedding>embedding models</a> when dealing with newly added relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1224 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1224.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1224/>Regular Expression Guided Entity Mention Mining from Noisy Web Data</a></strong><br><a href=/people/s/shanshan-zhang/>Shanshan Zhang</a>
|
<a href=/people/l/lihong-he/>Lihong He</a>
|
<a href=/people/s/slobodan-vucetic/>Slobodan Vucetic</a>
|
<a href=/people/e/eduard-dragut/>Eduard Dragut</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1224><div class="card-body p-3 small">Many important entity types in web documents, such as dates, times, email addresses, and course numbers, follow or closely resemble patterns that can be described by Regular Expressions (REs). Due to a vast diversity of web documents and ways in which they are being generated, even seemingly straightforward tasks such as identifying mentions of date in a document become very challenging. It is reasonable to claim that it is impossible to create a RE that is capable of identifying such entities from <a href=https://en.wikipedia.org/wiki/Web_page>web documents</a> with perfect <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>. Rather than abandoning REs as a go-to approach for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity detection</a>, this paper explores ways to combine the expressive power of REs, ability of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> to learn from large data, and human-in-the loop approach into a new integrated framework for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity identification</a> from web data. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> starts by creating or collecting the existing REs for a particular type of an entity. Those REs are then used over a large document corpus to collect weak labels for the entity mentions and a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> is trained to predict those RE-generated weak labels. Finally, a human expert is asked to label a small set of documents and the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> is fine tuned on those documents. The experimental evaluation on several entity identification problems shows that the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves impressive <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, while requiring very modest human effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1226 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1226/>Neural Adaptation Layers for Cross-domain Named Entity Recognition</a></strong><br><a href=/people/b/bill-yuchen-lin/>Bill Yuchen Lin</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1226><div class="card-body p-3 small">Recent research efforts have shown that neural architectures can be effective in conventional information extraction tasks such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, yielding state-of-the-art results on standard newswire datasets. However, despite significant resources required for training such models, the performance of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on one domain typically degrades dramatically when applied to a different domain, yet extracting entities from new emerging domains such as <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> can be of significant interest. In this paper, we empirically investigate effective methods for conveniently adapting an existing, well-trained neural NER model for a new domain. Unlike existing approaches, we propose lightweight yet effective <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for performing <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for neural models. Specifically, we introduce adaptation layers on top of existing neural architectures, where no re-training using the source domain data is required. We conduct extensive empirical studies and show that our approach significantly outperforms state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1228 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1228/>Annotation of a Large Clinical Entity Corpus</a></strong><br><a href=/people/p/pinal-patel/>Pinal Patel</a>
|
<a href=/people/d/disha-davey/>Disha Davey</a>
|
<a href=/people/v/vishal-panchal/>Vishal Panchal</a>
|
<a href=/people/p/parth-pathak/>Parth Pathak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1228><div class="card-body p-3 small">Having an entity annotated corpus of the clinical domain is one of the basic requirements for detection of clinical entities using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning (ML) approaches</a>. Past researches have shown the superiority of statistical / ML approaches over the rule based approaches. But in order to take full advantage of the ML approaches, an accurately annotated corpus becomes an essential requirement. Though there are a few annotated corpora available either on a small data set, or covering a narrower domain (like cancer patients records, lab reports), <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> of a large data set representing the entire clinical domain has not been created yet. In this paper, we have described in detail the annotation guidelines, annotation process and our approaches in creating a CER (clinical entity recognition) corpus of 5,160 clinical documents from forty different clinical specialities. The clinical entities range across various types such as diseases, <a href=https://en.wikipedia.org/wiki/Medical_procedure>procedures</a>, <a href=https://en.wikipedia.org/wiki/Medication>medications</a>, <a href=https://en.wikipedia.org/wiki/Medical_device>medical devices</a> and so on. We have classified them into eleven categories for <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. Our <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> also reflects the relations among the group of entities that constitute larger concepts altogether.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1230 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1230" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1230/>Learning Named Entity Tagger using Domain-Specific Dictionary</a></strong><br><a href=/people/j/jingbo-shang/>Jingbo Shang</a>
|
<a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/x/xiaotao-gu/>Xiaotao Gu</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/t/teng-ren/>Teng Ren</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1230><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural models</a> allow us to build reliable named entity recognition (NER) systems without handcrafting features. However, such <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> require large amounts of manually-labeled training data. There have been efforts on replacing human annotations with distant supervision (in conjunction with external dictionaries), but the generated noisy labels pose significant challenges on learning effective neural models. Here we propose two neural models to suit noisy distant supervision from the dictionary. First, under the traditional sequence labeling framework, we propose a revised fuzzy CRF layer to handle tokens with multiple possible labels. After identifying the nature of noisy labels in distant supervision, we go beyond the traditional framework and propose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1233.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1233 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1233 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1233.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1233/>Interpretation of Natural Language Rules in Conversational Machine Reading</a></strong><br><a href=/people/m/marzieh-saeidi/>Marzieh Saeidi</a>
|
<a href=/people/m/max-bartolo/>Max Bartolo</a>
|
<a href=/people/p/patrick-lewis/>Patrick Lewis</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/t/tim-rocktaschel/>Tim Rocktäschel</a>
|
<a href=/people/m/mike-sheldon/>Mike Sheldon</a>
|
<a href=/people/g/guillaume-bouchard/>Guillaume Bouchard</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1233><div class="card-body p-3 small">Most work in <a href=https://en.wikipedia.org/wiki/Machine_reading>machine reading</a> focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader&#8217;s background knowledge. One example is the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> of interpreting regulations to answer Can I...? or Do I have to...? questions such as I am working in Canada. Do I have to carry on paying UK National Insurance? after reading a UK government website about this topic. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspecified, and a human assistant will regularly have to ask clarification questions such as How long have you been working abroad? when the answer can not be directly derived from the question and text. In this paper, we formalise this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and develop a crowd-sourcing strategy to collect 37k <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task instances</a> based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and assess its difficulty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1234 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1234.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1234/>A State-transition Framework to Answer Complex Questions over Knowledge Base</a></strong><br><a href=/people/s/sen-hu/>Sen Hu</a>
|
<a href=/people/l/lei-zou/>Lei Zou</a>
|
<a href=/people/x/xinbo-zhang/>Xinbo Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1234><div class="card-body p-3 small">Although natural language question answering over <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> have been studied in the literature, existing methods have some limitations in answering complex questions. To address that, in this paper, we propose a State Transition-based approach to translate a complex natural language question N to a semantic query graph (SQG), which is used to match the underlying knowledge graph to find the answers to question N. In order to generate SQG, we propose four primitive operations (expand, fold, connect and merge) and a learning-based state transition approach. Extensive experiments on several benchmarks (such as QALD, WebQuestions and ComplexQuestions) with two knowledge bases (DBpedia and Freebase) confirm the superiority of our approach compared with state-of-the-arts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1236 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1236/>Logician and Orator : Learning from the Duality between Language and Knowledge in Open Domain</a></strong><br><a href=/people/m/mingming-sun/>Mingming Sun</a>
|
<a href=/people/x/xu-li/>Xu Li</a>
|
<a href=/people/p/ping-li/>Ping Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1236><div class="card-body p-3 small">We propose the task of Open-Domain Information Narration (OIN) as the reverse task of Open Information Extraction (OIE), to implement the dual structure between language and knowledge in the open domain. Then, we develop an agent, called Orator, to accomplish the OIN task, and assemble the Orator and the recently proposed OIE agent Logician into a dual system to utilize the duality structure with a reinforcement learning paradigm. Experimental results reveal the dual structure between OIE and OIN tasks helps to build better both OIE agents and OIN agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1237 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1237.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1237/>MemoReader : Large-Scale Reading Comprehension through Neural Memory Controller<span class=acl-fixed-case>M</span>emo<span class=acl-fixed-case>R</span>eader: Large-Scale Reading Comprehension through Neural Memory Controller</a></strong><br><a href=/people/s/seohyun-back/>Seohyun Back</a>
|
<a href=/people/s/seunghak-yu/>Seunghak Yu</a>
|
<a href=/people/s/sathish-reddy-indurthi/>Sathish Reddy Indurthi</a>
|
<a href=/people/j/jihie-kim/>Jihie Kim</a>
|
<a href=/people/j/jaegul-choo/>Jaegul Choo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1237><div class="card-body p-3 small">Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text. Existing approaches made a significant progress comparable to human-level performance, but they are still limited in understanding, up to a few paragraphs, failing to properly comprehend lengthy document. In this paper, we propose a novel deep neural network architecture to handle a <a href=https://en.wikipedia.org/wiki/Long-range_dependency>long-range dependency</a> in RC tasks. In detail, our method has two novel aspects : (1) an advanced memory-augmented architecture and (2) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the <a href=https://en.wikipedia.org/wiki/Memory>memory</a>. Our proposed <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> is widely applicable to other <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. We have performed extensive experiments with well-known benchmark datasets such as TriviaQA, QUASAR-T, and SQuAD. The experimental results demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>, especially for lengthy documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1238 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1238/>Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension</a></strong><br><a href=/people/y/yi-tay/>Yi Tay</a>
|
<a href=/people/a/anh-tuan-luu/>Anh Tuan Luu</a>
|
<a href=/people/s/siu-cheung-hui/>Siu Cheung Hui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1238><div class="card-body p-3 small">Sequence encoders are crucial components in many neural architectures for learning to read and comprehend. This paper presents a new compositional encoder for reading comprehension (RC). Our proposed <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> is not only aimed at being fast but also expressive. Specifically, the key novelty behind our <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> is that it explicitly models across multiple granularities using a new dilated composition mechanism. In our approach, gating functions are learned by modeling relationships and reasoning over multi-granular sequence information, enabling compositional learning that is aware of both long and short term information. We conduct experiments on three RC datasets, showing that our proposed <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> demonstrates very promising results both as a standalone encoder as well as a complementary building block. Empirical results show that simple Bi-Attentive architectures augmented with our proposed <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> not only achieves state-of-the-art / highly competitive results but is also considerably faster than other published works.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1239 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1239.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1239/>Neural Compositional Denotational Semantics for Question Answering</a></strong><br><a href=/people/n/nitish-gupta/>Nitish Gupta</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1239><div class="card-body p-3 small">Answering compositional questions requiring multi-step reasoning is challenging. We introduce an end-to-end differentiable model for interpreting questions about a knowledge graph (KG), which is inspired by formal approaches to <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. Each span of text is represented by a denotation in a KG and a <a href=https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)>vector</a> that captures ungrounded aspects of meaning. Learned composition modules recursively combine constituent spans, culminating in a grounding for the complete sentence which answers the question. For example, to interpret not green, the model represents <a href=https://en.wikipedia.org/wiki/Green>green</a> as a set of KG entities and not as a trainable ungrounded vectorand then uses this vector to parameterize a <a href=https://en.wikipedia.org/wiki/Function_composition>composition function</a> that performs a <a href=https://en.wikipedia.org/wiki/Complement_(set_theory)>complement operation</a>. For each sentence, we build a parse chart subsuming all possible <a href=https://en.wikipedia.org/wiki/Parsing>parses</a>, allowing the model to jointly learn both the composition operators and output structure by <a href=https://en.wikipedia.org/wiki/Gradient_descent>gradient descent</a> from end-task supervision. The model learns a variety of challenging semantic operators, such as <a href=https://en.wikipedia.org/wiki/Quantifier_(linguistics)>quantifiers</a>, <a href=https://en.wikipedia.org/wiki/Logical_disjunction>disjunctions</a> and composed relations, and infers latent syntactic structure. It also generalizes well to longer questions than seen in its training data, in contrast to RNN, its tree-based variants, and semantic parsing baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1240 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1240" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1240/>Cross-Pair Text Representations for Answer Sentence Selection</a></strong><br><a href=/people/k/kateryna-tymoshenko/>Kateryna Tymoshenko</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1240><div class="card-body p-3 small">High-level semantics tasks, e.g., paraphrasing, textual entailment or question answering, involve modeling of text pairs. Before the emergence of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, this has been mostly performed using intra-pair features, which incorporate similarity scores or rewrite rules computed between the members within the same pair. In this paper, we compute scalar products between vectors representing similarity between members of different pairs, in place of simply using a single vector for each pair. This allows us to obtain a representation specific to any pair of pairs, which delivers the state of the art in answer sentence selection. Most importantly, our approach can outperform much more complex <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1242 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1242" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1242/>Knowledge Base Question Answering via Encoding of Complex Query Graphs</a></strong><br><a href=/people/k/kangqi-luo/>Kangqi Luo</a>
|
<a href=/people/f/fengli-lin/>Fengli Lin</a>
|
<a href=/people/x/xusheng-luo/>Xusheng Luo</a>
|
<a href=/people/k/kenny-zhu/>Kenny Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1242><div class="card-body p-3 small">Answering complex questions that involve multiple entities and multiple relations using a standard <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> is an open and challenging task. Most existing KBQA approaches focus on simpler questions and do not work very well on complex questions because they were not able to simultaneously represent the question and the corresponding complex query structure. In this work, we encode such complex query structure into a uniform vector representation, and thus successfully capture the interactions between individual semantic components within a complex question. This approach consistently outperforms existing methods on <a href=https://en.wikipedia.org/wiki/Complex_question>complex questions</a> while staying competitive on simple questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1244.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1244 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1244 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1244.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1244" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1244/>Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</a></strong><br><a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/p/peng-qi/>Peng Qi</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1244><div class="card-body p-3 small">Dependency trees help relation extraction models capture long-range relations between words. However, existing dependency-based models either neglect crucial information (e.g., negation) by pruning the dependency trees too aggressively, or are computationally inefficient because it is difficult to parallelize over different <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree structures</a>. We propose an extension of graph convolutional networks that is tailored for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, which pools information over arbitrary dependency structures efficiently in parallel. To incorporate relevant information while maximally removing irrelevant content, we further apply a novel pruning strategy to the input <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a> by keeping words immediately around the shortest path between the two entities among which a relation might hold. The resulting <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on the large-scale TACRED dataset, outperforming existing sequence and dependency-based neural models. We also show through detailed analysis that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has complementary strengths to sequence models, and combining them further improves the state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1245.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1245 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1245 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1245/>Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction</a></strong><br><a href=/people/j/jinhua-du/>Jinhua Du</a>
|
<a href=/people/j/jingguang-han/>Jingguang Han</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/d/dadong-wan/>Dadong Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1245><div class="card-body p-3 small">Attention mechanism is often used in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> for distantly supervised relation extraction (DS-RE) to distinguish valid from noisy instances. However, traditional 1-D vector attention model is insufficient for learning of different contexts in the selection of valid instances to predict the relationship for an entity pair. To alleviate this issue, we propose a novel multi-level structured (2-D matrix) self-attention mechanism for DS-RE in a multi-instance learning (MIL) framework using bidirectional recurrent neural networks (BiRNN). In the proposed method, a structured word-level self-attention learns a <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>2-D matrix</a> where each row vector represents a <a href=https://en.wikipedia.org/wiki/Weight_distribution>weight distribution</a> for different aspects of an instance regarding two entities. Targeting the MIL issue, the structured sentence-level attention learns a <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>2-D matrix</a> where each row vector represents a <a href=https://en.wikipedia.org/wiki/Weight_distribution>weight distribution</a> on selection of different valid instances. Experiments conducted on two publicly available DS-RE datasets show that the proposed framework with multi-level structured self-attention mechanism significantly outperform baselines in terms of PR curves, P@N and F1 measures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1248 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1248/>Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding</a></strong><br><a href=/people/g/guanying-wang/>Guanying Wang</a>
|
<a href=/people/w/wen-zhang/>Wen Zhang</a>
|
<a href=/people/r/ruoxu-wang/>Ruoxu Wang</a>
|
<a href=/people/y/yalin-zhou/>Yalin Zhou</a>
|
<a href=/people/x/xi-chen/>Xi Chen</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/h/hai-zhu/>Hai Zhu</a>
|
<a href=/people/h/huajun-chen/>Huajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1248><div class="card-body p-3 small">Distant supervision is an effective method to generate large scale labeled data for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, which assumes that if a pair of entities appears in some relation of a Knowledge Graph (KG), all sentences containing those entities in a large unlabeled corpus are then labeled with that relation to train a relation classifier. However, when the pair of entities has multiple relationships in the <a href=https://en.wikipedia.org/wiki/Kinetic_theory_of_gases>KG</a>, this assumption may produce noisy relation labels. This paper proposes a label-free distant supervision method, which makes no use of the relation labels under this inadequate assumption, but only uses the prior knowledge derived from the KG to supervise the learning of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> directly and softly. Specifically, we make use of the type information and the translation law derived from typical KG embedding model to learn <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for certain sentence patterns. As the supervision signal is only determined by the two aligned entities, neither hard relation labels nor extra noise-reduction model for the bag of sentences is needed in this way. The experiments show that the <a href=https://en.wikipedia.org/wiki/Design_of_experiments>approach</a> performs well in current distant supervision dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1251.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1251 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1251 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1251/>Possessors Change Over Time : A Case Study with <a href=https://en.wikipedia.org/wiki/Work_of_art>Artworks</a></a></strong><br><a href=/people/d/dhivya-chinnappa/>Dhivya Chinnappa</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1251><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and experimental results to extract possession relations over time. We work with Wikipedia articles about artworks, and extract possession relations along with temporal information indicating when these <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> are true. The annotation scheme yields many possessors over time for a given artwork, and experimental results show that an LSTM ensemble can automate the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1252.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1252 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1252 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1252.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305936322 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1252/>Using Lexical Alignment and Referring Ability to Address Data Sparsity in Situated Dialog Reference Resolution</a></strong><br><a href=/people/t/todd-shore/>Todd Shore</a>
|
<a href=/people/g/gabriel-skantze/>Gabriel Skantze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1252><div class="card-body p-3 small">Referring to entities in situated dialog is a collaborative process, whereby interlocutors often expand, repair and/or replace referring expressions in an iterative process, converging on conceptual pacts of referring language use in doing so. Nevertheless, much work on exophoric reference resolution (i.e. resolution of references to entities outside of a given text) follows a literary model, whereby individual referring expressions are interpreted as unique identifiers of their referents given the state of the dialog the referring expression is initiated. In this paper, we address this collaborative nature to improve dialogic reference resolution in two ways : First, we trained a words-as-classifiers logistic regression model of word semantics and incrementally adapt the model to idiosyncratic language between dyad partners during evaluation of the dialog. We then used these semantic models to learn the general referring ability of each word, which is independent of referent features. These methods facilitate accurate automatic reference resolution in situated dialog without annotation of referring expressions, even with little background data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1253.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1253 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1253 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305937184 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1253/>Subgoal Discovery for Hierarchical Dialogue Policy Learning</a></strong><br><a href=/people/d/da-tang/>Da Tang</a>
|
<a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/c/chong-wang/>Chong Wang</a>
|
<a href=/people/l/lihong-li/>Lihong Li</a>
|
<a href=/people/t/tony-jebara/>Tony Jebara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1253><div class="card-body p-3 small">Developing <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> to engage in complex goal-oriented dialogues is challenging partly because the main learning signals are very sparse in long conversations. In this paper, we propose a divide-and-conquer approach that discovers and exploits the hidden structure of the task to enable efficient <a href=https://en.wikipedia.org/wiki/Policy_learning>policy learning</a>. First, given successful example dialogues, we propose the Subgoal Discovery Network (SDN) to divide a complex goal-oriented task into a set of simpler subgoals in an unsupervised fashion. We then use these <a href=https://en.wikipedia.org/wiki/Goal>subgoals</a> to learn a multi-level policy by hierarchical reinforcement learning. We demonstrate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> by building a dialogue agent for the composite task of travel planning. Experiments with simulated and real users show that our approach performs competitively against a state-of-the-art method that requires human-defined subgoals. Moreover, we show that the learned <a href=https://en.wikipedia.org/wiki/Goal>subgoals</a> are often human comprehensible.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1254 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305938531 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1254/>Supervised Clustering of Questions into Intents for Dialog System Applications</a></strong><br><a href=/people/i/iryna-haponchyk/>Iryna Haponchyk</a>
|
<a href=/people/a/antonio-uva/>Antonio Uva</a>
|
<a href=/people/s/seunghak-yu/>Seunghak Yu</a>
|
<a href=/people/o/olga-uryupina/>Olga Uryupina</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1254><div class="card-body p-3 small">Modern automated dialog systems require complex dialog managers able to deal with <a href=https://en.wikipedia.org/wiki/User_intent>user intent</a> triggered by high-level semantic questions. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for automatically clustering questions into <a href=https://en.wikipedia.org/wiki/User_intent>user intents</a> to help the design tasks. Since questions are short texts, uncovering their <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> to group them together can be very challenging. We approach the problem by using powerful semantic classifiers from question duplicate / matching research along with a novel idea of supervised clustering methods based on structured output. We test our approach on two intent clustering corpora, showing an impressive improvement over previous methods for two languages / domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1255 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1255.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305939688 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1255" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1255/>Towards Exploiting Background Knowledge for Building Conversation Systems</a></strong><br><a href=/people/n/nikita-moghe/>Nikita Moghe</a>
|
<a href=/people/s/siddhartha-arora/>Siddhartha Arora</a>
|
<a href=/people/s/suman-banerjee/>Suman Banerjee</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1255><div class="card-body p-3 small">Existing dialog datasets contain a sequence of utterances and responses without any explicit background knowledge associated with them. This has resulted in the development of <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> which treat <a href=https://en.wikipedia.org/wiki/Conversation>conversation</a> as a sequence-to-sequence generation task (i.e., given a sequence of utterances generate the response sequence). This is not only an overly simplistic view of conversation but it is also emphatically different from the way humans converse by heavily relying on their background knowledge about the topic (as opposed to simply relying on the previous sequence of utterances). For example, it is common for humans to (involuntarily) produce utterances which are copied or suitably modified from background articles they have read about the topic. To facilitate the development of such natural conversation models which mimic the human process of conversing, we create a new dataset containing movie chats wherein each response is explicitly generated by copying and/or modifying sentences from unstructured background knowledge such as plots, comments and reviews about the movie. We establish baseline results on this dataset (90 K utterances from 9 K conversations) using three different models : (i) pure generation based models which ignore the background knowledge (ii) generation based models which learn to copy information from the background knowledge when required and (iii) span prediction based models which predict the appropriate response span in the background knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1256 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1256.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305940786 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1256" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1256/>Decoupling Strategy and Generation in Negotiation Dialogues</a></strong><br><a href=/people/h/he-he/>He He</a>
|
<a href=/people/d/derek-chen/>Derek Chen</a>
|
<a href=/people/a/anusha-balakrishnan/>Anusha Balakrishnan</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1256><div class="card-body p-3 small">We consider negotiation settings in which two agents use <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> to bargain on goods. Agents need to decide on both high-level strategy (e.g., proposing $ 50) and the execution of that strategy (e.g., generating The bike is brand new. Selling for just $ 50 !). Recent work on <a href=https://en.wikipedia.org/wiki/Negotiation>negotiation</a> trains neural models, but their end-to-end nature makes it hard to control their <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a>, and <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> tends to lead to degenerate solutions. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Modular_programming>modular approach</a> based on coarse dialogue acts (e.g., propose(price=50)) that decouples <a href=https://en.wikipedia.org/wiki/Strategy_(game_theory)>strategy</a> and generation. We show that we can flexibly set the strategy using <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>, <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances. We test our approach on the recently proposed DEALORNODEAL game, and we also collect a richer dataset based on real items on <a href=https://en.wikipedia.org/wiki/Craigslist>Craigslist</a>. Human evaluation shows that our <a href=https://en.wikipedia.org/wiki/System>systems</a> achieve higher task success rate and more human-like negotiation behavior than previous approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1257.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1257 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1257 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305886563 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1257" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1257/>Large-scale Cloze Test Dataset Created by Teachers</a></strong><br><a href=/people/q/qizhe-xie/>Qizhe Xie</a>
|
<a href=/people/g/guokun-lai/>Guokun Lai</a>
|
<a href=/people/z/zihang-dai/>Zihang Dai</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1257><div class="card-body p-3 small">Cloze tests are widely adopted in <a href=https://en.wikipedia.org/wiki/Test_(assessment)>language exams</a> to evaluate students&#8217; language proficiency. In this paper, we propose the first large-scale human-created cloze test dataset CLOTH, containing questions used in middle-school and high-school language exams. With missing blanks carefully created by teachers and candidate choices purposely designed to be nuanced, CLOTH requires a deeper language understanding and a wider <a href=https://en.wikipedia.org/wiki/Attention_span>attention span</a> than previously automatically-generated cloze datasets. We test the performance of dedicatedly designed baseline models including a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> trained on the One Billion Word Corpus and show humans outperform them by a significant margin. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending the long-term context to be the key bottleneck.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1258 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305887077 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1258" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1258/>emrQA : A Large Corpus for <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> on Electronic Medical Records<span class=acl-fixed-case>QA</span>: A Large Corpus for Question Answering on Electronic Medical Records</a></strong><br><a href=/people/a/anusri-pampari/>Anusri Pampari</a>
|
<a href=/people/p/preethi-raghavan/>Preethi Raghavan</a>
|
<a href=/people/j/jennifer-liang/>Jennifer Liang</a>
|
<a href=/people/j/jian-peng/>Jian Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1258><div class="card-body p-3 small">We propose a novel <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million questions-logical form and 400,000 + question-answer evidence pairs. We characterize the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and explore its learning potential by training baseline models for question to logical form and question to answer mapping.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1263.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1263 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1263 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306045906 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1263/>Semantics as a Foreign Language</a></strong><br><a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1263><div class="card-body p-3 small">We propose a novel approach to semantic dependency parsing (SDP) by casting the task as an instance of multi-lingual machine translation, where each semantic representation is a different foreign dialect. To that end, we first generalize syntactic linearization techniques to account for the richer semantic dependency graph structure. Following, we design a neural sequence-to-sequence framework which can effectively recover our graph linearizations, performing almost on-par with previous SDP state-of-the-art while requiring less parallel training annotations. Beyond SDP, our linearization technique opens the door to integration of graph-based semantic representations as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> in neural models for downstream applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1264 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306049123 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1264" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1264/>An AMR Aligner Tuned by Transition-based Parser<span class=acl-fixed-case>AMR</span> Aligner Tuned by Transition-based Parser</a></strong><br><a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/w/wanxiang-che/>Wanxiang Che</a>
|
<a href=/people/b/bo-zheng/>Bo Zheng</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1264><div class="card-body p-3 small">In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle parser</a> via picking the alignment that leads to the highest-scored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the current state-of-the-art parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1265.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1265 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1265 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1265.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306052219 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1265/>Dependency-based Hybrid Trees for Semantic Parsing</a></strong><br><a href=/people/z/zhanming-jie/>Zhanming Jie</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1265><div class="card-body p-3 small">We propose a novel dependency-based hybrid tree model for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, which converts natural language utterance into machine interpretable meaning representations. Unlike previous state-of-the-art models, the semantic information is interpreted as the latent dependency between the natural language words in our joint representation. Such dependency information can capture the interactions between the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language words</a>. We integrate a neural component into our model and propose an efficient dynamic-programming algorithm to perform tractable inference. Through extensive experiments on the standard multilingual GeoQuery dataset with eight languages, we demonstrate that our proposed approach is able to achieve state-of-the-art performance across several languages. Analysis also justifies the effectiveness of using our new dependency-based representation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1267 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1267.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305663630 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1267" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1267/>Sentence Compression for Arbitrary Languages via Multilingual Pivoting</a></strong><br><a href=/people/j/jonathan-mallinson/>Jonathan Mallinson</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1267><div class="card-body p-3 small">In this paper we advocate the use of bilingual corpora which are abundantly available for training sentence compression models. Our approach borrows much of its machinery from <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and leverages bilingual pivoting : compressions are obtained by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length. Our model can be trained for any language as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data. We release. Moss, a new parallel Multilingual Compression dataset for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and <a href=https://en.wikipedia.org/wiki/French_language>French</a> which can be used to evaluate compression models across languages and genres.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1269.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1269 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1269 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305665271 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1269" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1269/>XNLI : Evaluating Cross-lingual Sentence Representations<span class=acl-fixed-case>XNLI</span>: Evaluating Cross-lingual Sentence Representations</a></strong><br><a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/r/ruty-rinott/>Ruty Rinott</a>
|
<a href=/people/g/guillaume-lample/>Guillaume Lample</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a>
|
<a href=/people/h/holger-schwenk/>Holger Schwenk</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1269><div class="card-body p-3 small">State-of-the-art <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a> rely on <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> in the form of <a href=https://en.wikipedia.org/wiki/Annotation>annotated data</a> to learn competent <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. These <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are generally trained on data in a single language (usually English), and can not be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as <a href=https://en.wikipedia.org/wiki/Swahili_language>Swahili</a> and <a href=https://en.wikipedia.org/wiki/Urdu>Urdu</a>. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1273.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1273 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1273 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1273" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1273/>A Hybrid Approach to Automatic Corpus Generation for Chinese Spelling Check<span class=acl-fixed-case>C</span>hinese Spelling Check</a></strong><br><a href=/people/d/dingmin-wang/>Dingmin Wang</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/j/jialong-han/>Jialong Han</a>
|
<a href=/people/h/haisong-zhang/>Haisong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1273><div class="card-body p-3 small">Chinese spelling check (CSC) is a challenging yet meaningful task, which not only serves as a preprocessing in many natural language processing(NLP) applications, but also facilitates reading and understanding of running texts in peoples&#8217; daily lives. However, to utilize data-driven approaches for CSC, there is one major limitation that annotated corpora are not enough in applying <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> and building <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. In this paper, we propose a novel approach of constructing CSC corpus with automatically generated spelling errors, which are either visually or phonologically resembled characters, corresponding to the OCR- and ASR-based methods, respectively. Upon the constructed corpus, different <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are trained and evaluated for CSC with respect to three standard test sets. Experimental results demonstrate the effectiveness of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, therefore confirm the validity of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1274.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1274 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1274 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1274" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1274/>Neural Quality Estimation of Grammatical Error Correction</a></strong><br><a href=/people/s/shamil-chollampatt/>Shamil Chollampatt</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1274><div class="card-body p-3 small">Grammatical error correction (GEC) systems deployed in language learning environments are expected to accurately correct errors in learners&#8217; writing. However, in practice, they often produce spurious corrections and fail to correct many errors, thereby misleading learners. This necessitates the estimation of the quality of output sentences produced by GEC systems so that instructors can selectively intervene and re-correct the sentences which are poorly corrected by the <a href=https://en.wikipedia.org/wiki/System>system</a> and ensure that learners get accurate feedback. We propose the first neural approach to automatic quality estimation of GEC output sentences that does not employ any hand-crafted features. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is trained in a supervised manner on learner sentences and corresponding GEC system outputs with quality score labels computed using human-annotated references. Our neural quality estimation models for GEC show significant improvements over a strong feature-based baseline. We also show that a state-of-the-art GEC system can be improved when quality scores are used as features for re-ranking the N-best candidates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1275 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1275/>Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging<span class=acl-fixed-case>T</span>witter <span class=acl-fixed-case>POS</span> Tagging</a></strong><br><a href=/people/t/tao-gui/>Tao Gui</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/j/jingjing-gong/>Jingjing Gong</a>
|
<a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/d/di-liang/>Di Liang</a>
|
<a href=/people/k/keyu-ding/>Keyu Ding</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1275><div class="card-body p-3 small">Part-of-Speech (POS) tagging for <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> has received considerable attention in recent years. Because most POS tagging methods are based on <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a>, they usually require a large amount of labeled data for training. However, the existing labeled datasets for <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> are much smaller than those for newswire text. Hence, to help POS tagging for <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, most domain adaptation methods try to leverage newswire datasets by learning the shared features between the two domains. However, from a linguistic perspective, Twitter users not only tend to mimic the formal expressions of traditional media, like <a href=https://en.wikipedia.org/wiki/News>news</a>, but they also appear to be developing linguistically informal styles. Therefore, <a href=https://en.wikipedia.org/wiki/POS_tagging>POS tagging</a> for the formal Twitter context can be learned together with the newswire dataset, while <a href=https://en.wikipedia.org/wiki/POS_tagging>POS tagging</a> for the informal Twitter context should be learned separately. To achieve this task, in this work, we propose a hypernetwork-based method to generate different parameters to separately model contexts with different expression styles. Experimental results on three different datasets show that our approach achieves better performance than state-of-the-art methods in most cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1276.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1276 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1276 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1276.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1276/>Free as in Free Word Order : An Energy Based Model for Word Segmentation and Morphological Tagging in <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a><span class=acl-fixed-case>S</span>anskrit</a></strong><br><a href=/people/a/amrith-krishna/>Amrith Krishna</a>
|
<a href=/people/b/bishal-santra/>Bishal Santra</a>
|
<a href=/people/s/sasi-prasanth-bandaru/>Sasi Prasanth Bandaru</a>
|
<a href=/people/g/gaurav-sahu/>Gaurav Sahu</a>
|
<a href=/people/v/vishnu-dutt-sharma/>Vishnu Dutt Sharma</a>
|
<a href=/people/p/pavankumar-satuluri/>Pavankumar Satuluri</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1276><div class="card-body p-3 small">The configurational information in sentences of a free word order language such as <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a> is of limited use. Thus, the context of the entire sentence will be desirable even for basic processing tasks such as <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>. We propose a structured prediction framework that jointly solves the word segmentation and morphological tagging tasks in <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>. We build an energy based model where we adopt approaches generally employed in graph based parsing techniques (McDonald et al., 2005a ; Carreras, 2007). Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> outperforms the state of the art with an F-Score of 96.92 (percentage improvement of 7.06 %) while using less than one tenth of the task-specific training data. We find that the use of a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph based approach</a> instead of a traditional lattice-based sequential labelling approach leads to a percentage gain of 12.6 % in F-Score for the segmentation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1278.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1278 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1278 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1278.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1278/>What do character-level models learn about <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphology</a>? The case of dependency parsing</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/a/andreas-grivas/>Andreas Grivas</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1278><div class="card-body p-3 small">When parsing morphologically-rich languages with neural models, it is beneficial to model input at the character level, and it has been claimed that this is because character-level models learn <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>. We test these claims by comparing character-level models to an oracle with access to explicit morphological analysis on twelve languages with varying <a href=https://en.wikipedia.org/wiki/Morphological_typology>morphological typologies</a>. Our results highlight many strengths of character-level models, but also show that they are poor at disambiguating some words, particularly in the face of case syncretism. We then demonstrate that explicitly modeling morphological case improves our best model, showing that character-level models can benefit from targeted forms of explicit morphological modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1280 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1280/>ICON : Interactive Conversational Memory Network for Multimodal Emotion Detection<span class=acl-fixed-case>ICON</span>: Interactive Conversational Memory Network for Multimodal Emotion Detection</a></strong><br><a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1280><div class="card-body p-3 small">Emotion recognition in <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> is crucial for building empathetic machines. Present works in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues. To this end, we propose Interactive COnversational memory Network (ICON), a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the self- and inter-speaker emotional influences into global memories. Such <a href=https://en.wikipedia.org/wiki/Memory>memories</a> generate contextual summaries which aid in predicting the emotional orientation of utterance-videos. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art networks on multiple classification and regression tasks in two benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1281 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1281.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1281/>Discriminative Learning of Open-Vocabulary Object Retrieval and Localization by Negative Phrase Augmentation</a></strong><br><a href=/people/r/ryota-hinami/>Ryota Hinami</a>
|
<a href=/people/s/shinichi-satoh/>Shin’ichi Satoh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1281><div class="card-body p-3 small">Thanks to the success of object detection technology, we can retrieve objects of the specified classes even from huge image collections. However, the current state-of-the-art object detectors (such as Faster R-CNN) can only handle pre-specified classes. In addition, large amounts of positive and negative visual samples are required for training. In this paper, we address the problem of open-vocabulary object retrieval and localization, where the target object is specified by a textual query (e.g., a word or phrase). We first propose Query-Adaptive R-CNN, a simple extension of Faster R-CNN adapted to open-vocabulary queries, by transforming the text embedding vector into an object classifier and localization regressor. Then, for discriminative training, we then propose negative phrase augmentation (NPA) to mine hard negative samples which are visually similar to the query and at the same time semantically mutually exclusive of the query. The proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> can retrieve and localize objects specified by a <a href=https://en.wikipedia.org/wiki/Text-based_user_interface>textual query</a> from one million images in only 0.5 seconds with high precision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1282.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1282 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1282 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1282/>Grounding Semantic Roles in Images</a></strong><br><a href=/people/c/carina-silberer/>Carina Silberer</a>
|
<a href=/people/m/manfred-pinkal/>Manfred Pinkal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1282><div class="card-body p-3 small">We address the task of visual semantic role labeling (vSRL), the identification of the participants of a situation or event in a visual scene, and their labeling with their semantic relations to the event or situation. We render candidate participants as image regions of objects, and train a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> which learns to ground roles in the regions which depict the corresponding participant. Experimental results demonstrate that we can train a vSRL model without reliance on prohibitive image-based role annotations, by utilizing noisy data which we extract automatically from image captions using a linguistic SRL system. Furthermore, our model induces framesemantic visual representations, and their comparison to previous work on supervised visual verb sense disambiguation yields overall better results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1283 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1283" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1283/>Commonsense Justification for Action Explanation</a></strong><br><a href=/people/s/shaohua-yang/>Shaohua Yang</a>
|
<a href=/people/q/qiaozi-gao/>Qiaozi Gao</a>
|
<a href=/people/s/sari-saba-sadiya/>Sari Sadiya</a>
|
<a href=/people/j/joyce-chai/>Joyce Chai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1283><div class="card-body p-3 small">To enable collaboration and communication between humans and agents, this paper investigates learning to acquire commonsense evidence for action justification. In particular, we have developed an approach based on the generative Conditional Variational Autoencoder(CVAE) that models object relations / attributes of the world as latent variables and jointly learns a performer that predicts actions and an explainer that gathers commonsense evidence to justify the action. Our empirical results have shown that, compared to a typical attention-based model, CVAE achieves significantly higher performance in both action prediction and <a href=https://en.wikipedia.org/wiki/Theory_of_justification>justification</a>. A human subject study further shows that the commonsense evidence gathered by CVAE can be communicated to humans to achieve a significantly higher common ground between humans and agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1285.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1285 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1285 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1285/>Grounding <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a> by training semantic parsers using captioned videos</a></strong><br><a href=/people/c/candace-ross/>Candace Ross</a>
|
<a href=/people/a/andrei-barbu/>Andrei Barbu</a>
|
<a href=/people/y/yevgeni-berzak/>Yevgeni Berzak</a>
|
<a href=/people/b/battushig-myanganbayar/>Battushig Myanganbayar</a>
|
<a href=/people/b/boris-katz/>Boris Katz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1285><div class="card-body p-3 small">We develop a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> that is trained in a grounded setting using pairs of videos captioned with sentences. This setting is both data-efficient, requiring little <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, and similar to the experience of children where they observe their environment and listen to speakers. The <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> recovers the meaning of English sentences despite not having access to any annotated sentences. It does so despite the ambiguity inherent in vision where a sentence may refer to any combination of objects, object properties, relations or actions taken by any agent in a video. For this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we collected a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for grounded language acquisition. Learning a grounded semantic parser turning sentences into logical forms using captioned videos can significantly expand the range of data that parsers can be trained on, lower the effort of training a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a>, and ultimately lead to a better understanding of child language acquisition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1286.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1286 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1286 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1286.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1286/>Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation</a></strong><br><a href=/people/x/xiaoxue-zang/>Xiaoxue Zang</a>
|
<a href=/people/a/ashwini-pokle/>Ashwini Pokle</a>
|
<a href=/people/m/marynel-vazquez/>Marynel Vázquez</a>
|
<a href=/people/k/kevin-chen/>Kevin Chen</a>
|
<a href=/people/j/juan-carlos-niebles/>Juan Carlos Niebles</a>
|
<a href=/people/a/alvaro-soto/>Alvaro Soto</a>
|
<a href=/people/s/silvio-savarese/>Silvio Savarese</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1286><div class="card-body p-3 small">We propose an end-to-end deep learning model for translating free-form natural language instructions to a high-level plan for behavioral robot navigation. We use attention models to connect information from both the <a href=https://en.wikipedia.org/wiki/User_interface>user instructions</a> and a topological representation of the environment. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance on a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> containing 10,050 pairs of navigation instructions. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms baseline approaches. Furthermore, our results suggest that it is possible to leverage the <a href=https://en.wikipedia.org/wiki/Environment_map>environment map</a> as a relevant knowledge base to facilitate the translation of free-form navigational instruction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1288 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1288" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1288/>Deconvolutional Time Series Regression : A Technique for Modeling Temporally Diffuse Effects</a></strong><br><a href=/people/c/cory-shain/>Cory Shain</a>
|
<a href=/people/w/william-schuler/>William Schuler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1288><div class="card-body p-3 small">Researchers in computational psycholinguistics frequently use <a href=https://en.wikipedia.org/wiki/Linear_model>linear models</a> to study <a href=https://en.wikipedia.org/wiki/Time_series>time series data</a> generated by human subjects. However, <a href=https://en.wikipedia.org/wiki/Time_series>time series</a> may violate the assumptions of these models through temporal diffusion, where stimulus presentation has a lingering influence on the response as the rest of the experiment unfolds. This paper proposes a new <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical model</a> that borrows from <a href=https://en.wikipedia.org/wiki/Digital_signal_processing>digital signal processing</a> by recasting the predictors and response as convolutionally-related signals, using recent advances in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> to fit latent impulse response functions (IRFs) of arbitrary shape. A synthetic experiment shows successful recovery of true latent IRFs, and psycholinguistic experiments reveal plausible, replicable, and fine-grained estimates of latent temporal dynamics, with comparable or improved prediction quality to widely-used alternatives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1292.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1292 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1292 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1292" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1292/>Depth-bounding is effective : Improvements and evaluation of unsupervised PCFG induction<span class=acl-fixed-case>PCFG</span> induction</a></strong><br><a href=/people/l/lifeng-jin/>Lifeng Jin</a>
|
<a href=/people/f/finale-doshi-velez/>Finale Doshi-Velez</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/w/william-schuler/>William Schuler</a>
|
<a href=/people/l/lane-schwartz/>Lane Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1292><div class="card-body p-3 small">There have been several recent attempts to improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of grammar induction systems by bounding the recursive complexity of the <a href=https://en.wikipedia.org/wiki/Mathematical_induction>induction model</a>. Modern depth-bounded grammar inducers have been shown to be more accurate than early unbounded PCFG inducers, but this technique has never been compared against unbounded induction within the same system, in part because most previous depth-bounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. The present work instead applies depth bounds within a chart-based Bayesian PCFG inducer, where <a href=https://en.wikipedia.org/wiki/Upper_and_lower_bounds>bounding</a> can be switched on and off, and then samples trees with or without <a href=https://en.wikipedia.org/wiki/Upper_and_lower_bounds>bounding</a>. Results show that depth-bounding is indeed significantly effective in limiting the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> of the inducer and thereby increasing accuracy of resulting parsing model, independent of the contribution of modern Bayesian induction techniques. Moreover, parsing results on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a> show that this bounded model is able to produce parse trees more accurately than or competitively with state-of-the-art constituency grammar induction models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1293.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1293 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1293 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1293.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1293/>Incremental Computation of Infix Probabilities for Probabilistic Finite Automata</a></strong><br><a href=/people/m/marco-cognetta/>Marco Cognetta</a>
|
<a href=/people/y/yo-sub-han/>Yo-Sub Han</a>
|
<a href=/people/s/soon-chan-kwon/>Soon Chan Kwon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1293><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, a common task is to compute the probability of a phrase appearing in a document or to calculate the probability of all phrases matching a given pattern. For instance, one computes affix (prefix, <a href=https://en.wikipedia.org/wiki/Suffix>suffix</a>, <a href=https://en.wikipedia.org/wiki/Infix>infix</a>, etc.) probabilities of a string or a set of strings with respect to a probability distribution of patterns. The problem of computing infix probabilities of strings when the pattern distribution is given by a <a href=https://en.wikipedia.org/wiki/Probabilistic_context-free_grammar>probabilistic context-free grammar</a> or by a <a href=https://en.wikipedia.org/wiki/Probabilistic_finite_automaton>probabilistic finite automaton</a> is already solved, yet it was open to compute the infix probabilities in an incremental manner. The <a href=https://en.wikipedia.org/wiki/Incremental_computation>incremental computation</a> is crucial when a new query is built from a previous query. We tackle this problem and suggest a method that computes infix probabilities incrementally for <a href=https://en.wikipedia.org/wiki/Probabilistic_finite_automaton>probabilistic finite automata</a> by representing all the probabilities of matching strings as a series of transition matrix calculations. We show that the proposed approach is theoretically faster than the previous method and, using real world data, demonstrate that our approach has vastly better performance in practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1294.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1294 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1294 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1294/>Syntax Encoding with Application in Authorship Attribution</a></strong><br><a href=/people/r/richong-zhang/>Richong Zhang</a>
|
<a href=/people/z/zhiyuan-hu/>Zhiyuan Hu</a>
|
<a href=/people/h/hongyu-guo/>Hongyu Guo</a>
|
<a href=/people/y/yongyi-mao/>Yongyi Mao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1294><div class="card-body p-3 small">We propose a novel strategy to encode the syntax parse tree of sentence into a learnable distributed representation. The proposed syntax encoding scheme is provably information-lossless. In specific, an <a href=https://en.wikipedia.org/wiki/Embedding>embedding vector</a> is constructed for each word in the sentence, encoding the path in the <a href=https://en.wikipedia.org/wiki/Syntax_tree>syntax tree</a> corresponding to the word. The one-to-one correspondence between these syntax-embedding vectors and the words (hence their embedding vectors) in the sentence makes it easy to integrate such a representation with all word-level NLP models. We empirically show the benefits of the syntax embeddings on the Authorship Attribution domain, where our approach improves upon the prior art and achieves new performance records on five benchmarking data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1295.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1295 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1295 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1295/>Sanskrit Word Segmentation Using Character-level Recurrent and Convolutional Neural Networks<span class=acl-fixed-case>S</span>anskrit Word Segmentation Using Character-level Recurrent and Convolutional Neural Networks</a></strong><br><a href=/people/o/oliver-hellwig/>Oliver Hellwig</a>
|
<a href=/people/s/sebastian-nehrdich/>Sebastian Nehrdich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1295><div class="card-body p-3 small">The paper introduces end-to-end neural network models that tokenize <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a> by jointly splitting compounds and resolving phonetic merges (Sandhi). Tokenization of <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a> depends on local phonetic and distant semantic features that are incorporated using convolutional and recurrent elements. Contrary to most previous systems, our models do not require <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> or extern linguistic resources, but operate solely on parallel versions of raw and segmented text. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> discussed in this paper clearly improve over previous <a href=https://en.wikipedia.org/wiki/Conceptual_model>approaches</a> to Sanskrit word segmentation. As they are language agnostic, we will demonstrate that they also outperform the state of the art for the related task of German compound splitting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1296.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1296 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1296 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305942129 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1296/>Session-level Language Modeling for Conversational Speech</a></strong><br><a href=/people/w/wayne-xiong/>Wayne Xiong</a>
|
<a href=/people/l/lingfeng-wu/>Lingfeng Wu</a>
|
<a href=/people/j/jun-zhang/>Jun Zhang</a>
|
<a href=/people/a/andreas-stolcke/>Andreas Stolcke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1296><div class="card-body p-3 small">We propose to generalize language models for conversational speech recognition to allow them to operate across utterance boundaries and speaker changes, thereby capturing conversation-level phenomena such as <a href=https://en.wikipedia.org/wiki/Adjacency_pairs>adjacency pairs</a>, <a href=https://en.wikipedia.org/wiki/Lexical_entrainment>lexical entrainment</a>, and topical coherence. The model consists of a long-short-term memory (LSTM) recurrent network that reads the entire word-level history of a conversation, as well as information about turn taking and speaker overlap, in order to predict each next word. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is applied in a rescoring framework, where the word history prior to the current utterance is approximated with preliminary recognition results. In experiments in the conversational telephone speech domain (Switchboard) we find that such a model gives substantial perplexity reductions over a standard LSTM-LM with utterance scope, as well as improvements in word error rate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1297 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305942945 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1297" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1297/>Towards Less Generic Responses in Neural Conversation Models : A Statistical Re-weighting Method</a></strong><br><a href=/people/y/yahui-liu/>Yahui Liu</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/j/jun-gao/>Jun Gao</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a>
|
<a href=/people/j/jian-yao/>Jian Yao</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1297><div class="card-body p-3 small">Sequence-to-sequence neural generation models have achieved promising performance on short text conversation tasks. However, they tend to generate generic / dull responses, leading to unsatisfying dialogue experience. We observe that in the conversation tasks, each query could have multiple responses, which forms a 1-to-n or m-to-n relationship in the view of the total corpus. The <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> used in standard sequence-to-sequence models will be dominated by loss terms with generic patterns. Inspired by this observation, we introduce a statistical re-weighting method that assigns different weights for the multiple responses of the same query, and trains the common neural generation model with the weights. Experimental results on a large Chinese dialogue corpus show that our method improves the acceptance rate of generated responses compared with several baseline models and significantly reduces the number of generated generic responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1298.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1298 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1298 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305943582 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1298/>Training Millions of Personalized Dialogue Agents</a></strong><br><a href=/people/p/pierre-emmanuel-mazare/>Pierre-Emmanuel Mazaré</a>
|
<a href=/people/s/samuel-humeau/>Samuel Humeau</a>
|
<a href=/people/m/martin-raison/>Martin Raison</a>
|
<a href=/people/a/antoine-bordes/>Antoine Bordes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1298><div class="card-body p-3 small">Current dialogue systems fail at being engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> used in Zhang et al. (2018) is synthetic and only contains around 1k different <a href=https://en.wikipedia.org/wiki/Persona>personas</a>. In this paper we introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> providing 5 million <a href=https://en.wikipedia.org/wiki/Persona>personas</a> and 700 million persona-based dialogues. Our experiments show that, at this scale, training using <a href=https://en.wikipedia.org/wiki/Persona>personas</a> still improves the performance of <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end systems</a>. In addition, we show that other tasks benefit from the wide coverage of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by fine-tuning our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the data from Zhang et al. (2018) and achieving state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1299 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305944406 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1299/>Towards Universal Dialogue State Tracking</a></strong><br><a href=/people/l/liliang-ren/>Liliang Ren</a>
|
<a href=/people/k/kaige-xie/>Kaige Xie</a>
|
<a href=/people/l/lu-chen/>Lu Chen</a>
|
<a href=/people/k/kai-yu/>Kai Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1299><div class="card-body p-3 small">Dialogue state tracker is the core part of a <a href=https://en.wikipedia.org/wiki/Spoken_dialogue_system>spoken dialogue system</a>. It estimates the beliefs of possible user&#8217;s goals at every dialogue turn. However, for most current approaches, it&#8217;s difficult to scale to large dialogue domains. They have one or more of following limitations : (a) Some <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> do n&#8217;t work in the situation where slot values in ontology changes dynamically ; (b) The number of model parameters is proportional to the number of slots ; (c) Some <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> extract features based on hand-crafted lexicons. To tackle these challenges, we propose StateNet, a universal dialogue state tracker. It is independent of the number of values, shares parameters across all slots, and uses pre-trained word vectors instead of explicit semantic dictionaries. Our experiments on two datasets show that our approach not only overcomes the limitations, but also significantly outperforms the performance of state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1302 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305891047 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1302/>Reducing Gender Bias in Abusive Language Detection</a></strong><br><a href=/people/j/ji-ho-park/>Ji Ho Park</a>
|
<a href=/people/j/jamin-shin/>Jamin Shin</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1302><div class="card-body p-3 small">Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, You are a good woman was considered sexist when trained on an existing <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Such model bias is an obstacle for <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to be robust enough for practical use. In this work, we measure them on <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods : (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98 % and can be extended to correct model bias in other scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1305 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305892676 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1305/>WikiConv : A Corpus of the Complete Conversational History of a Large Online Collaborative Community<span class=acl-fixed-case>W</span>iki<span class=acl-fixed-case>C</span>onv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community</a></strong><br><a href=/people/y/yiqing-hua/>Yiqing Hua</a>
|
<a href=/people/c/cristian-danescu-niculescu-mizil/>Cristian Danescu-Niculescu-Mizil</a>
|
<a href=/people/d/dario-taraborelli/>Dario Taraborelli</a>
|
<a href=/people/n/nithum-thain/>Nithum Thain</a>
|
<a href=/people/j/jeffery-sorensen/>Jeffery Sorensen</a>
|
<a href=/people/l/lucas-dixon/>Lucas Dixon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1305><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> that encompasses the complete history of conversations between contributors to <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, one of the largest online collaborative communities. By recording the intermediate states of conversations-including not only comments and replies, but also their modifications, deletions and restorations-this <a href=https://en.wikipedia.org/wiki/Data>data</a> offers an unprecedented view of online conversation. Our framework is designed to be language agnostic, and we show that it extracts high quality data in both <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. This level of detail supports new research questions pertaining to the process (and challenges) of large-scale online collaboration. We illustrate the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>&#8217; potential with two case studies on <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a> that highlight new perspectives on earlier work. First, we explore how a person&#8217;s conversational behavior depends on how they relate to the discussion&#8217;s venue. Second, we show that community moderation of toxic behavior happens at a higher rate than previously estimated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1306 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1306.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306054703 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1306/>Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> for Biomedical Named Entity Recognition from Disjoint Label Sets</a></strong><br><a href=/people/n/nathan-greenberg/>Nathan Greenberg</a>
|
<a href=/people/t/trapit-bansal/>Trapit Bansal</a>
|
<a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1306><div class="card-body p-3 small">Extracting typed entity mentions from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> is a fundamental component to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding and reasoning</a>. While there exist substantial labeled text datasets for multiple subsets of biomedical entity typessuch as genes and proteins, or chemicals and diseasesit is rare to find large labeled datasets containing labels for all desired entity types together. This paper presents a method for training a single CRF extractor from multiple <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with disjoint or partially overlapping sets of entity types. Our approach employs marginal likelihood training to insist on labels that are present in the data, while filling in missing labels. This allows us to leverage all the available data within a single <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. In experimental results on the Biocreative V CDR (chemicals / diseases), Biocreative VI ChemProt (chemicals / proteins) and MedMentions (19 entity types) datasets, we show that joint training on multiple datasets improves NER F1 over training in isolation, and our methods achieve state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1308 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1308.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306056257 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1308" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1308/>Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree Decoding</a></strong><br><a href=/people/g/gaurav-singh-tomar/>Gaurav Singh</a>
|
<a href=/people/j/james-thomas/>James Thomas</a>
|
<a href=/people/i/iain-marshall/>Iain Marshall</a>
|
<a href=/people/j/john-shawe-taylor/>John Shawe-Taylor</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1308><div class="card-body p-3 small">We propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for tagging <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured texts</a> with an arbitrary number of terms drawn from a <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree-structured vocabulary</a> (i.e., an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a>). We treat this as a special case of sequence-to-sequence learning in which the decoder begins at the root node of an ontological tree and recursively elects to expand child nodes as a function of the input text, the current node, and the latent decoder state. We demonstrate that this method yields state-of-the-art results on the important task of assigning <a href=https://en.wikipedia.org/wiki/Medical_Subject_Headings>MeSH terms</a> to biomedical abstracts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1309 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306057139 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1309/>Deep Exhaustive Model for Nested Named Entity Recognition</a></strong><br><a href=/people/m/mohammad-golam-sohrab/>Mohammad Golam Sohrab</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1309><div class="card-body p-3 small">We propose a simple <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural model</a> for nested named entity recognition (NER). Most NER models focused on flat entities and ignored nested entities, which failed to fully capture underlying semantic information in texts. The key idea of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is to enumerate all possible regions or spans as potential entity mentions and classify them with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. To reduce the computational costs and capture the information of the contexts around the regions, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> represents the regions using the outputs of shared underlying bidirectional long short-term memory. We evaluate our exhaustive <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the GENIA and JNLPBA corpora in biomedical domain, and the results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art models on nested and flat NER, achieving 77.1 % and 78.4 % respectively in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>, without any external knowledge resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1310 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306112516 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1310" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1310/>Evaluating the Utility of <a href=https://en.wikipedia.org/wiki/Handicraft>Hand-crafted Features</a> in Sequence Labelling</a></strong><br><a href=/people/m/minghao-wu/>Minghao Wu</a>
|
<a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1310><div class="card-body p-3 small">Conventional wisdom is that hand-crafted features are redundant for deep learning models, as they already learn adequate representations of text automatically from corpora. In this work, we test this claim by proposing a new method for exploiting handcrafted features as part of a novel hybrid learning approach, incorporating a feature auto-encoder loss component. We evaluate on the task of named entity recognition (NER), where we show that including manual features for part-of-speech, word shapes and gazetteers can improve the performance of a neural CRF model. We obtain a F 1 of 91.89 for the CoNLL-2003 English shared task, which significantly outperforms a collection of highly competitive baseline models. We also present an ablation study showing the importance of auto-encoding, over using features as either inputs or outputs alone, and moreover, show including the autoencoder components reduces training requirements to 60 %, while retaining the same predictive accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1311 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305667813 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1311/>Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data</a></strong><br><a href=/people/w/wenhui-wang/>Wenhui Wang</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/m/mairgup-mansur/>Mairgup Mansur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1311><div class="card-body p-3 small">Pre-trained word embeddings and <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> have been shown useful in a lot of tasks. However, both of them can not directly capture word connections in a sentence, which is important for dependency parsing given its goal is to establish dependency relations between words. In this paper, we propose to implicitly capture word connections from unlabeled data by a word ordering model with self-attention mechanism. Experiments show that these implicit word connections do improve our parsing model. Furthermore, by combining with a pre-trained language model, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> gets state-of-the-art performance on the English PTB dataset, achieving 96.35 % UAS and 95.25 % LAS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1312 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1312.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305925065 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1312/>A Framework for Understanding the Role of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a> in Universal Dependency Parsing<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Parsing</a></strong><br><a href=/people/m/mathieu-dehouck/>Mathieu Dehouck</a>
|
<a href=/people/p/pascal-denis/>Pascal Denis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1312><div class="card-body p-3 small">This paper presents a simple <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for characterizing <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological complexity</a> and how it encodes <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>. In particular, we propose a new measure of morpho-syntactic complexity in terms of governor-dependent preferential attachment that explains <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance. Through experiments on dependency parsing with data from Universal Dependencies (UD), we show that representations derived from morphological attributes deliver important parsing performance improvements over standard word form embeddings when trained on the same datasets. We also show that the new morpho-syntactic complexity measure is predictive of the gains provided by using morphological attributes over plain forms on parsing scores, making it a tool to distinguish languages using <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> as a syntactic marker from others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1314 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305671668 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1314/>Imitation Learning for Neural Morphological String Transduction</a></strong><br><a href=/people/p/peter-makarov/>Peter Makarov</a>
|
<a href=/people/s/simon-clematide/>Simon Clematide</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1314><div class="card-body p-3 small">We employ imitation learning to train a neural transition-based string transducer for morphological tasks such as <a href=https://en.wikipedia.org/wiki/Inflection>inflection generation</a> and <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a>. Previous approaches to training this type of <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> either rely on an external character aligner for the production of gold action sequences, which results in a suboptimal <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> due to the unwarranted dependence on a single gold action sequence despite spurious ambiguity, or require warm starting with an MLE model. Our approach only requires a simple expert policy, eliminating the need for a character aligner or <a href=https://en.wikipedia.org/wiki/Warm_start>warm start</a>. It also addresses familiar MLE training biases and leads to strong and state-of-the-art performance on several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1315 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305674091 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1315/>An Encoder-Decoder Approach to the Paradigm Cell Filling Problem</a></strong><br><a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1315><div class="card-body p-3 small">The Paradigm Cell Filling Problem in <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> asks to complete word inflection tables from partial ones. We implement novel neural models for this task, evaluating them on 18 data sets in 8 languages, showing performance that is comparable with previous work with far less training data. We also publish a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and code implementing the <a href=https://en.wikipedia.org/wiki/System>system</a> described in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1317 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1317/>Multi-Head Attention with Disagreement Regularization</a></strong><br><a href=/people/j/jian-li/>Jian Li</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/b/baosong-yang/>Baosong Yang</a>
|
<a href=/people/m/michael-r-lyu/>Michael R. Lyu</a>
|
<a href=/people/t/tong-zhang/>Tong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1317><div class="card-body p-3 small">Multi-head attention is appealing for the ability to jointly attend to information from different representation subspaces at different positions. In this work, we introduce a disagreement regularization to explicitly encourage the diversity among multiple attention heads. Specifically, we propose three types of disagreement regularization, which respectively encourage the <a href=https://en.wikipedia.org/wiki/Linear_subspace>subspace</a>, the attended positions, and the output representation associated with each attention head to be different from other heads. Experimental results on widely-used WMT14 English-German and WMT17 Chinese-English translation tasks demonstrate the effectiveness and universality of the proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1318 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1318/>Deep Bayesian Active Learning for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> : Results of a Large-Scale Empirical Study<span class=acl-fixed-case>B</span>ayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study</a></strong><br><a href=/people/a/aditya-siddhant/>Aditya Siddhant</a>
|
<a href=/people/z/zachary-c-lipton/>Zachary C. Lipton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1318><div class="card-body p-3 small">Several recent papers investigate Active Learning (AL) for mitigating the <a href=https://en.wikipedia.org/wiki/Data_dependence>data dependence</a> of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. However, the applicability of AL to real-world problems remains an open question. While in <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>, practitioners can try many different methods, evaluating each against a validation set before selecting a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we have no opportunity to compare <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and acquisition functions. This paper provides a large-scale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement, using uncertainty estimates provided either by Dropout or <a href=https://en.wikipedia.org/wiki/Bayes&#8217;_rule>Bayes-by-Backprop</a> significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1319 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1319.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1319" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1319/>Bayesian Compression for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a><span class=acl-fixed-case>B</span>ayesian Compression for Natural Language Processing</a></strong><br><a href=/people/n/nadezhda-chirkova/>Nadezhda Chirkova</a>
|
<a href=/people/e/ekaterina-lobacheva/>Ekaterina Lobacheva</a>
|
<a href=/people/d/dmitry-vetrov/>Dmitry Vetrov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1319><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, a lot of the tasks are successfully solved with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>, but such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have a huge number of parameters. The majority of these parameters are often concentrated in the <a href=https://en.wikipedia.org/wiki/Embedding>embedding layer</a>, which size grows proportionally to the vocabulary length. We propose a Bayesian sparsification technique for RNNs which allows compressing the RNN dozens or hundreds of times without time-consuming hyperparameters tuning. We also generalize the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for vocabulary sparsification to filter out unnecessary words and compress the RNN even further. We show that the choice of the kept words is interpretable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1321 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1321" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1321/>Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet<span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>P</span>inyin Aided <span class=acl-fixed-case>IME</span>, Input What You Have Not Keystroked Yet</a></strong><br><a href=/people/y/yafang-huang/>Yafang Huang</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1321><div class="card-body p-3 small">Chinese pinyin input method engine (IME) converts pinyin into character so that <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a> can be conveniently inputted into computer through common keyboard. IMEs work relying on its core component, pinyin-to-character conversion (P2C). Usually Chinese IMEs simply predict a list of character sequences for user choice only according to user pinyin input at each turn. However, Chinese inputting is a multi-turn online procedure, which can be supposed to be exploited for further user experience promoting. This paper thus for the first time introduces a sequence-to-sequence model with gated-attention mechanism for the core task in <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>IMEs</a>. The proposed neural P2C model is learned by encoding previous input utterance as extra context to enable our IME capable of predicting character sequence with incomplete pinyin input. Our model is evaluated in different benchmark datasets showing great user experience improvement compared to traditional models, which demonstrates the first engineering practice of building Chinese aided IME.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1322 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1322/>Estimating Marginal Probabilities of n-grams for Recurrent Neural Language Models</a></strong><br><a href=/people/t/thanapon-noraset/>Thanapon Noraset</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1322><div class="card-body p-3 small">Recurrent neural network language models (RNNLMs) are the current standard-bearer for statistical language modeling. However, RNNLMs only estimate probabilities for complete sequences of text, whereas some applications require context-independent phrase probabilities instead. In this paper, we study how to compute an RNNLM&#8217;s em marginal probability : the probability that the model assigns to a short sequence of text when the preceding context is not known. We introduce a simple method of altering the RNNLM training to make the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> more accurate at marginal estimation. Our experiments demonstrate that the technique is effective compared to baselines including the traditional RNNLM probability and an importance sampling approach. Finally, we show how we can use the marginal estimation to improve an RNNLM by training the <a href=https://en.wikipedia.org/wiki/Marginal_distribution>marginals</a> to match n-gram probabilities from a larger corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1323 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1323.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1323/>How to represent a word and predict it, too : Improving tied architectures for language modelling</a></strong><br><a href=/people/k/kristina-gulordava/>Kristina Gulordava</a>
|
<a href=/people/l/laura-aina/>Laura Aina</a>
|
<a href=/people/g/gemma-boleda/>Gemma Boleda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1323><div class="card-body p-3 small">Recent state-of-the-art neural language models share the representations of words given by the input and output mappings. We propose a simple modification to these <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> that decouples the hidden state from the word embedding prediction. Our architecture leads to comparable or better results compared to previous tied models and <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> without tying, with a much smaller number of parameters. We also extend our proposal to word2vec models, showing that tying is appropriate for general word prediction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1324 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1324/>The Importance of Generation Order in Language Modeling</a></strong><br><a href=/people/n/nicolas-ford/>Nicolas Ford</a>
|
<a href=/people/d/daniel-duckworth/>Daniel Duckworth</a>
|
<a href=/people/m/mohammad-norouzi/>Mohammad Norouzi</a>
|
<a href=/people/g/george-dahl/>George Dahl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1324><div class="card-body p-3 small">Neural language models are a critical component of state-of-the-art systems for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>audio transcription</a>, and other tasks. These language models are almost universally autoregressive in nature, generating sentences one token at a time from left to right. This paper studies the influence of token generation order on model quality via a novel two-pass language model that produces partially-filled sentence templates and then fills in missing tokens. We compare various <a href=https://en.wikipedia.org/wiki/Strategy_(game_theory)>strategies</a> for structuring these two passes and observe a surprisingly large variation in model quality. We find the most effective <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> generates <a href=https://en.wikipedia.org/wiki/Function_word>function words</a> in the first pass followed by <a href=https://en.wikipedia.org/wiki/Content_word>content words</a> in the second. We believe these experimental results justify a more extensive investigation of the generation order for neural language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1326 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1326/>Three Strategies to Improve One-to-Many Multilingual Translation</a></strong><br><a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/f/feifei-zhai/>Feifei Zhai</a>
|
<a href=/people/j/jingfang-xu/>Jingfang Xu</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1326><div class="card-body p-3 small">Due to the benefits of model compactness, multilingual translation (including many-to-one, many-to-many and one-to-many) based on a universal encoder-decoder architecture attracts more and more attention. However, previous studies show that one-to-many translation based on this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> can not perform on par with the individually trained models. In this work, we introduce three <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> to improve one-to-many multilingual translation by balancing the shared and unique features. Within the architecture of one <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> for all target languages, we first exploit the use of unique initial states for different target languages. Then, we employ language-dependent positional embeddings. Finally and especially, we propose to divide the hidden cells of the <a href=https://en.wikipedia.org/wiki/Code>decoder</a> into shared and language-dependent ones. The extensive experiments demonstrate that our proposed methods can obtain remarkable improvements over the strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. Moreover, our <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> can achieve comparable or even better performance than the individually trained <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1327 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1327/>Multi-Source Syntactic Neural Machine Translation</a></strong><br><a href=/people/a/anna-currey/>Anna Currey</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1327><div class="card-body p-3 small">We introduce a novel multi-source technique for incorporating <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>source syntax</a> into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> using linearized parses. This is achieved by employing separate <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> for the sequential and parsed versions of the same source sentence ; the resulting representations are then combined using a hierarchical attention mechanism. The proposed model improves over both seq2seq and parsed baselines by over 1 BLEU on the WMT17 English-German task. Further analysis shows that our multi-source syntactic model is able to translate successfully without any parsed input, unlike standard parsed methods. In addition, performance does not deteriorate as much on long sentences as for the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1328 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1328" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1328/>Fixing Translation Divergences in Parallel Corpora for Neural MT<span class=acl-fixed-case>MT</span></a></strong><br><a href=/people/m/minh-quang-pham/>MinhQuang Pham</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1328><div class="card-body p-3 small">Corpus-based approaches to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> for detecting translation divergences in parallel sentences. We rely on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> that computes cross-lingual sentence similarity scores, which are then used to effectively filter out divergent translations. Furthermore, similarity scores predicted by the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a> are used to identify and fix some partial divergences, yielding additional parallel segments. We evaluate these methods for English-French and English-German machine translation tasks, and show that using filtered / corrected corpora actually improves MT performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1330 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1330.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1330" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1330/>Loss in Translation : Learning Bilingual Word Mapping with a Retrieval Criterion</a></strong><br><a href=/people/a/armand-joulin/>Armand Joulin</a>
|
<a href=/people/p/piotr-bojanowski/>Piotr Bojanowski</a>
|
<a href=/people/t/tomas-mikolov/>Tomas Mikolov</a>
|
<a href=/people/h/herve-jegou/>Hervé Jégou</a>
|
<a href=/people/e/edouard-grave/>Edouard Grave</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1330><div class="card-body p-3 small">Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve a quadratic problem to learn a <a href=https://en.wikipedia.org/wiki/Orthogonal_matrix>orthogonal matrix</a> aligning a <a href=https://en.wikipedia.org/wiki/Bilingual_lexicon>bilingual lexicon</a>, and use a retrieval criterion for <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. In this paper, we propose an unified formulation that directly optimizes a retrieval criterion in an end-to-end fashion. Our experiments on standard <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarks</a> show that our approach outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> on word translation, with the biggest improvements observed for distant language pairs such as <a href=https://en.wikipedia.org/wiki/Standard_Chinese>English-Chinese</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1334.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1334 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1334 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1334/>Getting Gender Right in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/e/eva-vanmassenhove/>Eva Vanmassenhove</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1334><div class="card-body p-3 small">Speakers of different languages must attend to and encode strikingly different aspects of the world in order to use their language correctly (Sapir, 1921 ; Slobin, 1996). One such difference is related to the way gender is expressed in a language. Saying I am happy in English, does not encode any additional knowledge of the speaker that uttered the sentence. However, many other languages do have <a href=https://en.wikipedia.org/wiki/Grammatical_gender>grammatical gender systems</a> and so such knowledge would be encoded. In order to correctly translate such a sentence into, say, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, the inherent gender information needs to be retained / recovered. The same sentence would become either Je suis heureux, for a male speaker or Je suis heureuse for a female one. Apart from <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological agreement</a>, <a href=https://en.wikipedia.org/wiki/Demography>demographic factors</a> (gender, age, etc.) also influence our use of language in terms of word choices or syntactic constructions (Tannen, 1991 ; Pennebaker et al., 2003). We integrate <a href=https://en.wikipedia.org/wiki/Gender>gender information</a> into <a href=https://en.wikipedia.org/wiki/Network_topology>NMT systems</a>. Our contribution is two-fold : (1) the compilation of large datasets with speaker information for 20 language pairs, and (2) a simple set of experiments that incorporate gender information into NMT for multiple language pairs. Our experiments show that adding a gender feature to an NMT system significantly improves the translation quality for some language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1338.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1338 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1338 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1338/>Training Deeper Neural Machine Translation Models with Transparent Attention</a></strong><br><a href=/people/a/ankur-bapna/>Ankur Bapna</a>
|
<a href=/people/m/mia-xu-chen/>Mia Chen</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a>
|
<a href=/people/y/yuan-cao/>Yuan Cao</a>
|
<a href=/people/y/yonghui-wu/>Yonghui Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1338><div class="card-body p-3 small">While current state-of-the-art NMT models, such as RNN seq2seq and Transformers, possess a large number of parameters, they are still shallow in comparison to convolutional models used for both text and vision applications. In this work we attempt to train significantly (2-3x) deeper Transformer and Bi-RNN encoders for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We propose a simple modification to the attention mechanism that eases the optimization of deeper models, and results in consistent gains of 0.7-1.1 BLEU on the benchmark WMT&#8217;14 English-German and WMT&#8217;15 Czech-English tasks for both <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1339.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1339 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1339 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1339/>Context and Copying in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/r/rebecca-knowles/>Rebecca Knowles</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1339><div class="card-body p-3 small">Neural machine translation systems with subword vocabularies are capable of <a href=https://en.wikipedia.org/wiki/Translation>translating</a> or copying unknown words. In this work, we show that they learn to copy words based on both the context in which the words appear as well as features of the words themselves. In contexts that are particularly copy-prone, they even copy words that they have already learned they should translate. We examine the influence of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context and subword features</a> on this and other types of <a href=https://en.wikipedia.org/wiki/Copying>copying behavior</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1342 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1342/>Breaking the Beam Search Curse : A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation</a></strong><br><a href=/people/y/yilin-yang/>Yilin Yang</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/m/mingbo-ma/>Mingbo Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1342><div class="card-body p-3 small">Beam search is widely used in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, and usually improves translation quality compared to <a href=https://en.wikipedia.org/wiki/Greedy_search>greedy search</a>. It has been widely observed that, however, beam sizes larger than 5 hurt translation quality. We explain why this happens, and propose several <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> to address this problem. Furthermore, we discuss the optimal stopping criteria for these methods. Results show that our hyperparameter-free methods outperform the widely-used hyperparameter-free heuristic of length normalization by +2.0 BLEU, and achieve the best results among all methods on Chinese-to-English translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1343 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1343.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1343" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1343/>Multi-Multi-View Learning : Multilingual and Multi-Representation Entity Typing</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1343><div class="card-body p-3 small">Accurate and complete knowledge bases (KBs) are paramount in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. We employ mul-itiview learning for increasing the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and coverage of entity type information in <a href=https://en.wikipedia.org/wiki/Knowledge_base>KBs</a>. We rely on two <a href=https://en.wikipedia.org/wiki/Metafiction>metaviews</a> : language and representation. For <a href=https://en.wikipedia.org/wiki/Language>language</a>, we consider high-resource and low-resource languages from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. For <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a>, we consider representations based on the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context distribution</a> of the entity (i.e., on its embedding), on the entity&#8217;s name (i.e., on its surface form) and on its description in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. The two metaviews language and representation can be freely combined : each pair of <a href=https://en.wikipedia.org/wiki/Language_(computer_science)>language</a> and representation (e.g., German embedding, English description, Spanish name) is a distinct view. Our experiments on entity typing with fine-grained classes demonstrate the effectiveness of multiview learning. We release MVET, a large multiview and, in particular, multilingual entity typing dataset we created. Mono- and multilingual fine-grained entity typing systems can be evaluated on this dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1344 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1344/>Word Embeddings for Code-Mixed Language Processing</a></strong><br><a href=/people/a/adithya-pratapa/>Adithya Pratapa</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1344><div class="card-body p-3 small">We compare three existing bilingual word embedding approaches, and a novel approach of training skip-grams on synthetic code-mixed text generated through linguistic models of code-mixing, on two tasks-sentiment analysis and POS tagging for code-mixed text. Our results show that while CVM and CCA based embeddings perform as well as the proposed embedding technique on semantic and syntactic tasks respectively, the proposed approach provides the best performance for both tasks overall. Thus, this study demonstrates that existing bilingual embedding techniques are not ideal for code-mixed text processing and there is a need for learning multilingual word embedding from the code-mixed text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1346.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1346 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1346 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1346/>Code-switched Language Models Using Dual RNNs and Same-Source Pretraining<span class=acl-fixed-case>RNN</span>s and Same-Source Pretraining</a></strong><br><a href=/people/s/saurabh-garg/>Saurabh Garg</a>
|
<a href=/people/t/tanmay-parekh/>Tanmay Parekh</a>
|
<a href=/people/p/preethi-jyothi/>Preethi Jyothi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1346><div class="card-body p-3 small">This work focuses on building language models (LMs) for code-switched text. We propose two techniques that significantly improve these LMs : 1) A novel recurrent neural network unit with dual components that focus on each language in the code-switched text separately 2) Pretraining the LM using synthetic text from a generative model estimated using the training data. We demonstrate the effectiveness of our proposed techniques by reporting perplexities on a Mandarin-English task and derive significant reductions in <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1347 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1347/>Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification</a></strong><br><a href=/people/k/kelsey-ball/>Kelsey Ball</a>
|
<a href=/people/d/dan-garrette/>Dan Garrette</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1347><div class="card-body p-3 small">Code-switching, the use of more than one language within a single utterance, is ubiquitous in much of the world, but remains a challenge for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> largely due to the lack of representative data for training models. In this paper, we present a novel model architecture that is trained exclusively on <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual resources</a>, but can be applied to unseen code-switched text at inference time. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> accomplishes this by jointly maintaining separate word representations for each of the possible languages, or <a href=https://en.wikipedia.org/wiki/Writing_system>scripts</a> in the case of <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a>, allowing each to contribute to inferences without forcing the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to commit to a language. Experiments on Hindi-English part-of-speech tagging demonstrate that our approach outperforms standard models when training on monolingual text without <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a>, and testing on code-switched text with alternate scripts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1349 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305946571 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1349" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1349/>Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts</a></strong><br><a href=/people/d/di-jin/>Di Jin</a>
|
<a href=/people/p/peter-szolovits/>Peter Szolovits</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1349><div class="card-body p-3 small">Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> is needed for better overall classification performance. In this work, we present a hierarchical sequential labeling network to make use of the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> within surrounding sentences to help classify the current sentence. Our model outperforms the state-of-the-art results by 2%-3 % on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1350.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1350 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1350 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305947408 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1350" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1350/>Investigating Capsule Networks with <a href=https://en.wikipedia.org/wiki/Dynamic_routing>Dynamic Routing</a> for Text Classification</a></strong><br><a href=/people/w/wei-zhao/>Wei Zhao</a>
|
<a href=/people/j/jianbo-ye/>Jianbo Ye</a>
|
<a href=/people/m/min-yang/>Min Yang</a>
|
<a href=/people/z/zeyang-lei/>Zeyang Lei</a>
|
<a href=/people/s/suofei-zhang/>Suofei Zhang</a>
|
<a href=/people/z/zhou-zhao/>Zhou Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1350><div class="card-body p-3 small">In this study, we explore capsule networks with <a href=https://en.wikipedia.org/wiki/Dynamic_routing>dynamic routing</a> for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. We propose three strategies to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain background information or have not been successfully trained. A series of experiments are conducted with capsule networks on six text classification benchmarks. Capsule networks achieve state of the art on 4 out of 6 datasets, which shows the effectiveness of capsule networks for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. We additionally show that capsule networks exhibit significant improvement when transfer single-label to multi-label text classification over strong baseline methods. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for text modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1352 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305948835 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1352" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1352/>Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces</a></strong><br><a href=/people/a/anthony-rios/>Anthony Rios</a>
|
<a href=/people/r/ramakanth-kavuluru/>Ramakanth Kavuluru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1352><div class="card-body p-3 small">Large multi-label datasets contain labels that occur thousands of times (frequent group), those that occur only a few times (few-shot group), and labels that never appear in the training dataset (zero-shot group). Multi-label few- and zero-shot label prediction is mostly unexplored on datasets with large label spaces, especially for text classification. In this paper, we perform a fine-grained evaluation to understand how state-of-the-art methods perform on infrequent labels. Furthermore, we develop few- and zero-shot methods for multi-label text classification when there is a known structure over the label space, and evaluate them on two publicly available medical text datasets : MIMIC II and MIMIC III. For few-shot labels we achieve improvements of 6.2 % and 4.8 % in <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R@10</a> for MIMIC II and MIMIC III, respectively, over prior efforts ; the corresponding <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R@10</a> improvements for zero-shot labels are 17.3 % and 19 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1355 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305927122 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1355" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1355/>Integrating Transformer and Paraphrase Rules for Sentence Simplification</a></strong><br><a href=/people/s/sanqiang-zhao/>Sanqiang Zhao</a>
|
<a href=/people/r/rui-meng/>Rui Meng</a>
|
<a href=/people/d/daqing-he/>Daqing He</a>
|
<a href=/people/a/andi-saptono/>Andi Saptono</a>
|
<a href=/people/b/bambang-parmanto/>Bambang Parmanto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1355><div class="card-body p-3 small">Sentence simplification aims to reduce the <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of a sentence while retaining its original meaning. Current models for <a href=https://en.wikipedia.org/wiki/Sentence_simplification>sentence simplification</a> adopted ideas from machine translation studies and implicitly learned simplification mapping rules from normal-simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we propose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits : (1) the integrated model outperforms multiple state-of-the-art baseline models for <a href=https://en.wikipedia.org/wiki/Sentence_simplification>sentence simplification</a> in the literature (2) through analysis of the rule utilization, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> seeks to select more accurate simplification rules. The code and models used in the paper are available at.<url>https://github.com/Sanqiang/text_simplification</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1357 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305929800 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1357/>Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation</a></strong><br><a href=/people/r/renjie-zheng/>Renjie Zheng</a>
|
<a href=/people/m/mingbo-ma/>Mingbo Ma</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1357><div class="card-body p-3 small">Neural text generation, including <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, image captioning, and <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, has been quite successful recently. However, during training time, typically only one reference is considered for each example, even though there are often multiple references available, e.g., 4 references in <a href=https://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology>NIST MT evaluations</a>, and 5 references in image captioning data. We first investigate several different ways of utilizing multiple human references during training. But more importantly, we then propose an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to generate exponentially many pseudo-references by first compressing existing human references into <a href=https://en.wikipedia.org/wiki/Lattice_(group)>lattices</a> and then traversing them to generate new pseudo-references. These approaches lead to substantial improvements over strong <a href=https://en.wikipedia.org/wiki/Baseline_(typography)>baselines</a> in both <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> (+1.5 BLEU) and image captioning (+3.1 BLEU / +11.7 CIDEr).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1364 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305679809 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1364/>Implicational Universals in Stochastic Constraint-Based Phonology</a></strong><br><a href=/people/g/giorgio-magri/>Giorgio Magri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1364><div class="card-body p-3 small">This paper focuses on the most basic implicational universals in <a href=https://en.wikipedia.org/wiki/Phonology>phonological theory</a>, called T-orders after Anttila and Andrus (2006). It shows that the T-orders predicted by stochastic (and partial order) Optimality Theory coincide with those predicted by categorical OT. Analogously, the T-orders predicted by stochastic Harmonic Grammar coincide with those predicted by categorical HG. In other words, these stochastic constraint-based frameworks do not tamper with the <a href=https://en.wikipedia.org/wiki/Typology_(linguistics)>typological structure</a> induced by the original <a href=https://en.wikipedia.org/wiki/Categorical_variable>categorical frameworks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1365 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1365.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305681577 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1365" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1365/>Explaining Character-Aware Neural Networks for Word-Level Prediction : Do They Discover Linguistic Rules?</a></strong><br><a href=/people/f/frederic-godin/>Fréderic Godin</a>
|
<a href=/people/k/kris-demuynck/>Kris Demuynck</a>
|
<a href=/people/j/joni-dambre/>Joni Dambre</a>
|
<a href=/people/w/wesley-de-neve/>Wesley De Neve</a>
|
<a href=/people/t/thomas-demeester/>Thomas Demeester</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1365><div class="card-body p-3 small">Character-level features are currently used in different neural network-based natural language processing algorithms. However, little is known about the character-level patterns those <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> learn. Moreover, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are often compared only quantitatively while a qualitative analysis is missing. In this paper, we investigate which character-level patterns neural networks learn and if those <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> coincide with manually-defined word segmentations and <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>. To that end, we extend the contextual decomposition technique (Murdoch et al. 2018) to <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> which allows us to compare <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> and bidirectional long short-term memory networks. We evaluate and compare these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for the task of morphological tagging on three morphologically different languages and show that these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> implicitly discover understandable linguistic rules.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1367.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1367 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1367 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1367/>A Computational Exploration of Exaggeration</a></strong><br><a href=/people/e/enrica-troiano/>Enrica Troiano</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a>
|
<a href=/people/g/gozde-ozbal/>Gözde Özbal</a>
|
<a href=/people/s/serra-sinem-tekiroglu/>Serra Sinem Tekiroğlu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1367><div class="card-body p-3 small">Several NLP studies address the problem of <a href=https://en.wikipedia.org/wiki/Figurative_language>figurative language</a>, but among non-literal phenomena, they have neglected <a href=https://en.wikipedia.org/wiki/Exaggeration>exaggeration</a>. This paper presents a first <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational approach</a> to this <a href=https://en.wikipedia.org/wiki/Figure_of_speech>figure of speech</a>. We explore the possibility to automatically detect <a href=https://en.wikipedia.org/wiki/Exaggeration>exaggerated sentences</a>. First, we introduce HYPO, a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> containing overstatements (or hyperboles) collected on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> and validated via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. Then, we evaluate a number of <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on HYPO, and bring evidence that the task of hyperbole identification can be successfully performed based on a small set of <a href=https://en.wikipedia.org/wiki/Semantic_feature>semantic features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1368.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1368 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1368 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1368/>Building Context-aware Clause Representations for Situation Entity Type Classification</a></strong><br><a href=/people/z/zeyu-dai/>Zeyu Dai</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1368><div class="card-body p-3 small">Capabilities to categorize a clause based on the type of situation entity (e.g., <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a>, states and generic statements) the clause introduces to the <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> can benefit many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP applications</a>. Observing that the situation entity type of a clause depends on discourse functions the clause plays in a paragraph and the interpretation of discourse functions depends heavily on paragraph-wide contexts, we propose to build context-aware clause representations for predicting situation entity types of clauses. Specifically, we propose a hierarchical recurrent neural network model to read a whole paragraph at a time and jointly learn representations for all the clauses in the paragraph by extensively modeling context influences and inter-dependencies of clauses. Experimental results show that our model achieves the state-of-the-art performance for clause-level situation entity classification on the genre-rich MASC+Wiki corpus, which approaches human-level performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1370.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1370 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1370 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1370.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1370/>Investigating the Role of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>Argumentation</a> in the <a href=https://en.wikipedia.org/wiki/Rhetorical_analysis>Rhetorical Analysis</a> of <a href=https://en.wikipedia.org/wiki/Scientific_literature>Scientific Publications</a> with Neural Multi-Task Learning Models</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/k/kai-eckert/>Kai Eckert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1370><div class="card-body p-3 small">Exponential growth in the number of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publications</a> yields the need for effective automatic analysis of rhetorical aspects of scientific writing. Acknowledging the argumentative nature of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific text</a>, in this work we investigate the link between the argumentative structure of scientific publications and rhetorical aspects such as discourse categories or <a href=https://en.wikipedia.org/wiki/Citation>citation contexts</a>. To this end, we (1) augment a corpus of scientific publications annotated with four layers of rhetoric annotations with argumentation annotations and (2) investigate neural multi-task learning architectures combining argument extraction with a set of rhetorical classification tasks. By coupling rhetorical classifiers with the extraction of argumentative components in a joint multi-task learning setting, we obtain significant performance gains for different rhetorical analysis tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1371.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1371 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1371 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1371" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1371/>Neural Ranking Models for Temporal Dependency Structure Parsing</a></strong><br><a href=/people/y/yuchen-zhang/>Yuchen Zhang</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1371><div class="card-body p-3 small">We design and build the first neural temporal dependency parser. It utilizes a neural ranking model with minimal feature engineering, and parses time expressions and events in a text into a temporal dependency tree structure. We evaluate our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> on two domains : <a href=https://en.wikipedia.org/wiki/News>news reports</a> and <a href=https://en.wikipedia.org/wiki/Narrative>narrative stories</a>. In a parsing-only evaluation setup where gold time expressions and <a href=https://en.wikipedia.org/wiki/Event_(computing)>events</a> are provided, our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> reaches 0.81 and 0.70 <a href=https://en.wikipedia.org/wiki/F-score>f-score</a> on unlabeled and labeled parsing respectively, a result that is very competitive against alternative approaches. In an end-to-end evaluation setup where time expressions and events are automatically recognized, our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> beats two strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on both data domains. Our experimental results and discussions shed light on the nature of temporal dependency structures in different domains and provide insights that we believe will be valuable to future research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1372.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1372 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1372 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1372.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1372/>Causal Explanation Analysis on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/y/youngseo-son/>Youngseo Son</a>
|
<a href=/people/n/nipun-bayas/>Nipun Bayas</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1372><div class="card-body p-3 small">Understanding causal explanations-reasons given for happenings in one&#8217;s life-has been found to be an important psychological factor linked to physical and mental health. Causal explanations are often studied through manual identification of phrases over limited samples of personal writing. Automatic identification of causal explanations in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, while challenging in relying on contextual and sequential cues, offers a larger-scale alternative to expensive manual ratings and opens the door for new applications (e.g. studying prevailing beliefs about causes, such as climate change). Here, we explore automating causal explanation analysis, building on discourse parsing, and presenting two novel subtasks : causality detection (determining whether a causal explanation exists at all) and causal explanation identification (identifying the specific phrase that is the explanation). We achieve strong accuracies for both tasks but find different approaches best : an SVM for causality prediction (F1 = 0.791) and a hierarchy of Bidirectional LSTMs for causal explanation identification (F1 = 0.853). Finally, we explore applications of our complete pipeline (F1 = 0.868), showing demographic differences in mentions of causal explanation and that the association between a word and sentiment can change when it is used within a <a href=https://en.wikipedia.org/wiki/Causality>causal explanation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1374.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1374 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1374 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1374.Attachment.tgz data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1374/>Content Explorer : Recommending Novel Entities for a Document Writer</a></strong><br><a href=/people/m/michal-lukasik/>Michal Lukasik</a>
|
<a href=/people/r/richard-zens/>Richard Zens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1374><div class="card-body p-3 small">Background research is an essential part of document writing. Search engines are great for retrieving information once we know what to look for. However, the bigger challenge is often identifying topics for further research. Automated tools could help significantly in this discovery process and increase the productivity of the writer. In this paper, we formulate the problem of recommending topics to a writer. We consider this as a supervised learning problem and run a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> to validate this approach. We propose an evaluation metric and perform an empirical comparison of state-of-the-art models for extreme multi-label classification on a large data set. We demonstrate how a simple modification of the cross-entropy loss function leads to improved results of the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1375.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1375 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1375 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1375" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1375/>A Genre-Aware Attention Model to Improve the Likability Prediction of Books</a></strong><br><a href=/people/s/suraj-maharjan/>Suraj Maharjan</a>
|
<a href=/people/m/manuel-montes/>Manuel Montes</a>
|
<a href=/people/f/fabio-a-gonzalez/>Fabio A. González</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1375><div class="card-body p-3 small">Likability prediction of books has many uses. Readers, writers, as well as the publishing industry, can all benefit from automatic book likability prediction systems. In order to make reliable decisions, these <a href=https://en.wikipedia.org/wiki/System>systems</a> need to assimilate information from different aspects of a book in a sensible way. We propose a novel multimodal neural architecture that incorporates genre supervision to assign weights to individual feature types. Our proposed method is capable of dynamically tailoring weights given to feature types based on the characteristics of each book. Our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> achieves competitive results and even outperforms <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1376.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1376 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1376 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1376.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1376/>Thread Popularity Prediction and Tracking with a Permutation-invariant Model</a></strong><br><a href=/people/h/hou-pong-chan/>Hou Pong Chan</a>
|
<a href=/people/i/irwin-king/>Irwin King</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1376><div class="card-body p-3 small">The task of thread popularity prediction and tracking aims to recommend a few popular comments to subscribed users when a batch of new comments arrive in a <a href=https://en.wikipedia.org/wiki/Conversation_threading>discussion thread</a>. This task has been formulated as a reinforcement learning problem, in which the reward of the agent is the sum of positive responses received by the recommended comments. In this work, we propose a novel <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>approach</a> to tackle this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. First, we propose a deep neural network architecture to model the expected cumulative reward (Q-value) of a recommendation (action). Unlike the state-of-the-art approach, which treats an action as a sequence, our model uses an attention mechanism to integrate information from a set of comments. Thus, the prediction of Q-value is invariant to the permutation of the comments, which leads to a more consistent agent behavior. Second, we employ a <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy procedure</a> to approximate the action that maximizes the predicted Q-value from a combinatorial action space. Different from the state-of-the-art approach, this procedure does not require an additional pre-trained model to generate candidate actions. Experiments on five real-world datasets show that our approach outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1378 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1378/>Limbic : Author-Based Sentiment Aspect Modeling Regularized with Word Embeddings and Discourse Relations<span class=acl-fixed-case>L</span>imbic: Author-Based Sentiment Aspect Modeling Regularized with Word Embeddings and Discourse Relations</a></strong><br><a href=/people/z/zhe-zhang/>Zhe Zhang</a>
|
<a href=/people/m/munindar-p-singh/>Munindar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1378><div class="card-body p-3 small">We propose <a href=https://en.wikipedia.org/wiki/Limbic>Limbic</a>, an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised probabilistic model</a> that addresses the problem of discovering aspects and sentiments and associating them with authors of opinionated texts. Limbic combines three ideas, incorporating <a href=https://en.wikipedia.org/wiki/Author>authors</a>, <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>, and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. For discourse relations, <a href=https://en.wikipedia.org/wiki/Limbic>Limbic</a> adopts a generative process regularized by a <a href=https://en.wikipedia.org/wiki/Markov_random_field>Markov Random Field</a>. To promote words with high <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> into the same topic, <a href=https://en.wikipedia.org/wiki/Limbic>Limbic</a> captures semantic regularities from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> via a generalized Plya Urn process. We demonstrate that Limbic (1) discovers aspects associated with sentiments with high <a href=https://en.wikipedia.org/wiki/Lexical_diversity>lexical diversity</a> ; (2) outperforms state-of-the-art models by a substantial margin in topic cohesion and sentiment classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1379.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1379 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1379 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1379/>An Interpretable Neural Network with Topical Information for Relevant Emotion Ranking</a></strong><br><a href=/people/y/yang-yang/>Yang Yang</a>
|
<a href=/people/d/deyu-zhou/>Deyu Zhou</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1379><div class="card-body p-3 small">Text might express or evoke multiple emotions with varying intensities. As such, it is crucial to predict and rank multiple relevant emotions by their intensities. Moreover, as <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> might be evoked by hidden topics, it is important to unveil and incorporate such topical information to understand how the <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> are evoked. We proposed a novel interpretable neural network approach for relevant emotion ranking. Specifically, motivated by <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> is initialized to make the hidden layer approximate the behavior of <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a>. Moreover, a novel <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error function</a> is defined to optimize the whole <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> for relevant emotion ranking. Experimental results on three real-world corpora show that the proposed approach performs remarkably better than the state-of-the-art emotion detection approaches and multi-label learning methods. Moreover, the extracted emotion-associated topic words indeed represent <a href=https://en.wikipedia.org/wiki/Emotion>emotion-evoking events</a> and are in line with our <a href=https://en.wikipedia.org/wiki/Common_sense>common-sense knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1381 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1381/>Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention for Sentiment Classification</a></strong><br><a href=/people/y/yi-tay/>Yi Tay</a>
|
<a href=/people/a/anh-tuan-luu/>Anh Tuan Luu</a>
|
<a href=/people/s/siu-cheung-hui/>Siu Cheung Hui</a>
|
<a href=/people/j/jian-su/>Jian Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1381><div class="card-body p-3 small">This paper proposes a new neural architecture that exploits readily available sentiment lexicon resources. The key idea is that that incorporating a word-level prior can aid in the representation learning process, eventually improving <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. To this end, our model employs two distinctly unique components, i.e., (1) we introduce a lexicon-driven contextual attention mechanism to imbue lexicon words with long-range contextual information and (2), we introduce a contrastive co-attention mechanism that models contrasting polarities between all positive and negative words in a sentence. Via extensive experiments, we show that our approach outperforms many other neural baselines on sentiment classification tasks on multiple benchmark datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1385 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1385" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1385/>Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content</a></strong><br><a href=/people/w/weiming-wen/>Weiming Wen</a>
|
<a href=/people/s/songwen-su/>Songwen Su</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1385><div class="card-body p-3 small">With the increasing popularity of <a href=https://en.wikipedia.org/wiki/Smart_device>smart devices</a>, <a href=https://en.wikipedia.org/wiki/Rumor>rumors</a> with <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia content</a> become more and more common on <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. The <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia information</a> usually makes <a href=https://en.wikipedia.org/wiki/Rumor>rumors</a> look more convincing. Therefore, finding an automatic approach to verify <a href=https://en.wikipedia.org/wiki/Rumor>rumors</a> with <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia content</a> is a pressing task. Previous rumor verification research only utilizes <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia</a> as input features. We propose not to use the <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia content</a> but to find <a href=https://en.wikipedia.org/wiki/Infographic>external information</a> in other <a href=https://en.wikipedia.org/wiki/Online_newspaper>news platforms</a> pivoting on it. We introduce a new features set, cross-lingual cross-platform features that leverage the semantic similarity between the rumors and the external information. When implemented, <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a> utilizing such <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> achieved the state-of-the-art rumor verification results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1386.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1386 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1386 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1386/>Extractive Adversarial Networks : High-Recall Explanations for Identifying Personal Attacks in Social Media Posts</a></strong><br><a href=/people/s/samuel-carton/>Samuel Carton</a>
|
<a href=/people/q/qiaozhu-mei/>Qiaozhu Mei</a>
|
<a href=/people/p/paul-resnick/>Paul Resnick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1386><div class="card-body p-3 small">We introduce an <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial method</a> for producing high-recall explanations of neural text classifier decisions. Building on an existing architecture for extractive explanations via hard attention, we add an adversarial layer which scans the residual of the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> for remaining predictive signal. Motivated by the important domain of detecting personal attacks in social media comments, we additionally demonstrate the importance of manually setting a semantically appropriate default behavior for the model by explicitly manipulating its bias term. We develop a validation set of human-annotated personal attacks to evaluate the impact of these changes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1387.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1387 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1387 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1387/>Automatic Detection of Vague Words and Sentences in Privacy Policies</a></strong><br><a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1387><div class="card-body p-3 small">Website privacy policies represent the single most important source of information for users to gauge how their personal data are collected, used and shared by companies. However, <a href=https://en.wikipedia.org/wiki/Privacy_policy>privacy policies</a> are often vague and people struggle to understand the content. Their opaqueness poses a significant challenge to both users and policy regulators. In this paper, we seek to identify vague content in <a href=https://en.wikipedia.org/wiki/Privacy_policy>privacy policies</a>. We construct the first <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of human-annotated vague words and sentences and present empirical studies on automatic vagueness detection. In particular, we investigate context-aware and context-agnostic models for predicting vague words, and explore auxiliary-classifier generative adversarial networks for characterizing sentence vagueness. Our experimental results demonstrate the effectiveness of proposed approaches. Finally, we provide suggestions for resolving <a href=https://en.wikipedia.org/wiki/Vagueness>vagueness</a> and improving the usability of <a href=https://en.wikipedia.org/wiki/Privacy_policy>privacy policies</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1388.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1388 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1388 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1388/>Multi-view Models for Political Ideology Detection of News Articles</a></strong><br><a href=/people/v/vivek-kulkarni/>Vivek Kulkarni</a>
|
<a href=/people/j/junting-ye/>Junting Ye</a>
|
<a href=/people/s/steven-skiena/>Steve Skiena</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1388><div class="card-body p-3 small">A news article&#8217;s title, content and link structure often reveal its <a href=https://en.wikipedia.org/wiki/Ideology>political ideology</a>. However, most existing works on automatic political ideology detection only leverage textual cues. Drawing inspiration from recent advances in neural inference, we propose a novel attention based multi-view model to leverage cues from all of the above views to identify the <a href=https://en.wikipedia.org/wiki/Ideology>ideology</a> evinced by a <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news article</a>. Our model draws on advances in representation learning in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Network_science>network science</a> to capture cues from both textual content and the network structure of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>. We empirically evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> against a battery of baselines and show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state of the art by 10 percentage points F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1389.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1389 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1389 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1389" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1389/>Predicting Factuality of Reporting and Bias of News Media Sources</a></strong><br><a href=/people/r/ramy-baly/>Ramy Baly</a>
|
<a href=/people/g/georgi-karadzhov/>Georgi Karadzhov</a>
|
<a href=/people/d/dimitar-alexandrov/>Dimitar Alexandrov</a>
|
<a href=/people/j/james-glass/>James Glass</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1389><div class="card-body p-3 small">We present a study on predicting the factuality of reporting and bias of <a href=https://en.wikipedia.org/wiki/News_media>news media</a>. While previous work has focused on studying the veracity of claims or documents, here we are interested in characterizing entire <a href=https://en.wikipedia.org/wiki/News_media>news media</a>. This is an under-studied, but arguably important research problem, both in its own right and as a prior for <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking systems</a>. We experiment with a large list of news websites and with a rich set of features derived from (i) a sample of articles from the target <a href=https://en.wikipedia.org/wiki/News_media>news media</a>, (ii) its Wikipedia page, (iii) its Twitter account, (iv) the structure of its URL, and (v) information about the Web traffic it attracts. The experimental results show sizable performance gains over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, and reveal the importance of each feature type.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1390 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1390" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1390/>Legal Judgment Prediction via Topological Learning</a></strong><br><a href=/people/h/haoxi-zhong/>Haoxi Zhong</a>
|
<a href=/people/z/zhipeng-guo/>Zhipeng Guo</a>
|
<a href=/people/c/cunchao-tu/>Cunchao Tu</a>
|
<a href=/people/c/chaojun-xiao/>Chaojun Xiao</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1390><div class="card-body p-3 small">Legal Judgment Prediction (LJP) aims to predict the judgment result based on the facts of a case and becomes a promising application of artificial intelligence techniques in the <a href=https://en.wikipedia.org/wiki/Law>legal field</a>. In real-world scenarios, <a href=https://en.wikipedia.org/wiki/Legal_judgment>legal judgment</a> usually consists of multiple subtasks, such as the decisions of applicable law articles, <a href=https://en.wikipedia.org/wiki/Criminal_charge>charges</a>, <a href=https://en.wikipedia.org/wiki/Fine_(penalty)>fines</a>, and the term of penalty. Moreover, there exist topological dependencies among these subtasks. While most existing works only focus on a specific subtask of judgment prediction and ignore the dependencies among subtasks, we formalize the dependencies among subtasks as a Directed Acyclic Graph (DAG) and propose a topological multi-task learning framework, TopJudge, which incorporates multiple subtasks and DAG dependencies into judgment prediction. We conduct experiments on several real-world large-scale datasets of criminal cases in the <a href=https://en.wikipedia.org/wiki/Civil_law_(legal_system)>civil law system</a>. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves consistent and significant improvements over baselines on all judgment prediction tasks. The source code can be obtained from.<url>https://github.com/thunlp/TopJudge</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1393.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1393 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1393 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1393.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1393/>Framing and Agenda-setting in Russian News : a Computational Analysis of Intricate Political Strategies<span class=acl-fixed-case>R</span>ussian News: a Computational Analysis of Intricate Political Strategies</a></strong><br><a href=/people/a/anjalie-field/>Anjalie Field</a>
|
<a href=/people/d/doron-kliger/>Doron Kliger</a>
|
<a href=/people/s/shuly-wintner/>Shuly Wintner</a>
|
<a href=/people/j/jennifer-pan/>Jennifer Pan</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1393><div class="card-body p-3 small">Amidst growing concern over <a href=https://en.wikipedia.org/wiki/Media_manipulation>media manipulation</a>, NLP attention has focused on overt strategies like <a href=https://en.wikipedia.org/wiki/Censorship>censorship</a> and <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation : <a href=https://en.wikipedia.org/wiki/Agenda-setting_theory>agenda-setting</a> (selecting what topics to cover) and <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a> (deciding how topics are covered). We analyze 13 years (100 K articles) of the <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_Russia>Russian newspaper</a> Izvestia and identify a strategy of distraction : articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1396.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1396 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1396 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1396.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306146050 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1396/>Beyond Error Propagation in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> : Characteristics of Language Also Matter</a></strong><br><a href=/people/l/lijun-wu/>Lijun Wu</a>
|
<a href=/people/x/xu-tan/>Xu Tan</a>
|
<a href=/people/d/di-he/>Di He</a>
|
<a href=/people/f/fei-tian/>Fei Tian</a>
|
<a href=/people/t/tao-qin/>Tao Qin</a>
|
<a href=/people/j/jianhuang-lai/>Jianhuang Lai</a>
|
<a href=/people/t/tie-yan-liu/>Tie-Yan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1396><div class="card-body p-3 small">Neural machine translation usually adopts autoregressive models and suffers from <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a> as well as the consequent error propagation problem. Many previous works have discussed the relationship between error propagation and the accuracy drop (i.e., the left part of the translated sentence is often better than its right part in left-to-right decoding models) problem. In this paper, we conduct a series of analyses to deeply understand this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> and get several interesting findings. (1) The role of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a> on accuracy drop is overstated in the literature, although it indeed contributes to the accuracy drop problem. (2) Characteristics of a language play a more important role in causing the accuracy drop : the left part of the translation result in a <a href=https://en.wikipedia.org/wiki/Branching_(linguistics)>right-branching language</a> (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a <a href=https://en.wikipedia.org/wiki/Branching_(linguistics)>left-branching language</a> (e.g., Japanese). Our discoveries are confirmed on different model structures including Transformer and <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNN</a>, and in other sequence generation tasks such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>.<i>accuracy drop</i> (i.e., the left part of the translated sentence is often better than its right part in left-to-right decoding models) problem. In this paper, we conduct a series of analyses to deeply understand this problem and get several interesting findings. (1) The role of error propagation on accuracy drop is overstated in the literature, although it indeed contributes to the accuracy drop problem. (2) Characteristics of a language play a more important role in causing the accuracy drop: the left part of the translation result in a right-branching language (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language (e.g., Japanese). Our discoveries are confirmed on different model structures including Transformer and RNN, and in other sequence generation tasks such as text summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1398 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306147573 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1398/>Meta-Learning for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/j/jiatao-gu/>Jiatao Gu</a>
|
<a href=/people/y/yong-wang/>Yong Wang</a>
|
<a href=/people/y/yun-chen/>Yun Chen</a>
|
<a href=/people/v/victor-o-k-li/>Victor O. K. Li</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1398><div class="card-body p-3 small">In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn, et al., 2017) for low-resource neural machine translation (NMT). We frame low-resource translation as a meta-learning problem where we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on Romanian-English WMT&#8217;16 by seeing only 16,000 translated words (~600 parallel sentences)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1400 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1400 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1400.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306149028 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1400/>A Visual Attention Grounding Neural Model for Multimodal Machine Translation</a></strong><br><a href=/people/m/mingyang-zhou/>Mingyang Zhou</a>
|
<a href=/people/r/runxiang-cheng/>Runxiang Cheng</a>
|
<a href=/people/y/yong-jae-lee/>Yong Jae Lee</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1400><div class="card-body p-3 small">We introduce a novel multimodal machine translation model that utilizes parallel visual and textual information. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> jointly optimizes the learning of a shared visual-language embedding and a <a href=https://en.wikipedia.org/wiki/Translation>translator</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> leverages a visual attention grounding mechanism that links the visual semantics with the corresponding textual semantics. Our approach achieves competitive state-of-the-art results on the Multi30 K and the Ambiguous COCO datasets. We also collected a new multilingual multimodal product description dataset to simulate a real-world international online shopping scenario. On this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, our visual attention grounding model outperforms other <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1404 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1404.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306129121 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1404" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1404/>CARER : Contextualized Affect Representations for Emotion Recognition<span class=acl-fixed-case>CARER</span>: Contextualized Affect Representations for Emotion Recognition</a></strong><br><a href=/people/e/elvis-saravia/>Elvis Saravia</a>
|
<a href=/people/h/hsien-chi-toby-liu/>Hsien-Chi Toby Liu</a>
|
<a href=/people/y/yen-hao-huang/>Yen-Hao Huang</a>
|
<a href=/people/j/junlin-wu/>Junlin Wu</a>
|
<a href=/people/y/yi-shin-chen/>Yi-Shin Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1404><div class="card-body p-3 small">Emotions are expressed in nuanced ways, which varies by collective or individual experiences, knowledge, and beliefs. Therefore, to understand <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>, as conveyed through text, a robust mechanism capable of capturing and modeling different linguistic nuances and phenomena is needed. We propose a semi-supervised, graph-based algorithm to produce rich structural descriptors which serve as the building blocks for constructing contextualized affect representations from text. The <a href=https://en.wikipedia.org/wiki/Pattern_recognition>pattern-based representations</a> are further enriched with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and evaluated through several <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition tasks</a>. Our experimental results demonstrate that the proposed method outperforms state-of-the-art techniques on emotion recognition tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1405 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1405.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306156327 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1405/>Noise Contrastive Estimation and Negative Sampling for Conditional Models : Consistency and Statistical Efficiency</a></strong><br><a href=/people/z/zhuang-ma/>Zhuang Ma</a>
|
<a href=/people/m/michael-collins/>Michael Collins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1405><div class="card-body p-3 small">Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for log-linear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely related to negative sampling methods, now widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice ; however there has not been a rigorous theoretical analysis of NCE in this setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models : one based on a <a href=https://en.wikipedia.org/wiki/Loss_function>classification objective</a>, the other based on a <a href=https://en.wikipedia.org/wiki/Loss_function>ranking objective</a>. We show that the ranking-based variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method ; we analyze the statistical efficiency of the ranking-based and classification-based variants of NCE ; finally we describe experiments on synthetic data and language modeling showing the effectiveness and tradeoffs of both methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1406 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306157322 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1406/>CaLcs : Continuously Approximating Longest Common Subsequence for Sequence Level Optimization<span class=acl-fixed-case>C</span>a<span class=acl-fixed-case>L</span>cs: Continuously Approximating Longest Common Subsequence for Sequence Level Optimization</a></strong><br><a href=/people/s/semih-yavuz/>Semih Yavuz</a>
|
<a href=/people/c/chung-cheng-chiu/>Chung-Cheng Chiu</a>
|
<a href=/people/p/patrick-nguyen/>Patrick Nguyen</a>
|
<a href=/people/y/yonghui-wu/>Yonghui Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1406><div class="card-body p-3 small">Maximum-likelihood estimation (MLE) is one of the most widely used approaches for training structured prediction models for text-generation based natural language processing applications. However, besides <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias</a>, models trained with MLE suffer from wrong objective problem where they are trained to maximize the word-level correct next step prediction, but are evaluated with respect to sequence-level discrete metrics such as ROUGE and <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. Several variants of policy-gradient methods address some of these problems by optimizing for final discrete evaluation metrics and showing improvements over MLE training for downstream tasks like <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. However, policy-gradient methods suffers from high <a href=https://en.wikipedia.org/wiki/Variance>sample variance</a>, making the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training process</a> very difficult and unstable. In this paper, we present an alternative direction towards mitigating this problem by introducing a new objective (CaLcs) based on a differentiable surrogate of longest common subsequence (LCS) measure that captures sequence-level structure similarity. Experimental results on abstractive summarization and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> validate the effectiveness of the proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1408 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306159624 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1408/>Phrase-level Self-Attention Networks for Universal Sentence Encoding</a></strong><br><a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a>
|
<a href=/people/t/tianyu-liu/>Tianyu Liu</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1408><div class="card-body p-3 small">Universal sentence encoding is a hot topic in recent <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP research</a>. Attention mechanism has been an integral part in many sentence encoding models, allowing the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to capture context dependencies regardless of the distance between the elements in the sequence. Fully attention-based models have recently attracted enormous interest due to their highly <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallelizable computation</a> and significantly less <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a>. However, the <a href=https://en.wikipedia.org/wiki/Memory>memory consumption</a> of their models grows quadratically with the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>, and the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> is neglected. To this end, we propose Phrase-level Self-Attention Networks (PSAN) that perform self-attention across words inside a phrase to capture context dependencies at the phrase level, and use the gated memory updating mechanism to refine each word&#8217;s representation hierarchically with longer-term context dependencies captured in a larger phrase. As a result, the memory consumption can be reduced because the self-attention is performed at the <a href=https://en.wikipedia.org/wiki/Phrase>phrase level</a> instead of the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level</a>. At the same time, <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> can be easily integrated in the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. Experiment results show that PSAN can achieve the state-of-the-art performance across a plethora of NLP tasks including binary and multi-class classification, natural language inference and sentence similarity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1409 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1409.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306160623 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1409" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1409/>BanditSum : Extractive Summarization as a Contextual Bandit<span class=acl-fixed-case>B</span>andit<span class=acl-fixed-case>S</span>um: Extractive Summarization as a Contextual Bandit</a></strong><br><a href=/people/y/yue-dong/>Yue Dong</a>
|
<a href=/people/y/yikang-shen/>Yikang Shen</a>
|
<a href=/people/e/eric-crawford/>Eric Crawford</a>
|
<a href=/people/h/herke-van-hoof/>Herke van Hoof</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1409><div class="card-body p-3 small">In this work, we propose a novel method for training <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to perform single-document extractive summarization without heuristically-generated extractive labels. We call our approach BanditSum as it treats extractive summarization as a contextual bandit (CB) problem, where the model receives a document to summarize (the context), and chooses a sequence of sentences to include in the summary (the action). A policy gradient reinforcement learning algorithm is used to train the model to select sequences of sentences that maximize ROUGE score. We perform a series of experiments demonstrating that BanditSum is able to achieve ROUGE scores that are better than or comparable to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for extractive summarization, and converges using significantly fewer update steps than competing approaches. In addition, we show empirically that BanditSum performs significantly better than competing approaches when good summary sentences appear late in the source document.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1410 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306116474 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1410" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1410/>A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification</a></strong><br><a href=/people/m/mounica-maddela/>Mounica Maddela</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1410><div class="card-body p-3 small">Current lexical simplification approaches rely heavily on <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>heuristics</a> and corpus level features that do not always align with <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a>. We create a human-rated word-complexity lexicon of 15,000 English words and propose a novel neural readability ranking model with a Gaussian-based feature vectorization layer that utilizes these human ratings to measure the complexity of any given word or phrase. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better than the state-of-the-art systems for different lexical simplification tasks and evaluation datasets. Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by applying our model to the Paraphrase Database (PPDB).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1412 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1412.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306118515 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1412" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1412/>Syntactic Scaffolds for Semantic Structures</a></strong><br><a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/s/sam-thomson/>Sam Thomson</a>
|
<a href=/people/k/kenton-lee/>Kenton Lee</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1412><div class="card-body p-3 small">We introduce the syntactic scaffold, an approach to incorporating <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> into <a href=https://en.wikipedia.org/wiki/Semantics>semantic tasks</a>. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, achieving competitive performance on all three tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1414 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306119942 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1414" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1414/>Semantic Role Labeling for Learner Chinese : the Importance of Syntactic Parsing and L2-L1 Parallel Data<span class=acl-fixed-case>C</span>hinese: the Importance of Syntactic Parsing and <span class=acl-fixed-case>L</span>2-<span class=acl-fixed-case>L</span>1 Parallel Data</a></strong><br><a href=/people/z/zi-lin/>Zi Lin</a>
|
<a href=/people/y/yuguang-duan/>Yuguang Duan</a>
|
<a href=/people/y/yuanyuan-zhao/>Yuanyuan Zhao</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1414><div class="card-body p-3 small">This paper studies <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> for interlanguage (L2), taking semantic role labeling (SRL) as a case task and learner Chinese as a case language. We first manually annotate the semantic roles for a set of learner texts to derive a gold standard for automatic SRL. Based on the new data, we then evaluate three off-the-shelf SRL systems, i.e., the PCFGLA-parser-based, neural-parser-based and neural-syntax-agnostic systems, to gauge how successful SRL for learner Chinese can be. We find two non-obvious facts : 1) the L1-sentence-trained systems performs rather badly on the L2 data ; 2) the performance drop from the L1 data to the L2 data of the two parser-based systems is much smaller, indicating the importance of syntactic parsing in SRL for interlanguages. Finally, the paper introduces a new agreement-based model to explore the semantic coherency information in the large-scale L2-L1 parallel data. We then show such information is very effective to enhance SRL for learner texts. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 72.06, which is a 2.02 point improvement over the best <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1417 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1417/>A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding</a></strong><br><a href=/people/c/changliang-li/>Changliang Li</a>
|
<a href=/people/l/liang-li/>Liang Li</a>
|
<a href=/people/j/ji-qi/>Ji Qi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1417><div class="card-body p-3 small">Spoken Language Understanding (SLU), which typically involves intent determination and slot filling, is a core component of spoken dialogue systems. Joint learning has shown to be effective in SLU given that slot tags and intents are supposed to share knowledge with each other. However, most existing joint learning methods only consider joint learning by sharing parameters on surface level rather than semantic level. In this work, we propose a novel self-attentive model with gate mechanism to fully utilize the semantic correlation between slot and <a href=https://en.wikipedia.org/wiki/Intention>intent</a>. Our model first obtains intent-augmented embeddings based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> with self-attention mechanism. And then the intent semantic representation is utilized as the gate for labelling slot tags. The objectives of both <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> are optimized simultaneously via joint learning in an end-to-end way. We conduct experiment on popular benchmark ATIS. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art and outperforms other popular methods by a large margin in terms of both intent detection error rate and slot filling F1-score. This paper gives a new perspective for research on SLU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1418 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1418/>Learning End-to-End Goal-Oriented Dialog with Multiple Answers</a></strong><br><a href=/people/j/janarthanan-rajendran/>Janarthanan Rajendran</a>
|
<a href=/people/j/jatin-ganhotra/>Jatin Ganhotra</a>
|
<a href=/people/s/satinder-singh/>Satinder Singh</a>
|
<a href=/people/l/lazaros-polymenakos/>Lazaros Polymenakos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1418><div class="card-body p-3 small">In a <a href=https://en.wikipedia.org/wiki/Dialogue>dialog</a>, there could be multiple valid next utterances at any point. The present end-to-end neural methods for <a href=https://en.wikipedia.org/wiki/Dialog_(software)>dialog</a> do not take this into account. They learn with the assumption that at any time there is only one correct next utterance. In this work, we focus on this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> in the goal-oriented dialog setting where there are different paths to reach a goal. We propose a new method, that uses a combination of supervised learning and reinforcement learning approaches to address this issue. We also propose a new and more effective <a href=https://en.wikipedia.org/wiki/Testbed>testbed</a>, permuted-bAbI dialog tasks, by introducing multiple valid next utterances to the original-bAbI dialog tasks, which allows evaluation of end-to-end goal-oriented dialog systems in a more realistic setting. We show that there is a significant drop in performance of existing <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end neural methods</a> from 81.5 % per-dialog accuracy on original-bAbI dialog tasks to 30.3 % on permuted-bAbI dialog tasks. We also show that our proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> improves the performance and achieves 47.3 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>per-dialog accuracy</a> on permuted-bAbI dialog tasks. We also release permuted-bAbI dialog tasks, our proposed testbed, to the community for evaluating dialog systems in a goal-oriented setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1420 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1420/>QuaSE : Sequence Editing under Quantifiable Guidance<span class=acl-fixed-case>Q</span>ua<span class=acl-fixed-case>SE</span>: Sequence Editing under Quantifiable Guidance</a></strong><br><a href=/people/y/yi-liao/>Yi Liao</a>
|
<a href=/people/l/lidong-bing/>Lidong Bing</a>
|
<a href=/people/p/piji-li/>Piji Li</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/w/wai-lam/>Wai Lam</a>
|
<a href=/people/t/tong-zhang/>Tong Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1420><div class="card-body p-3 small">We propose the task of Quantifiable Sequence Editing (QuaSE): editing an input sequence to generate an output sequence that satisfies a given numerical outcome value measuring a certain property of the sequence, with the requirement of keeping the main content of the input sequence. For example, an input sequence could be a word sequence, such as review sentence and <a href=https://en.wikipedia.org/wiki/Advertising>advertisement text</a>. For a review sentence, the outcome could be the review rating ; for an <a href=https://en.wikipedia.org/wiki/Advertising>advertisement</a>, the outcome could be the <a href=https://en.wikipedia.org/wiki/Click-through_rate>click-through rate</a>. The major challenge in performing QuaSE is how to perceive the outcome-related wordings, and only edit them to change the outcome. In this paper, the proposed framework contains two latent factors, namely, outcome factor and content factor, disentangled from the input sentence to allow convenient editing to change the outcome and keep the content. Our framework explores the pseudo-parallel sentences by modeling their content similarity and outcome differences to enable a better disentanglement of the latent factors, which allows generating an output to better satisfy the desired outcome and keep the content. The dual reconstruction structure further enhances the capability of generating expected output by exploiting the couplings of latent factors of pseudo-parallel sentences. For evaluation, we prepared a dataset of Yelp review sentences with the ratings as outcome. Extensive experimental results are reported and discussed to elaborate the peculiarities of our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1421 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1421/>Paraphrase Generation with Deep Reinforcement Learning</a></strong><br><a href=/people/z/zichao-li/>Zichao Li</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/h/hang-li/>Hang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1421><div class="card-body p-3 small">Automatic generation of paraphrases from a given sentence is an important yet challenging task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. In this paper, we present a deep reinforcement learning approach to <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a>. Specifically, we propose a new <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>, which consists of a <a href=https://en.wikipedia.org/wiki/Generator_(computer_programming)>generator</a> and an evaluator, both of which are learned from data. The <a href=https://en.wikipedia.org/wiki/Generator_(mathematics)>generator</a>, built as a sequence-to-sequence learning model, can produce <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> given a sentence. The evaluator, constructed as a deep matching model, can judge whether two sentences are paraphrases of each other. The generator is first trained by <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and then further fine-tuned by <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> in which the reward is given by the evaluator. For the learning of the evaluator, we propose two methods based on <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> and <a href=https://en.wikipedia.org/wiki/Inverse_reinforcement_learning>inverse reinforcement learning</a> respectively, depending on the type of available training data. Experimental results on two datasets demonstrate the proposed models (the generators) can produce more accurate paraphrases and outperform the state-of-the-art methods in <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> in both automatic evaluation and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1422 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1422/>Operation-guided Neural Networks for High Fidelity Data-To-Text Generation</a></strong><br><a href=/people/f/feng-nie/>Feng Nie</a>
|
<a href=/people/j/jinpeng-wang/>Jinpeng Wang</a>
|
<a href=/people/j/jin-ge-yao/>Jin-Ge Yao</a>
|
<a href=/people/r/rong-pan/>Rong Pan</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1422><div class="card-body p-3 small">Recent neural models for data-to-text generation are mostly based on data-driven end-to-end training over encoder-decoder networks. Even though the generated texts are mostly fluent and informative, they often generate descriptions that are not consistent with the input structured data. This is a critical issue especially in domains that require <a href=https://en.wikipedia.org/wiki/Inference>inference</a> or calculations over <a href=https://en.wikipedia.org/wiki/Raw_data>raw data</a>. In this paper, we attempt to improve the <a href=https://en.wikipedia.org/wiki/Fidelity>fidelity</a> of neural data-to-text generation by utilizing pre-executed symbolic operations. We propose a framework called Operation-guided Attention-based sequence-to-sequence network (OpAtt), with a specifically designed gating mechanism as well as a quantization module for operation results to utilize information from pre-executed operations. Experiments on two sports datasets show our proposed method clearly improves the <a href=https://en.wikipedia.org/wiki/Fidelity>fidelity</a> of the generated texts to the input structured data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1424 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1424/>Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks</a></strong><br><a href=/people/y/yao-zhao/>Yao Zhao</a>
|
<a href=/people/x/xiaochuan-ni/>Xiaochuan Ni</a>
|
<a href=/people/y/yuanyuan-ding/>Yuanyuan Ding</a>
|
<a href=/people/q/qifa-ke/>Qifa Ke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1424><div class="card-body p-3 small">Question generation, the task of automatically creating questions that can be answered by a certain span of text within a given passage, is important for question-answering and conversational systems in digital assistants such as <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a>, <a href=https://en.wikipedia.org/wiki/Cortana>Cortana</a>, <a href=https://en.wikipedia.org/wiki/Google_Assistant>Google Assistant</a> and <a href=https://en.wikipedia.org/wiki/Siri>Siri</a>. Recent sequence to sequence neural models have outperformed previous <a href=https://en.wikipedia.org/wiki/Rule-based_system>rule-based systems</a>. Existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> mainly focused on using one or two sentences as the input. Long text has posed challenges for sequence to sequence neural models in question generation worse performances were reported if using the whole paragraph (with multiple sentences) as the input. In reality, however, it often requires the whole paragraph as context in order to generate high quality questions. In this paper, we propose a maxout pointer mechanism with gated self-attention encoder to address the challenges of processing long text inputs for question generation. With sentence-level inputs, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous approaches with either sentence-level or paragraph-level inputs. Furthermore, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can effectively utilize paragraphs as inputs, pushing the state-of-the-art result from 13.9 to 16.3 (BLEU_4).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1426 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1426/>Unsupervised Natural Language Generation with Denoising Autoencoders</a></strong><br><a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/s/scott-roy/>Scott Roy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1426><div class="card-body p-3 small">Generating text from <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> is important for various tasks such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> into training examples that do not contain <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a>, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1427 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1427.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1427/>Answer-focused and Position-aware Neural Question Generation</a></strong><br><a href=/people/x/xingwu-sun/>Xingwu Sun</a>
|
<a href=/people/j/jing-liu/>Jing Liu</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/w/wei-he/>Wei He</a>
|
<a href=/people/y/yanjun-ma/>Yanjun Ma</a>
|
<a href=/people/s/shi-wang/>Shi Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1427><div class="card-body p-3 small">In this paper, we focus on the problem of question generation (QG). Recent neural network-based approaches employ the sequence-to-sequence model which takes an answer and its context as input and generates a relevant question as output. However, we observe two major issues with these approaches : (1) The generated interrogative words (or question words) do not match the answer type. (2) The model copies the context words that are far from and irrelevant to the answer, instead of the words that are close and relevant to the answer. To address these two issues, we propose an answer-focused and position-aware neural question generation model. (1) By answer-focused, we mean that we explicitly model question word generation by incorporating the answer embedding, which can help generate an interrogative word matching the answer type. (2) By position-aware, we mean that we model the relative distance between the context words and the answer. Hence the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be aware of the position of the context words when copying them to generate a question. We conduct extensive experiments to examine the effectiveness of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. The experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improves the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> and outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1429 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1429" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1429/>Towards a Better Metric for Evaluating Question Generation Systems</a></strong><br><a href=/people/p/preksha-nema/>Preksha Nema</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1429><div class="card-body p-3 small">There has always been criticism for using n-gram based similarity metrics, such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, <a href=https://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology>NIST</a>, etc, for evaluating the performance of NLG systems. However, these <a href=https://en.wikipedia.org/wiki/Performance_metric>metrics</a> continue to remain popular and are recently being used for evaluating the performance of systems which automatically generate questions from <a href=https://en.wikipedia.org/wiki/Document>documents</a>, <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>, <a href=https://en.wikipedia.org/wiki/Image>images</a>, etc. Given the rising interest in such automatic question generation (AQG) systems, it is important to objectively examine whether these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are suitable for this task. In particular, it is important to verify whether such metrics used for evaluating AQG systems focus on answerability of the generated question by preferring questions which contain all relevant information such as question type (Wh-types), entities, relations, etc. In this work, we show that current automatic evaluation metrics based on n-gram similarity do not always correlate well with human judgments about answerability of a question. To alleviate this problem and as a first step towards better evaluation metrics for AQG, we introduce a scoring function to capture answerability and show that when this scoring function is integrated with existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>, they correlate significantly better with human judgments. The scripts and data developed as a part of this work are made publicly available.<tex-math>n</tex-math>-gram based similarity metrics, such as BLEU, NIST, <i>etc</i>, for evaluating the performance of NLG systems. However, these metrics continue to remain popular and are recently being used for evaluating the performance of systems which automatically generate questions from documents, knowledge graphs, images, <i>etc</i>. Given the rising interest in such automatic question generation (AQG) systems, it is important to objectively examine whether these metrics are suitable for this task. In particular, it is important to verify whether such metrics used for evaluating AQG systems focus on <i>answerability</i> of the generated question by preferring questions which contain all relevant information such as question type (Wh-types), entities, relations, <i>etc</i>. In this work, we show that current automatic evaluation metrics based on <tex-math>n</tex-math>-gram similarity do not always correlate well with human judgments about <i>answerability</i> of a question. To alleviate this problem and as a first step towards better evaluation metrics for AQG, we introduce a scoring function to capture <i>answerability</i> and show that when this scoring function is integrated with existing metrics, they correlate significantly better with human judgments. The scripts and data developed as a part of this work are made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1430.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1430 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1430 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1430.Attachment.txt data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1430/>Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement<span class=acl-fixed-case>C</span>hinese Poetry Generation via Unsupervised Style Disentanglement</a></strong><br><a href=/people/c/cheng-yang/>Cheng Yang</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/x/xiaoyuan-yi/>Xiaoyuan Yi</a>
|
<a href=/people/w/wenhao-li/>Wenhao Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1430><div class="card-body p-3 small">The ability to write diverse poems in different styles under the same poetic imagery is an important characteristic of human poetry writing. Most previous works on automatic Chinese poetry generation focused on improving the coherency among lines. Some work explored <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>style transfer</a> but suffered from expensive expert labeling of poem styles. In this paper, we target on stylistic poetry generation in a fully unsupervised manner for the first time. We propose a novel model which requires no supervised style labeling by incorporating <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a>, a concept in <a href=https://en.wikipedia.org/wiki/Information_theory>information theory</a>, into modeling. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to generate stylistic poems without losing fluency and <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherency</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1431 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1431" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1431/>Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints</a></strong><br><a href=/people/a/ashutosh-baheti/>Ashutosh Baheti</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/j/jiwei-li/>Jiwei Li</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1431><div class="card-body p-3 small">Neural conversation models tend to generate safe, generic responses for most inputs. This is due to the limitations of likelihood-based decoding objectives in generation tasks with diverse outputs, such as <a href=https://en.wikipedia.org/wiki/Conversation>conversation</a>. To address this challenge, we propose a simple yet effective approach for incorporating side information in the form of <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional constraints</a> over the generated responses. We propose two constraints that help generate more content rich responses that are based on a model of syntax and topics (Griffiths et al., 2005) and semantic similarity (Arora et al., 2016). We evaluate our approach against a variety of competitive baselines, using both automatic metrics and human judgments, showing that our proposed approach generates responses that are much less generic without sacrificing plausibility. A working demo of our code can be found at.<url>https://github.com/abaheti95/DC-NeuralConversation</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1434 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1434.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1434/>Multimodal Differential Network for Visual Question Generation</a></strong><br><a href=/people/b/badri-narayana-patro/>Badri Narayana Patro</a>
|
<a href=/people/s/sandeep-kumar/>Sandeep Kumar</a>
|
<a href=/people/v/vinod-kumar-kurmi/>Vinod Kumar Kurmi</a>
|
<a href=/people/v/vinay-namboodiri/>Vinay Namboodiri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1434><div class="card-body p-3 small">Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely <a href=https://en.wikipedia.org/wiki/Location>places</a>, <a href=https://en.wikipedia.org/wiki/Photo_caption>captions</a>, and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tags</a>. In this paper, we propose the use of <a href=https://en.wikipedia.org/wiki/Exemplar_(textual_criticism)>exemplars</a> for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, <a href=https://en.wikipedia.org/wiki/METEOR>METEOR</a>, ROUGE, and CIDEr).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1435.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1435 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1435 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1435/>Entity-aware Image Caption Generation</a></strong><br><a href=/people/d/di-lu/>Di Lu</a>
|
<a href=/people/s/spencer-whitehead/>Spencer Whitehead</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/s/shih-fu-chang/>Shih-Fu Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1435><div class="card-body p-3 small">Current image captioning approaches generate descriptions which lack specific information, such as <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> that are involved in the <a href=https://en.wikipedia.org/wiki/Image>images</a>. In this paper we propose a new task which aims to generate informative image captions, given <a href=https://en.wikipedia.org/wiki/Image>images</a> and <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> as input. We propose a simple but effective <a href=https://en.wikipedia.org/wiki/Scientific_method>approach</a> to tackle this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. We first train a convolutional neural networks-long short term memory networks (CNN-LSTM) model to generate a template caption based on the input image. Then we use a knowledge graph based collective inference algorithm to fill in the template with specific named entities retrieved via the hashtags. Experiments on a new benchmark dataset collected from <a href=https://en.wikipedia.org/wiki/Flickr>Flickr</a> show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates news-style image descriptions with much richer information. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms <a href=https://en.wikipedia.org/wiki/Unimodality>unimodal baselines</a> significantly with various evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1436 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1436" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1436/>Learning to Describe Differences Between Pairs of Similar Images</a></strong><br><a href=/people/h/harsh-jhamtani/>Harsh Jhamtani</a>
|
<a href=/people/t/taylor-berg-kirkpatrick/>Taylor Berg-Kirkpatrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1436><div class="card-body p-3 small">In this paper, we introduce the task of automatically generating text to describe the differences between two similar images. We collect a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by crowd-sourcing difference descriptions for pairs of <a href=https://en.wikipedia.org/wiki/Film_frame>image frames</a> extracted from <a href=https://en.wikipedia.org/wiki/Closed-circuit_television>video-surveillance footage</a>. Annotators were asked to succinctly describe all the differences in a short paragraph. As a result, our novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> provides an opportunity to explore <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that align language and <a href=https://en.wikipedia.org/wiki/Visual_perception>vision</a>, and capture <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>visual salience</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> may also be a useful benchmark for coherent multi-sentence generation. We perform a first-pass visual analysis that exposes clusters of differing pixels as a proxy for object-level differences. We propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that captures visual salience by using a <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> to align clusters of differing pixels with output sentences. We find that, for both single-sentence generation and as well as multi-sentence generation, the proposed model outperforms the models that use <a href=https://en.wikipedia.org/wiki/Attention>attention</a> alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1438 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1438/>Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN<span class=acl-fixed-case>RNN</span></a></strong><br><a href=/people/j/jingqiang-chen/>Jingqiang Chen</a>
|
<a href=/people/h/hai-zhuge/>Hai Zhuge</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1438><div class="card-body p-3 small">Rapid growth of multi-modal documents on the <a href=https://en.wikipedia.org/wiki/Internet>Internet</a> makes multi-modal summarization research necessary. Most previous research summarizes texts or images separately. Recent neural summarization research shows the strength of the Encoder-Decoder model in <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>. This paper proposes an abstractive text-image summarization model using the attentional hierarchical Encoder-Decoder model to summarize a text document and its accompanying images simultaneously, and then to align the sentences and images in summaries. A multi-modal attentional mechanism is proposed to attend original sentences, images, and captions when decoding. The DailyMail dataset is extended by collecting <a href=https://en.wikipedia.org/wiki/Image>images</a> and captions from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>. Experiments show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the neural abstractive and extractive text summarization methods that do not consider <a href=https://en.wikipedia.org/wiki/Digital_image>images</a>. In addition, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate informative summaries of images.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1440.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1440 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1440 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1440.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1440/>Closed-Book Training to Improve Summarization Encoder Memory</a></strong><br><a href=/people/y/yichen-jiang/>Yichen Jiang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1440><div class="card-body p-3 small">A good neural sequence-to-sequence summarization model should have a strong <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> that can distill and memorize the important information from long input texts so that the decoder can generate salient summaries based on the <a href=https://en.wikipedia.org/wiki/Encoder>encoder&#8217;s memory</a>. In this paper, we aim to improve the memorization capabilities of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> of a pointer-generator model by adding an additional &#8216;closed-book&#8217; decoder without attention and pointer mechanisms. Such a <a href=https://en.wikipedia.org/wiki/Code>decoder</a> forces the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to be more selective in the information encoded in its memory state because the <a href=https://en.wikipedia.org/wiki/Code>decoder</a> ca n&#8217;t rely on the extra information provided by the <a href=https://en.wikipedia.org/wiki/Attention>attention</a> and possibly copy modules, and hence improves the entire model. On the CNN / Daily Mail dataset, our 2-decoder model outperforms the baseline significantly in terms of ROUGE and METEOR metrics, for both cross-entropy and reinforced setups (and on human evaluation). Moreover, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> also achieves higher scores in a test-only DUC-2002 generalizability setup. We further present a memory ability test, two saliency metrics, as well as several sanity-check ablations (based on fixed-encoder, gradient-flow cut, and model capacity) to prove that the encoder of our 2-decoder model does in fact learn stronger memory representations than the baseline encoder.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1442 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1442.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1442" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1442/>Iterative Document Representation Learning Towards <a href=https://en.wikipedia.org/wiki/Summarization>Summarization</a> with Polishing</a></strong><br><a href=/people/x/xiuying-chen/>Xiuying Chen</a>
|
<a href=/people/s/shen-gao/>Shen Gao</a>
|
<a href=/people/c/chongyang-tao/>Chongyang Tao</a>
|
<a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1442><div class="card-body p-3 small">In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based model for supervised extractive text summarization, inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation, resulting in a sub-optimal representation. To address this issue we introduce a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> which iteratively polishes the document representation on many passes through the document. As part of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> should be updated. Experimental results on the CNN / DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1444.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1444 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1444 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1444" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1444/>Controlling Length in Abstractive Summarization Using a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Network</a></a></strong><br><a href=/people/y/yizhu-liu/>Yizhu Liu</a>
|
<a href=/people/z/zhiyi-luo/>Zhiyi Luo</a>
|
<a href=/people/k/kenny-zhu/>Kenny Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1444><div class="card-body p-3 small">Convolutional neural networks (CNNs) have met great success in abstractive summarization, but they can not effectively generate summaries of desired lengths. Because generated summaries are used in difference scenarios which may have space or length constraints, the ability to control the summary length in abstractive summarization is an important problem. In this paper, we propose an approach to constrain the summary length by extending a convolutional sequence to sequence model. The results show that this approach generates high-quality summaries with user defined length, and outperforms the baselines consistently in terms of ROUGE score, length variations and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1446 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1446" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1446/>Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization</a></strong><br><a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/k/kaiqiang-song/>Kaiqiang Song</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1446><div class="card-body p-3 small">Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>. In contrast, parallel data for <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a> are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1447 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1447/>Semi-Supervised Learning for Neural Keyphrase Generation</a></strong><br><a href=/people/h/hai-ye/>Hai Ye</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1447><div class="card-body p-3 small">We study the problem of generating keyphrases that summarize the key points for a given document. While sequence-to-sequence (seq2seq) models have achieved remarkable performance on this task (Meng et al., 2017), model training often relies on large amounts of labeled data, which is only applicable to resource-rich domains. In this paper, we propose semi-supervised keyphrase generation methods by leveraging both labeled data and large-scale unlabeled samples for <a href=https://en.wikipedia.org/wiki/Machine_learning>learning</a>. Two strategies are proposed. First, unlabeled documents are first tagged with synthetic keyphrases obtained from unsupervised keyphrase extraction methods or a self-learning algorithm, and then combined with labeled samples for training. Furthermore, we investigate a multi-task learning framework to jointly learn to generate keyphrases as well as the titles of the articles. Experimental results show that our semi-supervised learning-based methods outperform a state-of-the-art model trained with labeled data only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1448 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1448.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1448/>MSMO : Multimodal Summarization with Multimodal Output<span class=acl-fixed-case>MSMO</span>: Multimodal Summarization with Multimodal Output</a></strong><br><a href=/people/j/junnan-zhu/>Junnan Zhu</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/t/tianshang-liu/>Tianshang Liu</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1448><div class="card-body p-3 small">Multimodal summarization has drawn much attention due to the rapid growth of <a href=https://en.wikipedia.org/wiki/Multimedia>multimedia data</a>. The output of the current multimodal summarization systems is usually represented in texts. However, we have found through experiments that multimodal output can significantly improve user satisfaction for informativeness of summaries. In this paper, we propose a novel task, multimodal summarization with multimodal output (MSMO). To handle this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we first collect a <a href=https://en.wikipedia.org/wiki/Data_set>large-scale dataset</a> for MSMO research. We then propose a multimodal attention model to jointly generate text and select the most relevant image from the multimodal input. Finally, to evaluate multimodal outputs, we construct a novel multimodal automatic evaluation (MMAE) method which considers both intra-modality salience and inter-modality relevance. The experimental results show the effectiveness of <a href=https://en.wikipedia.org/wiki/MMAE>MMAE</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1449 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1449.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1449.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1449/>Frustratingly Easy Model Ensemble for Abstractive Summarization</a></strong><br><a href=/people/h/hayato-kobayashi/>Hayato Kobayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1449><div class="card-body p-3 small">Ensemble methods, which combine multiple models at decoding time, are now widely known to be effective for text-generation tasks. However, they generally increase computational costs, and thus, there have been many studies on compressing or distilling ensemble models. In this paper, we propose an alternative, simple but effective unsupervised ensemble method, post-ensemble, that combines multiple models by selecting a majority-like output in post-processing. We theoretically prove that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is closely related to <a href=https://en.wikipedia.org/wiki/Kernel_density_estimation>kernel density estimation</a> based on the von Mises-Fisher kernel. Experimental results on a news-headline-generation task show that the proposed method performs better than the current <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble methods</a>.<i>post-ensemble</i>, that combines multiple models by selecting a majority-like output in post-processing. We theoretically prove that our method is closely related to kernel density estimation based on the von Mises-Fisher kernel. Experimental results on a news-headline-generation task show that the proposed method performs better than the current ensemble methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1450 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1450/>Automatic Pyramid Evaluation Exploiting EDU-based Extractive Reference Summaries<span class=acl-fixed-case>EDU</span>-based Extractive Reference Summaries</a></strong><br><a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/h/hidetaka-kamigaito/>Hidetaka Kamigaito</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1450><div class="card-body p-3 small">This paper tackles automation of the pyramid method, a reliable manual evaluation framework. To construct a pyramid, we transform human-made reference summaries into extractive reference summaries that consist of Elementary Discourse Units (EDUs) obtained from source documents and then weight every EDU by counting the number of extractive reference summaries that contain the EDU. A summary is scored by the correspondences between EDUs in the summary and those in the pyramid. Experiments on DUC and TAC data sets show that our methods strongly correlate with various manual evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1451 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1451.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1451/>Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks</a></strong><br><a href=/people/y/yaushian-wang/>Yaushian Wang</a>
|
<a href=/people/h/hung-yi-lee/>Hung-Yi Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1451><div class="card-body p-3 small">Auto-encoders compress input data into a latent-space representation and reconstruct the original data from the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a>. This latent representation is not easily interpreted by humans. In this paper, we propose training an auto-encoder that encodes input text into human-readable sentences, and unpaired abstractive summarization is thereby achieved. The <a href=https://en.wikipedia.org/wiki/Auto-encoder>auto-encoder</a> is composed of a <a href=https://en.wikipedia.org/wiki/Generator_(computer_programming)>generator</a> and a <a href=https://en.wikipedia.org/wiki/Reconstructor>reconstructor</a>. The <a href=https://en.wikipedia.org/wiki/Generator_(mathematics)>generator</a> encodes the input text into a shorter word sequence, and the reconstructor recovers the <a href=https://en.wikipedia.org/wiki/Generator_(mathematics)>generator input</a> from the <a href=https://en.wikipedia.org/wiki/Generator_(mathematics)>generator output</a>. To make the generator output human-readable, a <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a> restricts the output of the <a href=https://en.wikipedia.org/wiki/Generator_(mathematics)>generator</a> to resemble human-written sentences. By taking the generator output as the summary of the input text, abstractive summarization is achieved without document-summary pairs as training data. Promising results are shown on both English and Chinese corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1452.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1452 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1452 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306149753 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1452/>Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings</a></strong><br><a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/l/lluis-marquez/>Lluís Màrquez</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1452><div class="card-body p-3 small">We address jointly two important tasks for Question Answering in community forums : given a new question, (i) find related existing questions, and (ii) find relevant answers to this new question. We further use an auxiliary task to complement the previous two, i.e., (iii) find good answers with respect to the thread question in a question-comment thread. We use deep neural networks (DNNs) to learn meaningful task-specific embeddings, which we then incorporate into a conditional random field (CRF) model for the multitask setting, performing joint learning over a complex graph structure. While <a href=https://en.wikipedia.org/wiki/Deep_learning>DNNs</a> alone achieve competitive results when trained to produce the embeddings, the CRF, which makes use of the embeddings and the dependencies between the tasks, improves the results significantly and consistently across a variety of evaluation metrics, thus showing the complementarity of <a href=https://en.wikipedia.org/wiki/Deep_learning>DNNs</a> and <a href=https://en.wikipedia.org/wiki/Structured_learning>structured learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1453.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1453 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1453 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306150555 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1453" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1453/>What Makes Reading Comprehension Questions Easier?</a></strong><br><a href=/people/s/saku-sugawara/>Saku Sugawara</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/s/satoshi-sekine/>Satoshi Sekine</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1453><div class="card-body p-3 small">A challenge in creating a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for machine reading comprehension (MRC) is to collect questions that require a sophisticated understanding of language to answer beyond using superficial cues. In this work, we investigate what makes questions easier across recent 12 MRC datasets with three question styles (answer extraction, description, and multiple choice). We propose to employ simple <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> to split each <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> into easy and hard subsets and examine the performance of two baseline models for each of the <a href=https://en.wikipedia.org/wiki/Subset>subsets</a>. We then manually annotate questions sampled from each <a href=https://en.wikipedia.org/wiki/Subset>subset</a> with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiple-choice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in <a href=https://en.wikipedia.org/wiki/Microscopic_scale>MRC</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1454.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1454 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1454 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1454.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306151626 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1454" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1454/>Commonsense for Generative Multi-Hop Question Answering Tasks</a></strong><br><a href=/people/l/lisa-bauer/>Lisa Bauer</a>
|
<a href=/people/y/yicheng-wang/>Yicheng Wang</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1454><div class="card-body p-3 small">Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-finding extractive QA. We instead focus on a more challenging multi-hop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs substantially better than previous generative models, and is competitive with current state-of-the-art span prediction models. We next introduce a novel system for selecting grounded multi-hop relational commonsense information from <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a> via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops, using a selectively-gated attention mechanism. This boosts the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance significantly (also verified via human evaluation), establishing a new state-of-the-art for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We also show that our background knowledge enhancements are generalizable and improve performance on QAngaroo-WikiHop, another multi-hop reasoning dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1456.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1456 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1456 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306152896 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1456" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1456/>A Nil-Aware Answer Extraction Framework for <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a></a></strong><br><a href=/people/s/souvik-kundu/>Souvik Kundu</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1456><div class="card-body p-3 small">Recently, there has been a surge of interest in reading comprehension-based (RC) question answering (QA). However, current approaches suffer from an impractical assumption that every question has a valid answer in the associated passage. A practical QA system must possess the ability to determine whether a valid answer exists in a given text passage. In this paper, we focus on developing QA systems that can extract an answer for a question if and only if the associated passage contains an answer. If the associated passage does not contain any valid answer, the QA system will correctly return Nil. We propose a novel nil-aware answer span extraction framework that is capable of returning Nil or a text span from the associated passage as an answer in a single step. We show that our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can be easily integrated with several recently proposed QA models developed for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> and can be trained in an end-to-end fashion. Our proposed nil-aware answer extraction neural network decomposes pieces of evidence into relevant and irrelevant parts and then combines them to infer the existence of any answer. Experiments on the NewsQA dataset show that the integration of our proposed framework significantly outperforms several strong baseline systems that use pipeline or threshold-based approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1458.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1458 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1458 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306131741 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1458" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1458/>Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures</a></strong><br><a href=/people/g/gongbo-tang/>Gongbo Tang</a>
|
<a href=/people/m/mathias-muller/>Mathias Müller</a>
|
<a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1458><div class="card-body p-3 small">Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks : subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that : 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances ; 2) self-attentional networks perform distinctly better than RNNs and CNNs on <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1459.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1459 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1459 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1459.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306132998 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1459" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1459/>Simplifying <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Addition-Subtraction Twin-Gated Recurrent Networks</a></strong><br><a href=/people/b/biao-zhang/>Biao Zhang</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a>
|
<a href=/people/j/jinsong-su/>Jinsong Su</a>
|
<a href=/people/q/qian-lin/>Qian Lin</a>
|
<a href=/people/h/huiji-zhang/>Huiji Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1459><div class="card-body p-3 small">In this paper, we propose an additionsubtraction twin-gated recurrent network (ATR) to simplify <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs. With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated. Despite this simplification, the essential <a href=https://en.wikipedia.org/wiki/Nonlinear_system>non-linearities</a> and capability of modeling long-distance dependencies are preserved. Additionally, the proposed <a href=https://en.wikipedia.org/wiki/Atchison,_Topeka_and_Santa_Fe_Railway>ATR</a> is more transparent than LSTM / GRU due to the simplification. Forward self-attention can be easily established in ATR, which makes the proposed <a href=https://en.wikipedia.org/wiki/Telecommunications_network>network</a> interpretable. Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English-German and English-French language pairs in terms of both translation quality and speed. Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1460 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306134160 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1460/>Speeding Up Neural Machine Translation Decoding by Cube Pruning</a></strong><br><a href=/people/w/wen-zhang/>Wen Zhang</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/y/yang-feng/>Yang Feng</a>
|
<a href=/people/l/lei-shen/>Lei Shen</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1460><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> has achieved promising results, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> suffers from slow translation speed. The direct consequence is that a trade-off has to be made between translation quality and speed, thus its performance can not come into full play. We apply cube pruning, a popular technique to speed up <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming</a>, into <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> to speed up the <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. To construct the <a href=https://en.wikipedia.org/wiki/Equivalence_class>equivalence class</a>, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3x on <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPUs</a> and 3.5x on <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPUs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1464 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1464.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306164201 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1464/>A Neural Local Coherence Model for Text Quality Assessment</a></strong><br><a href=/people/m/mohsen-mesgar/>Mohsen Mesgar</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1464><div class="card-body p-3 small">We propose a local coherence model that captures the flow of what semantically connects adjacent sentences in a text. We represent the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of a sentence by a <a href=https://en.wikipedia.org/wiki/Euclidean_vector>vector</a> and capture its state at each word of the sentence. We model what relates two adjacent sentences based on the two most similar semantic states, each of which is in one of the sentences. We encode the perceived coherence of a text by a <a href=https://en.wikipedia.org/wiki/Euclidean_vector>vector</a>, which represents patterns of changes in salient information that relates adjacent sentences. Our experiments demonstrate that our approach is beneficial for two downstream tasks : Readability assessment, in which our model achieves new state-of-the-art results ; and essay scoring, in which the combination of our coherence vectors and other task-dependent features significantly improves the performance of a strong essay scorer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1466.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1466 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1466 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306166016 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1466/>Getting to Hearer-old : Charting Referring Expressions Across Time</a></strong><br><a href=/people/i/ieva-staliunaite/>Ieva Staliūnaitė</a>
|
<a href=/people/h/hannah-rohde/>Hannah Rohde</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/a/annie-louis/>Annie Louis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1466><div class="card-body p-3 small">When a reader is first introduced to an entity, its <a href=https://en.wikipedia.org/wiki/Referring_expression>referring expression</a> must describe the entity. For entities that are widely known, a single word or phrase often suffices. This paper presents the first study of how <a href=https://en.wikipedia.org/wiki/Expression_(mathematics)>expressions</a> that refer to the same entity develop over time. We track thousands of person and organization entities over 20 years of New York Times (NYT). As entities move from hearer-new (first introduction to the NYT audience) to hearer-old (common knowledge) status, we show empirically that the referring expressions along this trajectory depend on the type of the entity, and exhibit linguistic properties related to becoming common knowledge (e.g., shorter length, less use of appositives, more definiteness). These properties can also be used to build a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to predict how long it will take for an entity to reach hearer-old status. Our results reach 10-30 % absolute improvement over a majority-class baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1467.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1467 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1467 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306120421 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1467/>Making fetch happen : The influence of social and linguistic context on nonstandard word growth and decline</a></strong><br><a href=/people/i/ian-stewart/>Ian Stewart</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1467><div class="card-body p-3 small">In an <a href=https://en.wikipedia.org/wiki/Online_community>online community</a>, new words come and go : today&#8217;s haha may be replaced by tomorrow&#8217;s lol. Changes in online writing are usually studied as a social process, with innovations diffusing through a network of individuals in a <a href=https://en.wikipedia.org/wiki/Speech_community>speech community</a>. But unlike other types of <a href=https://en.wikipedia.org/wiki/Innovation>innovation</a>, <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> is shaped and constrained by the <a href=https://en.wikipedia.org/wiki/Grammar>grammatical system</a> in which it takes part. To investigate the role of social and structural factors in <a href=https://en.wikipedia.org/wiki/Language_change>language change</a>, we undertake a large-scale analysis of the frequencies of <a href=https://en.wikipedia.org/wiki/Nonstandard_dialect>non-standard words</a> in <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. Dissemination across many linguistic contexts is a predictor of success : words that appear in more linguistic contexts grow faster and survive longer. Furthermore, social dissemination plays a less important role in explaining word growth and decline than previously hypothesized.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1468.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1468 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1468 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1468.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306121200 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1468" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1468/>Analyzing Correlated Evolution of Multiple Features Using Latent Representations</a></strong><br><a href=/people/y/yugo-murawaki/>Yugo Murawaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1468><div class="card-body p-3 small">Statistical phylogenetic models have allowed the quantitative analysis of the evolution of a single categorical feature and a pair of binary features, but correlated evolution involving multiple discrete features is yet to be explored. Here we propose latent representation-based analysis in which (1) a sequence of discrete surface features is projected to a sequence of <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>independent binary variables</a> and (2) <a href=https://en.wikipedia.org/wiki/Phylogenetic_inference>phylogenetic inference</a> is performed on the latent space. In the experiments, we analyze the features of <a href=https://en.wikipedia.org/wiki/Linguistic_typology>linguistic typology</a>, with a special focus on the order of subject, object and verb. Our analysis suggests that <a href=https://en.wikipedia.org/wiki/Language>languages</a> sharing the same <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> are not necessarily a coherent group but exhibit varying degrees of diachronic stability depending on other features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1469.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1469 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1469 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306121832 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1469/>Capturing <a href=https://en.wikipedia.org/wiki/Regional_variation>Regional Variation</a> with Distributed Place Representations and Geographic Retrofitting</a></strong><br><a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/c/christoph-purschke/>Christoph Purschke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1469><div class="card-body p-3 small">Dialects are one of the main drivers of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>language variation</a>, a major challenge for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tools</a>. In most languages, <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a> exist along a continuum, and are commonly discretized by combining the extent of several preselected linguistic variables. However, the selection of these <a href=https://en.wikipedia.org/wiki/Variable_(mathematics)>variables</a> is theory-driven and itself insensitive to change. We use Doc2Vec on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of 16.8 M anonymous online posts</a> in the <a href=https://en.wikipedia.org/wiki/List_of_territorial_entities_where_German_is_an_official_language>German-speaking area</a> to learn continuous document representations of cities. These <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> capture continuous regional linguistic distinctions, and can serve as input to downstream NLP tasks sensitive to <a href=https://en.wikipedia.org/wiki/Regional_variation>regional variation</a>. By incorporating geographic information via <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting</a> and <a href=https://en.wikipedia.org/wiki/Agglomerative_clustering>agglomerative clustering</a> with <a href=https://en.wikipedia.org/wiki/Structure>structure</a>, we recover dialect areas at various levels of <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a>. Evaluating these <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a> against an existing dialect map, we achieve a match of up to 0.77 V-score (harmonic mean of cluster completeness and homogeneity). Our results show that representation learning with <a href=https://en.wikipedia.org/wiki/Retrofitting>retrofitting</a> offers a robust general method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1472.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1472 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1472 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1472.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1472" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1472/>Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks<span class=acl-fixed-case>NLP</span> tasks</a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/p/paul-youssef/>Paul Youssef</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1472><div class="card-body p-3 small">Activation functions play a crucial role in <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> because they are the <a href=https://en.wikipedia.org/wiki/Nonlinear_system>nonlinearities</a> which have been attributed to the success story of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>. One of the currently most popular activation functions is <a href=https://en.wikipedia.org/wiki/ReLU>ReLU</a>, but several competitors have recently been proposed or &#8216;discovered&#8217;, including LReLU functions and swish. While most works compare newly proposed <a href=https://en.wikipedia.org/wiki/Activation_function>activation functions</a> on few tasks (usually from image classification) and against few competitors (usually ReLU), we perform the first largescale comparison of 21 <a href=https://en.wikipedia.org/wiki/Activation_function>activation functions</a> across eight different NLP tasks. We find that a largely unknown activation function performs most stably across all tasks, the so-called penalized tanh function. We also show that it can successfully replace the sigmoid and tanh gates in LSTM cells, leading to a 2 percentage point (pp) improvement over the standard choices on a challenging NLP task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1473.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1473 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1473 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1473" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1473/>Hard Non-Monotonic Attention for Character-Level Transduction</a></strong><br><a href=/people/s/shijie-wu/>Shijie Wu</a>
|
<a href=/people/p/pamela-shapiro/>Pamela Shapiro</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1473><div class="card-body p-3 small">Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks and has required a stochastic approximation to compute the <a href=https://en.wikipedia.org/wiki/Gradient>gradient</a>. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> significantly improves performance over the <a href=https://en.wikipedia.org/wiki/Stochastic_approximation>stochastic approximation</a> and outperforms soft attention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1477 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1477.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1477" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1477/>Simple Recurrent Units for Highly Parallelizable Recurrence</a></strong><br><a href=/people/t/tao-lei/>Tao Lei</a>
|
<a href=/people/y/yu-zhang/>Yu Zhang</a>
|
<a href=/people/s/sida-i-wang/>Sida I. Wang</a>
|
<a href=/people/h/hui-dai/>Hui Dai</a>
|
<a href=/people/y/yoav-artzi/>Yoav Artzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1477><div class="card-body p-3 small">Common <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural architectures</a> scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and <a href=https://en.wikipedia.org/wiki/Scalability>scalability</a>. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep models</a>. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 59x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model (Vaswani et al., 2017) on <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translation</a> by incorporating SRU into the <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1479.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1479 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1479 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1479/>Co-Stack Residual Affinity Networks with Multi-level Attention Refinement for Matching Text Sequences</a></strong><br><a href=/people/y/yi-tay/>Yi Tay</a>
|
<a href=/people/a/anh-tuan-luu/>Anh Tuan Luu</a>
|
<a href=/people/s/siu-cheung-hui/>Siu Cheung Hui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1479><div class="card-body p-3 small">Learning a <a href=https://en.wikipedia.org/wiki/Matching_function>matching function</a> between two text sequences is a long standing problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP research</a>. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> enables many potential <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and paraphrase identification. This paper proposes Co-Stack Residual Affinity Networks (CSRAN), a new and universal neural architecture for this problem. CSRAN is a deep architecture, involving stacked (multi-layered) recurrent encoders. Stacked / Deep architectures are traditionally difficult to train, due to the inherent weaknesses such as difficulty with feature propagation and <a href=https://en.wikipedia.org/wiki/Vanishing_gradient>vanishing gradients</a>. CSRAN incorporates two novel <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> to take advantage of the stacked architecture. Firstly, it introduces a new bidirectional alignment mechanism that learns <a href=https://en.wikipedia.org/wiki/Ligand_(biochemistry)>affinity weights</a> by fusing sequence pairs across stacked hierarchies. Secondly, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> leverages a multi-level attention refinement component between stacked recurrent layers. The key intuition is that, by leveraging information across all <a href=https://en.wikipedia.org/wiki/Hierarchy>network hierarchies</a>, we can not only improve <a href=https://en.wikipedia.org/wiki/Gradient_flow>gradient flow</a> but also improve overall performance. We conduct extensive experiments on six well-studied text sequence matching datasets, achieving state-of-the-art performance on all.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1480.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1480 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1480 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1480" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1480/>Spherical Latent Spaces for Stable Variational Autoencoders</a></strong><br><a href=/people/j/jiacheng-xu/>Jiacheng Xu</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1480><div class="card-body p-3 small">A hallmark of variational autoencoders (VAEs) for <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a> is their combination of powerful encoder-decoder models, such as LSTMs, with simple latent distributions, typically multivariate Gaussians. These models pose a difficult optimization problem : there is an especially bad local optimum where the variational posterior always equals the prior and the model does not use the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> at all, a kind of collapse which is encouraged by the KL divergence term of the objective. In this work, we experiment with another choice of latent distribution, namely the von Mises-Fisher (vMF) distribution, which places mass on the surface of the <a href=https://en.wikipedia.org/wiki/Unit_hypersphere>unit hypersphere</a>. With this choice of prior and posterior, the KL divergence term now only depends on the variance of the vMF distribution, giving us the ability to treat it as a fixed hyperparameter. We show that doing so not only averts the KL collapse, but consistently gives better likelihoods than <a href=https://en.wikipedia.org/wiki/List_of_things_named_after_Carl_Friedrich_Gauss>Gaussians</a> across a range of modeling conditions, including recurrent language modeling and bag-of-words document modeling. An analysis of the properties of our vMF representations shows that they learn richer and more nuanced structures in their latent representations than their Gaussian counterparts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1482 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1482.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1482" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1482/>Word Mover’s Embedding : From Word2Vec to Document Embedding<span class=acl-fixed-case>W</span>ord2<span class=acl-fixed-case>V</span>ec to Document Embedding</a></strong><br><a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/i/ian-en-hsu-yen/>Ian En-Hsu Yen</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/f/fangli-xu/>Fangli Xu</a>
|
<a href=/people/a/avinash-balakrishnan/>Avinash Balakrishnan</a>
|
<a href=/people/p/pin-yu-chen/>Pin-Yu Chen</a>
|
<a href=/people/p/pradeep-ravikumar/>Pradeep Ravikumar</a>
|
<a href=/people/m/michael-j-witbrock/>Michael J. Witbrock</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1482><div class="card-body p-3 small">While the celebrated Word2Vec technique yields semantically rich representations for individual words, there has been relatively less success in extending to generate unsupervised sentences or documents embeddings. Recent work has demonstrated that a distance measure between documents called Word Mover&#8217;s Distance (WMD) that aligns semantically similar words, yields unprecedented KNN classification accuracy. However, WMD is expensive to compute, and it is hard to extend its use beyond a KNN classifier. In this paper, we propose the Word Mover&#8217;s Embedding (WME), a novel approach to building an unsupervised document (sentence) embedding from pre-trained word embeddings. In our experiments on 9 benchmark text classification datasets and 22 textual similarity tasks, the proposed technique consistently matches or outperforms state-of-the-art techniques, with significantly higher accuracy on problems of short length.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1484.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1484 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1484 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1484/>Multi-Task Label Embedding for Text Classification</a></strong><br><a href=/people/h/honglun-zhang/>Honglun Zhang</a>
|
<a href=/people/l/liqiang-xiao/>Liqiang Xiao</a>
|
<a href=/people/w/wenqing-chen/>Wenqing Chen</a>
|
<a href=/people/y/yongkun-wang/>Yongkun Wang</a>
|
<a href=/people/y/yaohui-jin/>Yaohui Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1484><div class="card-body p-3 small">Multi-task learning in <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> leverages implicit correlations among related tasks to extract common features and yield performance gains. However, a large body of previous work treats labels of each task as independent and meaningless one-hot vectors, which cause a loss of potential label information. In this paper, we propose Multi-Task Label Embedding to convert labels in text classification into semantic vectors, thereby turning the original <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> into vector matching tasks. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> utilizes semantic correlations among tasks and makes it convenient to scale or transfer when new tasks are involved. Extensive experiments on five benchmark datasets for text classification show that our model can effectively improve the performances of related tasks with semantic representations of labels and additional information from each other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1487.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1487 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1487 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1487" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1487/>Uncertainty-aware generative models for inferring document class prevalence</a></strong><br><a href=/people/k/katherine-keith/>Katherine Keith</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1487><div class="card-body p-3 small">Prevalence estimation is the task of inferring the relative frequency of classes of unlabeled examples in a groupfor example, the proportion of a document collection with positive sentiment. Previous work has focused on aggregating and adjusting discriminative individual classifiers to obtain prevalence point estimates. But imperfect classifier accuracy ought to be reflected in uncertainty over the predicted prevalence for scientifically valid inference. In this work, we present (1) a generative probabilistic modeling approach to prevalence estimation, and (2) the construction and evaluation of prevalence confidence intervals ; in particular, we demonstrate that an off-the-shelf discriminative classifier can be given a generative re-interpretation, by backing out an implicit individual-level likelihood function, which can be used to conduct fast and simple group-level Bayesian inference. Empirically, we demonstrate our approach provides better confidence interval coverage than an alternative, and is dramatically more robust to shifts in the class prior between training and testing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1490.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1490 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1490 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1490" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1490/>Disfluency Detection using Auto-Correlational Neural Networks</a></strong><br><a href=/people/p/paria-jamshid-lou/>Paria Jamshid Lou</a>
|
<a href=/people/p/peter-anderson/>Peter Anderson</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1490><div class="card-body p-3 small">In recent years, the natural language processing community has moved away from task-specific feature engineering, i.e., researchers discovering ad-hoc feature representations for various tasks, in favor of <a href=https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units>general-purpose methods</a> that learn the input representation by themselves. However, state-of-the-art approaches to disfluency detection in spontaneous speech transcripts currently still depend on an array of hand-crafted features, and other representations derived from the output of pre-existing systems such as <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> or dependency parsers. As an alternative, this paper proposes a simple yet effective <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for automatic disfluency detection, called an auto-correlational neural network (ACNN). The model uses a convolutional neural network (CNN) and augments it with a new auto-correlation operator at the lowest layer that can capture the kinds of rough copy dependencies that are characteristic of repair disfluencies in speech. In experiments, the ACNN model outperforms the baseline CNN on a disfluency detection task with a 5 % increase in <a href=https://en.wikipedia.org/wiki/F-score>f-score</a>, which is close to the previous best result on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1492.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1492 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1492 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1492.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1492" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1492/>On Tree-Based Neural Sentence Modeling</a></strong><br><a href=/people/h/haoyue-shi/>Haoyue Shi</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a>
|
<a href=/people/j/jiaze-chen/>Jiaze Chen</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1492><div class="card-body p-3 small">Neural networks with tree-based sentence encoders have shown better results on many <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a>. Most of existing tree-based encoders adopt syntactic parsing trees as the explicit structure prior. To study the effectiveness of different <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree structures</a>, we replace the parsing trees with <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trivial trees</a> (i.e., binary balanced tree, left-branching tree and right-branching tree) in the <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a>. Though trivial trees contain no <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>, those <a href=https://en.wikipedia.org/wiki/Encoder>encoders</a> get competitive or even better results on all of the ten <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a> we investigated. This surprising result indicates that explicit syntax guidance may not be the main contributor to the superior performances of tree-based neural sentence modeling. Further analysis show that <a href=https://en.wikipedia.org/wiki/Tree_model>tree modeling</a> gives better results when crucial words are closer to the final representation. Additional experiments give more clues on how to design an effective tree-based encoder. Our code is open-source and available at.<url>https://github.com/ExplorerFreda/TreeEnc</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1494.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1494 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1494 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1494/>Siamese Network-Based Supervised Topic Modeling<span class=acl-fixed-case>S</span>iamese Network-Based Supervised Topic Modeling</a></strong><br><a href=/people/m/minghui-huang/>Minghui Huang</a>
|
<a href=/people/y/yanghui-rao/>Yanghui Rao</a>
|
<a href=/people/y/yuwei-liu/>Yuwei Liu</a>
|
<a href=/people/h/haoran-xie/>Haoran Xie</a>
|
<a href=/people/f/fu-lee-wang/>Fu Lee Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1494><div class="card-body p-3 small">Label-specific topics can be widely used for supporting <a href=https://en.wikipedia.org/wiki/Personality_psychology>personality psychology</a>, aspect-level sentiment analysis, and cross-domain sentiment classification. To generate label-specific topics, several supervised topic models which adopt likelihood-driven objective functions have been proposed. However, it is hard for them to get a precise estimation on both topic discovery and <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>. In this study, we propose a supervised topic model based on the <a href=https://en.wikipedia.org/wiki/Siamese_network>Siamese network</a>, which can trade off label-specific word distributions with document-specific label distributions in a uniform framework. Experiments on real-world datasets validate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs competitive in topic discovery quantitatively and qualitatively. Furthermore, the proposed model can effectively predict categorical or real-valued labels for new documents by generating word embeddings from a label-specific topical space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1497.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1497 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1497 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1497.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1497" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1497/>Learning Disentangled Representations of Texts with Application to Biomedical Abstracts</a></strong><br><a href=/people/s/sarthak-jain/>Sarthak Jain</a>
|
<a href=/people/e/edward-banner/>Edward Banner</a>
|
<a href=/people/j/jan-willem-van-de-meent/>Jan-Willem van de Meent</a>
|
<a href=/people/i/iain-marshall/>Iain J. Marshall</a>
|
<a href=/people/b/byron-c-wallace/>Byron C. Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1497><div class="card-body p-3 small">We propose a method for learning disentangled representations of texts that code for distinct and complementary aspects, with the aim of affording efficient model transfer and <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. To induce disentangled embeddings, we propose an adversarial objective based on the (dis)similarity between triplets of documents with respect to specific aspects. Our motivating application is embedding biomedical abstracts describing <a href=https://en.wikipedia.org/wiki/Clinical_trial>clinical trials</a> in a manner that disentangles the populations, interventions, and outcomes in a given trial. We show that our method learns <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> that encode these clinically salient aspects, and that these can be effectively used to perform aspect-specific retrieval. We demonstrate that the approach generalizes beyond our motivating application in experiments on two multi-aspect review corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1498.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1498 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1498 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1498" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1498/>Multi-Source Domain Adaptation with Mixture of Experts</a></strong><br><a href=/people/j/jiang-guo/>Jiang Guo</a>
|
<a href=/people/d/darsh-shah/>Darsh Shah</a>
|
<a href=/people/r/regina-barzilay/>Regina Barzilay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1498><div class="card-body p-3 small">We propose a mixture-of-experts approach for unsupervised domain adaptation from multiple sources. The key idea is to explicitly capture the relationship between a target example and different source domains. This relationship, expressed by a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>point-to-set metric</a>, determines how to combine predictors trained on various domains. The <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> is learned in an unsupervised fashion using meta-training. Experimental results on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> demonstrate that our approach consistently outperforms multiple baselines and can robustly handle negative transfer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1505 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1505.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306136412 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1505" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1505/>Revisiting the Importance of Encoding Logic Rules in Sentiment Classification</a></strong><br><a href=/people/k/kalpesh-krishna/>Kalpesh Krishna</a>
|
<a href=/people/p/preethi-jyothi/>Preethi Jyothi</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1505><div class="card-body p-3 small">We analyze the performance of different sentiment classification models on syntactically complex inputs like <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>A-but-B sentences</a>. The first contribution of this analysis addresses reproducible research : to meaningfully compare different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, their accuracies must be averaged over far more random seeds than what has traditionally been reported. With proper averaging in place, we notice that the distillation model described in Hu et al. (2016), which incorporates explicit logic rules for sentiment classification, is ineffective. In contrast, using contextualized ELMo embeddings (Peters et al., 2018a) instead of logic rules yields significantly better performance. Additionally, we provide analysis and visualizations that demonstrate ELMo&#8217;s ability to implicitly learn <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a>. Finally, a <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourced analysis</a> reveals how ELMo outperforms baseline models even on sentences with ambiguous sentiment labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1506 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306136988 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1506/>A Co-Attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness</a></strong><br><a href=/people/x/xiangju-li/>Xiangju Li</a>
|
<a href=/people/k/kaisong-song/>Kaisong Song</a>
|
<a href=/people/s/shi-feng/>Shi Feng</a>
|
<a href=/people/d/daling-wang/>Daling Wang</a>
|
<a href=/people/y/yifei-zhang/>Yifei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1506><div class="card-body p-3 small">Emotion cause analysis has been a key topic in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Existing methods ignore the contexts around the emotion word which can provide an emotion cause clue. Meanwhile, the clauses in a document play different roles on stimulating a certain emotion, depending on their content relevance. Therefore, we propose a co-attention neural network model for emotion cause analysis with emotional context awareness. The method encodes the clauses with a co-attention based bi-directional long short-term memory into high-level input representations, which are further fed into a convolutional layer for emotion cause analysis. Experimental results show that our approach outperforms the state-of-the-art baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1507 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306137544 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1507" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1507/>Modeling Empathy and Distress in Reaction to News Stories</a></strong><br><a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/a/anneke-buffone/>Anneke Buffone</a>
|
<a href=/people/b/barry-slaff/>Barry Slaff</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a>
|
<a href=/people/j/joao-sedoc/>João Sedoc</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1507><div class="card-body p-3 small">Computational detection and understanding of empathy is an important factor in advancing <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer interaction</a>. Yet to date, text-based empathy prediction has the following major limitations : It underestimates the psychological complexity of the phenomenon, adheres to a weak notion of ground truth where empathic states are ascribed by third parties, and lacks a shared corpus. In contrast, this contribution presents the first publicly available gold standard for empathy prediction. It is constructed using a novel annotation methodology which reliably captures empathy assessments by the writer of a statement using <a href=https://en.wikipedia.org/wiki/Scale_(social_sciences)>multi-item scales</a>. This is also the first computational work distinguishing between multiple forms of <a href=https://en.wikipedia.org/wiki/Empathy>empathy</a>, <a href=https://en.wikipedia.org/wiki/Empathic_concern>empathic concern</a>, and <a href=https://en.wikipedia.org/wiki/Distress_(medicine)>personal distress</a>, as recognized throughout <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a>. Finally, we present experimental results for three different <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a>, of which a CNN performs the best.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1509 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1509.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306166768 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1509" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1509/>A Tree-based Decoder for Neural Machine Translation</a></strong><br><a href=/people/x/xinyi-wang/>Xinyi Wang</a>
|
<a href=/people/h/hieu-pham/>Hieu Pham</a>
|
<a href=/people/p/pengcheng-yin/>Pengcheng Yin</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1509><div class="card-body p-3 small">Recent advances in Neural Machine Translation (NMT) show that adding syntactic information to NMT systems can improve the quality of their translations. Most existing work utilizes some specific types of linguistically-inspired tree structures, like <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>constituency</a> and dependency parse trees. This is often done via a standard RNN decoder that operates on a linearized target tree structure. However, it is an open question of what specific linguistic formalism, if any, is the best structural representation for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>. In this paper, we (1) propose an NMT model that can naturally generate the <a href=https://en.wikipedia.org/wiki/Topology>topology</a> of an arbitrary <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree structure</a> on the target side, and (2) experiment with various target tree structures. Our experiments show the surprising result that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> delivers the best improvements with balanced binary trees constructed without any linguistic knowledge ; this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms standard seq2seq models by up to 2.1 BLEU points, and other methods for incorporating target-side syntax by up to 0.7 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1512 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1512.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306169001 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1512" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1512/>Has <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> Achieved Human Parity? A Case for Document-level Evaluation</a></strong><br><a href=/people/s/samuel-laubli/>Samuel Läubli</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/m/martin-volk/>Martin Volk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1512><div class="card-body p-3 small">Recent research suggests that <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> achieves parity with <a href=https://en.wikipedia.org/wiki/Translation>professional human translation</a> on the WMT ChineseEnglish news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, <a href=https://en.wikipedia.org/wiki/Human>human raters</a> assessing adequacy and fluency show a stronger preference for <a href=https://en.wikipedia.org/wiki/Human>human</a> over <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1518 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306126460 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1518/>Learning Word Representations with Cross-Sentence Dependency for End-to-End Co-reference Resolution</a></strong><br><a href=/people/h/hongyin-luo/>Hongyin Luo</a>
|
<a href=/people/j/jim-glass/>Jim Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1518><div class="card-body p-3 small">In this work, we present a word embedding model that learns cross-sentence dependency for improving end-to-end co-reference resolution (E2E-CR). While the traditional E2E-CR model generates word representations by running long short-term memory (LSTM) recurrent neural networks on each sentence of an input article or conversation separately, we propose linear sentence linking and attentional sentence linking models to learn cross-sentence dependency. Both sentence linking strategies enable the LSTMs to make use of valuable information from context sentences while calculating the representation of the current input word. With this approach, the LSTMs learn <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> considering knowledge not only from the current sentence but also from the entire input document. Experiments show that learning cross-sentence dependency enriches information contained by the word representations, and improves the performance of the co-reference resolution model compared with our baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1523 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1523" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1523/>Word Sense Induction with Neural biLM and Symmetric Patterns<span class=acl-fixed-case>LM</span> and Symmetric Patterns</a></strong><br><a href=/people/a/asaf-amrami/>Asaf Amrami</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1523><div class="card-body p-3 small">An established method for Word Sense Induction (WSI) uses a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to predict probable substitutes for target words, and induces senses by clustering these resulting substitute vectors. We replace the ngram-based language model (LM) with a recurrent one. Beyond being more accurate, the use of the recurrent LM allows us to effectively query it in a creative way, using what we call dynamic symmetric patterns. The combination of the RNN-LM and the dynamic symmetric patterns results in strong substitute vectors for WSI, allowing to surpass the current state-of-the-art on the SemEval 2013 WSI shared task by a large margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1525 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1525/>Similarity-Based Reconstruction Loss for Meaning Representation</a></strong><br><a href=/people/o/olga-kovaleva/>Olga Kovaleva</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/a/alexey-romanov/>Alexey Romanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1525><div class="card-body p-3 small">This paper addresses the problem of <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. Using an autoencoder framework, we propose and evaluate several <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a> that can be used as an alternative to the commonly used cross-entropy reconstruction loss. The proposed <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a> use similarities between words in the <a href=https://en.wikipedia.org/wiki/Glossary_of_computer_graphics>embedding space</a>, and can be used to train any neural model for <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a>. We show that the introduced <a href=https://en.wikipedia.org/wiki/Loss_function>loss functions</a> amplify semantic diversity of reconstructed sentences, while preserving the original meaning of the input. We test the derived autoencoder-generated representations on paraphrase detection and language inference tasks and demonstrate performance improvement compared to the traditional cross-entropy loss.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1526 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1526.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1526/>What can we learn from Semantic Tagging?</a></strong><br><a href=/people/m/mostafa-abdou/>Mostafa Abdou</a>
|
<a href=/people/a/artur-kulmizev/>Artur Kulmizev</a>
|
<a href=/people/v/vinit-ravishankar/>Vinit Ravishankar</a>
|
<a href=/people/l/lasha-abzianidze/>Lasha Abzianidze</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1526><div class="card-body p-3 small">We investigate the effects of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> using the recently introduced task of <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>semantic tagging</a>. We employ semantic tagging as an auxiliary task for three different NLP tasks : <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, Universal Dependency parsing, and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a>. We compare full neural network sharing, partial neural network sharing, and what we term the learning what to share setting where negative transfer between tasks is less likely. Our findings show considerable improvements for all <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, particularly in the learning what to share setting which shows consistent gains across all tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1527 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1527.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1527/>Conditional Word Embedding and <a href=https://en.wikipedia.org/wiki/Hypothesis_testing>Hypothesis Testing</a> via Bayes-by-Backprop<span class=acl-fixed-case>B</span>ayes-by-Backprop</a></strong><br><a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/m/michael-gill/>Michael Gill</a>
|
<a href=/people/a/arthur-spirling/>Arthur Spirling</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1527><div class="card-body p-3 small">Conventional word embedding models do not leverage information from document meta-data, and they do not model <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a>. We address these concerns with a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that incorporates document covariates to estimate conditional word embedding distributions. Our model allows for (a) hypothesis tests about the meanings of terms, (b) assessments as to whether a word is near or far from another conditioned on different covariate values, and (c) assessments as to whether estimated differences are statistically significant.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1530 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1530.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1530/>Sanskrit Sandhi Splitting using seq2(seq)2<span class=acl-fixed-case>S</span>anskrit Sandhi Splitting using seq2(seq)2</a></strong><br><a href=/people/r/rahul-aralikatte/>Rahul Aralikatte</a>
|
<a href=/people/n/neelamadhav-gantayat/>Neelamadhav Gantayat</a>
|
<a href=/people/n/naveen-panwar/>Naveen Panwar</a>
|
<a href=/people/a/anush-sankaran/>Anush Sankaran</a>
|
<a href=/people/s/senthil-mani/>Senthil Mani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1530><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>, small words (morphemes) are combined to form <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound words</a> through a process known as <a href=https://en.wikipedia.org/wiki/Sandhi>Sandhi</a>. Sandhi splitting is the process of splitting a given <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound word</a> into its constituent morphemes. Although <a href=https://en.wikipedia.org/wiki/Linguistic_prescription>rules</a> governing <a href=https://en.wikipedia.org/wiki/Word_splitting>word splitting</a> exists in the language, it is highly challenging to identify the location of the splits in a <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound word</a>. Though existing Sandhi splitting systems incorporate these pre-defined splitting rules, they have a low <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> as the same <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>compound word</a> might be broken down in multiple ways to provide syntactically correct splits. In this research, we propose a novel deep learning architecture called Double Decoder RNN (DD-RNN), which (i) predicts the location of the split(s) with 95 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, and (ii) predicts the constituent words (learning the Sandhi splitting rules) with 79.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, outperforming the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-art</a> by 20 %. Additionally, we show the generalization capability of our deep learning model, by showing competitive results in the problem of Chinese word segmentation, as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1531 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1531" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1531/>Unsupervised Neural Word Segmentation for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> via Segmental Language Modeling<span class=acl-fixed-case>C</span>hinese via Segmental Language Modeling</a></strong><br><a href=/people/z/zhiqing-sun/>Zhiqing Sun</a>
|
<a href=/people/z/zhi-hong-deng/>Zhi-Hong Deng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1531><div class="card-body p-3 small">Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative models</a> into neural version by using neural language models, those of <a href=https://en.wikipedia.org/wiki/Generative_model>generative ones</a> are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, as well as preserves several properties of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1532 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1532.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1532" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1532/>LemmaTag : Jointly Tagging and <a href=https://en.wikipedia.org/wiki/Lemmatization>Lemmatizing</a> for Morphologically Rich Languages with BRNNs<span class=acl-fixed-case>L</span>emma<span class=acl-fixed-case>T</span>ag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with <span class=acl-fixed-case>BRNN</span>s</a></strong><br><a href=/people/d/daniel-kondratyuk/>Daniel Kondratyuk</a>
|
<a href=/people/t/tomas-gavenciak/>Tomáš Gavenčiak</a>
|
<a href=/people/m/milan-straka/>Milan Straka</a>
|
<a href=/people/j/jan-hajic/>Jan Hajič</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1532><div class="card-body p-3 small">We present LemmaTag, a featureless neural network architecture that jointly generates part-of-speech tags and lemmas for sentences by using bidirectional RNNs with character-level and word-level embeddings. We demonstrate that both tasks benefit from sharing the encoding part of the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a>, predicting tag subcategories, and using the tagger output as an input to the <a href=https://en.wikipedia.org/wiki/Lemmatizer>lemmatizer</a>. We evaluate our model across several languages with complex morphology, which surpasses state-of-the-art accuracy in both <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Lemmatization>lemmatization</a> in <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1533 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1533.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1533/>Recovering Missing Characters in Old Hawaiian Writing<span class=acl-fixed-case>H</span>awaiian Writing</a></strong><br><a href=/people/b/brendan-shillingford/>Brendan Shillingford</a>
|
<a href=/people/o/oiwi-parker-jones/>Oiwi Parker Jones</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1533><div class="card-body p-3 small">In contrast to the older writing system of the 19th century, modern Hawaiian orthography employs characters for <a href=https://en.wikipedia.org/wiki/Vowel_length>long vowels</a> and <a href=https://en.wikipedia.org/wiki/Glottal_stop>glottal stops</a>. These extra characters account for about one-third of the <a href=https://en.wikipedia.org/wiki/Phoneme>phonemes</a> in <a href=https://en.wikipedia.org/wiki/Hawaiian_language>Hawaiian</a>, so including them makes a big difference to <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> and <a href=https://en.wikipedia.org/wiki/Pronunciation>pronunciation</a>. However, <a href=https://en.wikipedia.org/wiki/Transliteration>transliterating</a> between older and newer texts is a laborious task when performed manually. We introduce two related methods to help solve this <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration problem</a> automatically. One approach is implemented, end-to-end, using finite state transducers (FSTs). The other is a hybrid deep learning approach, which approximately composes an FST with a recurrent neural network language model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1535.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1535 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1535 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1535.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1535/>Bridging Knowledge Gaps in Neural Entailment via Symbolic Models</a></strong><br><a href=/people/d/dongyeop-kang/>Dongyeop Kang</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1535><div class="card-body p-3 small">Most textual entailment models focus on lexical gaps between the premise text and the hypothesis, but rarely on knowledge gaps. We focus on filling these knowledge gaps in the Science Entailment task, by leveraging an external structured knowledge base (KB) of science facts. Our new <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> combines standard neural entailment models with a knowledge lookup module. To facilitate this lookup, we propose a fact-level decomposition of the hypothesis, and verifying the resulting sub-facts against both the textual premise and the structured KB. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, NSNet, learns to aggregate predictions from these heterogeneous data formats. On the SciTail dataset, NSNet outperforms a simpler combination of the two predictions by 3 % and the base entailment model by 5 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1536.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1536 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1536 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1536/>The BQ Corpus : A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification<span class=acl-fixed-case>BQ</span> Corpus: A Large-scale Domain-specific <span class=acl-fixed-case>C</span>hinese Corpus For Sentence Semantic Equivalence Identification</a></strong><br><a href=/people/j/jing-chen/>Jing Chen</a>
|
<a href=/people/q/qingcai-chen/>Qingcai Chen</a>
|
<a href=/people/x/xin-liu/>Xin Liu</a>
|
<a href=/people/h/haijun-yang/>Haijun Yang</a>
|
<a href=/people/d/daohe-lu/>Daohe Lu</a>
|
<a href=/people/b/buzhou-tang/>Buzhou Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1536><div class="card-body p-3 small">This paper introduces the Bank Question (BQ) corpus, a Chinese corpus for sentence semantic equivalence identification (SSEI). The BQ corpus contains 120,000 question pairs from 1-year online bank custom service logs. To efficiently process and annotate questions from such a large scale of logs, this paper proposes a clustering based annotation method to achieve questions with the same intent. First, the deduplicated questions with the same answer are clustered into stacks by the Word Mover&#8217;s Distance (WMD) based Affinity Propagation (AP) algorithm. Then, the annotators are asked to assign the clustered questions into different intent categories. Finally, the positive and negative question pairs for SSEI are selected in the same intent category and between different intent categories respectively. We also present six SSEI benchmark performance on our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, including state-of-the-art algorithms. As the largest manually annotated public Chinese SSEI corpus in the bank domain, the BQ corpus is not only useful for Chinese question semantic matching research, but also a significant resource for cross-lingual and cross-domain SSEI research. The <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>corpus</a> is available in public.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1537 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1537.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1537/>Interpreting Recurrent and Attention-Based Neural Models : a Case Study on Natural Language Inference</a></strong><br><a href=/people/r/reza-ghaeini/>Reza Ghaeini</a>
|
<a href=/people/x/xiaoli-fern/>Xiaoli Fern</a>
|
<a href=/people/p/prasad-tadepalli/>Prasad Tadepalli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1537><div class="card-body p-3 small">Deep learning models have achieved remarkable success in natural language inference (NLI) tasks. While these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are widely explored, they are hard to interpret and it is often unclear how and why they actually work. In this paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLI</a>. In particular, we propose to interpret the intermediate layers of NLI models by visualizing the <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>saliency of attention</a> and LSTM gating signals. We present several examples for which our methods are able to reveal interesting insights and identify the critical information contributing to the model decisions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1539.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1539 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1539 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-1539/>Identifying Domain Adjacent Instances for Semantic Parsers</a></strong><br><a href=/people/j/james-ferguson/>James Ferguson</a>
|
<a href=/people/j/janara-christensen/>Janara Christensen</a>
|
<a href=/people/e/edward-li/>Edward Li</a>
|
<a href=/people/e/edgar-gonzalez-pellicer/>Edgar Gonzàlez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1539><div class="card-body p-3 small">When the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of a sentence are not representable in a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a>&#8217;s output schema, <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> will inevitably fail. Detection of these instances is commonly treated as an out-of-domain classification problem. However, there is also a more subtle scenario in which the test data is drawn from the same domain. In addition to formalizing this problem of domain-adjacency, we present a comparison of various baselines that could be used to solve it. We also propose a new simple <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representation</a> that emphasizes words which are unexpected. This approach improves the performance of a downstream semantic parser run on in-domain and domain-adjacent instances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1540 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1540" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1540/>Mapping <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language commands</a> to web elements</a></strong><br><a href=/people/p/panupong-pasupat/>Panupong Pasupat</a>
|
<a href=/people/t/tian-shun-jiang/>Tian-Shun Jiang</a>
|
<a href=/people/e/evan-liu/>Evan Liu</a>
|
<a href=/people/k/kelvin-guu/>Kelvin Guu</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1540><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> provides a rich, open-domain environment with textual, structural, and spatial properties. We propose a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> for grounding language in this environment : given a natural language command (e.g., click on the second article), choose the correct element on the web page (e.g., a hyperlink or text box). We collected a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of over 50,000 <a href=https://en.wikipedia.org/wiki/Command_(computing)>commands</a> that capture various phenomena such as functional references (e.g. find who made this site), relational reasoning (e.g. article by john), and <a href=https://en.wikipedia.org/wiki/Visual_reasoning>visual reasoning</a> (e.g. top-most article). We also implemented and analyzed three baseline models that capture different phenomena present in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1541 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1541" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1541/>Wronging a Right : Generating Better Errors to Improve Grammatical Error Detection</a></strong><br><a href=/people/s/sudhanshu-kasewa/>Sudhanshu Kasewa</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1541><div class="card-body p-3 small">Grammatical error correction, like other machine learning tasks, greatly benefits from large quantities of high quality training data, which is typically expensive to produce. While writing a program to automatically generate realistic <a href=https://en.wikipedia.org/wiki/Grammatical_error>grammatical errors</a> would be difficult, one could learn the distribution of naturally-occurring errors and attempt to introduce them into other <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Initial work on inducing errors in this way using <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> has shown promise ; we investigate cheaply constructing synthetic samples, given a small corpus of human-annotated data, using an off-the-rack attentive sequence-to-sequence model and a straight-forward post-processing procedure. Our approach yields error-filled artificial data that helps a vanilla bi-directional LSTM to outperform the previous state of the art at grammatical error detection, and a previously introduced model to gain further improvements of over 5 % F0.5 score. When attempting to determine if a given sentence is synthetic, a human annotator at best achieves 39.39 F1 score, indicating that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates mostly <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-like instances</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1542.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1542 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1542 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1542.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1542" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1542/>Modeling Input Uncertainty in Neural Network Dependency Parsing</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1542><div class="card-body p-3 small">Recently introduced neural network parsers allow for new approaches to circumvent data sparsity issues by modeling character level information and by exploiting raw data in a semi-supervised setting. Data sparsity is especially prevailing when transferring to <a href=https://en.wikipedia.org/wiki/Standardization>non-standard domains</a>. In this setting, lexical normalization has often been used in the past to circumvent data sparsity. In this paper, we investigate whether these new neural approaches provide similar functionality as lexical normalization, or whether they are complementary. We provide experimental results which show that a separate normalization component improves performance of a neural network parser even if it has access to character level information as well as external word embeddings. Further improvements are obtained by a straightforward but novel approach in which the top-N best candidates provided by the <a href=https://en.wikipedia.org/wiki/Parsing>normalization component</a> are available to the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1543 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1543.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1543.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1543" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1543/>Parameter sharing between dependency parsers for related languages</a></strong><br><a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1543><div class="card-body p-3 small">Previous work has suggested that parameter sharing between transition-based neural dependency parsers for related languages can lead to better performance, but there is no consensus on what parameters to share. We present an evaluation of 27 different parameter sharing strategies across 10 languages, representing five pairs of related languages, each pair from a different <a href=https://en.wikipedia.org/wiki/Language_family>language family</a>. We find that sharing transition classifier parameters always helps, whereas the usefulness of sharing word and/or character LSTM parameters varies. Based on this result, we propose an architecture where the transition classifier is shared, and the sharing of word and character parameters is controlled by a parameter that can be tuned on validation data. This <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is linguistically motivated and obtains significant improvements over a monolingually trained baseline. We also find that sharing transition classifier parameters helps when training a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> on unrelated language pairs, but we find that, in the case of unrelated languages, sharing too many parameters does not help.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1544.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1544 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1544 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1544.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1544/>Grammar Induction with Neural Language Models : An Unusual Replication</a></strong><br><a href=/people/p/phu-mon-htut/>Phu Mon Htut</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1544><div class="card-body p-3 small">A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report near-state-of-the-art results on the target task of <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust : All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for <a href=https://en.wikipedia.org/wiki/Grammar_induction>grammar induction</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1545 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1545" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-1545/>Data Augmentation via Dependency Tree Morphing for Low-Resource Languages</a></strong><br><a href=/people/g/gozde-gul-sahin/>Gözde Gül Şahin</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1545><div class="card-body p-3 small">Neural NLP systems achieve high scores in the presence of sizable training dataset. Lack of such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> leads to poor <a href=https://en.wikipedia.org/wiki/System>system</a> performances in the case low-resource languages. We present two simple text augmentation techniques using dependency trees, inspired from <a href=https://en.wikipedia.org/wiki/Digital_image_processing>image processing</a>. We crop sentences by removing dependency links, and we rotate sentences by moving the tree fragments around the root. We apply these techniques to augment the training sets of low-resource languages in Universal Dependencies project. We implement a character-level sequence tagging model and evaluate the augmented datasets on part-of-speech tagging task. We show that <a href=https://en.wikipedia.org/wiki/Crop_factor>crop</a> and rotate provides improvements over the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with non-augmented data for majority of the languages, especially for languages with rich case marking systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1546.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1546 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1546 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306140720 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1546/>How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks</a></strong><br><a href=/people/d/divyansh-kaushik/>Divyansh Kaushik</a>
|
<a href=/people/z/zachary-c-lipton/>Zachary C. Lipton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1546><div class="card-body p-3 small">Many recent papers address <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, where examples consist of (question, passage, answer) tuples. Presumably, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding that question- and passage-only models often perform surprisingly well. On 14 out of 20 bAbI tasks, passage-only models achieve greater than 50 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, sometimes matching the full <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Interestingly, while <a href=https://en.wikipedia.org/wiki/Cognitive_behavioral_therapy>CBT</a> provides 20-sentence passages, only the last is needed for accurate prediction. By comparison, SQuAD and <a href=https://en.wikipedia.org/wiki/CNN>CNN</a> appear better-constructed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1547 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1547.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306141298 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1547" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1547/>MultiWOZ-A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>WOZ</span> - A Large-Scale Multi-Domain <span class=acl-fixed-case>W</span>izard-of-<span class=acl-fixed-case>O</span>z Dataset for Task-Oriented Dialogue Modelling</a></strong><br><a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/o/osman-ramadan/>Osman Ramadan</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1547><div class="card-body p-3 small">Even though <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available. To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora. The contribution of this work apart from the open-sourced dataset is two-fold : firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-1548.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-1548 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-1548 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/D18-1548.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/306141078 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-1548" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/D18-1548/>Linguistically-Informed Self-Attention for Semantic Role Labeling</a></strong><br><a href=/people/e/emma-strubell/>Emma Strubell</a>
|
<a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/d/daniel-andor/>Daniel Andor</a>
|
<a href=/people/d/david-weiss/>David Weiss</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-1548><div class="card-body p-3 small">Current state-of-the-art semantic role labeling (SRL) uses a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, predicate detection and <a href=https://en.wikipedia.org/wiki/Speech_recognition>SRL</a>. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> using merely raw tokens as input, encoding the sequence only once to simultaneously perform <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>, predicate detection and role labeling for all predicates. Syntax is incorporated by training one <a href=https://en.wikipedia.org/wiki/Head_(linguistics)>attention head</a> to attend to syntactic parents for each token. Moreover, if a high-quality syntactic parse is already available, it can be beneficially injected at test time without re-training our SRL model. In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, attaining 2.5 F1 absolute higher than the previous state-of-the-art on <a href=https://en.wikipedia.org/wiki/News_agency>newswire</a> and more than 3.5 F1 on out-of-domain data, nearly 10 % reduction in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1.</div></div></div><hr><div id=d18-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/D18-2/>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2000/>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></strong><br><a href=/people/e/eduardo-blanco/>Eduardo Blanco</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2001/>SyntaViz : Visualizing Voice Queries through a Syntax-Driven Hierarchical Ontology<span class=acl-fixed-case>S</span>ynta<span class=acl-fixed-case>V</span>iz: Visualizing Voice Queries through a Syntax-Driven Hierarchical Ontology</a></strong><br><a href=/people/m/md-iftekhar-tanveer/>Md Iftekhar Tanveer</a>
|
<a href=/people/f/ferhan-ture/>Ferhan Ture</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2001><div class="card-body p-3 small">This paper describes SyntaViz, a visualization interface specifically designed for analyzing natural-language queries that were created by users of a voice-enabled product. SyntaViz provides a platform for browsing the ontology of user queries from a syntax-driven perspective, providing quick access to high-impact failure points of the existing intent understanding system and evidence for data-driven decisions in the development cycle. A case study on Xfinity X1 (a voice-enabled entertainment platform from Comcast) reveals that SyntaViz helps developers identify multiple action items in a short amount of time without any special training. SyntaViz has been open-sourced for the benefit of the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2005/>MorAz : an Open-source Morphological Analyzer for Azerbaijani Turkish<span class=acl-fixed-case>M</span>or<span class=acl-fixed-case>A</span>z: an Open-source Morphological Analyzer for <span class=acl-fixed-case>A</span>zerbaijani <span class=acl-fixed-case>T</span>urkish</a></strong><br><a href=/people/b/berke-ozenc/>Berke Özenç</a>
|
<a href=/people/r/razieh-ehsani/>Razieh Ehsani</a>
|
<a href=/people/e/ercan-solak/>Ercan Solak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2005><div class="card-body p-3 small">MorAz is an open-source <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyzer</a> for <a href=https://en.wikipedia.org/wiki/Azerbaijani_language>Azerbaijani Turkish</a>. The analyzer is available through both as a website for interactive exploration and as a <a href=https://en.wikipedia.org/wiki/Representational_state_transfer>RESTful web service</a> for integration into a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>natural language processing pipeline</a>. MorAz implements the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> of <a href=https://en.wikipedia.org/wiki/Azerbaijani_language>Azerbaijani Turkish</a> in two-level using Helsinki finite-state transducer and wraps the analyzer with python scripts in a Django instance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2007/>Visual Interrogation of Attention-Based Models for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a> and Machine Comprehension</a></strong><br><a href=/people/s/shusen-liu/>Shusen Liu</a>
|
<a href=/people/t/tao-li/>Tao Li</a>
|
<a href=/people/z/zhimin-li/>Zhimin Li</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a>
|
<a href=/people/v/valerio-pascucci/>Valerio Pascucci</a>
|
<a href=/people/p/peer-timo-bremer/>Peer-Timo Bremer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2007><div class="card-body p-3 small">Neural networks models have gained unprecedented popularity in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> due to their state-of-the-art performance and the flexible end-to-end training scheme. Despite their advantages, the lack of interpretability hinders the deployment and refinement of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. In this work, we present a flexible visualization library for creating customized visual analytic environments, in which the user can investigate and interrogate the relationships among the input, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model internals</a> (i.e., <a href=https://en.wikipedia.org/wiki/Attentional_control>attention</a>), and the output predictions, which in turn shed light on the <a href=https://en.wikipedia.org/wiki/Decision-making_software>model decision-making process</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2008/>DERE : A Task and Domain-Independent Slot Filling Framework for Declarative Relation Extraction<span class=acl-fixed-case>DERE</span>: A Task and Domain-Independent Slot Filling Framework for Declarative Relation Extraction</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/l/laura-ana-maria-oberlander/>Laura Ana Maria Bostan</a>
|
<a href=/people/s/sean-papay/>Sean Papay</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2008><div class="card-body p-3 small">Most <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning systems</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> are tailored to specific tasks. As a result, comparability of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> across tasks is missing and their applicability to new tasks is limited. This affects end users without machine learning experience as well as model developers. To address these limitations, we present DERE, a novel <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for declarative specification and compilation of template-based information extraction. It uses a generic <a href=https://en.wikipedia.org/wiki/Specification_language>specification language</a> for the task and for data annotations in terms of spans and <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>frames</a>. This formalism enables the representation of a large variety of natural language processing challenges. The <a href=https://en.wikipedia.org/wiki/Front_and_back_ends>backend</a> can be instantiated by different <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, following different paradigms. The clear separation of frame specification and <a href=https://en.wikipedia.org/wiki/Modeling_language>model backend</a> will ease the implementation of new <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and the evaluation of different <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> across different tasks. Furthermore, it simplifies <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, joint learning across tasks and/or domains as well as the assessment of model generalizability. DERE is available as open-source software.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2009/>Demonstrating Par4Sem-A Semantic Writing Aid with Adaptive Paraphrasing<span class=acl-fixed-case>P</span>ar4<span class=acl-fixed-case>S</span>em - A Semantic Writing Aid with Adaptive Paraphrasing</a></strong><br><a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2009><div class="card-body p-3 small">In this paper, we present Par4Sem, a semantic writing aid tool based on adaptive paraphrasing. Unlike many annotation tools that are primarily used to collect training examples, Par4Sem is integrated into a real word application, in this case a writing aid tool, in order to collect training examples from usage data. Par4Sem is a tool, which supports an adaptive, iterative, and interactive process where the underlying <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> are updated for each iteration using new training examples from usage data. After motivating the use of ever-learning tools in NLP applications, we evaluate Par4Sem by adopting it to a text simplification task through mere usage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2010/>Juman++ : A Morphological Analysis Toolkit for Scriptio Continua<span class=acl-fixed-case>J</span>uman++: A Morphological Analysis Toolkit for Scriptio Continua</a></strong><br><a href=/people/a/arseny-tolmachev/>Arseny Tolmachev</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2010><div class="card-body p-3 small">We present a three-part toolkit for developing <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyzers</a> for languages without natural word boundaries. The first part is a C++11/14 lattice-based morphological analysis library that uses a combination of linear and recurrent neural net language models for analysis. The other parts are a tool for exposing problems in the trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> and a partial annotation tool. Our morphological analyzer of <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> achieves new SOTA on Jumandic-based corpora while being 250 times faster than the previous one. We also perform a small experiment and quantitive analysis and experience of using <a href=https://en.wikipedia.org/wiki/Programming_tool>development tools</a>. All <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> of the <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is open source and available under a permissive Apache 2 License.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2012/>SentencePiece : A simple and language independent subword tokenizer and detokenizer for Neural Text Processing<span class=acl-fixed-case>S</span>entence<span class=acl-fixed-case>P</span>iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a></strong><br><a href=/people/t/taku-kudo/>Taku Kudo</a>
|
<a href=/people/j/john-richardson/>John Richardson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2012><div class="card-body p-3 small">This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>. It provides open-source <a href=https://en.wikipedia.org/wiki/C++>C++</a> and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at.<url>https://github.com/google/sentencepiece</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2013 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2013/>CogCompTime : A Tool for Understanding Time in <a href=https://en.wikipedia.org/wiki/Natural_language>Natural Language</a><span class=acl-fixed-case>C</span>og<span class=acl-fixed-case>C</span>omp<span class=acl-fixed-case>T</span>ime: A Tool for Understanding Time in Natural Language</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/b/ben-zhou/>Ben Zhou</a>
|
<a href=/people/z/zhili-feng/>Zhili Feng</a>
|
<a href=/people/h/haoruo-peng/>Haoruo Peng</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2013><div class="card-body p-3 small">Automatic extraction of temporal information is important for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. It involves two basic <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> : (1) Understanding time expressions that are mentioned explicitly in text (e.g., February 27, 1998 or tomorrow), and (2) Understanding <a href=https://en.wikipedia.org/wiki/Time>temporal information</a> that is conveyed implicitly via <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. This paper introduces CogCompTime, a <a href=https://en.wikipedia.org/wiki/System>system</a> that has these two important functionalities. It incorporates the most recent progress, achieves state-of-the-art performance, and is publicly available at.<url>http://cogcomp.org/page/publication_view/844</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2014/>A Multilingual Information Extraction Pipeline for Investigative Journalism</a></strong><br><a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2014><div class="card-body p-3 small">We introduce an advanced information extraction pipeline to automatically process very large collections of <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured textual data</a> for the purpose of <a href=https://en.wikipedia.org/wiki/Investigative_journalism>investigative journalism</a>. The <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> serves as a new input processor for the upcoming major release of our New / s / leak 2.0 software, which we develop in cooperation with a large German news organization. The use case is that journalists receive a large collection of files up to several Gigabytes containing unknown contents. Collections may originate either from official disclosures of documents, e.g. Freedom of Information Act requests, or unofficial data leaks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2016/>KT-Speech-Crawler : Automatic Dataset Construction for <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Recognition</a> from YouTube Videos<span class=acl-fixed-case>KT</span>-Speech-Crawler: Automatic Dataset Construction for Speech Recognition from <span class=acl-fixed-case>Y</span>ou<span class=acl-fixed-case>T</span>ube Videos</a></strong><br><a href=/people/e/egor-lakomkin/>Egor Lakomkin</a>
|
<a href=/people/s/sven-magg/>Sven Magg</a>
|
<a href=/people/c/cornelius-weber/>Cornelius Weber</a>
|
<a href=/people/s/stefan-wermter/>Stefan Wermter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2016><div class="card-body p-3 small">We describe KT-Speech-Crawler : an approach for automatic dataset construction for <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> by crawling <a href=https://en.wikipedia.org/wiki/YouTube>YouTube videos</a>. We outline several filtering and post-processing steps, which extract samples that can be used for training end-to-end neural speech recognition systems. In our experiments, we demonstrate that a single-core version of the <a href=https://en.wikipedia.org/wiki/Web_crawler>crawler</a> can obtain around 150 hours of <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcribed speech</a> within a day, containing an estimated 3.5 % <a href=https://en.wikipedia.org/wiki/Word_error_rate>word error rate</a> in the <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcriptions</a>. Automatically collected samples contain reading and spontaneous speech recorded in various conditions including background noise and music, distant microphone recordings, and a variety of <a href=https://en.wikipedia.org/wiki/Accent_(sociolinguistics)>accents</a> and <a href=https://en.wikipedia.org/wiki/Reverberation>reverberation</a>. When training a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> on <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, we observed around 40 % word error rate reduction on the Wall Street Journal dataset by integrating 200 hours of the collected samples into the training set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2018/>An Interface for Annotating Science Questions</a></strong><br><a href=/people/m/michael-boratko/>Michael Boratko</a>
|
<a href=/people/h/harshit-padigela/>Harshit Padigela</a>
|
<a href=/people/d/divyendra-mikkilineni/>Divyendra Mikkilineni</a>
|
<a href=/people/p/pritish-yuvraj/>Pritish Yuvraj</a>
|
<a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a>
|
<a href=/people/m/maria-chang/>Maria Chang</a>
|
<a href=/people/a/achille-fokoue-nkoutche/>Achille Fokoue</a>
|
<a href=/people/p/pavan-kapanipathi/>Pavan Kapanipathi</a>
|
<a href=/people/n/nicholas-mattei/>Nicholas Mattei</a>
|
<a href=/people/r/ryan-musa/>Ryan Musa</a>
|
<a href=/people/k/kartik-talamadupula/>Kartik Talamadupula</a>
|
<a href=/people/m/michael-j-witbrock/>Michael Witbrock</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2018><div class="card-body p-3 small">Recent work introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into an Easy Set and a Challenge Set. That work includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them. However, it does not include clear definitions of these types, nor does it offer information about the quality of the labels or the annotation process used. In this paper, we introduce a novel interface for human annotation of science question-answer pairs with their respective knowledge and reasoning types, in order that the classification of new questions may be improved. We build on the classification schema proposed by prior work on the ARC dataset, and evaluate the effectiveness of our interface with a preliminary study involving 10 participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2019/>APLenty : annotation tool for creating high-quality datasets using active and proactive learning<span class=acl-fixed-case>APL</span>enty: annotation tool for creating high-quality datasets using active and proactive learning</a></strong><br><a href=/people/m/minh-quoc-nghiem/>Minh-Quoc Nghiem</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2019><div class="card-body p-3 small">In this paper, we present APLenty, an annotation tool for creating high-quality sequence labeling datasets using active and proactive learning. A major innovation of our tool is the integration of automatic annotation with <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> and <a href=https://en.wikipedia.org/wiki/Proactive_learning>proactive learning</a>. This makes the task of creating labeled datasets easier, less time-consuming and requiring less human effort. APLenty is highly flexible and can be adapted to various other <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2020/>Interactive Instance-based Evaluation of Knowledge Base Question Answering</a></strong><br><a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2020><div class="card-body p-3 small">Most approaches to Knowledge Base Question Answering are based on <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. In this paper, we present a tool that aids in debugging of question answering systems that construct a structured semantic representation for the input question. Previous work has largely focused on building <a href=https://en.wikipedia.org/wiki/Question_answering>question answering interfaces</a> or evaluation frameworks that unify multiple data sets. The primary objective of our <a href=https://en.wikipedia.org/wiki/System>system</a> is to enable interactive debugging of model predictions on individual instances (questions) and to simplify manual error analysis. Our interactive interface helps researchers to understand the shortcomings of a particular <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, qualitatively analyze the complete <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> and compare different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. A set of sit-by sessions was used to validate our <a href=https://en.wikipedia.org/wiki/User_interface_design>interface design</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2021/>Magnitude : A Fast, Efficient Universal Vector Embedding Utility Package<span class=acl-fixed-case>M</span>agnitude: A Fast, Efficient Universal Vector Embedding Utility Package</a></strong><br><a href=/people/a/ajay-patel/>Ajay Patel</a>
|
<a href=/people/a/alexander-sands/>Alexander Sands</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2021><div class="card-body p-3 small">Vector space embedding models like <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a>, <a href=https://en.wikipedia.org/wiki/GloVe_(machine_learning)>GloVe</a>, and <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> are extremely popular representations in natural language processing (NLP) applications. We present Magnitude, a fast, lightweight tool for utilizing and processing <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. Magnitude is an open source <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python package</a> with a compact vector storage file format that allows for efficient manipulation of huge numbers of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. Magnitude performs common <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>operations</a> up to 60 to 6,000 times faster than <a href=https://en.wikipedia.org/wiki/Gensim>Gensim</a>. Magnitude introduces several novel <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> for improved <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> like out-of-vocabulary lookups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2022 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2022/>Integrating Knowledge-Supported Search into the INCEpTION Annotation Platform<span class=acl-fixed-case>INCE</span>p<span class=acl-fixed-case>TION</span> Annotation Platform</a></strong><br><a href=/people/b/beto-boullosa/>Beto Boullosa</a>
|
<a href=/people/r/richard-eckart-de-castilho/>Richard Eckart de Castilho</a>
|
<a href=/people/n/naveen-kumar-laskari/>Naveen Kumar</a>
|
<a href=/people/j/jan-christoph-klie/>Jan-Christoph Klie</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2022><div class="card-body p-3 small">Annotating entity mentions and linking them to a knowledge resource are essential tasks in many domains. It disambiguates mentions, introduces cross-document coreferences, and the resources contribute extra information, e.g. taxonomic relations. Such tasks benefit from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text annotation tools</a> that integrate a <a href=https://en.wikipedia.org/wiki/Search_engine_technology>search</a> which covers the <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>, as well as the knowledge resource. However, to the best of our knowledge, no current tools integrate knowledge-supported search as well as entity linking support. We address this gap by introducing knowledge-supported search functionality into the INCEpTION text annotation platform. In our approach, cross-document references are created by linking entity mentions to a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> in the form of a structured hierarchical vocabulary. The resulting <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> are then indexed to enable fast and yet complex <a href=https://en.wikipedia.org/wiki/Information_retrieval>queries</a> taking into account the text, the annotations, and the vocabulary structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2023" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2023/>CytonMT : an Efficient Neural Machine Translation Open-source Toolkit Implemented in C++<span class=acl-fixed-case>C</span>yton<span class=acl-fixed-case>MT</span>: an Efficient Neural Machine Translation Open-source Toolkit Implemented in <span class=acl-fixed-case>C</span>++</a></strong><br><a href=/people/x/xiaolin-wang/>Xiaolin Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2023><div class="card-body p-3 small">This paper presents an open-source neural machine translation toolkit named CytonMT. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is built from scratch only using <a href=https://en.wikipedia.org/wiki/C++>C++</a> and NVIDIA&#8217;s GPU-accelerated libraries. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> features training efficiency, code simplicity and translation quality. Benchmarks show that cytonMT accelerates the training speed by 64.5 % to 110.8 % on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> of various sizes, and achieves competitive translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2024" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2024/>OpenKE : An Open Toolkit for Knowledge Embedding<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>KE</span>: An Open Toolkit for Knowledge Embedding</a></strong><br><a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/s/shulin-cao/>Shulin Cao</a>
|
<a href=/people/x/xin-lv/>Xin Lv</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2024><div class="card-body p-3 small">We release an open toolkit for knowledge embedding (OpenKE), which provides a unified framework and various fundamental models to embed knowledge graphs into a continuous low-dimensional space. OpenKE prioritizes <a href=https://en.wikipedia.org/wiki/Operational_efficiency>operational efficiency</a> to support quick model validation and large-scale knowledge representation learning. Meanwhile, OpenKE maintains sufficient modularity and extensibility to easily incorporate new <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> into the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. Besides the toolkit, the embeddings of some existing large-scale knowledge graphs pre-trained by OpenKE are also available, which can be directly applied for many applications including <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, personalized recommendation and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a>, documentation, and pre-trained embeddings are all released on.<url>http://openke.thunlp.org/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2025/>LIA : A Natural Language Programmable Personal Assistant<span class=acl-fixed-case>LIA</span>: A Natural Language Programmable Personal Assistant</a></strong><br><a href=/people/i/igor-labutov/>Igor Labutov</a>
|
<a href=/people/s/shashank-srivastava/>Shashank Srivastava</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2025><div class="card-body p-3 small">We present LIA, an intelligent personal assistant that can be programmed using <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> demonstrates multiple competencies towards learning from <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-like interactions</a>. These include the ability to be taught reusable conditional procedures, the ability to be taught new knowledge about the world (concepts in an ontology) and the ability to be taught how to ground that knowledge in a set of <a href=https://en.wikipedia.org/wiki/Sensor>sensors</a> and effectors. Building such a system highlights design questions regarding the overall architecture that such an <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> should have, as well as questions about <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and grounding language in situational contexts. We outline key properties of this <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>, and demonstrate a <a href=https://en.wikipedia.org/wiki/Prototype>prototype</a> that embodies them in the form of a <a href=https://en.wikipedia.org/wiki/Personal_assistant>personal assistant</a> on an <a href=https://en.wikipedia.org/wiki/Android_(operating_system)>Android device</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2026/>PizzaPal : Conversational Pizza Ordering using a High-Density Conversational AI Platform<span class=acl-fixed-case>P</span>izza<span class=acl-fixed-case>P</span>al: Conversational Pizza Ordering using a High-Density Conversational <span class=acl-fixed-case>AI</span> Platform</a></strong><br><a href=/people/a/antoine-raux/>Antoine Raux</a>
|
<a href=/people/y/yi-ma/>Yi Ma</a>
|
<a href=/people/p/paul-yang/>Paul Yang</a>
|
<a href=/people/f/felicia-wong/>Felicia Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2026><div class="card-body p-3 small">This paper describes PizzaPal, a voice-only agent for ordering pizza, as well as the Conversational AI architecture built at b4.ai. Based on the principles of high-density conversational AI, it supports natural and flexible interactions through neural conversational language understanding, robust dialog state tracking, and hierarchical task decomposition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2027 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2027/>Developing Production-Level Conversational Interfaces with Shallow Semantic Parsing</a></strong><br><a href=/people/a/arushi-raghuvanshi/>Arushi Raghuvanshi</a>
|
<a href=/people/l/lucien-carroll/>Lucien Carroll</a>
|
<a href=/people/k/karthik-raghunathan/>Karthik Raghunathan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2027><div class="card-body p-3 small">We demonstrate an end-to-end approach for building conversational interfaces from prototype to production that has proven to work well for a number of applications across diverse verticals. Our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> improves on the standard domain-intent-entity classification hierarchy and dialogue management architecture by leveraging shallow semantic parsing. We observe that NLU systems for industry applications often require more structured representations of entity relations than provided by the standard <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchy</a>, yet without requiring full semantic parses which are often inaccurate on real-world conversational data. We distinguish two kinds of semantic properties that can be provided through shallow semantic parsing : <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity groups</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity roles</a>. We also provide live demos of conversational apps built for two different <a href=https://en.wikipedia.org/wiki/Use_case>use cases</a> : <a href=https://en.wikipedia.org/wiki/Online_food_ordering>food ordering</a> and meeting control.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2028/>When <a href=https://en.wikipedia.org/wiki/Science_journalism>science journalism</a> meets <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a> : An interactive demonstration</a></strong><br><a href=/people/r/raghuram-vadapalli/>Raghuram Vadapalli</a>
|
<a href=/people/b/bakhtiyar-syed/>Bakhtiyar Syed</a>
|
<a href=/people/n/nishant-prabhu/>Nishant Prabhu</a>
|
<a href=/people/b/balaji-vasan-srinivasan/>Balaji Vasan Srinivasan</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2028><div class="card-body p-3 small">We present an online interactive tool that generates titles of blog titles and thus take the first step toward automating <a href=https://en.wikipedia.org/wiki/Science_journalism>science journalism</a>. Science journalism aims to transform jargon-laden scientific articles into a form that the common reader can comprehend while ensuring that the underlying meaning of the article is retained. In this work, we present a tool, which, given the title and abstract of a research paper will generate a blog title by mimicking a human science journalist. The tool makes use of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 87,328 pairs of research papers and their corresponding <a href=https://en.wikipedia.org/wiki/Blog>blogs</a>, built from two <a href=https://en.wikipedia.org/wiki/News_aggregator>science news aggregators</a>. The architecture of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is a two-stage mechanism which generates <a href=https://en.wikipedia.org/wiki/Blog>blog titles</a>. Evaluation using standard <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> indicate the viability of the proposed <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2029/>Universal Sentence Encoder for English<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/d/daniel-cer/>Daniel Cer</a>
|
<a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/s/sheng-yi-kong/>Sheng-yi Kong</a>
|
<a href=/people/n/nan-hua/>Nan Hua</a>
|
<a href=/people/n/nicole-limtiaco/>Nicole Limtiaco</a>
|
<a href=/people/r/rhomni-st-john/>Rhomni St. John</a>
|
<a href=/people/n/noah-constant/>Noah Constant</a>
|
<a href=/people/m/mario-guajardo-cespedes/>Mario Guajardo-Cespedes</a>
|
<a href=/people/s/steve-yuan/>Steve Yuan</a>
|
<a href=/people/c/chris-tar/>Chris Tar</a>
|
<a href=/people/b/brian-strope/>Brian Strope</a>
|
<a href=/people/r/ray-kurzweil/>Ray Kurzweil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2029><div class="card-body p-3 small">We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Computational_resource>compute resources</a>. We report the relationship between model complexity, <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resources</a>, and transfer performance. Comparisons are made with <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> without <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and to <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.</div></div></div><hr><div id=d18-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/D18-3/>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D18-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-3001/>Joint models for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-3001><div class="card-body p-3 small">Joint models have received much research attention in NLP, allowing relevant tasks to share common information while avoiding error propagation in multi-stage pepelines. Several main approaches have been taken by statistical joint modeling, while neural models allow parameter sharing and adversarial training. This tutorial reviews main approaches to joint modeling for both statistical and neural methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D18-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-3002/>Graph Formalisms for Meaning Representations</a></strong><br><a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/s/sorcha-gilroy/>Sorcha Gilroy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-3002><div class="card-body p-3 small">In this tutorial we will focus on Hyperedge Replacement Languages (HRL; Drewes et al. 1997), a context-free graph rewriting system. HRL are one of the most popular graph formalisms to be studied in NLP (Chiang et al., 2013; Peng et al., 2015; Bauer and Rambow, 2016). We will discuss HRL by formally defining them, studying several examples, discussing their properties, and providing exercises for the tutorial. While HRL have been used in NLP in the past, there is some speculation that they are more expressive than is necessary for graphs representing natural language (Drewes, 2017). Part of our own research has been exploring what restrictions of HRL could yield languages that are more useful for NLP and also those that have desirable properties for NLP models, such as being closed under intersection. With that in mind, we also plan to discuss Regular Graph Languages (RGL; Courcelle 1991), a subfamily of HRL which are closed under intersection. The definition of RGL is relatively simple after being introduced to HRL. We do not plan on discussing any proofs of why RGL are also a subfamily of MSOL, as described in Gilroy et al. (2017b). We will briefly mention the other formalisms shown in Figure 1 such as MSOL and DAGAL but this will focus on their properties rather than any formal definitions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D18-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-3003/>Writing Code for <span class=acl-fixed-case>NLP</span> Research</a></strong><br><a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/m/mark-neumann/>Mark Neumann</a>
|
<a href=/people/j/joel-grus/>Joel Grus</a>
|
<a href=/people/n/nicholas-lourie/>Nicholas Lourie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-3003><div class="card-body p-3 small">Doing modern NLP research requires writing code. Good code enables fast prototyping, easy debugging, controlled experiments, and accessible visualizations that help researchers understand what a model is doing. Bad code leads to research that is at best hard to reproduce and extend, and at worst simply incorrect. Indeed, there is a growing recognition of the importance of having good tools to assist good research in our field, as the upcoming workshop on open source software for NLP demonstrates. This tutorial aims to share best practices for writing code for NLP research, drawing on the instructors' experience designing the recently-released AllenNLP toolkit, a PyTorch-based library for deep learning NLP research. We will explain how a library with the right abstractions and components enables better code and better science, using models implemented in AllenNLP as examples. Participants will learn how to write research code in a way that facilitates good science and easy experimentation, regardless of what framework they use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D18-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-3004/>Deep Latent Variable Models of Natural Language</a></strong><br><a href=/people/a/alexander-m-rush/>Alexander Rush</a>
|
<a href=/people/y/yoon-kim/>Yoon Kim</a>
|
<a href=/people/s/sam-wiseman/>Sam Wiseman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-3004><div class="card-body p-3 small">The proposed tutorial will cover deep latent variable models both in the case where exact inference over the latent variables is tractable and when it is not. The former case includes neural extensions of unsupervised tagging and parsing models. Our discussion of the latter case, where inference cannot be performed tractably, will restrict itself to continuous latent variables. In particular, we will discuss recent developments both in neural variational inference (e.g., relating to Variational Auto-encoders) and in implicit density modeling (e.g., relating to Generative Adversarial Networks). We will highlight the challenges of applying these families of methods to NLP problems, and discuss recent successes and best practices.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D18-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-3005/>Standardized Tests as benchmarks for Artificial Intelligence</a></strong><br><a href=/people/m/mrinmaya-sachan/>Mrinmaya Sachan</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-3005><div class="card-body p-3 small">Standardized tests have recently been proposed as replacements to the Turing test as a driver for progress in AI (Clark, 2015). These include tests on understanding passages and stories and answering questions about them (Richardson et al., 2013; Rajpurkar et al., 2016a, inter alia), science question answering (Schoenick et al., 2016, inter alia), algebra word problems (Kushman et al., 2014, inter alia), geometry problems (Seo et al., 2015; Sachan et al., 2016), visual question answering (Antol et al., 2015), etc. Many of these tests require sophisticated understanding of the world, aiming to push the boundaries of AI. For this tutorial, we broadly categorize these tests into two categories: open domain tests such as reading comprehensions and elementary school tests where the goal is to find the support for an answer from the student curriculum, and closed domain tests such as intermediate level math and science tests (algebra, geometry, Newtonian physics problems, etc.). Unlike open domain tests, closed domain tests require the system to have significant domain knowledge and reasoning capabilities. For example, geometry questions typically involve a number of geometry primitives (lines, quadrilaterals, circles, etc) and require students to use axioms and theorems of geometry (Pythagoras theorem, alternating angles, etc) to solve them. These closed domains often have a formal logical basis and the question can be mapped to a formal language by semantic parsing. The formal question representation can then provided as an input to an expert system to solve the question.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-D18-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-3006/>Deep Chit-Chat: Deep Learning for <span class=acl-fixed-case>C</span>hat<span class=acl-fixed-case>B</span>ots</a></strong><br><a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/r/rui-yan/>Rui Yan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-3006><div class="card-body p-3 small">The tutorial is based on the long-term efforts on building conversational models with deep learning approaches for chatbots. We will summarize the fundamental challenges in modeling open domain dialogues, clarify the difference from modeling goal-oriented dialogues, and give an overview of state-of-the-art methods for open domain conversation including both retrieval-based methods and generation-based methods. In addition to these, our tutorial will also cover some new trends of research of chatbots, such as how to design a reasonable evaluation system and how to "control" conversations from a chatbot with some specific information such as personas, styles, and emotions, etc.</div></div></div><hr><div id=w18-51><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-51.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-51/>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5100/>Proceedings of the 2nd Workshop on Abusive Language Online (<span class=acl-fixed-case>ALW</span>2)</a></strong><br><a href=/people/d/darja-fiser/>Darja Fišer</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/r/rob-voigt/>Rob Voigt</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/j/jacqueline-wernimont/>Jacqueline Wernimont</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5102/>Hate Speech Dataset from a White Supremacy Forum</a></strong><br><a href=/people/o/ona-de-gibert/>Ona de Gibert</a>
|
<a href=/people/n/naiara-perez/>Naiara Perez</a>
|
<a href=/people/a/aitor-garcia-pablos/>Aitor García-Pablos</a>
|
<a href=/people/m/montse-cuadros/>Montse Cuadros</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5102><div class="card-body p-3 small">Hate speech is commonly defined as any communication that disparages a target group of people based on some characteristic such as <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>race</a>, colour, <a href=https://en.wikipedia.org/wiki/Ethnic_group>ethnicity</a>, <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, <a href=https://en.wikipedia.org/wiki/Sexual_orientation>sexual orientation</a>, <a href=https://en.wikipedia.org/wiki/Nationality>nationality</a>, <a href=https://en.wikipedia.org/wiki/Religion>religion</a>, or other characteristic. Due to the massive rise of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated web content</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, the amount of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> is also steadily increasing. Over the past years, interest in online hate speech detection and, particularly, the automation of this task has continuously grown, along with the societal impact of the phenomenon. This paper describes a <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech dataset</a> composed of thousands of sentences manually labelled as containing <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> or not. The sentences have been extracted from <a href=https://en.wikipedia.org/wiki/Stormfront_(website)>Stormfront</a>, a white supremacist forum. A custom annotation tool has been developed to carry out the manual labelling task which, among other things, allows the annotators to choose whether to read the context of a sentence before labelling it. The paper also provides a thoughtful qualitative and quantitative study of the resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and several baseline experiments with different <a href=https://en.wikipedia.org/wiki/Statistical_model>classification models</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5104/>Predictive Embeddings for Hate Speech Detection on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/r/rohan-kshirsagar/>Rohan Kshirsagar</a>
|
<a href=/people/t/tyrus-cukuvac/>Tyrus Cukuvac</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a>
|
<a href=/people/s/susan-mcgregor/>Susan McGregor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5104><div class="card-body p-3 small">We present a neural-network based approach to classifying online hate speech in general, as well as racist and sexist speech in particular. Using pre-trained word embeddings and max / mean pooling from simple, fully-connected transformations of these embeddings, we are able to predict the occurrence of hate speech on three commonly used publicly available datasets. Our models match or outperform state of the art F1 performance on all three datasets using significantly fewer parameters and minimal feature preprocessing compared to previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5105/>Challenges for Toxic Comment Classification : An In-Depth Error Analysis</a></strong><br><a href=/people/b/betty-van-aken/>Betty van Aken</a>
|
<a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a>
|
<a href=/people/a/alexander-loser/>Alexander Löser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5105><div class="card-body p-3 small">Toxic comment classification has become an active research field with many recently proposed approaches. However, while these approaches address some of the task&#8217;s challenges others still remain unsolved and directions for further research are needed. To this end, we compare different deep learning and shallow approaches on a new, large comment dataset and propose an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> that outperforms all individual models. Further, we validate our findings on a second <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The results of the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> enable us to perform an extensive error analysis, which reveals open challenges for state-of-the-art methods and directions towards pending future research. These challenges include missing paradigmatic context and inconsistent dataset labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5106" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5106/>Aggression Detection on Social Media Text Using Deep Neural Networks</a></strong><br><a href=/people/v/vinay-singh/>Vinay Singh</a>
|
<a href=/people/a/aman-varshney/>Aman Varshney</a>
|
<a href=/people/s/syed-sarfaraz-akhtar/>Syed Sarfaraz Akhtar</a>
|
<a href=/people/d/deepanshu-vijay/>Deepanshu Vijay</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5106><div class="card-body p-3 small">In the past few years, bully and aggressive posts on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> have grown significantly, causing serious consequences for victims / users of all demographics. Majority of the work in this field has been done for English only. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning based classification system</a> for <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook posts</a> and comments of Hindi-English Code-Mixed text to detect the aggressive behaviour of / towards users. Our work focuses on text from users majorly in the Indian Subcontinent. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that we used for our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> is provided by <a href=https://en.wikipedia.org/wiki/TRAC-1>TRAC-1</a> in their shared task. Our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a> assigns each <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook post / comment</a> to one of the three predefined categories : Overtly Aggressive, Covertly Aggressive and Non-Aggressive. We experimented with 6 classification models and our CNN model on a 10 K-fold cross-validation gave the best result with the prediction accuracy of 73.2 %.<b>TRAC-1</b>in their shared task. Our classification model assigns each Facebook post/comment to one of the three predefined categories: &#8220;Overtly Aggressive&#8221;, &#8220;Covertly Aggressive&#8221; and &#8220;Non-Aggressive&#8221;. We experimented with 6 classification models and our CNN model on a 10 K-fold cross-validation gave the best result with the prediction accuracy of 73.2%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5107/>Creating a WhatsApp Dataset to Study Pre-teen Cyberbullying<span class=acl-fixed-case>W</span>hats<span class=acl-fixed-case>A</span>pp Dataset to Study Pre-teen Cyberbullying</a></strong><br><a href=/people/r/rachele-sprugnoli/>Rachele Sprugnoli</a>
|
<a href=/people/s/stefano-menini/>Stefano Menini</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/f/filippo-oncini/>Filippo Oncini</a>
|
<a href=/people/e/enrico-piras/>Enrico Piras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5107><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/WhatsApp>WhatsApp</a> is used by teenagers as one major channel of <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>, such interactions remain invisible due to the <a href=https://en.wikipedia.org/wiki/Privacy_policy>app privacy policies</a> that do not allow ex-post data collection. Indeed, most of the information on these phenomena rely on surveys regarding <a href=https://en.wikipedia.org/wiki/Self-report_study>self-reported data</a>. In order to overcome this limitation, we describe in this paper the activities that led to the creation of a WhatsApp dataset to study <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> among Italian students aged 12-13. We present not only the collected chats with annotations about user role and type of offense, but also the living lab created in a collaboration between researchers and schools to monitor and analyse <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>. Finally, we discuss some open issues, dealing with ethical, operational and epistemic aspects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5109/>Aggressive language in an online hacking forum</a></strong><br><a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/s/sergio-pastrana/>Sergio Pastrana</a>
|
<a href=/people/a/alice-hutchings/>Alice Hutchings</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5109><div class="card-body p-3 small">We probe the heterogeneity in levels of abusive language in different sections of the Internet, using an annotated corpus of Wikipedia page edit comments to train a <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classifier</a> for abuse detection. Our test data come from the CrimeBB Corpus of hacking-related forum posts and we find that (a) forum interactions are rarely abusive, (b) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain, and tends to involve aggressive posturing rather than <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> or threats of violence. We observe that the purpose of conversations in online forums tend to be more constructive and informative than those in Wikipedia page edit comments which are geared more towards adversarial interactions, and that this may explain the lower levels of abuse found in our forum data than in Wikipedia comments. Further work remains to be done to compare these results with other inter-domain classification experiments, and to understand the impact of aggressive language in forum conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5110/>The Effects of User Features on Twitter Hate Speech Detection<span class=acl-fixed-case>T</span>witter Hate Speech Detection</a></strong><br><a href=/people/e/elise-fehn-unsvag/>Elise Fehn Unsvåg</a>
|
<a href=/people/b/bjorn-gamback/>Björn Gambäck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5110><div class="card-body p-3 small">The paper investigates the potential effects user features have on hate speech classification. A quantitative analysis of Twitter data was conducted to better understand user characteristics, but no correlations were found between hateful text and the characteristics of the users who had posted it. However, experiments with a hate speech classifier based on datasets from three different languages showed that combining certain user features with textual features gave slight improvements of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance. While the incorporation of <a href=https://en.wikipedia.org/wiki/Software_feature>user features</a> resulted in varying impact on performance for the different datasets used, user network-related features provided the most consistent improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5112/>Determining Code Words in Euphemistic Hate Speech Using Word Embedding Networks</a></strong><br><a href=/people/r/rijul-magu/>Rijul Magu</a>
|
<a href=/people/j/jiebo-luo/>Jiebo Luo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5112><div class="card-body p-3 small">While analysis of online explicit abusive language detection has lately seen an ever-increasing focus, implicit abuse detection remains a largely unexplored space. We carry out a study on a subcategory of implicit hate : euphemistic hate speech. We propose a method to assist in identifying unknown euphemisms (or code words) given a set of hateful tweets containing a known code word. Our approach leverages <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and <a href=https://en.wikipedia.org/wiki/Network_theory>network analysis</a> (through centrality measures and community detection) in a manner that can be generalized to identify euphemisms across contexts- not just <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5117 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5117/>Cross-Domain Detection of Abusive Language Online</a></strong><br><a href=/people/m/mladen-karan/>Mladen Karan</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5117><div class="card-body p-3 small">We investigate to what extent the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained to detect general abusive language generalize between different datasets labeled with different abusive language types. To this end, we compare the cross-domain performance of simple classification models on nine different <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, finding that the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> fail to generalize to out-domain datasets and that having at least some in-domain data is important. We also show that using the frustratingly simple domain adaptation (Daume III, 2007) in most cases improves the results over in-domain training, especially when used to augment a smaller dataset with a larger one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5118 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5118/>Did you offend me? Classification of Offensive Tweets in Hinglish Language<span class=acl-fixed-case>H</span>inglish Language</a></strong><br><a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/m/meghna-ayyar/>Meghna Ayyar</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5118><div class="card-body p-3 small">The use of code-switched languages (e.g., <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish</a>, which is derived by the blending of <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> with the English language) is getting much popular on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> due to their ease of communication in <a href=https://en.wikipedia.org/wiki/First_language>native languages</a>. However, spelling variations and absence of <a href=https://en.wikipedia.org/wiki/Grammar>grammar rules</a> introduce <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> and make it difficult to understand the text automatically. This paper presents the Multi-Input Multi-Channel Transfer Learning based model (MIMCT) to detect offensive (hate speech or abusive) Hinglish tweets from the proposed Hinglish Offensive Tweet (HOT) dataset using transfer learning coupled with multiple feature inputs. Specifically, it takes multiple primary word embedding along with secondary extracted features as inputs to train a multi-channel CNN-LSTM architecture that has been pre-trained on English tweets through <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. The proposed MIMCT model outperforms the baseline supervised classification models, transfer learning based CNN and LSTM models to establish itself as the state of the art in the unexplored domain of Hinglish offensive text classification.<i>e.g.</i>, Hinglish, which is derived by the blending of Hindi with the English language) is getting much popular on Twitter due to their ease of communication in native languages. However, spelling variations and absence of grammar rules introduce ambiguity and make it difficult to understand the text automatically. This paper presents the Multi-Input Multi-Channel Transfer Learning based model (MIMCT) to detect offensive (hate speech or abusive) Hinglish tweets from the proposed Hinglish Offensive Tweet (HOT) dataset using transfer learning coupled with multiple feature inputs. Specifically, it takes multiple primary word embedding along with secondary extracted features as inputs to train a multi-channel CNN-LSTM architecture that has been pre-trained on English tweets through transfer learning. The proposed MIMCT model outperforms the baseline supervised classification models, transfer learning based CNN and LSTM models to establish itself as the state of the art in the unexplored domain of Hinglish offensive text classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5119 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5119/>Decipherment for Adversarial Offensive Language Detection</a></strong><br><a href=/people/z/zhelun-wu/>Zhelun Wu</a>
|
<a href=/people/n/nishant-kambhatla/>Nishant Kambhatla</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5119><div class="card-body p-3 small">Automated filters are commonly used by online services to stop users from sending age-inappropriate, bullying messages, or asking others to expose personal information. Previous work has focused on rules or classifiers to detect and filter offensive messages, but these are vulnerable to cleverly disguised plaintext and unseen expressions especially in an adversarial setting where the users can repeatedly try to bypass the filter. In this paper, we model the disguised messages as if they are produced by encrypting the original message using an invented cipher. We apply automatic decipherment techniques to decode the disguised malicious text, which can be then filtered using <a href=https://en.wikipedia.org/wiki/Rule-based_system>rules</a> or <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. We provide experimental results on three different datasets and show that <a href=https://en.wikipedia.org/wiki/Decipherment>decipherment</a> is an effective tool for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5120 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5120/>The Linguistic Ideologies of Deep Abusive Language Classification</a></strong><br><a href=/people/m/michael-castelle/>Michael Castelle</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5120><div class="card-body p-3 small">This paper brings together theories from <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a> and <a href=https://en.wikipedia.org/wiki/Linguistic_anthropology>linguistic anthropology</a> to critically evaluate the so-called language ideologies the set of beliefs and ways of speaking about languagein the practices of abusive language classification in modern machine learning-based NLP. This argument is made at both a conceptual and empirical level, as we review approaches to <a href=https://en.wikipedia.org/wiki/Abusive_language>abusive language</a> from different fields, and use two neural network methods to analyze three datasets developed for <a href=https://en.wikipedia.org/wiki/Abusive_language>abusive language classification tasks</a> (drawn from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a>, and StackOverflow). By evaluating and comparing these results, we argue for the importance of incorporating theories of <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatics</a> and <a href=https://en.wikipedia.org/wiki/Metapragmatics>metapragmatics</a> into both the design of classification tasks as well as in ML architectures.</div></div></div><hr><div id=w18-52><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-52.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-52/>Proceedings of the 5th Workshop on Argument Mining</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5200/>Proceedings of the 5th Workshop on Argument Mining</a></strong><br><a href=/people/n/noam-slonim/>Noam Slonim</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5201" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5201/>Argumentative Link Prediction using <a href=https://en.wikipedia.org/wiki/Residual_network>Residual Networks</a> and Multi-Objective Learning</a></strong><br><a href=/people/a/andrea-galassi/>Andrea Galassi</a>
|
<a href=/people/m/marco-lippi/>Marco Lippi</a>
|
<a href=/people/p/paolo-torroni/>Paolo Torroni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5201><div class="card-body p-3 small">We explore the use of <a href=https://en.wikipedia.org/wiki/Residual_network>residual networks</a> for argumentation mining, with an emphasis on link prediction. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> we propose makes no assumptions on document or argument structure. We evaluate <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> on a challenging <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated comments</a> collected from an <a href=https://en.wikipedia.org/wiki/Online_platform>online platform</a>. Results show that our model outperforms an equivalent <a href=https://en.wikipedia.org/wiki/Deep_learning>deep network</a> and offers results comparable with state-of-the-art methods that rely on <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5202/>End-to-End Argument Mining for Discussion Threads Based on Parallel Constrained Pointer Architecture</a></strong><br><a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/k/katsuhide-fujita/>Katsuhide Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5202><div class="card-body p-3 small">Argument Mining (AM) is a relatively recent discipline, which concentrates on extracting claims or premises from discourses, and inferring their structures. However, many existing works do not consider micro-level AM studies on <a href=https://en.wikipedia.org/wiki/Conversation_threading>discussion threads</a> sufficiently. In this paper, we tackle <a href=https://en.wikipedia.org/wiki/Amplitude_modulation>AM</a> for <a href=https://en.wikipedia.org/wiki/Internet_forum>discussion threads</a>. Our main contributions are follows : (1) A novel combination scheme focusing on micro-level inner- and inter- post schemes for a discussion thread. (2) Annotation of large-scale civic discussion threads with the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a>. (3) Parallel constrained pointer architecture (PCPA), a novel end-to-end technique to discriminate sentence types, inner-post relations, and inter-post interactions simultaneously. The experimental results demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> shows better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in terms of relations extraction, in comparison to existing state-of-the-art models.<i>Parallel constrained pointer architecture</i> (PCPA), a novel end-to-end technique to discriminate sentence types, inner-post relations, and inter-post interactions simultaneously. The experimental results demonstrate that our proposed model shows better accuracy in terms of relations extraction, in comparison to existing state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5203" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5203/>ArguminSci : A Tool for Analyzing Argumentation and Rhetorical Aspects in Scientific Writing<span class=acl-fixed-case>A</span>rgumin<span class=acl-fixed-case>S</span>ci: A Tool for Analyzing Argumentation and Rhetorical Aspects in Scientific Writing</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/k/kai-eckert/>Kai Eckert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5203><div class="card-body p-3 small">Argumentation is arguably one of the central features of scientific language. We present ArguminSci, an easy-to-use tool that analyzes <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a> and other rhetorical aspects of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific writing</a>, which we collectively dub scitorics. The main aspect we focus on is the fine-grained argumentative analysis of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific text</a> through identification of argument components. The functionality of ArguminSci is accessible via three interfaces : as a <a href=https://en.wikipedia.org/wiki/Command-line_interface>command line tool</a>, via a <a href=https://en.wikipedia.org/wiki/Representational_state_transfer>RESTful application programming interface</a>, and as a <a href=https://en.wikipedia.org/wiki/Web_application>web application</a>.<i>ArguminSci</i>, an easy-to-use tool that analyzes argumentation and other rhetorical aspects of scientific writing, which we collectively dub <i>scitorics</i>. The main aspect we focus on is the fine-grained argumentative analysis of scientific text through identification of argument components. The functionality of <i>ArguminSci</i> is accessible via three interfaces: as a command line tool, via a RESTful application programming interface, and as a web application.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5204/>Evidence Type Classification in Randomized Controlled Trials</a></strong><br><a href=/people/t/tobias-mayer/>Tobias Mayer</a>
|
<a href=/people/e/elena-cabrio/>Elena Cabrio</a>
|
<a href=/people/s/serena-villata/>Serena Villata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5204><div class="card-body p-3 small">Randomized Controlled Trials (RCT) are a common type of experimental studies in the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a> for evidence-based decision making. The ability to automatically extract the arguments proposed therein can be of valuable support for clinicians and practitioners in their daily evidence-based decision making activities. Given the peculiarity of the medical domain and the required level of detail, standard approaches to argument component detection in argument(ation) mining are not fine-grained enough to support such activities. In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>sub-task</a> of the argument component identification task : evidence type classification. To address it, we propose a supervised approach and we test <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> on a set of <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>RCT abstracts</a> on different <a href=https://en.wikipedia.org/wiki/Medicine>medical topics</a>.<i>arguments</i> proposed therein can be of valuable support for clinicians and practitioners in their daily evidence-based decision making activities. Given the peculiarity of the medical domain and the required level of detail, standard approaches to argument component detection in <i>argument(ation) mining</i> are not fine-grained enough to support such activities. In this paper, we introduce a new sub-task of the argument component identification task: <i>evidence type classification</i>. To address it, we propose a supervised approach and we test it on a set of RCT abstracts on different medical topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5206/>An Argument-Annotated Corpus of Scientific Publications</a></strong><br><a href=/people/a/anne-lauscher/>Anne Lauscher</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5206><div class="card-body p-3 small">Argumentation is an essential feature of scientific language. We present an annotation study resulting in a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of scientific publications</a> annotated with argumentative components and relations. The argumentative annotations have been added to the existing Dr. Inventor Corpus, already annotated for four other rhetorical aspects. We analyze the annotated argumentative structures and investigate the relations between <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a> and other rhetorical aspects of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific writing</a>, such as discourse roles and citation contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5207/>Annotating Claims in the Vaccination Debate</a></strong><br><a href=/people/b/benedetta-torsi/>Benedetta Torsi</a>
|
<a href=/people/r/roser-morante/>Roser Morante</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5207><div class="card-body p-3 small">In this paper we present <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> experiments with three different <a href=https://en.wikipedia.org/wiki/Annotation>annotation schemes</a> for the identification of argument components in texts related to the vaccination debate. Identifying claims about vaccinations made by participants in the debate is of great societal interest, as the decision to vaccinate or not has impact in public health and safety. Since most corpora that have been annotated with argumentation information contain texts that belong to a specific genre and have a well defined argumentation structure, we needed to adjust the annotation schemes to our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, which contains heterogeneous texts from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>. We started with a complex annotation scheme that had to be simplified due to low IAA. In our final experiment, which focused on annotating claims, annotators reached 57.3 % IAA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5208/>Argument Component Classification for Classroom Discussions</a></strong><br><a href=/people/l/luca-lugini/>Luca Lugini</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5208><div class="card-body p-3 small">This paper focuses on argument component classification for transcribed spoken classroom discussions, with the goal of automatically classifying student utterances into claims, evidence, and warrants. We show that an existing <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for argument component classification developed for another educationally-oriented domain performs poorly on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We then show that feature sets from prior work on <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> for student essays and online dialogues can be used to improve performance considerably. We also provide a comparison between <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> and <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> when trained under different conditions to classify argument components in classroom discussions. While neural network models are not always able to outperform a logistic regression model, we were able to gain some useful insights : convolutional networks are more robust than recurrent networks both at the character and at the word level, and specificity information can help boost performance in multi-task training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5209/>Evidence Types, Credibility Factors, and Patterns or Soft Rules for Weighing Conflicting Evidence : Argument Mining in the Context of Legal Rules Governing Evidence Assessment</a></strong><br><a href=/people/v/vern-walker/>Vern R. Walker</a>
|
<a href=/people/d/dina-foerster/>Dina Foerster</a>
|
<a href=/people/j/julia-monica-ponce/>Julia Monica Ponce</a>
|
<a href=/people/m/matthew-rosen/>Matthew Rosen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5209><div class="card-body p-3 small">This paper reports on the results of an empirical study of adjudicatory decisions about veterans&#8217; claims for disability benefits in the United States. It develops a typology of kinds of relevant evidence (argument premises) employed in cases, and it identifies factors that the tribunal considers when assessing the credibility or trustworthiness of individual items of evidence. It also reports on patterns or <a href=https://en.wikipedia.org/wiki/Soft_law>soft rules</a> that the tribunal uses to comparatively weigh the probative value of conflicting evidence. These evidence types, credibility factors, and comparison patterns are developed to be inter-operable with <a href=https://en.wikipedia.org/wiki/Legal_doctrine>legal rules</a> governing the <a href=https://en.wikipedia.org/wiki/Evidence_(law)>evidence assessment process</a> in the U.S. This approach should be transferable to other legal and non-legal domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5210/>Feasible Annotation Scheme for Capturing Policy Argument Reasoning using Argument Templates</a></strong><br><a href=/people/p/paul-reisert/>Paul Reisert</a>
|
<a href=/people/n/naoya-inoue/>Naoya Inoue</a>
|
<a href=/people/t/tatsuki-kuribayashi/>Tatsuki Kuribayashi</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5210><div class="card-body p-3 small">Most of the existing works on <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> cast the problem of argumentative structure identification as classification tasks (e.g. attack-support relations, stance, explicit premise / claim). This paper goes a step further by addressing the task of automatically identifying reasoning patterns of arguments using predefined templates, which is called argument template (AT) instantiation. The contributions of this work are three-fold. First, we develop a simple, yet expressive set of easily annotatable ATs that can represent a majority of writer&#8217;s reasoning for texts with diverse policy topics while maintaining the computational feasibility of the task. Second, we create a small, but highly reliable annotated corpus of instantiated ATs on top of reliably annotated support and attack relations and conduct an annotation study. Third, we formulate the task of AT instantiation as <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> constrained by a feasible set of <a href=https://en.wikipedia.org/wiki/Template_processor>templates</a>. Our evaluation demonstrates that we can annotate ATs with a reasonably high inter-annotator agreement, and the use of template-constrained inference is useful for instantiating ATs with only partial reasoning comprehension clues.<i>argument template (AT) instantiation</i>. The contributions of this work are three-fold. First, we develop a simple, yet expressive set of easily annotatable ATs that can represent a majority of writer&#8217;s reasoning for texts with diverse policy topics while maintaining the computational feasibility of the task. Second, we create a small, but highly reliable annotated corpus of instantiated ATs on top of reliably annotated support and attack relations and conduct an annotation study. Third, we formulate the task of AT instantiation as structured prediction constrained by a feasible set of templates. Our evaluation demonstrates that we can annotate ATs with a reasonably high inter-annotator agreement, and the use of template-constrained inference is useful for instantiating ATs with only partial reasoning comprehension clues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5211" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5211/>Frame- and Entity-Based Knowledge for Common-Sense Argumentative Reasoning</a></strong><br><a href=/people/t/teresa-botschen/>Teresa Botschen</a>
|
<a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5211><div class="card-body p-3 small">Common-sense argumentative reasoning is a challenging task that requires holistic understanding of the argumentation where external knowledge about the world is hypothesized to play a key role. We explore the idea of using event knowledge about prototypical situations from <a href=https://en.wikipedia.org/wiki/FrameNet>FrameNet</a> and fact knowledge about concrete entities from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> to solve the task. We find that both resources can contribute to an improvement over the non-enriched approach and point out two persisting challenges : first, integration of many <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> of the same type, and second, fusion of complementary annotations. After our explorations, we question the key role of external world knowledge with respect to the argumentative reasoning task and rather point towards a logic-based analysis of the chain of reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5213/>Proposed Method for Annotation of Scientific Arguments in Terms of Semantic Relations and Argument Schemes</a></strong><br><a href=/people/n/nancy-green/>Nancy Green</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5213><div class="card-body p-3 small">This paper presents a proposed method for annotation of scientific arguments in biological / biomedical journal articles. Semantic entities and <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> are used to represent the propositional content of arguments in instances of argument schemes. We describe an experiment in which we encoded the arguments in a <a href=https://en.wikipedia.org/wiki/Article_(publishing)>journal article</a> to identify issues in this approach. Our catalogue of argument schemes and a copy of the annotated article are now publically available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5215/>Dave the debater : a retrieval-based and generative argumentative dialogue agent</a></strong><br><a href=/people/d/dieu-thu-le/>Dieu Thu Le</a>
|
<a href=/people/c/cam-tu-nguyen/>Cam-Tu Nguyen</a>
|
<a href=/people/k/kim-anh-nguyen/>Kim Anh Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5215><div class="card-body p-3 small">In this paper, we explore the problem of developing an argumentative dialogue agent that can be able to discuss with human users on controversial topics. We describe two systems that use retrieval-based and generative models to make argumentative responses to the users. The experiments show promising results although they have been trained on a <a href=https://en.wikipedia.org/wiki/Data_set>small dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5216" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5216/>PD3 : Better Low-Resource Cross-Lingual Transfer By Combining Direct Transfer and Annotation Projection<span class=acl-fixed-case>PD</span>3: Better Low-Resource Cross-Lingual Transfer By Combining Direct Transfer and Annotation Projection</a></strong><br><a href=/people/s/steffen-eger/>Steffen Eger</a>
|
<a href=/people/a/andreas-ruckle/>Andreas Rücklé</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5216><div class="card-body p-3 small">We consider unsupervised cross-lingual transfer on two tasks, viz., sentence-level argumentation mining and standard POS tagging. We combine direct transfer using bilingual embeddings with annotation projection, which projects labels across unlabeled parallel data. We do so by either merging respective source and target language datasets or alternatively by using <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Our combination strategy considerably improves upon both direct transfer and <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection</a> with few available parallel sentences, the most realistic scenario for many low-resource target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5218 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5218/>More or less controlled elicitation of argumentative text : Enlarging a <a href=https://en.wikipedia.org/wiki/Microtext>microtext corpus</a> via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a></a></strong><br><a href=/people/m/maria-skeppstedt/>Maria Skeppstedt</a>
|
<a href=/people/a/andreas-peldszus/>Andreas Peldszus</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5218><div class="card-body p-3 small">We present an extension of an annotated corpus of short argumentative texts that had originally been built in a controlled text production experiment. Our <a href=https://en.wikipedia.org/wiki/Extension_(semantics)>extension</a> more than doubles the size of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by means of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We report on the setup of this experiment and on the consequences that <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> had for assembling the <a href=https://en.wikipedia.org/wiki/Data>data</a>, and in particular for <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. We labeled the argumentative structure by marking claims, premises, and relations between them, following the scheme used in the original <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, but had to make a few modifications in response to interesting phenomena in the data. Finally, we report on an experiment with the automatic prediction of this argumentation structure : We first replicated the approach of an earlier study on the original <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, and compare the performance to various settings involving the extension.</div></div></div><hr><div id=w18-53><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-53.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-53/>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5300/>Proceedings of the 6th <span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>ASQ</span> Workshop A challenge on large-scale biomedical semantic indexing and question answering</a></strong><br><a href=/people/i/ioannis-kakadiaris/>Ioannis A. Kakadiaris</a>
|
<a href=/people/g/george-paliouras/>George Paliouras</a>
|
<a href=/people/a/anastasia-krithara/>Anastasia Krithara</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5301/>Results of the sixth edition of the BioASQ Challenge<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>ASQ</span> Challenge</a></strong><br><a href=/people/a/anastasios-nentidis/>Anastasios Nentidis</a>
|
<a href=/people/a/anastasia-krithara/>Anastasia Krithara</a>
|
<a href=/people/k/konstantinos-bougiatiotis/>Konstantinos Bougiatiotis</a>
|
<a href=/people/g/georgios-paliouras/>Georgios Paliouras</a>
|
<a href=/people/i/ioannis-kakadiaris/>Ioannis Kakadiaris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5301><div class="card-body p-3 small">This paper presents the results of the sixth edition of the BioASQ challenge. The BioASQ challenge aims at the promotion of systems and methodologies through the organization of a challenge on two tasks : semantic indexing and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. In total, 26 teams with more than 90 systems participated in this year&#8217;s challenge. As in previous years, the best systems were able to outperform the strong baselines. This suggests that state-of-the-art systems are continuously improving, pushing the frontier of research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5306/>AttentionMeSH : Simple, Effective and Interpretable Automatic MeSH Indexer<span class=acl-fixed-case>A</span>ttention<span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>SH</span>: Simple, Effective and Interpretable Automatic <span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>SH</span> Indexer</a></strong><br><a href=/people/q/qiao-jin/>Qiao Jin</a>
|
<a href=/people/b/bhuwan-dhingra/>Bhuwan Dhingra</a>
|
<a href=/people/w/william-cohen/>William Cohen</a>
|
<a href=/people/x/xinghua-lu/>Xinghua Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5306><div class="card-body p-3 small">There are millions of articles in <a href=https://en.wikipedia.org/wiki/PubMed>PubMed database</a>. To facilitate <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, curators in the National Library of Medicine (NLM) assign a set of Medical Subject Headings (MeSH) to each article. MeSH is a hierarchically-organized vocabulary, containing about 28 K different concepts, covering the fields from <a href=https://en.wikipedia.org/wiki/Medicine>clinical medicine</a> to <a href=https://en.wikipedia.org/wiki/Information_science>information sciences</a>. Several automatic MeSH indexing models have been developed to improve the time-consuming and financially expensive manual annotation, including the NLM official tool Medical Text Indexer, and the winner of BioASQ Task5a challenge DeepMeSH. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are complex and not interpretable. We propose a novel <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end model</a>, AttentionMeSH, which utilizes <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and attention mechanism to index <a href=https://en.wikipedia.org/wiki/Medical_Subject_Headings>MeSH terms</a> to biomedical text. The attention mechanism enables the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to associate textual evidence with annotations, thus providing <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> at the word level. The <a href=https://en.wikipedia.org/wiki/Physical_model>model</a> also uses a novel masking mechanism to enhance <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Speed>speed</a>. In the final week of BioASQ Chanllenge Task6a, we ranked 2nd by average MiF using an on-construction model. After the contest, we achieve close to state-of-the-art MiF performance of 0.684 using our final model. Human evaluations show AttentionMeSH also provides high level of interpretability, retrieving about 90 % of all expert-labeled relevant words given an MeSH-article pair at 20 output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5308/>UNCC QA : Biomedical Question Answering system<span class=acl-fixed-case>UNCC</span> <span class=acl-fixed-case>QA</span>: Biomedical Question Answering system</a></strong><br><a href=/people/a/abhishek-bhandwaldar/>Abhishek Bhandwaldar</a>
|
<a href=/people/w/wlodek-zadrozny/>Wlodek Zadrozny</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5308><div class="card-body p-3 small">In this paper, we detail our submission to the BioASQ competition&#8217;s Biomedical Semantic Question and Answering task. Our system uses extractive summarization techniques to generate answers and has scored highest ROUGE-2 and Rogue-SU4 in all test batch sets. Our contributions are named-entity based method for answering factoid and list questions, and an extractive summarization techniques for building paragraph-sized summaries, based on lexical chains. Our system got highest ROUGE-2 and ROUGE-SU4 scores for ideal-type answers in all test batch sets. We also discuss the limitations of the described <a href=https://en.wikipedia.org/wiki/System>system</a>, such lack of the evaluation on other criteria (e.g. manual). Also, for factoid- and list -type question our system got low <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> (which suggests that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> needs to improve in the ranking of entities).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5309/>An Adaption of BIOASQ Question Answering dataset for Machine Reading systems by Manual Annotations of Answer Spans.<span class=acl-fixed-case>BIOASQ</span> Question Answering dataset for Machine Reading systems by Manual Annotations of Answer Spans.</a></strong><br><a href=/people/s/sanjay-kamath/>Sanjay Kamath</a>
|
<a href=/people/b/brigitte-grau/>Brigitte Grau</a>
|
<a href=/people/y/yue-ma/>Yue Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5309><div class="card-body p-3 small">BIOASQ Task B Phase B challenge focuses on extracting answers from snippets for a given question. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> provided by the organizers contains answers, but not all their variants. Henceforth a manual annotation was performed to extract all forms of correct answers. This article shows the impact of using all occurrences of correct answers for training on the evaluation scores which are improved significantly.</div></div></div><hr><div id=w18-54><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-54.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-54/>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5400/>Proceedings of the 2018 <span class=acl-fixed-case>EMNLP</span> Workshop <span class=acl-fixed-case>B</span>lackbox<span class=acl-fixed-case>NLP</span>: Analyzing and Interpreting Neural Networks for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a>
|
<a href=/people/a/afra-alishahi/>Afra Alishahi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5403/>Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures</a></strong><br><a href=/people/d/danilo-croce/>Danilo Croce</a>
|
<a href=/people/d/daniele-rossini/>Daniele Rossini</a>
|
<a href=/people/r/roberto-basili/>Roberto Basili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5403><div class="card-body p-3 small">Nonlinear methods such as <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> achieve state-of-the-art performances in several semantic NLP tasks. However epistemologically transparent decisions are not provided as for the limited interpretability of the underlying acquired neural models. In neural-based semantic inference tasks epistemological transparency corresponds to the ability of tracing back causal connections between the linguistic properties of a input instance and the produced classification output. In this paper, we propose the use of a methodology, called Layerwise Relevance Propagation, over linguistically motivated neural architectures, namely Kernel-based Deep Architectures (KDA), to guide argumentations and explanation inferences. In such a way, each decision provided by a KDA can be linked to real examples, linguistically related to the input instance : these can be used to motivate the network output. Quantitative analysis shows that richer explanations about the semantic and syntagmatic structures of the examples characterize more convincing arguments in two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, i.e. question classification and <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>.<i>Layerwise Relevance Propagation</i>, over linguistically motivated neural architectures, namely <i>Kernel-based Deep Architectures</i> (KDA), to guide argumentations and explanation inferences. In such a way, each decision provided by a KDA can be linked to real examples, linguistically related to the input instance: these can be used to motivate the network output. Quantitative analysis shows that richer explanations about the semantic and syntagmatic structures of the examples characterize more convincing arguments in two tasks, i.e. question classification and semantic role labeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5405/>Evaluating Textual Representations through Image Generation</a></strong><br><a href=/people/g/graham-spinks/>Graham Spinks</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5405><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for determining the quality of textual representations through the ability to generate <a href=https://en.wikipedia.org/wiki/Image>images</a> from them. Continuous representations of textual input are ubiquitous in modern Natural Language Processing techniques either at the core of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> or as the by-product at any given layer of a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>. While current techniques to evaluate such representations focus on their performance on particular tasks, they do n&#8217;t provide a clear understanding of the level of informational detail that is stored within them, especially their ability to represent spatial information. The central premise of this paper is that visual inspection or analysis is the most convenient method to quickly and accurately determine information content. Through the use of text-to-image neural networks, we propose a new technique to compare the quality of textual representations by visualizing their information content. The method is illustrated on a <a href=https://en.wikipedia.org/wiki/Medical_imaging>medical dataset</a> where the correct representation of spatial information and <a href=https://en.wikipedia.org/wiki/Shorthand>shorthands</a> are of particular importance. For four different well-known textual representations, we show with a quantitative analysis that some <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> are consistently able to deliver higher quality visualizations of the information content. Additionally, we show that the quantitative analysis technique correlates with the judgment of a human expert evaluator in terms of alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5409/>Linguistic representations in multi-task neural networks for ellipsis resolution</a></strong><br><a href=/people/o/ola-ronning/>Ola Rønning</a>
|
<a href=/people/d/daniel-hardt/>Daniel Hardt</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5409><div class="card-body p-3 small">Sluicing resolution is the task of identifying the antecedent to a question ellipsis. Antecedents are often <a href=https://en.wikipedia.org/wiki/Constituent_(linguistics)>sentential constituents</a>, and previous work has therefore relied on <a href=https://en.wikipedia.org/wiki/Parsing>syntactic parsing</a>, together with complex linguistic features. A recent model instead used partial parsing as an auxiliary task in sequential neural network architectures to inject <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>. We explore the linguistic information being brought to bear by such <a href=https://en.wikipedia.org/wiki/Social_network>networks</a>, both by defining subsets of the data exhibiting relevant linguistic characteristics, and by examining the internal representations of the <a href=https://en.wikipedia.org/wiki/Social_network>network</a>. Both perspectives provide evidence for substantial <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic knowledge</a> being deployed by the <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5413/>Rearranging the Familiar : Testing Compositional Generalization in Recurrent Networks</a></strong><br><a href=/people/j/joao-loula/>João Loula</a>
|
<a href=/people/m/marco-baroni/>Marco Baroni</a>
|
<a href=/people/b/brenden-lake/>Brenden Lake</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5413><div class="card-body p-3 small">Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it&#8217;s seen as key to the human capacity for generalization in language. Recent work (Lake and Baroni, 2018) has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool. Lake and Baroni&#8217;s main experiment required the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as around and right) in novel contexts. Our findings confirm and strengthen the earlier ones : seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of X around right to jump around right), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of around right from those of right and around).<i>around</i>&#8221; and &#8220;<i>right</i>&#8221;) in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of &#8220;X <i>around right</i>&#8221; to &#8220;<i>jump around right</i>&#8221;), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of &#8220;<i>around right</i>&#8221; from those of &#8220;<i>right</i>&#8221; and &#8220;<i>around</i>&#8221;).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5415/>Interpretable Neural Architectures for Attributing an Ad’s Performance to its Writing Style</a></strong><br><a href=/people/r/reid-pryzant/>Reid Pryzant</a>
|
<a href=/people/s/sugato-basu/>Sugato Basu</a>
|
<a href=/people/k/kazoo-sone/>Kazoo Sone</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5415><div class="card-body p-3 small">How much does free shipping ! help an advertisement&#8217;s ability to persuade? This paper presents two methods for <a href=https://en.wikipedia.org/wiki/Performance_attribution>performance attribution</a> : finding the degree to which an outcome can be attributed to parts of a text while controlling for potential confounders. Both <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> are based on interpreting the behaviors and parameters of trained <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. One method uses a CNN to encode the text, an adversarial objective function to control for confounders, and projects its weights onto its activations to interpret the importance of each phrase towards each output class. The other method leverages <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>residualization</a> to control for <a href=https://en.wikipedia.org/wiki/Confounding>confounds</a> and performs <a href=https://en.wikipedia.org/wiki/Interpretation_(logic)>interpretation</a> by aggregating over learned word vectors. We demonstrate these algorithms&#8217; efficacy on 118,000 internet search advertisements and outcomes, finding language indicative of high and low click through rate (CTR) regardless of who the ad is by or what it is for. Our results suggest the proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> are high performance and data efficient, able to glean actionable insights from fewer than 10,000 data points. We find that quick, easy, and authoritative language is associated with success, while lackluster embellishment is related to failure. These findings agree with the advertising industry&#8217;s emperical wisdom, automatically revealing insights which previously required manual A / B testing to discover.<i>performance attribution</i>: finding the degree to which an outcome can be attributed to parts of a text while controlling for potential confounders. Both algorithms are based on interpreting the behaviors and parameters of trained neural networks. One method uses a CNN to encode the text, an adversarial objective function to control for confounders, and projects its weights onto its activations to interpret the importance of each phrase towards each output class. The other method leverages residualization to control for confounds and performs interpretation by aggregating over learned word vectors. We demonstrate these algorithms&#8217; efficacy on 118,000 internet search advertisements and outcomes, finding language indicative of high and low click through rate (CTR) regardless of who the ad is by or what it is for. Our results suggest the proposed algorithms are high performance and data efficient, able to glean actionable insights from fewer than 10,000 data points. We find that quick, easy, and authoritative language is associated with success, while lackluster embellishment is related to failure. These findings agree with the advertising industry&#8217;s emperical wisdom, automatically revealing insights which previously required manual A/B testing to discover.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5418/>LISA : Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation<span class=acl-fixed-case>LISA</span>: Explaining Recurrent Neural Network Judgments via Layer-w<span class=acl-fixed-case>I</span>se Semantic Accumulation and Example to Pattern Transformation</a></strong><br><a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5418><div class="card-body p-3 small">Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of <a href=https://en.wikipedia.org/wiki/Neural_network>RNN</a> via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while <a href=https://en.wikipedia.org/wiki/Decision-making>decision making</a>. We demonstrate (1) LISA : How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response (2) Example2pattern : How the saliency patterns look like for each category in the data according to the network in decision making. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the <a href=https://en.wikipedia.org/wiki/Neural_network>network</a>. We employ two relation classification datasets : SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern.<i>Layer-wIse-Semantic-Accumulation</i> (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making. We demonstrate (1) <i>LISA</i>: &#8220;How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response&#8221; (2) <i>Example2pattern</i>: &#8220;How the saliency patterns look like for each category in the data according to the network in decision making&#8221;. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the <i>LISA</i> and <i>example2pattern</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5420/>An Operation Sequence Model for Explainable Neural Machine Translation</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5420><div class="card-body p-3 small">We propose to achieve explainable neural machine translation (NMT) by changing the output representation to explain itself. We present a novel approach to NMT which generates the target sentence by monotonically walking through the source sentence. Word reordering is modeled by operations which allow setting markers in the target sentence and move a target-side write head between those markers. In contrast to many modern neural models, our system emits explicit word alignment information which is often crucial to practical <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> as it improves explainability. Our technique can outperform a plain text system in terms of BLEU score under the recent Transformer architecture on Japanese-English and Portuguese-English, and is within 0.5 BLEU difference on Spanish-English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5421/>Introspection for convolutional automatic speech recognition</a></strong><br><a href=/people/a/andreas-krug/>Andreas Krug</a>
|
<a href=/people/s/sebastian-stober/>Sebastian Stober</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5421><div class="card-body p-3 small">Artificial Neural Networks (ANNs) have experienced great success in the past few years. The increasing <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> leads to less understanding about their <a href=https://en.wikipedia.org/wiki/Decision-making>decision processes</a>. Therefore, introspection techniques have been proposed, mostly for <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> as input data. Patterns or relevant regions in <a href=https://en.wikipedia.org/wiki/Image>images</a> can be intuitively interpreted by a human observer. This is not the case for more complex data like speech recordings. In this work, we investigate the application of common introspection techniques from <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> to an Automatic Speech Recognition (ASR) task. To this end, we use a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> similar to <a href=https://en.wikipedia.org/wiki/Image_classification>image classification</a>, which predicts <a href=https://en.wikipedia.org/wiki/Letter_(alphabet)>letters</a> from <a href=https://en.wikipedia.org/wiki/Optical_spectrometer>spectrograms</a>. We show difficulties in applying image introspection to ASR. To tackle these problems, we propose normalized averaging of aligned inputs (NAvAI): a data-driven method to reveal learned patterns for prediction of specific classes. Our method integrates information from many data examples through local introspection techniques for Convolutional Neural Networks (CNNs). We demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> provides better interpretability of letter-specific patterns than existing methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5422/>Learning and Evaluating Sparse Interpretable Sentence Embeddings</a></strong><br><a href=/people/v/valentin-trifonov/>Valentin Trifonov</a>
|
<a href=/people/o/octavian-eugen-ganea/>Octavian-Eugen Ganea</a>
|
<a href=/people/a/anna-potapenko/>Anna Potapenko</a>
|
<a href=/people/t/thomas-hofmann/>Thomas Hofmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5422><div class="card-body p-3 small">Previous research on word embeddings has shown that sparse representations, which can be either learned on top of existing dense embeddings or obtained through model constraints during training time, have the benefit of increased interpretability properties : to some degree, each dimension can be understood by a human and associated with a recognizable feature in the data. In this paper, we transfer this idea to <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> and explore several approaches to obtain a <a href=https://en.wikipedia.org/wiki/Sparse_representation>sparse representation</a>. We further introduce a novel, quantitative and automated evaluation metric for sentence embedding interpretability, based on topic coherence methods. We observe an increase in <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> compared to dense models, on a dataset of movie dialogs and on the scene descriptions from the MS COCO dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5425 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5425/>Closing Brackets with Recurrent Neural Networks</a></strong><br><a href=/people/n/natalia-skachkova/>Natalia Skachkova</a>
|
<a href=/people/t/thomas-alexander-trost/>Thomas Trost</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5425><div class="card-body p-3 small">Many natural and formal languages contain words or symbols that require a matching counterpart for making an expression well-formed. The combination of opening and closing brackets is a typical example of such a construction. Due to their commonness, the ability to follow such <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> is important for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. Currently, recurrent neural networks (RNNs) are extensively used for this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. We investigate whether they are capable of learning the rules of opening and closing brackets by applying them to synthetic Dyck languages that consist of different types of <a href=https://en.wikipedia.org/wiki/Bracket>brackets</a>. We provide an analysis of the statistical properties of these languages as a baseline and show strengths and limits of Elman-RNNs, GRUs and LSTMs in experiments on random samples of these languages. In terms of perplexity and prediction accuracy, the RNNs get close to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>theoretical baseline</a> in most cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5426 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5426/>Under the Hood : Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information</a></strong><br><a href=/people/m/mario-giulianelli/>Mario Giulianelli</a>
|
<a href=/people/j/jack-harding/>Jack Harding</a>
|
<a href=/people/f/florian-mohnert/>Florian Mohnert</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a>
|
<a href=/people/w/willem-zuidema/>Willem Zuidema</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5426><div class="card-body p-3 small">How do neural language models keep track of <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>number agreement</a> between subject and verb? We show that &#8216;diagnostic classifiers&#8217;, trained to predict number from the internal states of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>, provide a detailed understanding of how, when, and where this information is represented. Moreover, they give us insight into when and where <a href=https://en.wikipedia.org/wiki/Number>number information</a> is corrupted in cases where the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> ends up making <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>agreement errors</a>. To demonstrate the causal role played by the representations we find, we then use agreement information to influence the course of the LSTM during the processing of difficult sentences. Results from such an intervention reveal a large increase in the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>&#8217;s accuracy. Together, these results show that diagnostic classifiers give us an unrivalled detailed look into the representation of linguistic information in neural models, and demonstrate that this knowledge can be used to improve their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5427 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5427/>Iterative Recursive Attention Model for Interpretable Sequence Classification</a></strong><br><a href=/people/m/martin-tutek/>Martin Tutek</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5427><div class="card-body p-3 small">Natural language processing has greatly benefited from the introduction of the attention mechanism. However, standard attention models are of limited interpretability for tasks that involve a series of <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference steps</a>. We describe an iterative recursive attention model, which constructs incremental representations of input data through reusing results of previously computed queries. We train our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on sentiment classification datasets and demonstrate its capacity to identify and combine different aspects of the input in an easily interpretable manner, while obtaining performance close to the state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5429/>Importance of Self-Attention for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/g/gael-letarte/>Gaël Letarte</a>
|
<a href=/people/f/frederik-paradis/>Frédérik Paradis</a>
|
<a href=/people/p/philippe-giguere/>Philippe Giguère</a>
|
<a href=/people/f/francois-laviolette/>François Laviolette</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5429><div class="card-body p-3 small">Despite their superior performance, deep learning models often lack <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. In this paper, we explore the modeling of insightful relations between words, in order to understand and enhance predictions. To this effect, we propose the Self-Attention Network (SANet), a flexible and interpretable architecture for text classification. Experiments indicate that gains obtained by self-attention is task-dependent. For instance, experiments on sentiment analysis tasks showed an improvement of around 2 % when using self-attention compared to a baseline without <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, while topic classification showed no gain. Interpretability brought forward by our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> highlighted the importance of neighboring word interactions to extract <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5431/>An Analysis of Encoder Representations in Transformer-Based Machine Translation</a></strong><br><a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5431><div class="card-body p-3 small">The attention mechanism is a successful technique in modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, especially in tasks like <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The recently proposed <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a> of the Transformer is based entirely on <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> and achieves new state of the art results in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> while higher layers tend to encode more <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>.<i>Transformer</i> is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5432 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5432/>Evaluating Grammaticality in Seq2seq Models with a Broad Coverage HPSG Grammar : A Case Study on Machine Translation<span class=acl-fixed-case>HPSG</span> Grammar: A Case Study on Machine Translation</a></strong><br><a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/k/khiem-pham/>Khiem Pham</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a>
|
<a href=/people/b/brian-w-dillon/>Brian Dillon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5432><div class="card-body p-3 small">Sequence to sequence (seq2seq) models are often employed in settings where the target output is <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. However, the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic properties</a> of the language generated from these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are not well understood. We explore whether such output belongs to a formal and realistic grammar, by employing the English Resource Grammar (ERG), a broad coverage, linguistically precise HPSG-based grammar of English. From a French to English parallel corpus, we analyze the parseability and <a href=https://en.wikipedia.org/wiki/Grammar>grammatical constructions</a> occurring in output from a seq2seq translation model. Over 93 % of the model translations are parseable, suggesting that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> learns to generate conforming to a <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has trouble learning the distribution of rarer syntactic rules, and we pinpoint several constructions that differentiate translations between the references and our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5434 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5434" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5434/>Learning Explanations from Language Data</a></strong><br><a href=/people/d/david-harbecke/>David Harbecke</a>
|
<a href=/people/r/robert-schwarzenberg/>Robert Schwarzenberg</a>
|
<a href=/people/c/christoph-alt/>Christoph Alt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5434><div class="card-body p-3 small">PatternAttribution is a recent method, introduced in the <a href=https://en.wikipedia.org/wiki/Visual_system>vision domain</a>, that explains classifications of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. We demonstrate that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> also generates meaningful interpretations in the language domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5435.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5435 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5435 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5435/>How much should you ask? On the question structure in <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA systems</a>.<span class=acl-fixed-case>QA</span> systems.</a></strong><br><a href=/people/b/barbara-rychalska/>Barbara Rychalska</a>
|
<a href=/people/d/dominika-basaj/>Dominika Basaj</a>
|
<a href=/people/a/anna-wroblewska/>Anna Wróblewska</a>
|
<a href=/people/p/przemyslaw-biecek/>Przemyslaw Biecek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5435><div class="card-body p-3 small">Datasets that boosted state-of-the-art solutions for Question Answering (QA) systems prove that it is possible to ask questions in natural language manner. However, users are still used to query-like systems where they type in <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> to search for answer. In this study we validate which parts of questions are essential for obtaining valid answer. In order to conclude that, we take advantage of LIME-a framework that explains <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> by local approximation. We find that <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> and <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> is disregarded by QA. State-of-the-art model can answer properly even if&#8217; asked&#8217; only with a few words with high coefficients calculated with <a href=https://en.wikipedia.org/wiki/LIME>LIME</a>. According to our knowledge, it is the first time that QA model is being explained by LIME.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5438 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5438/>Language Models Learn POS First<span class=acl-fixed-case>POS</span> First</a></strong><br><a href=/people/n/naomi-saphra/>Naomi Saphra</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5438><div class="card-body p-3 small">A glut of recent research shows that <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> capture linguistic structure. Such work answers the question of whether a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> represents linguistic structure. But how and when are these <a href=https://en.wikipedia.org/wiki/List_of_nonbuilding_structure_types>structures</a> acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with <a href=https://en.wikipedia.org/wiki/Part_of_speech_tagging>part of speech tagging</a> acquired early and global topic information learned continuously.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5439 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5439/>Predicting and interpreting embeddings for out of vocabulary words in downstream tasks</a></strong><br><a href=/people/n/nicolas-garneau/>Nicolas Garneau</a>
|
<a href=/people/j/jean-samuel-leboeuf/>Jean-Samuel Leboeuf</a>
|
<a href=/people/l/luc-lamontagne/>Luc Lamontagne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5439><div class="card-body p-3 small">We propose a novel way to handle out of vocabulary (OOV) words in downstream natural language processing (NLP) tasks. We implement a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> that predicts useful embeddings for OOV words based on their <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> and on the context in which they appear. Our model also incorporates an attention mechanism indicating the focus allocated to the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>left context words</a>, the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>right context words</a> or the word&#8217;s characters, hence making the prediction more interpretable. The model is a drop-in module that is jointly trained with the downstream task&#8217;s <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, thus producing embeddings specialized for the task at hand. When the task is mostly syntactical, we observe that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> aims most of its attention on surface form characters. On the other hand, for tasks more semantical, the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>network</a> allocates more attention to the surrounding words. In all our tests, the <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> helps the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> to achieve better performances in comparison to the use of simple random embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5441 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/305194062 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-5441/>Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation</a></strong><br><a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/a/aparajita-haldar/>Aparajita Haldar</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/j/j-edward-hu/>J. Edward Hu</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a>
|
<a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5441><div class="card-body p-3 small">We present a large scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence representation</a> encoded by a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> captures distinct types of <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. Our collection of diverse datasets is available at, and will grow over time as additional resources are recast and added from novel sources.<url>http://www.decomp.net/</url>, and will grow over time as additional resources are recast and added from novel sources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5442 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5442/>Interpretable Word Embedding Contextualization</a></strong><br><a href=/people/k/kyoung-rok-jang/>Kyoung-Rok Jang</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a>
|
<a href=/people/s/sang-bum-kim/>Sang-Bum Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5442><div class="card-body p-3 small">In this paper, we propose a method of calibrating a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>, so that the semantic it conveys becomes more relevant to the context. Our method is novel because the output shows clearly which senses that were originally presented in a target <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> become stronger or weaker. This is possible by utilizing the technique of using <a href=https://en.wikipedia.org/wiki/Sparse_coding>sparse coding</a> to recover senses that comprises a <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5444.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5444 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5444 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5444/>Extracting Syntactic Trees from Transformer Encoder Self-Attentions</a></strong><br><a href=/people/d/david-marecek/>David Mareček</a>
|
<a href=/people/r/rudolf-rosa/>Rudolf Rosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5444><div class="card-body p-3 small">This is a work in progress about extracting the sentence tree structures from the encoder&#8217;s self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing <a href=https://en.wikipedia.org/wiki/Syntax>syntactic theories</a> and <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5445 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5445/>Portable, layer-wise task performance monitoring for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a><span class=acl-fixed-case>NLP</span> models</a></strong><br><a href=/people/t/tom-lippincott/>Tom Lippincott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5445><div class="card-body p-3 small">There is a long-standing interest in understanding the internal behavior of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Deep neural architectures for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a> are often accompanied by explanations for their effectiveness, from general observations (e.g. RNNs can represent unbounded dependencies in a sequence) to specific arguments about linguistic phenomena (early layers encode lexical information, deeper layers syntactic). The recent ascendancy of DNNs is fueling efforts in the NLP community to explore these claims. Previous work has tended to focus on easily-accessible representations like word or sentence embeddings, with deeper structure requiring more ad hoc methods to extract and examine. In this work, we introduce Vivisect, a toolkit that aims at a general solution for broad and fine-grained monitoring in the major DNN frameworks, with minimal change to research patterns.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5447 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5447/>Explicitly modeling case improves neural dependency parsing</a></strong><br><a href=/people/c/clara-vania/>Clara Vania</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5447><div class="card-body p-3 small">Neural dependency parsing models that compose word representations from <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> can presumably exploit <a href=https://en.wikipedia.org/wiki/Morphosyntax>morphosyntax</a> when making attachment decisions. How much do they know about <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphology</a>? We investigate how well they handle <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological case</a>, which is important for <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Our experiments on <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> suggest that adding explicit morphological caseeither oracle or predictedimproves neural dependency parsing, indicating that the learned representations in these models do not fully encode the morphological knowledge that they need, and can still benefit from targeted forms of explicit linguistic modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5449 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5449/>Representation of Word Meaning in the Intermediate Projection Layer of a Neural Language Model</a></strong><br><a href=/people/s/steven-derby/>Steven Derby</a>
|
<a href=/people/p/paul-miller/>Paul Miller</a>
|
<a href=/people/b/brian-murphy/>Brian Murphy</a>
|
<a href=/people/b/barry-devereux/>Barry Devereux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5449><div class="card-body p-3 small">Performance in <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a> has been significantly improved by training <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> on large corpora. This progress has come at the cost of interpretability and an understanding of how these <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> function, making principled development of better <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> more difficult. We look inside a state-of-the-art neural language model to analyse how this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> represents high-level lexico-semantic information. In particular, we investigate how the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> represents words by extracting activation patterns where they occur in the text, and compare these <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> directly to human semantic knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5450.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5450 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5450 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5450/>Interpretable Structure Induction via Sparse Attention</a></strong><br><a href=/people/b/ben-peters/>Ben Peters</a>
|
<a href=/people/v/vlad-niculae/>Vlad Niculae</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5450><div class="card-body p-3 small">Neural network methods are experiencing wide adoption in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, thanks to their empirical performance on many tasks. Modern neural architectures go way beyond simple feedforward and recurrent models : they are complex pipelines that perform soft, differentiable computation instead of <a href=https://en.wikipedia.org/wiki/Discrete_mathematics>discrete logic</a>. The price of such <a href=https://en.wikipedia.org/wiki/Soft_computing>soft computing</a> is the introduction of dense dependencies, which make it hard to disentangle the patterns that trigger a prediction. Our recent work on sparse and structured latent computation presents a promising avenue for enhancing interpretability of such neural pipelines. Through this extended abstract, we aim to discuss and explore the potential and impact of our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5451 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5451/>Debugging Sequence-to-Sequence Models with Seq2Seq-Vis<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq-Vis</a></strong><br><a href=/people/h/hendrik-strobelt/>Hendrik Strobelt</a>
|
<a href=/people/s/sebastian-gehrmann/>Sebastian Gehrmann</a>
|
<a href=/people/m/michael-behrisch/>Michael Behrisch</a>
|
<a href=/people/a/adam-perer/>Adam Perer</a>
|
<a href=/people/h/hanspeter-pfister/>Hanspeter Pfister</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5451><div class="card-body p-3 small">Neural attention-based sequence-to-sequence models (seq2seq) (Sutskever et al., 2014 ; Bahdanau et al., 2014) have proven to be accurate and robust for many sequence prediction tasks. They have become the standard approach for automatic translation of text, at the cost of increased model complexity and <a href=https://en.wikipedia.org/wiki/Uncertainty>uncertainty</a>. End-to-end trained neural models act as a black box, which makes it difficult to examine model decisions and attribute errors to a specific part of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. The highly connected and high-dimensional internal representations pose a challenge for analysis and visualization tools. The development of methods to understand seq2seq predictions is crucial for systems in production settings, as mistakes involving language are often very apparent to human readers. For instance, a widely publicized incident resulted from a translation system mistakenly translating good morning into attack them leading to a wrongful arrest (Hern, 2017).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5453.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5453 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5453 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5453/>Does Syntactic Knowledge in Multilingual Language Models Transfer Across Languages?</a></strong><br><a href=/people/p/prajit-dhar/>Prajit Dhar</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5453><div class="card-body p-3 small">Recent work has shown that neural models can be successfully trained on multiple languages simultaneously. We investigate whether such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> learn to share and exploit common syntactic knowledge among the languages on which they are trained. This extended abstract presents our preliminary results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5455.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5455 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5455 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5455" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5455/>End-to-end Image Captioning Exploits Distributional Similarity in Multimodal Space</a></strong><br><a href=/people/p/pranava-swaroop-madhyastha/>Pranava Swaroop Madhyastha</a>
|
<a href=/people/j/josiah-wang/>Josiah Wang</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5455><div class="card-body p-3 small">We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn &#8216;distributional similarity&#8217; in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the &#8216;image&#8217; side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. Our analysis indicates that image captioning models (i) are capable of separating structure from noisy input representations ; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space ; (iii) cluster images with similar visual and linguistic information together. Our experiments all point to one fact : that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.</div></div></div><hr><div id=w18-55><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-55.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-55/>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5500/>Proceedings of the First Workshop on Fact Extraction and <span class=acl-fixed-case>VER</span>ification (<span class=acl-fixed-case>FEVER</span>)</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5501/>The Fact Extraction and VERification (FEVER) Shared Task<span class=acl-fixed-case>VER</span>ification (<span class=acl-fixed-case>FEVER</span>) Shared Task</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/o/oana-cocarascu/>Oana Cocarascu</a>
|
<a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a>
|
<a href=/people/a/arpit-mittal/>Arpit Mittal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5501><div class="card-body p-3 small">We present the results of the first Fact Extraction and VERification (FEVER) Shared Task. The task challenged participants to classify whether human-written factoid claims could be SUPPORTED or REFUTED using evidence retrieved from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We received entries from 23 competing teams, 19 of which scored higher than the previously published baseline. The best performing <a href=https://en.wikipedia.org/wiki/System>system</a> achieved a <a href=https://en.wikipedia.org/wiki/Fever>FEVER score</a> of 64.21 %. In this paper, we present the results of the shared task and a summary of the <a href=https://en.wikipedia.org/wiki/System>systems</a>, highlighting commonalities and innovations among participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5502" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5502/>The Data Challenge in Misinformation Detection : Source Reputation vs. Content Veracity</a></strong><br><a href=/people/f/fatemeh-torabi-asr/>Fatemeh Torabi Asr</a>
|
<a href=/people/m/maite-taboada/>Maite Taboada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5502><div class="card-body p-3 small">Misinformation detection at the level of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>full news articles</a> is a text classification problem. Reliably labeled data in this <a href=https://en.wikipedia.org/wiki/Domain_(mathematical_analysis)>domain</a> is rare. Previous work relied on <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> collected from so-called reputable and suspicious websites and labeled accordingly. We leverage fact-checking websites to collect individually-labeled news articles with regard to the veracity of their content and use this data to test the cross-domain generalization of a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> trained on bigger text collections but labeled according to source reputation. Our results suggest that reputation-based classification is not sufficient for predicting the veracity level of the majority of news articles, and that the system performance on different test datasets depends on topic distribution. Therefore collecting well-balanced and carefully-assessed training data is a priority for developing robust misinformation detection systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5503" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5503/>Crowdsourcing Semantic Label Propagation in Relation Classification</a></strong><br><a href=/people/a/anca-dumitrache/>Anca Dumitrache</a>
|
<a href=/people/l/lora-aroyo/>Lora Aroyo</a>
|
<a href=/people/c/chris-welty/>Chris Welty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5503><div class="card-body p-3 small">Distant supervision is a popular method for performing <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> that is known to produce noisy labels. Most progress in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> has been made with crowdsourced corrections to distant-supervised labels, and there is evidence that indicates still more would be better. In this paper, we explore the problem of propagating human annotation signals gathered for open-domain relation classification through the CrowdTruth methodology for <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>, that captures ambiguity in annotations by measuring inter-annotator disagreement. Our approach propagates annotations to sentences that are similar in a low dimensional embedding space, expanding the number of labels by two orders of magnitude. Our experiments show significant improvement in a sentence-level multi-class relation classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5506/>Joint Modeling for <a href=https://en.wikipedia.org/wiki/Query_expansion>Query Expansion</a> and <a href=https://en.wikipedia.org/wiki/Information_extraction>Information Extraction</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5506><div class="card-body p-3 small">Information extraction about an event can be improved by incorporating <a href=https://en.wikipedia.org/wiki/Empirical_evidence>external evidence</a>. In this study, we propose a joint model for pseudo-relevance feedback based query expansion and <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates an event-specific query to effectively retrieve documents relevant to the event. We demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is comparable or has better performance than the previous <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in two publicly available datasets. Furthermore, we analyzed the influences of the retrieval effectiveness in our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the <a href=https://en.wikipedia.org/wiki/Information_retrieval>extraction</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5508/>Belittling the Source : Trustworthiness Indicators to Obfuscate Fake News on the Web</a></strong><br><a href=/people/d/diego-esteves/>Diego Esteves</a>
|
<a href=/people/a/aniketh-janardhan-reddy/>Aniketh Janardhan Reddy</a>
|
<a href=/people/p/piyush-chawla/>Piyush Chawla</a>
|
<a href=/people/j/jens-lehmann/>Jens Lehmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5508><div class="card-body p-3 small">With the growth of the <a href=https://en.wikipedia.org/wiki/Internet>internet</a>, the number of fake-news online has been proliferating every year. The consequences of such phenomena are manifold, ranging from lousy decision-making process to bullying and violence episodes. Therefore, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking algorithms</a> became a valuable asset. To this aim, an important step to detect <a href=https://en.wikipedia.org/wiki/Fake_news>fake-news</a> is to have access to a credibility score for a given <a href=https://en.wikipedia.org/wiki/Source_(journalism)>information source</a>. However, most of the widely used Web indicators have either been shutdown to the public (e.g., Google PageRank) or are not free for use (Alexa Rank). Further existing databases are short-manually curated lists of online sources, which do not scale. Finally, most of the research on the topic is theoretical-based or explore <a href=https://en.wikipedia.org/wiki/Confidentiality>confidential data</a> in a restricted simulation environment. In this paper we explore current research, highlight the challenges and propose solutions to tackle the problem of classifying websites into a credibility scale. The proposed model automatically extracts source reputation cues and computes a credibility factor, providing valuable insights which can help in belittling dubious and confirming trustful unknown websites. Experimental results outperform state of the art in the 2-classes and 5-classes setting.<i>fake-news</i> online has been proliferating every year. The consequences of such phenomena are manifold, ranging from lousy decision-making process to bullying and violence episodes. Therefore, fact-checking algorithms became a valuable asset. To this aim, an important step to detect fake-news is to have access to a credibility score for a given information source. However, most of the widely used Web indicators have either been shutdown to the public (e.g., Google PageRank) or are not free for use (Alexa Rank). Further existing databases are short-manually curated lists of online sources, which do not scale. Finally, most of the research on the topic is theoretical-based or explore confidential data in a restricted simulation environment. In this paper we explore current research, highlight the challenges and propose solutions to tackle the problem of classifying websites into a credibility scale. The proposed model automatically extracts source reputation cues and computes a credibility factor, providing valuable insights which can help in belittling dubious and confirming trustful unknown websites. Experimental results outperform state of the art in the 2-classes and 5-classes setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5510/>Stance Detection in Fake News A Combined Feature Representation</a></strong><br><a href=/people/b/bilal-ghanem/>Bilal Ghanem</a>
|
<a href=/people/p/paolo-rosso/>Paolo Rosso</a>
|
<a href=/people/f/francisco-rangel/>Francisco Rangel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5510><div class="card-body p-3 small">With the uncontrolled increasing of <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and rumors over the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>, different approaches have been proposed to address the problem. In this paper, we present an approach that combines lexical, word embeddings and <a href=https://en.wikipedia.org/wiki/N-gram>n-gram features</a> to detect the stance in <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>. Our approach has been tested on the Fake News Challenge (FNC-1) dataset. Given a news title-article pair, the FNC-1 task aims at determining the relevance of the article and the title. Our proposed approach has achieved an accurate result (59.6 % Macro F1) that is close to the state-of-the-art result with 0.013 difference using a simple <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>feature representation</a>. Furthermore, we have investigated the importance of different <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> in the detection of the classification labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5513" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5513/>Where is Your Evidence : Improving <a href=https://en.wikipedia.org/wiki/Fact-checking>Fact-checking</a> by Justification Modeling</a></strong><br><a href=/people/t/tariq-alhindi/>Tariq Alhindi</a>
|
<a href=/people/s/savvas-petridis/>Savvas Petridis</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5513><div class="card-body p-3 small">Fact-checking is a <a href=https://en.wikipedia.org/wiki/Journalism>journalistic practice</a> that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a>. However, approaches based on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5514/>Affordance Extraction and Inference based on Semantic Role Labeling</a></strong><br><a href=/people/d/daniel-loureiro/>Daniel Loureiro</a>
|
<a href=/people/a/alipio-jorge/>Alípio Jorge</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5514><div class="card-body p-3 small">Common-sense reasoning is becoming increasingly important for the advancement of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. While word embeddings have been very successful, they can not explain which aspects of &#8216;coffee&#8217; and &#8216;tea&#8217; make them similar, or how they could be related to &#8216;shop&#8217;. In this paper, we propose an explicit word representation that builds upon the Distributional Hypothesis to represent meaning from semantic roles, and allow inference of relations from their meshing, as supported by the affordance-based Indexical Hypothesis. We find that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves the state-of-the-art on unsupervised word similarity tasks while allowing for direct inference of new relations from the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5515 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5515/>UCL Machine Reading Group : Four Factor Framework For Fact Finding (HexaF)<span class=acl-fixed-case>UCL</span> Machine Reading Group: Four Factor Framework For Fact Finding (<span class=acl-fixed-case>H</span>exa<span class=acl-fixed-case>F</span>)</a></strong><br><a href=/people/t/takuma-yoneda/>Takuma Yoneda</a>
|
<a href=/people/j/jeff-mitchell/>Jeff Mitchell</a>
|
<a href=/people/j/johannes-welbl/>Johannes Welbl</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5515><div class="card-body p-3 small">In this paper we describe our 2nd place FEVER shared-task system that achieved a FEVER score of 62.52 % on the provisional test set (without additional human evaluation), and 65.41 % on the development set. Our system is a four stage model consisting of <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence retrieval</a>, <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a> and aggregation. Retrieval is performed leveraging task-specific features, and then a natural language inference model takes each of the retrieved sentences paired with the claimed fact. The resulting predictions are aggregated across retrieved sentences with a Multi-Layer Perceptron, and re-ranked corresponding to the final prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5516" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5516/>UKP-Athene : Multi-Sentence Textual Entailment for Claim Verification<span class=acl-fixed-case>UKP</span>-Athene: Multi-Sentence Textual Entailment for Claim Verification</a></strong><br><a href=/people/a/andreas-hanselowski/>Andreas Hanselowski</a>
|
<a href=/people/h/hao-zhang/>Hao Zhang</a>
|
<a href=/people/z/zile-li/>Zile Li</a>
|
<a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/b/benjamin-schiller/>Benjamin Schiller</a>
|
<a href=/people/c/claudia-schulz/>Claudia Schulz</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5516><div class="card-body p-3 small">The Fact Extraction and VERification (FEVER) shared task was launched to support the development of systems able to verify claims by extracting supporting or refuting facts from raw text. The shared task organizers provide a large-scale dataset for the consecutive steps involved in <a href=https://en.wikipedia.org/wiki/Verification_and_validation>claim verification</a>, in particular, <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, fact extraction, and <a href=https://en.wikipedia.org/wiki/Statistical_classification>claim classification</a>. In this paper, we present our claim verification pipeline approach, which, according to the preliminary results, scored third in the shared task, out of 23 competing systems. For the <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, we implemented a new entity linking approach. In order to be able to rank candidate facts and classify a claim on the basis of several selected facts, we introduce two extensions to the Enhanced LSTM (ESIM).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5517.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5517 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5517 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5517" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5517/>Team Papelo : Transformer Networks at FEVER<span class=acl-fixed-case>FEVER</span></a></strong><br><a href=/people/c/christopher-malon/>Christopher Malon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5517><div class="card-body p-3 small">We develop a system for the FEVER fact extraction and verification challenge that uses a high precision entailment classifier based on transformer networks pretrained with <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, to classify a broad set of potential evidence. The precision of the <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment classifier</a> allows us to enhance <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> by considering every statement from several articles to decide upon each claim. We include not only the articles best matching the claim text by TFIDF score, but read additional articles whose titles match named entities and capitalized expressions occurring in the claim text. The entailment module evaluates potential evidence one statement at a time, together with the title of the page the evidence came from (providing a hint about possible pronoun antecedents). In preliminary evaluation, the system achieves.5736 FEVER score,.6108 label accuracy, and.6485 evidence F1 on the FEVER shared task test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5519.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5519 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5519 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5519/>SIRIUS-LTG : An Entity Linking Approach to Fact Extraction and Verification<span class=acl-fixed-case>SIRIUS</span>-<span class=acl-fixed-case>LTG</span>: An Entity Linking Approach to Fact Extraction and Verification</a></strong><br><a href=/people/f/farhad-nooralahzadeh/>Farhad Nooralahzadeh</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5519><div class="card-body p-3 small">This article presents the SIRIUS-LTG system for the Fact Extraction and VERification (FEVER) Shared Task. It consists of three components : 1) Wikipedia Page Retrieval : First we extract the entities in the claim, then we find potential Wikipedia URI candidates for each of the entities using a SPARQL query over <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a> 2) Sentence selection : We investigate various <a href=https://en.wikipedia.org/wiki/List_of_Internet_phenomena>techniques</a> i.e. Smooth Inverse Frequency (SIF), Word Mover&#8217;s Distance (WMD), Soft-Cosine Similarity, Cosine similarity with unigram Term Frequency Inverse Document Frequency (TF-IDF) to rank sentences by their similarity to the claim. 3) Textual Entailment : We compare three models for the task of claim classification. We apply a Decomposable Attention (DA) model (Parikh et al., 2016), a Decomposed Graph Entailment (DGE) model (Khot et al., 2018) and a Gradient-Boosted Decision Trees (TalosTree) model (Sean et al., 2017) for this task. The experiments show that the pipeline with simple <a href=https://en.wikipedia.org/wiki/Cosine_similarity>Cosine Similarity</a> using TFIDF in sentence selection along with DA model as labelling model achieves the best results on the development set (F1 evidence : 32.17, label accuracy : 59.61 and FEVER score : 0.3778). Furthermore, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> obtains 30.19, 48.87 and 36.55 in terms of F1 evidence, label accuracy and FEVER score, respectively, on the test set. Our system ranks 15th among 23 participants in the shared task prior to any human-evaluation of the evidence.<i>Wikipedia Page Retrieval</i>: First we extract the entities in the claim, then we find potential Wikipedia URI candidates for each of the entities using a SPARQL query over DBpedia 2) <i>Sentence selection</i>: We investigate various techniques i.e. Smooth Inverse Frequency (SIF), Word Mover&#8217;s Distance (WMD), Soft-Cosine Similarity, Cosine similarity with unigram Term Frequency Inverse Document Frequency (TF-IDF) to rank sentences by their similarity to the claim. 3) <i>Textual Entailment</i>: We compare three models for the task of claim classification. We apply a Decomposable Attention (DA) model (Parikh et al., 2016), a Decomposed Graph Entailment (DGE) model (Khot et al., 2018) and a Gradient-Boosted Decision Trees (TalosTree) model (Sean et al., 2017) for this task. The experiments show that the pipeline with simple Cosine Similarity using TFIDF in sentence selection along with DA model as labelling model achieves the best results on the development set (F1 evidence: 32.17, label accuracy: 59.61 and FEVER score: 0.3778). Furthermore, it obtains 30.19, 48.87 and 36.55 in terms of F1 evidence, label accuracy and FEVER score, respectively, on the test set. Our system ranks 15th among 23 participants in the shared task prior to any human-evaluation of the evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5520 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5520/>Integrating Entity Linking and Evidence Ranking for Fact Extraction and Verification</a></strong><br><a href=/people/m/motoki-taniguchi/>Motoki Taniguchi</a>
|
<a href=/people/t/tomoki-taniguchi/>Tomoki Taniguchi</a>
|
<a href=/people/t/takumi-takahashi/>Takumi Takahashi</a>
|
<a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/t/tomoko-ohkuma/>Tomoko Ohkuma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5520><div class="card-body p-3 small">We describe here our <a href=https://en.wikipedia.org/wiki/System>system</a> and results on the FEVER shared task. We prepared a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline system</a> which composes of a document selection, a sentence retrieval, and a recognizing textual entailment (RTE) components. A simple entity linking approach with text match is used as the document selection component, this component identifies relevant documents for a given claim by using mentioned entities as clues. The sentence retrieval component selects relevant sentences as candidate evidence from the documents based on <a href=https://en.wikipedia.org/wiki/TF-IDF>TF-IDF</a>. Finally, the RTE component selects evidence sentences by ranking the sentences and classifies the claim simultaneously. The experimental results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved the <a href=https://en.wikipedia.org/wiki/FEVER>FEVER score</a> of 0.4016 and outperformed the official baseline system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5522.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5522 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5522 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5522" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5522/>DeFactoNLP : Fact Verification using Entity Recognition, TFIDF Vector Comparison and Decomposable Attention<span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>F</span>acto<span class=acl-fixed-case>NLP</span>: Fact Verification using Entity Recognition, <span class=acl-fixed-case>TFIDF</span> Vector Comparison and Decomposable Attention</a></strong><br><a href=/people/a/aniketh-janardhan-reddy/>Aniketh Janardhan Reddy</a>
|
<a href=/people/g/gil-rocha/>Gil Rocha</a>
|
<a href=/people/d/diego-esteves/>Diego Esteves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5522><div class="card-body p-3 small">In this paper, we describe DeFactoNLP, the <a href=https://en.wikipedia.org/wiki/System>system</a> we designed for the FEVER 2018 Shared Task. The aim of this task was to conceive a system that can not only automatically assess the veracity of a claim but also retrieve evidence supporting this assessment from Wikipedia. In our approach, the Wikipedia documents whose Term Frequency-Inverse Document Frequency (TFIDF) vectors are most similar to the vector of the claim and those documents whose names are similar to those of the named entities (NEs) mentioned in the claim are identified as the documents which might contain evidence. The sentences in these documents are then supplied to a textual entailment recognition module. This module calculates the probability of each sentence supporting the claim, contradicting the claim or not providing any relevant information to assess the veracity of the claim. Various <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> computed using these <a href=https://en.wikipedia.org/wiki/Probability>probabilities</a> are finally used by a Random Forest classifier to determine the overall truthfulness of the claim. The sentences which support this <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification</a> are returned as evidence. Our approach achieved a 0.4277 evidence F1-score, a 0.5136 label accuracy and a 0.3833 FEVER score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5523/>An End-to-End Multi-task Learning Model for <a href=https://en.wikipedia.org/wiki/Fact-checking>Fact Checking</a></a></strong><br><a href=/people/s/sizhen-li/>Sizhen Li</a>
|
<a href=/people/s/shuai-zhao/>Shuai Zhao</a>
|
<a href=/people/b/bo-cheng/>Bo Cheng</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5523><div class="card-body p-3 small">With huge amount of information generated every day on the web, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a> is an important and challenging task which can help people identify the authenticity of most claims as well as providing evidences selected from knowledge source like <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. Here we decompose this problem into two parts : an entity linking task (retrieving relative Wikipedia pages) and recognizing textual entailment between the claim and selected pages. In this paper, we present an end-to-end multi-task learning with bi-direction attention (EMBA) model to classify the claim as supports, refutes or not enough info with respect to the pages retrieved and detect sentences as evidence at the same time. We conduct experiments on the FEVER (Fact Extraction and VERification) paper test dataset and shared task test dataset, a new public dataset for verification against textual sources. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves comparable performance compared with the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5524 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5524/>Team GESIS Cologne : An all in all sentence-based approach for FEVER<span class=acl-fixed-case>GESIS</span> Cologne: An all in all sentence-based approach for <span class=acl-fixed-case>FEVER</span></a></strong><br><a href=/people/w/wolfgang-otto/>Wolfgang Otto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5524><div class="card-body p-3 small">In this system description of our <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> to participate at the Fever Shared Task, we describe our sentence-based approach. Throughout all steps of our <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a>, we regarded single sentences as our processing unit. In our IR-Component, we searched in the set of all possible Wikipedia introduction sentences without limiting sentences to a fixed number of relevant documents. In the entailment module, we judged every sentence separately and combined the result of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> for the top 5 sentences with the help of an ensemble classifier to make a judgment whether the truth of a statement can be derived from the given claim.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5525 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5525/>Team SWEEPer : Joint Sentence Extraction and Fact Checking with Pointer Networks<span class=acl-fixed-case>SWEEP</span>er: Joint Sentence Extraction and Fact Checking with Pointer Networks</a></strong><br><a href=/people/c/christopher-hidey/>Christopher Hidey</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5525><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> rely on information extracted from unreliable sources. These <a href=https://en.wikipedia.org/wiki/System>systems</a> would thus benefit from knowing whether a statement from an unreliable source is correct. We present experiments on the FEVER (Fact Extraction and VERification) task, a shared task that involves selecting sentences from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and predicting whether a claim is supported by those sentences, refuted, or there is not enough information. Fact checking is a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that benefits from not only asserting or disputing the veracity of a claim but also finding evidence for that position. As these tasks are dependent on each other, an ideal model would consider the veracity of the claim when finding evidence and also find only the evidence that is relevant. We thus jointly model <a href=https://en.wikipedia.org/wiki/Sentence_extraction>sentence extraction</a> and <a href=https://en.wikipedia.org/wiki/Verification_and_validation>verification</a> on the FEVER shared task. Among all participants, we ranked 5th on the <a href=https://en.wikipedia.org/wiki/Blinded_experiment>blind test set</a> (prior to any additional human evaluation of the evidence).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5527 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5527/>Team UMBC-FEVER : Claim verification using Semantic Lexical Resources<span class=acl-fixed-case>UMBC</span>-<span class=acl-fixed-case>FEVER</span> : Claim verification using Semantic Lexical Resources</a></strong><br><a href=/people/a/ankur-padia/>Ankur Padia</a>
|
<a href=/people/f/francis-ferraro/>Francis Ferraro</a>
|
<a href=/people/t/tim-finin/>Tim Finin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5527><div class="card-body p-3 small">We describe our <a href=https://en.wikipedia.org/wiki/System>system</a> used in the 2018 FEVER shared task. The system employed a frame-based information retrieval approach to select Wikipedia sentences providing evidence and used a <a href=https://en.wikipedia.org/wiki/Multilayer_perceptron>two-layer multilayer perceptron</a> to classify a claim as correct or not. Our submission achieved a score of 0.3966 on the Evidence F1 metric with accuracy of 44.79 %, and FEVER score of 0.2628 F1 points.</div></div></div><hr><div id=w18-56><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-56.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-56/>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5600/>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</a></strong><br><a href=/people/a/alberto-lavelli/>Alberto Lavelli</a>
|
<a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/f/fabio-rinaldi/>Fabio Rinaldi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5604/>Supervised Machine Learning for Extractive Query Based Summarisation of Biomedical Data</a></strong><br><a href=/people/m/mandeep-kaur/>Mandeep Kaur</a>
|
<a href=/people/d/diego-molla/>Diego Mollá</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5604><div class="card-body p-3 small">The automation of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarisation</a> of <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical publications</a> is a pressing need due to the plethora of information available online. This paper explores the impact of several supervised machine learning approaches for extracting multi-document summaries for given queries. In particular, we compare classification and regression approaches for query-based extractive summarisation using data provided by the BioASQ Challenge. We tackled the problem of annotating sentences for training classification systems and show that a simple annotation approach outperforms regression-based summarisation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5605.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5605 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5605 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5605/>Comparing CNN and LSTM character-level embeddings in BiLSTM-CRF models for chemical and disease named entity recognition<span class=acl-fixed-case>CNN</span> and <span class=acl-fixed-case>LSTM</span> character-level embeddings in <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> models for chemical and disease named entity recognition</a></strong><br><a href=/people/z/zenan-zhai/>Zenan Zhai</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5605><div class="card-body p-3 small">We compare the use of LSTM-based and CNN-based character-level word embeddings in BiLSTM-CRF models to approach chemical and disease named entity recognition (NER) tasks. Empirical results over the BioCreative V CDR corpus show that the use of either type of character-level word embeddings in conjunction with the BiLSTM-CRF models leads to comparable state-of-the-art performance. However, the models using CNN-based character-level word embeddings have a computational performance advantage, increasing <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a> over word-based models by 25 % while the LSTM-based character-level word embeddings more than double the required <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5607/>Investigating the Challenges of Temporal Relation Extraction from Clinical Text</a></strong><br><a href=/people/d/diana-galvan/>Diana Galvan</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/koji-matsuda/>Koji Matsuda</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5607><div class="card-body p-3 small">Temporal reasoning remains as an unsolved task for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>, particularly demonstrated in the <a href=https://en.wikipedia.org/wiki/Clinical_psychology>clinical domain</a>. The complexity of temporal representation in language is evident as results of the 2016 Clinical TempEval challenge indicate : the current state-of-the-art systems perform well in solving mention-identification tasks of event and time expressions but poorly in temporal relation extraction, showing a gap of around 0.25 point below human performance. We explore to adapt the tree-based LSTM-RNN model proposed by Miwa and Bansal (2016) to temporal relation extraction from clinical text, obtaining a five point improvement over the best 2016 Clinical TempEval system and two points over the state-of-the-art. We deliver a deep analysis of the results and discuss the next step towards human-like temporal reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5608 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5608/>De-identifying Free Text of Japanese Dummy Electronic Health Records<span class=acl-fixed-case>J</span>apanese Dummy Electronic Health Records</a></strong><br><a href=/people/k/kohei-kajiyama/>Kohei Kajiyama</a>
|
<a href=/people/h/hiromasa-horiguchi/>Hiromasa Horiguchi</a>
|
<a href=/people/t/takashi-okumura/>Takashi Okumura</a>
|
<a href=/people/m/mizuki-morita/>Mizuki Morita</a>
|
<a href=/people/y/yoshinobu-kano/>Yoshinobu Kano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5608><div class="card-body p-3 small">A new law was established in Japan to promote utilization of <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHRs</a> for research and developments, while <a href=https://en.wikipedia.org/wiki/De-identification>de-identification</a> is required to use <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EHRs</a>. However, studies of automatic de-identification in the healthcare domain is not active for <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese language</a>, no de-identification tool available in practical performance for Japanese medical domains, as far as we know. Previous work shows that rule-based methods are still effective, while <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> are reported to be better recently. In order to implement and evaluate a de-identification tool in a practical level, we implemented three methods, rule-based, CRF, and LSTM. We prepared three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> of pseudo EHRs with de-identification tags manually annotated. These <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are derived from shared task data to compare with previous work, and our new data to increase training data. Our result shows that our LSTM-based method is better and robust, which leads to our future work that plans to apply our <a href=https://en.wikipedia.org/wiki/System>system</a> to actual de-identification tasks in hospitals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5609/>Unsupervised Identification of Study Descriptors in Toxicology Research : An Experimental Study</a></strong><br><a href=/people/d/drahomira-herrmannova/>Drahomira Herrmannova</a>
|
<a href=/people/s/steve-young/>Steven Young</a>
|
<a href=/people/r/robert-patton/>Robert Patton</a>
|
<a href=/people/c/christopher-stahl/>Christopher Stahl</a>
|
<a href=/people/n/nicole-kleinstreuer/>Nicole Kleinstreuer</a>
|
<a href=/people/m/mary-wolfe/>Mary Wolfe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5609><div class="card-body p-3 small">Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised manner</a>. Specifically, provided a set of criteria describing specific study parameters, such as species, route of administration, and dosing regimen, we develop an unsupervised approach to identify text segments (sentences) relevant to the criteria. A <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classifier</a> trained to identify publications that met the criteria performs better when trained on the candidate sentences than when trained on sentences randomly picked from the text, supporting the intuition that our method is able to accurately identify study descriptors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5613.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5613 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5613 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5613/>Iterative development of family history annotation guidelines using a synthetic corpus of clinical text</a></strong><br><a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/p/pal-brekke/>Pål Brekke</a>
|
<a href=/people/o/oystein-nytro/>Øystein Nytrø</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5613><div class="card-body p-3 small">In this article, we describe the development of annotation guidelines for <a href=https://en.wikipedia.org/wiki/Family_history_(medicine)>family history information</a> in <a href=https://en.wikipedia.org/wiki/Medical_literature>Norwegian clinical text</a>. We make use of incrementally developed synthetic clinical text describing patients&#8217; family history relating to cases of cardiac disease and present a general methodology which integrates the synthetically produced clinical statements and guideline development. We analyze <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a> based on the developed guidelines and present results from experiments aimed at evaluating the validity and applicability of the <a href=https://en.wikipedia.org/wiki/Text_corpus>annotated corpus</a> using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning techniques</a>. The resulting annotated corpus contains 477 sentences and 6030 tokens. Both the annotation guidelines and the annotated corpus are made freely available and as such constitutes the first publicly available resource of Norwegian clinical text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5615.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5615 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5615 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5615/>Analysis of Risk Factor Domains in Psychosis Patient Health Records</a></strong><br><a href=/people/e/eben-holderness/>Eben Holderness</a>
|
<a href=/people/n/nicholas-miller/>Nicholas Miller</a>
|
<a href=/people/k/kirsten-bolton/>Kirsten Bolton</a>
|
<a href=/people/p/philip-cawkwell/>Philip Cawkwell</a>
|
<a href=/people/m/marie-meteer/>Marie Meteer</a>
|
<a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/m/mei-hua-hall/>Mei Hua-Hall</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5615><div class="card-body p-3 small">Readmission after discharge from a hospital is disruptive and costly, regardless of the reason. However, it can be particularly problematic for psychiatric patients, so predicting which patients may be readmitted is critically important but also very difficult. Clinical narratives in psychiatric electronic health records (EHRs) span a wide range of topics and vocabulary ; therefore, a psychiatric readmission prediction model must begin with a robust and interpretable topic extraction component. We created a data pipeline for using document vector similarity metrics to perform topic extraction on psychiatric EHR data in service of our long-term goal of creating a readmission risk classifier. We show initial results for our topic extraction model and identify additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> we will be incorporating in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5616.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5616 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5616 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5616/>Patient Risk Assessment and Warning Symptom Detection Using Deep Attention-Based Neural Networks</a></strong><br><a href=/people/i/ivan-girardi/>Ivan Girardi</a>
|
<a href=/people/p/pengfei-ji/>Pengfei Ji</a>
|
<a href=/people/a/an-phi-nguyen/>An-phi Nguyen</a>
|
<a href=/people/n/nora-hollenstein/>Nora Hollenstein</a>
|
<a href=/people/a/adam-ivankay/>Adam Ivankay</a>
|
<a href=/people/l/lorenz-kuhn/>Lorenz Kuhn</a>
|
<a href=/people/c/chiara-marchiori/>Chiara Marchiori</a>
|
<a href=/people/c/ce-zhang/>Ce Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5616><div class="card-body p-3 small">We present an operational component of a real-world patient triage system. Given a specific patient presentation, the system is able to assess the level of medical urgency and issue the most appropriate recommendation in terms of best point of care and time to treat. We use an attention-based convolutional neural network architecture trained on 600,000 <a href=https://en.wikipedia.org/wiki/Medical_record>doctor notes</a> in <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We compare two approaches, one that uses the full text of the medical notes and one that uses only a selected list of medical entities extracted from the text. These approaches achieve 79 % and 66 % <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a>, respectively, but on a <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence threshold</a> of 0.6, <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> increases to 85 % and 75 %, respectively. In addition, a method to detect <a href=https://en.wikipedia.org/wiki/Atelectasis>warning symptoms</a> is implemented to render the classification task transparent from a medical perspective. The <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is based on the learning of attention scores and a method of <a href=https://en.wikipedia.org/wiki/Data_validation>automatic validation</a> using the same data.<i>point of care</i> and <i>time to treat</i>. We use an attention-based convolutional neural network architecture trained on 600,000 doctor notes in German. We compare two approaches, one that uses the full text of the medical notes and one that uses only a selected list of medical entities extracted from the text. These approaches achieve 79% and 66% precision, respectively, but on a confidence threshold of 0.6, precision increases to 85% and 75%, respectively. In addition, a method to detect <i>warning symptoms</i> is implemented to render the classification task transparent from a medical perspective. The method is based on the learning of attention scores and a method of automatic validation using the same data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5617.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5617 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5617 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5617/>Syntax-based Transfer Learning for the Task of Biomedical Relation Extraction</a></strong><br><a href=/people/j/joel-legrand/>Joël Legrand</a>
|
<a href=/people/y/yannick-toussaint/>Yannick Toussaint</a>
|
<a href=/people/c/chedy-raissi/>Chedy Raïssi</a>
|
<a href=/people/a/adrien-coulet/>Adrien Coulet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5617><div class="card-body p-3 small">Transfer learning (TL) proposes to enhance <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> performance on a <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>, by reusing labeled data originally designed for a related problem. In particular, <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> consists, for a specific <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, in reusing training data developed for the same <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> but a distinct domain. This is particularly relevant to the applications of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>, because those usually require large annotated corpora that may not exist for the targeted domain, but exist for side domains. In this paper, we experiment with TL for the task of Relation Extraction (RE) from biomedical texts, using the TreeLSTM model. We empirically show the impact of TreeLSTM alone and with domain adaptation by obtaining better performances than the state of the art on two biomedical RE tasks and equal performances for two others, for which few annotated data are available. Furthermore, we propose an analysis of the role that syntactic features may play in TL for RE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5618.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5618 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5618/>In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition</a></strong><br><a href=/people/g/golnar-sheikhshab/>Golnar Sheikhshabbafghi</a>
|
<a href=/people/i/inanc-birol/>Inanc Birol</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5618><div class="card-body p-3 small">Rapidly expanding volume of publications in the biomedical domain makes it increasingly difficult for a timely evaluation of the latest literature. That, along with a push for automated evaluation of clinical reports, present opportunities for effective natural language processing methods. In this study we target the problem of <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, where texts are processed to annotate terms that are relevant for <a href=https://en.wikipedia.org/wiki/Medical_research>biomedical studies</a>. Terms of interest in the domain include gene and protein names, and cell lines and types. Here we report on a pipeline built on Embeddings from Language Models (ELMo) and a deep learning package for natural language processing (AllenNLP). We trained context-aware token embeddings on a dataset of biomedical papers using ELMo, and incorporated these embeddings in the LSTM-CRF model used by AllenNLP for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. We show these <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> improve <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> for different types of biomedical named entities. We also achieve a new state of the art in gene mention detection on the BioCreative II gene mention shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5619.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5619 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5619/>Self-training improves <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a> performance for Temporal Relation Extraction</a></strong><br><a href=/people/c/chen-lin/>Chen Lin</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/d/dmitriy-dligach/>Dmitriy Dligach</a>
|
<a href=/people/h/hadi-amiri/>Hadi Amiri</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5619><div class="card-body p-3 small">Neural network models are oftentimes restricted by limited labeled instances and resort to advanced <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> and <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> for cutting edge performance. We propose to build a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> with multiple semantically heterogeneous embeddings within a self-training framework. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> makes use of labeled, unlabeled, and social media data, operates on basic features, and is scalable and generalizable. With this method, we establish the state-of-the-art result for both in- and cross-domain for a clinical temporal relation extraction task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5620.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5620 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5620 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5620" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5620/>Listwise temporal ordering of events in clinical notes</a></strong><br><a href=/people/s/serena-jeblee/>Serena Jeblee</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5620><div class="card-body p-3 small">We present metrics for listwise temporal ordering of events in clinical notes, as well as a baseline listwise temporal ranking model that generates a timeline of events that can be used in downstream medical natural language processing tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5622 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5622" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5622/>Evaluation of a Sequence Tagging Tool for Biomedical Texts</a></strong><br><a href=/people/j/julien-tourille/>Julien Tourille</a>
|
<a href=/people/m/matthieu-doutreligne/>Matthieu Doutreligne</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/n/nicolas-paris/>Nicolas Paris</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5622><div class="card-body p-3 small">Many applications in biomedical natural language processing rely on sequence tagging as an initial step to perform more complex analysis. To support text analysis in the biomedical domain, we introduce Yet Another SEquence Tagger (YASET), an open-source multi purpose sequence tagger that implements state-of-the-art deep learning algorithms for sequence tagging. Herein, we evaluate YASET on <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> in a variety of text genres including articles from the biomedical literature in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and clinical narratives in <a href=https://en.wikipedia.org/wiki/French_language>French</a>. To further characterize performance, we report distributions over 30 runs and different sizes of training datasets. YASET provides state-of-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81). We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts.</div></div></div><hr><div id=w18-57><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-57.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-57/>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5700/>Proceedings of the 2018 <span class=acl-fixed-case>EMNLP</span> Workshop <span class=acl-fixed-case>SCAI</span>: The 2nd International Workshop on Search-Oriented Conversational <span class=acl-fixed-case>AI</span></a></strong><br><a href=/people/a/aleksandr-chuklin/>Aleksandr Chuklin</a>
|
<a href=/people/j/jeff-dalton/>Jeff Dalton</a>
|
<a href=/people/j/julia-kiseleva/>Julia Kiseleva</a>
|
<a href=/people/a/alexey-borisov/>Alexey Borisov</a>
|
<a href=/people/m/mikhail-burtsev/>Mikhail Burtsev</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5701 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5701" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5701/>Neural Response Ranking for Social Conversation : A Data-Efficient Approach</a></strong><br><a href=/people/i/igor-shalyminov/>Igor Shalyminov</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/o/oliver-lemon/>Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5701><div class="card-body p-3 small">The overall objective of &#8216;social&#8217; dialogue systems is to support engaging, entertaining, and lengthy conversations on a wide variety of topics, including social chit-chat. Apart from raw dialogue data, user-provided ratings are the most common signal used to train such systems to produce engaging responses. In this paper we show that social dialogue systems can be trained effectively from <a href=https://en.wikipedia.org/wiki/Raw_data>raw unannotated data</a>. Using a dataset of real conversations collected in the 2017 Alexa Prize challenge, we developed a neural ranker for selecting &#8216;good&#8217; system responses to <a href=https://en.wikipedia.org/wiki/User-generated_content>user utterances</a>, i.e. responses which are likely to lead to long and engaging conversations. We show that (1) our neural ranker consistently outperforms several strong baselines when trained to optimise for user ratings ; (2) when trained on larger amounts of data and only using conversation length as the objective, the <a href=https://en.wikipedia.org/wiki/Ranker>ranker</a> performs better than the one trained using ratings ultimately reaching a Precision@1 of 0.87. This advance will make <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> for social conversational agents simpler and less expensive in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5708/>Data Augmentation for Neural Online Chats Response Selection</a></strong><br><a href=/people/w/wenchao-du/>Wenchao Du</a>
|
<a href=/people/a/alan-w-black/>Alan Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5708><div class="card-body p-3 small">Data augmentation seeks to manipulate the available data for training to improve the generalization ability of models. We investigate two data augmentation proxies, permutation and flipping, for neural dialog response selection task on various models over multiple datasets, including both Chinese and English languages. Different from standard data augmentation techniques, our method combines the original and synthesized data for <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. Empirical results show that our approach can gain 1 to 3 recall-at-1 points over baseline models in both full-scale and small-scale settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5710 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5710/>Embedding Individual Table Columns for Resilient SQL Chatbots<span class=acl-fixed-case>SQL</span> Chatbots</a></strong><br><a href=/people/b/bojan-petrovski/>Bojan Petrovski</a>
|
<a href=/people/i/ignacio-aguado/>Ignacio Aguado</a>
|
<a href=/people/a/andreea-hossmann/>Andreea Hossmann</a>
|
<a href=/people/m/michael-baeriswyl/>Michael Baeriswyl</a>
|
<a href=/people/c/claudiu-musat/>Claudiu Musat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5710><div class="card-body p-3 small">Most of the world&#8217;s data is stored in <a href=https://en.wikipedia.org/wiki/Relational_database>relational databases</a>. Accessing these requires specialized knowledge of the Structured Query Language (SQL), putting them out of the reach of many people. A recent research thread in Natural Language Processing (NLP) aims to alleviate this problem by automatically translating natural language questions into SQL queries. While the proposed solutions are a great start, they lack robustness and do not easily generalize : the methods require high quality descriptions of the <a href=https://en.wikipedia.org/wiki/Column_(database)>database table columns</a>, and the most widely used training dataset, WikiSQL, is heavily biased towards using those descriptions as part of the questions. In this work, we propose solutions to both problems : we entirely eliminate the need for column descriptions, by relying solely on their contents, and we augment the WikiSQL dataset by paraphrasing column names to reduce bias. We show that the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of existing methods drops when trained on our augmented, column-agnostic dataset, and that our own method reaches state of the art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, while relying on column contents only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5711/>Exploring Named Entity Recognition As an Auxiliary Task for Slot Filling in Conversational Language Understanding</a></strong><br><a href=/people/s/samuel-louvan/>Samuel Louvan</a>
|
<a href=/people/b/bernardo-magnini/>Bernardo Magnini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5711><div class="card-body p-3 small">Slot filling is a crucial task in the Natural Language Understanding (NLU) component of a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a>. Most approaches for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> rely solely on the domain-specific datasets for training. We propose a joint model of slot filling and Named Entity Recognition (NER) in a multi-task learning (MTL) setup. Our experiments on three slot filling datasets show that using NER as an auxiliary task improves slot filling performance and achieve competitive performance compared with <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. In particular, NER is effective when supervised at the lower layer of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. For low-resource scenarios, we found that MTL is effective for one dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5712.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5712 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5712 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5712/>Why are Sequence-to-Sequence Models So Dull? Understanding the Low-Diversity Problem of Chatbots</a></strong><br><a href=/people/s/shaojie-jiang/>Shaojie Jiang</a>
|
<a href=/people/m/maarten-de-rijke/>Maarten de Rijke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5712><div class="card-body p-3 small">Diversity is a long-studied topic in <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> that usually refers to the requirement that retrieved results should be non-repetitive and cover different aspects. In a conversational setting, an additional dimension of <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> matters : an engaging response generation system should be able to output responses that are diverse and interesting. Sequence-to-sequence (Seq2Seq) models have been shown to be very effective for response generation. However, dialogue responses generated by Seq2Seq models tend to have <a href=https://en.wikipedia.org/wiki/Diversity_index>low diversity</a>. In this paper, we review known sources and existing approaches to this low-diversity problem. We also identify a source of low diversity that has been little studied so far, namely model over-confidence. We sketch several directions for tackling model over-confidence and, hence, the low-diversity problem, including <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence penalties</a> and label smoothing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5713 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5713/>Retrieve and Refine : Improved Sequence Generation Models For Dialogue</a></strong><br><a href=/people/j/jason-weston/>Jason Weston</a>
|
<a href=/people/e/emily-dinan/>Emily Dinan</a>
|
<a href=/people/a/alexander-miller/>Alexander Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5713><div class="card-body p-3 small">Sequence generation models for <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> are known to have several problems : they tend to produce short, generic sentences that are uninformative and unengaging. Retrieval models on the other hand can surface interesting responses, but are restricted to the given retrieval set leading to erroneous replies that can not be tuned to the specific context. In this work we develop a model that combines the two approaches to avoid both their deficiencies : first retrieve a response and then refine it the final sequence generator treating the retrieval as additional context. We show on the recent ConvAI2 challenge task our approach produces responses superior to both standard retrieval and generation models in human evaluations.</div></div></div><hr><div id=w18-58><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-58.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-58/>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5800/>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</a></strong><br><a href=/people/s/sandra-kubler/>Sandra Kuebler</a>
|
<a href=/people/g/garrett-nicolai/>Garrett Nicolai</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5801/>Efficient Computation of Implicational Universals in Constraint-Based Phonology Through the <a href=https://en.wikipedia.org/wiki/Hyperplane_separation_theorem>Hyperplane Separation Theorem</a></a></strong><br><a href=/people/g/giorgio-magri/>Giorgio Magri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5801><div class="card-body p-3 small">This paper focuses on the most basic implicational universals in <a href=https://en.wikipedia.org/wiki/Phonology>phonological theory</a>, called T-orders after Anttila and Andrus (2006). It develops necessary and sufficient constraint characterizations of T-orders within Harmonic Grammar and <a href=https://en.wikipedia.org/wiki/Optimality_theory>Optimality Theory</a>. These conditions rest on the rich convex geometry underlying these frameworks. They are phonologically intuitive and have significant algorithmic implications.<i>implicational universals</i> in phonological theory, called <i>T-orders</i> after Anttila and Andrus (2006). It develops necessary and sufficient constraint characterizations of T-orders within <i>Harmonic Grammar</i> and <i>Optimality Theory</i>. These conditions rest on the rich convex geometry underlying these frameworks. They are phonologically intuitive and have significant algorithmic implications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5802/>Lexical Networks in ! Xung<span class=acl-fixed-case>X</span>ung</a></strong><br><a href=/people/s/syed-amad-hussain/>Syed-Amad Hussain</a>
|
<a href=/people/m/micha-elsner/>Micha Elsner</a>
|
<a href=/people/a/amanda-miller/>Amanda Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5802><div class="card-body p-3 small">We investigate the lexical network properties of the large phoneme inventory Southern African language Mangetti Dune ! Xung as it compares to <a href=https://en.wikipedia.org/wiki/English_language>English</a> and other commonly-studied languages. Lexical networks are graphs in which nodes (words) are linked to their minimal pairs ; global properties of these <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>networks</a> are believed to mediate <a href=https://en.wikipedia.org/wiki/Lexical_access>lexical access</a> in the minds of speakers. We show that the network properties of ! Xung are within the range found in previously-studied languages. By simulating data (pseudolexicons) with varying levels of phonotactic structure, we find that the lexical network properties of ! Xung diverge from previously-studied languages when fewer phonotactic constraints are retained. We conclude that lexical network properties are representative of an underlying cognitive structure which is necessary for efficient word retrieval and that the <a href=https://en.wikipedia.org/wiki/Phonotactics>phonotactics</a> of ! Xung may be shaped by a selective pressure which preserves network properties within this cognitively useful range.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5803/>Acoustic Word Disambiguation with Phonogical Features in Danish ASR<span class=acl-fixed-case>D</span>anish <span class=acl-fixed-case>ASR</span></a></strong><br><a href=/people/a/andreas-soeborg-kirkedal/>Andreas Søeborg Kirkedal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5803><div class="card-body p-3 small">Phonological features can indicate <a href=https://en.wikipedia.org/wiki/Word_class>word class</a> and we can use <a href=https://en.wikipedia.org/wiki/Word_class>word class information</a> to disambiguate both <a href=https://en.wikipedia.org/wiki/Homophone>homophones</a> and <a href=https://en.wikipedia.org/wiki/Homograph>homographs</a> in automatic speech recognition (ASR). We show Danish std can be predicted from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> and used to improve <a href=https://en.wikipedia.org/wiki/Speech_recognition>ASR</a>. We discover which acoustic features contain the signal of <a href=https://en.wikipedia.org/wiki/Standardized_test>std</a>, how to use these features to predict <a href=https://en.wikipedia.org/wiki/Standardized_test>std</a> and how we can make use of <a href=https://en.wikipedia.org/wiki/Standardized_test>std</a> and stdpredictive acoustic features to improve overall ASR accuracy and decoding speed. In the process, we discover acoustic features that are novel to the phonetic characterisation of std.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5804/>Adaptor Grammars for the Linguist : <a href=https://en.wikipedia.org/wiki/Word_segmentation>Word Segmentation</a> Experiments for Very Low-Resource Languages<span class=acl-fixed-case>A</span>daptor <span class=acl-fixed-case>G</span>rammars for the Linguist: Word Segmentation Experiments for Very Low-Resource Languages</a></strong><br><a href=/people/p/pierre-godard/>Pierre Godard</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a>
|
<a href=/people/m/martine-adda-decker/>Martine Adda-Decker</a>
|
<a href=/people/g/gilles-adda/>Gilles Adda</a>
|
<a href=/people/h/helene-bonneau-maynard/>Hélène Maynard</a>
|
<a href=/people/a/annie-rialland/>Annie Rialland</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5804><div class="card-body p-3 small">Computational Language Documentation attempts to make the most recent research in speech and language technologies available to linguists working on <a href=https://en.wikipedia.org/wiki/Language_preservation>language preservation</a> and documentation. In this paper, we pursue two main goals along these lines. The first is to improve upon a strong baseline for the unsupervised word discovery task on two very low-resource Bantu languages, taking advantage of the expertise of linguists on these particular languages. The second consists in exploring the Adaptor Grammar framework as a decision and prediction tool for linguists studying a new language. We experiment 162 grammar configurations for each language and show that using Adaptor Grammars for <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> enables us to test hypotheses about a language. Specializing a generic grammar with language specific knowledge leads to great improvements for the word discovery task, ultimately achieving a leap of about 30 % token F-score from the results of a strong baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5805/>String Transduction with Target Language Models and Insertion Handling</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/s/saeed-najafi/>Saeed Najafi</a>
|
<a href=/people/g/grzegorz-kondrak/>Grzegorz Kondrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5805><div class="card-body p-3 small">Many character-level tasks can be framed as sequence-to-sequence transduction, where the target is a word from a <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. We show that leveraging target language models derived from unannotated target corpora, combined with a precise alignment of the training data, yields state-of-the art results on cognate projection, inflection generation, and phoneme-to-grapheme conversion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5807/>Modeling Reduplication with 2-way Finite-State Transducers</a></strong><br><a href=/people/h/hossep-dolatian/>Hossep Dolatian</a>
|
<a href=/people/j/jeffrey-heinz/>Jeffrey Heinz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5807><div class="card-body p-3 small">This article describes a novel approach to the computational modeling of reduplication. Reduplication is a well-studied <a href=https://en.wikipedia.org/wiki/Phenomenon>linguistic phenomenon</a>. However, it is often treated as a stumbling block within finite-state treatments of <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphology</a>. Most finite-state implementations of computational morphology can not adequately capture the productivity of unbounded copying in <a href=https://en.wikipedia.org/wiki/Reduplication>reduplication</a>, nor can they adequately capture bounded copying. We show that an understudied type of finite-state machines, two-way finite-state transducers (2-way FSTs), captures virtually all reduplicative processes, including total reduplication. 2-way FSTs can model reduplicative typology in a way which is convenient, easy to design and debug in practice, and linguistically-motivated. By virtue of being finite-state, 2-way FSTs are likewise incorporable into existing <a href=https://en.wikipedia.org/wiki/Finite-state_machine>finite-state systems</a> and <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a>. A small but representative typology of reduplicative processes is described in this article, alongside their corresponding 2-way FST models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5809 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5809/>A Comparison of Entity Matching Methods between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Japanese Katakana<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>J</span>apanese Katakana</a></strong><br><a href=/people/m/michiharu-yamashita/>Michiharu Yamashita</a>
|
<a href=/people/h/hideki-awashima/>Hideki Awashima</a>
|
<a href=/people/h/hidekazu-oiwa/>Hidekazu Oiwa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5809><div class="card-body p-3 small">Japanese Katakana is one component of the <a href=https://en.wikipedia.org/wiki/Japanese_writing_system>Japanese writing system</a> and is used to express English terms, <a href=https://en.wikipedia.org/wiki/Loanword>loanwords</a>, and <a href=https://en.wikipedia.org/wiki/Onomatopoeia>onomatopoeia</a> in <a href=https://en.wikipedia.org/wiki/Japanese_writing_system>Japanese characters</a> based on the <a href=https://en.wikipedia.org/wiki/Japanese_phonology>phonemes</a>. The main purpose of this research is to find the best entity matching methods between <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Katakana. We built two research questions to clarify which types of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity matching systems</a> works better than others. The first question is what <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> should be used for <a href=https://en.wikipedia.org/wiki/Conversion_(word_formation)>conversion</a>. We need to transliterate English or Katakana terms into the same form in order to compute the <a href=https://en.wikipedia.org/wiki/String_similarity>string similarity</a>. We consider five conversions that transliterate <a href=https://en.wikipedia.org/wiki/English_language>English</a> to Katakana directly, <a href=https://en.wikipedia.org/wiki/Katakana>Katakana</a> to English directly, <a href=https://en.wikipedia.org/wiki/English_language>English</a> to Katakana via phoneme, Katakana to <a href=https://en.wikipedia.org/wiki/English_language>English</a> via phoneme, and both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Katakana to phoneme. The second question is what should be used for the <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measure</a> at <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity matching</a>. To investigate the problem, we choose six methods, which are <a href=https://en.wikipedia.org/wiki/Overlap_coefficient>Overlap Coefficient</a>, <a href=https://en.wikipedia.org/wiki/Cosine>Cosine</a>, <a href=https://en.wikipedia.org/wiki/Jaccard>Jaccard</a>, Jaro-Winkler, Levenshtein, and the similarity of the phoneme probability predicted by RNN. Our results show that 1) matching using <a href=https://en.wikipedia.org/wiki/Phoneme>phonemes</a> and conversion of Katakana to <a href=https://en.wikipedia.org/wiki/English_language>English</a> works better than other methods, and 2) the similarity of phonemes outperforms other methods while other similarity score is changed depending on data and models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5810.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5810 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5810 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5810/>Seq2Seq Models with Dropout can Learn Generalizable Reduplication<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq Models with Dropout can Learn Generalizable Reduplication</a></strong><br><a href=/people/b/brandon-prickett/>Brandon Prickett</a>
|
<a href=/people/a/aaron-traylor/>Aaron Traylor</a>
|
<a href=/people/j/joe-pater/>Joe Pater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5810><div class="card-body p-3 small">Natural language reduplication can pose a challenge to neural models of language, and has been argued to require <a href=https://en.wikipedia.org/wiki/Variable_(mathematics)>variables</a> (Marcus et al., 1999). Sequence-to-sequence neural networks have been shown to perform well at a number of other morphological tasks (Cotterell et al., 2016), and produce results that highly correlate with human behavior (Kirov, 2017 ; Kirov & Cotterell, 2018) but do not include any explicit variables in their architecture. We find that they can learn a reduplicative pattern that generalizes to novel segments if they are trained with dropout (Srivastava et al., 2014). We argue that this matches the scope of <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> observed in human reduplication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5811" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5811/>A Characterwise Windowed Approach to Hebrew Morphological Segmentation<span class=acl-fixed-case>H</span>ebrew Morphological Segmentation</a></strong><br><a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5811><div class="card-body p-3 small">This paper presents a novel approach to the segmentation of orthographic word forms in contemporary <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>, focusing purely on splitting without carrying out <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a> or <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>disambiguation</a>. Casting the analysis task as character-wise binary classification and using adjacent character and word-based lexicon-lookup features, this approach achieves over 98 % accuracy on the benchmark SPMRL shared task data for <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>, and 97 % accuracy on a new out of domain Wikipedia dataset, an improvement of 4 % and 5 % over previous state of the art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5812.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5812 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5812 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5812/>Phonetic Vector Representations for Sound Sequence Alignment</a></strong><br><a href=/people/p/pavel-sofroniev/>Pavel Sofroniev</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5812><div class="card-body p-3 small">This study explores a number of data-driven vector representations of the IPA-encoded sound segments for the purpose of sound sequence alignment. We test the alternative <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> based on the alignment accuracy in the context of computational historical linguistics. We show that the data-driven methods consistently do better than linguistically-motivated articulatory-acoustic features. The similarity scores obtained using the data-driven representations in a monolingual context, however, performs worse than the state-of-the-art distance (or similarity) scoring methods proposed in earlier studies of computational historical linguistics. We also show that adapting representations to the task at hand improves the results, yielding alignment accuracy comparable to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5813.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5813 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5813 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5813/>Sounds Wilde. Phonetically Extended Embeddings for Author-Stylized Poetry Generation</a></strong><br><a href=/people/a/aleksey-tikhonov/>Aleksey Tikhonov</a>
|
<a href=/people/i/ivan-p-yamshchikov/>Ivan P. Yamshchikov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5813><div class="card-body p-3 small">This paper addresses author-stylized text generation. Using a version of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> with extended phonetic and semantic embeddings for poetry generation we show that <a href=https://en.wikipedia.org/wiki/Phonetics>phonetics</a> has comparable contribution to the overall model performance as the information on the target author. Phonetic information is shown to be important for <a href=https://en.wikipedia.org/wiki/English_language>English and Russian language</a>. Humans tend to attribute machine generated texts to the target author.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5814 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5814/>On Hapax Legomena and Morphological Productivity</a></strong><br><a href=/people/j/janet-pierrehumbert/>Janet Pierrehumbert</a>
|
<a href=/people/r/ramon-granell/>Ramon Granell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5814><div class="card-body p-3 small">Quantifying and predicting morphological productivity is a long-standing challenge in <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus linguistics</a> and <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a>. The same challenge reappears in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> in the context of handling words that were not seen in the training set (out-of-vocabulary, or OOV, words). Prior research showed that a good indicator of the productivity of a <a href=https://en.wikipedia.org/wiki/Morpheme>morpheme</a> is the number of words involving it that occur exactly once (the hapax legomena). A technical connection was adduced between this result and <a href=https://en.wikipedia.org/wiki/Good-Turing_smoothing>Good-Turing smoothing</a>, which assigns <a href=https://en.wikipedia.org/wiki/Probability_mass_function>probability mass</a> to unseen events on the basis of the simplifying assumption that <a href=https://en.wikipedia.org/wiki/Word_frequency>word frequencies</a> are stationary. In a large-scale study of 133 affixes in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, we develop evidence that success in fact depends on tapping the frequency range in which the assumptions of Good-Turing are violated.<i>hapax legomena</i>). A technical connection was adduced between this result and Good-Turing smoothing, which assigns probability mass to unseen events on the basis of the simplifying assumption that word frequencies are stationary. In a large-scale study of 133 affixes in Wikipedia, we develop evidence that success in fact depends on tapping the frequency range in which the assumptions of Good-Turing are violated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5816.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5816 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5816 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5816/>An Arabic Morphological Analyzer and Generator with Copious Features<span class=acl-fixed-case>A</span>rabic Morphological Analyzer and Generator with Copious Features</a></strong><br><a href=/people/d/dima-taji/>Dima Taji</a>
|
<a href=/people/s/salam-khalifa/>Salam Khalifa</a>
|
<a href=/people/o/ossama-obeid/>Ossama Obeid</a>
|
<a href=/people/f/fadhl-eryani/>Fadhl Eryani</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5816><div class="card-body p-3 small">We introduce CALIMA-Star, a very rich Arabic morphological analyzer and generator that provides functional and form-based morphological features as well as built-in tokenization, phonological representation, lexical rationality and much more. This tool includes a fast engine that can be easily integrated into other <a href=https://en.wikipedia.org/wiki/System>systems</a>, as well as an easy-to-use API and a <a href=https://en.wikipedia.org/wiki/User_interface>web interface</a>. CALIMA-Star also supports morphological reinflection. We evaluate CALIMA-Star against four commonly used analyzers for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> in terms of speed and morphological content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5817.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5817 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5817 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5817/>Sanskrit n-Retroflexion is Input-Output Tier-Based Strictly Local<span class=acl-fixed-case>S</span>anskrit n-Retroflexion is Input-Output Tier-Based Strictly Local</a></strong><br><a href=/people/t/thomas-graf/>Thomas Graf</a>
|
<a href=/people/c/connor-mayer/>Connor Mayer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5817><div class="card-body p-3 small">Sanskrit /n/-retroflexion is one of the most complex segmental processes in <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a>. While <a href=https://en.wikipedia.org/wiki/It_(novel)>it</a> is still star-free, <a href=https://en.wikipedia.org/wiki/It_(novel)>it</a> does not fit in any of the subregular classes that are commonly entertained in the literature. We show that when construed as a phonotactic dependency, the process fits into a <a href=https://en.wikipedia.org/wiki/Class_(computer_programming)>class</a> we call input-output tier-based strictly local (IO-TSL), a natural extension of the familiar class TSL. IO-TSL increases the power of TSL&#8217;s tier projection function by making it an input-output strictly local transduction. Assuming that /n/-retroflexion represents the upper bound on the complexity of segmental phonology, this shows that all of segmental phonology can be captured by combining the intuitive notion of tiers with the independently motivated machinery of strictly local mappings.<i>input-output tier-based strictly local</i> (IO-TSL), a natural extension of the familiar class TSL. IO-TSL increases the power of TSL&#8217;s tier projection function by making it an input-output strictly local transduction. Assuming that /n/-retroflexion represents the upper bound on the complexity of segmental phonology, this shows that all of segmental phonology can be captured by combining the intuitive notion of tiers with the independently motivated machinery of strictly local mappings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5818.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5818 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5818 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5818/>Phonological Features for Morphological Inflection</a></strong><br><a href=/people/a/adam-wiemerslage/>Adam Wiemerslage</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5818><div class="card-body p-3 small">Modeling morphological inflection is an important task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. In contrast to earlier work that has largely used <a href=https://en.wikipedia.org/wiki/Orthographic_representation>orthographic representations</a>, we experiment with this task in a phonetic character space, representing inputs as either <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>IPA segments</a> or bundles of <a href=https://en.wikipedia.org/wiki/Distinctive_feature>phonological distinctive features</a>. We show that both of these inputs, somewhat counterintuitively, achieve similar accuracies on morphological inflection, slightly lower than orthographic models. We conclude that providing detailed phonological representations is largely redundant when compared to <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>IPA segments</a>, and that articulatory distinctions relevant for <a href=https://en.wikipedia.org/wiki/Inflection>word inflection</a> are already latently present in the distributional properties of many graphemic writing systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5819.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5819 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5819 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5819/>Extracting Morphophonology from Small Corpora</a></strong><br><a href=/people/m/marina-ermolaeva/>Marina Ermolaeva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5819><div class="card-body p-3 small">Probabilistic approaches have proven themselves well in learning <a href=https://en.wikipedia.org/wiki/Phonology>phonological structure</a>. In contrast, <a href=https://en.wikipedia.org/wiki/Theoretical_linguistics>theoretical linguistics</a> usually works with deterministic generalizations. The goal of this paper is to explore possible interactions between information-theoretic methods and deterministic linguistic knowledge and to examine some ways in which both can be used in tandem to extract phonological and morphophonological patterns from a small annotated dataset. Local and nonlocal processes in Mishar Tatar (Turkic / Kipchak) are examined as a case study.</div></div></div><hr><div id=w18-59><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-59.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-59/>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop & Shared Task</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5900/>Proceedings of the 2018 <span class=acl-fixed-case>EMNLP</span> Workshop <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span>: The 3rd Social Media Mining for Health Applications Workshop & Shared Task</a></strong><br><a href=/people/g/graciela-gonzalez/>Graciela Gonzalez-Hernandez</a>
|
<a href=/people/d/davy-weissenbacher/>Davy Weissenbacher</a>
|
<a href=/people/a/abeed-sarker/>Abeed Sarker</a>
|
<a href=/people/m/michael-paul/>Michael Paul</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5901 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5901/>Football and Beer - a Social Media Analysis on <span class=acl-fixed-case>T</span>witter in Context of the <span class=acl-fixed-case>FIFA</span> Football World Cup 2018</a></strong><br><a href=/people/r/roland-roller/>Roland Roller</a>
|
<a href=/people/p/philippe-thomas/>Philippe Thomas</a>
|
<a href=/people/s/sven-schmeier/>Sven Schmeier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5901><div class="card-body p-3 small">In many societies alcohol is a legal and common recreational substance and socially accepted. Alcohol consumption often comes along with social events as it helps people to increase their sociability and to overcome their inhibitions. On the other hand we know that increased alcohol consumption can lead to serious health issues, such as cancer, cardiovascular diseases and diseases of the digestive system, to mention a few. This work examines alcohol consumption during the FIFA Football World Cup 2018, particularly the usage of alcohol related information on Twitter. For this we analyse the tweeting behaviour and show that the tournament strongly increases the interest in beer. Furthermore we show that countries who had to leave the tournament at early stage might have done something good to their fans as the interest in beer decreased again.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5902/>Stance-Taking in Topics Extracted from Vaccine-Related Tweets and Discussion Forum Posts</a></strong><br><a href=/people/m/maria-skeppstedt/>Maria Skeppstedt</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a>
|
<a href=/people/a/andreas-kerren/>Andreas Kerren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5902><div class="card-body p-3 small">The occurrence of stance-taking towards vaccination was measured in documents extracted by topic modelling from two different <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, one discussion forum corpus and one tweet corpus. For some of the topics extracted, their most closely associated documents contained a proportion of vaccine stance-taking texts that exceeded the corpus average by a large margin. These extracted document sets would, therefore, form a useful resource in a process for computer-assisted analysis of argumentation on the subject of vaccination.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5904/>Overview of the Third Social Media Mining for Health (SMM4H) Shared Tasks at EMNLP 2018<span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span>) Shared Tasks at <span class=acl-fixed-case>EMNLP</span> 2018</a></strong><br><a href=/people/d/davy-weissenbacher/>Davy Weissenbacher</a>
|
<a href=/people/a/abeed-sarker/>Abeed Sarker</a>
|
<a href=/people/m/michael-paul/>Michael J. Paul</a>
|
<a href=/people/g/graciela-gonzalez/>Graciela Gonzalez-Hernandez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5904><div class="card-body p-3 small">The goals of the SMM4H shared tasks are to release annotated social media based health related datasets to the research community, and to compare the performances of natural language processing and machine learning systems on tasks involving these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. The third execution of the SMM4H shared tasks, co-hosted with EMNLP-2018, comprised of four subtasks. These subtasks involve annotated user posts from Twitter (tweets) and focus on the (i) automatic classification of tweets mentioning a drug name, (ii) automatic classification of tweets containing reports of first-person medication intake, (iii) automatic classification of tweets presenting self-reports of adverse drug reaction (ADR) detection, and (iv) automatic classification of vaccine behavior mentions in tweets. A total of 14 teams participated and 78 system runs were submitted (23 for task 1, 20 for task 2, 18 for task 3, 17 for task 4).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-5909" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-5909/>Detecting Tweets Mentioning Drug Name and Adverse Drug Reaction with Hierarchical Tweet Representation and Multi-Head Self-Attention</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/j/junxin-liu/>Junxin Liu</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a>
|
<a href=/people/x/xing-xie/>Xing Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5909><div class="card-body p-3 small">This paper describes our system for the first and third shared tasks of the third Social Media Mining for Health Applications (SMM4H) workshop, which aims to detect the tweets mentioning drug names and adverse drug reactions. In our system we propose a neural approach with hierarchical tweet representation and multi-head self-attention (HTR-MSA) for both tasks. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved the first place in both the first and third shared tasks of SMM4H with an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 91.83 % and 52.20 % respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5911/>Shot Or Not : Comparison of NLP Approaches for Vaccination Behaviour Detection<span class=acl-fixed-case>NLP</span> Approaches for Vaccination Behaviour Detection</a></strong><br><a href=/people/a/aditya-joshi/>Aditya Joshi</a>
|
<a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/r/ross-sparks/>Ross Sparks</a>
|
<a href=/people/c/cecile-paris/>Cécile Paris</a>
|
<a href=/people/c/c-raina-macintyre/>C Raina MacIntyre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5911><div class="card-body p-3 small">Vaccination behaviour detection deals with predicting whether or not a person received / was about to receive a vaccine. We present our submission for vaccination behaviour detection shared task at the SMM4H workshop. Our findings are based on three prevalent text classification approaches : rule-based, statistical and deep learning-based. Our final submissions are : (1) an ensemble of statistical classifiers with task-specific features derived using lexicons, language processing tools and word embeddings ; and, (2) a LSTM classifier with pre-trained language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5913/>IRISA at SMM4H 2018 : <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Network</a> and Bagging for Tweet Classification<span class=acl-fixed-case>IRISA</span> at <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> 2018: Neural Network and Bagging for Tweet Classification</a></strong><br><a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/c/christian-raymond/>Christian Raymond</a>
|
<a href=/people/v/vincent-claveau/>Vincent Claveau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5913><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>systems</a> developed by IRISA to participate to the four <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> of the SMM4H 2018 challenge. For these tweet classification tasks, we adopt a common approach based on recurrent neural networks (BiLSTM). Our main contributions are the use of certain <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, the use of Bagging in order to deal with unbalanced datasets, and on the automatic selection of difficult examples. These techniques allow us to reach 91.4, 46.5, 47.8, 85.0 as F1-scores for Tasks 1 to 4.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5914/>Drug-Use Identification from Tweets with Word and Character N-Grams</a></strong><br><a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/t/taraka-rama/>Taraka Rama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5914><div class="card-body p-3 small">This paper describes our systems in <a href=https://en.wikipedia.org/wiki/Social_media_mining>social media mining</a> for health applications (SMM4H) shared task. We participated in all four tracks of the shared task using <a href=https://en.wikipedia.org/wiki/Linear_model>linear models</a> with a combination of character and word n-gram features. We did not use any external data or domain specific information. The resulting <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved above-average scores among other participating systems, with F1-scores of 91.22, 46.8, 42.4, and 85.53 on <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> 1, 2, 3, and 4 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5917/>Deep Learning for Social Media Health Text Classification</a></strong><br><a href=/people/s/santosh-tokala/>Santosh Tokala</a>
|
<a href=/people/v/vaibhav-gambhir/>Vaibhav Gambhir</a>
|
<a href=/people/a/animesh-mukherjee/>Animesh Mukherjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5917><div class="card-body p-3 small">This paper describes the systems developed for 1st and 2nd tasks of the 3rd Social Media Mining for Health Applications Shared Task at EMNLP 2018. The first <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> focuses on automatic detection of posts mentioning a drug name or <a href=https://en.wikipedia.org/wiki/Dietary_supplement>dietary supplement</a>, a <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a>. The second <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is about distinguishing the tweets that present <a href=https://en.wikipedia.org/wiki/Medication>personal medication intake</a>, possible <a href=https://en.wikipedia.org/wiki/Medication>medication intake</a> and <a href=https://en.wikipedia.org/wiki/Drug>non-intake</a>. We performed extensive experiments with various classifiers like Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDT) and deep learning architectures such as Long Short-Term Memory Networks (LSTM), jointed Convolutional Neural Networks (CNN) and LSTM architecture, and attention based LSTM architecture both at word and character level. We have also explored using various pre-trained embeddings like Global Vectors for Word Representation (GloVe), Word2Vec and task-specific embeddings learned using CNN-LSTM and LSTMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5919.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5919 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5919 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5919/>Leveraging Web Based Evidence Gathering for Drug Information Identification from Tweets</a></strong><br><a href=/people/r/rupsa-saha/>Rupsa Saha</a>
|
<a href=/people/a/abir-naskar/>Abir Naskar</a>
|
<a href=/people/t/tirthankar-dasgupta/>Tirthankar Dasgupta</a>
|
<a href=/people/l/lipika-dey/>Lipika Dey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5919><div class="card-body p-3 small">In this paper, we have explored web-based evidence gathering and different linguistic features to automatically extract drug names from tweets and further classify such <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> into <a href=https://en.wikipedia.org/wiki/Adverse_drug_reaction>Adverse Drug Events</a> or not. We have evaluated our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> as released by the SMM4H workshop shared Task-1 and Task-3 respectively. Our evaluation results shows that the proposed model achieved good results, with <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>Precision</a>, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a> and <a href=https://en.wikipedia.org/wiki/F-number>F-scores</a> of 78.5 %, 88 % and 82.9 % respectively for Task1 and 33.2 %, 54.7 % and 41.3 % for Task3.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-5920.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-5920 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-5920 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-5920/>CLaC at SMM4H Task 1, 2, and 4<span class=acl-fixed-case>CL</span>a<span class=acl-fixed-case>C</span> at <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> Task 1, 2, and 4</a></strong><br><a href=/people/p/parsa-bagherzadeh/>Parsa Bagherzadeh</a>
|
<a href=/people/n/nadia-sheikh/>Nadia Sheikh</a>
|
<a href=/people/s/sabine-bergler/>Sabine Bergler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-5920><div class="card-body p-3 small">CLaC Labs participated in Tasks 1, 2, and 4 using the same base architecture for all tasks with various parameter variations. This was our first exploration of this data and the SMM4H Tasks, thus a unified system was useful to compare the behavior of our architecture over the different datasets and how they interact with different linguistic features.</div></div></div><hr><div id=w18-60><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-60.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-60/>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6000/>Proceedings of the Second Workshop on Universal Dependencies (<span class=acl-fixed-case>UDW</span> 2018)</a></strong><br><a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/t/teresa-lynn/>Teresa Lynn</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6001 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6001/>Assessing the Impact of Incremental Error Detection and Correction. A Case Study on the Italian Universal Dependency Treebank<span class=acl-fixed-case>I</span>talian <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Treebank</a></strong><br><a href=/people/c/chiara-alzetta/>Chiara Alzetta</a>
|
<a href=/people/f/felice-dellorletta/>Felice Dell’Orletta</a>
|
<a href=/people/s/simonetta-montemagni/>Simonetta Montemagni</a>
|
<a href=/people/m/maria-simi/>Maria Simi</a>
|
<a href=/people/g/giulia-venturi/>Giulia Venturi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6001><div class="card-body p-3 small">Detection and correction of errors and inconsistencies in gold treebanks are becoming more and more central topics of corpus annotation. The paper illustrates a new incremental method for enhancing <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>, with particular emphasis on the extension of <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error patterns</a> across different textual genres and registers. Impact and role of corrections have been assessed in a dependency parsing experiment carried out with four different <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>, whose results are promising. For both evaluation datasets, the performance of parsers increases, in terms of the standard LAS and UAS measures and of a more focused measure taking into account only relations involved in error patterns, and at the level of individual dependencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6002/>Using Universal Dependencies in cross-linguistic complexity research<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies in cross-linguistic complexity research</a></strong><br><a href=/people/a/aleksandrs-berdicevskis/>Aleksandrs Berdicevskis</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/k/katharina-ehret/>Katharina Ehret</a>
|
<a href=/people/k/kilu-von-prince/>Kilu von Prince</a>
|
<a href=/people/d/daniel-ross/>Daniel Ross</a>
|
<a href=/people/b/bill-thompson/>Bill Thompson</a>
|
<a href=/people/c/chunxiao-yan/>Chunxiao Yan</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/g/gary-lupyan/>Gary Lupyan</a>
|
<a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/c/christian-bentz/>Christian Bentz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6002><div class="card-body p-3 small">We evaluate corpus-based measures of linguistic complexity obtained using Universal Dependencies (UD) treebanks. We propose a method of estimating <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a> of the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>complexity</a> values obtained using a given <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a> and a given <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a>. The results indicate that measures of syntactic complexity might be on average less robust than those of morphological complexity. We also estimate the validity of <a href=https://en.wikipedia.org/wiki/Complexity_measure>complexity measures</a> by comparing the results for very similar languages and checking for unexpected differences. We show that some of those differences that arise can be diminished by using parallel treebanks and, more importantly from the practical point of view, by harmonizing the language-specific solutions in the UD annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6003/>Expletives in Universal Dependency Treebanks<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Treebanks</a></strong><br><a href=/people/g/gosse-bouma/>Gosse Bouma</a>
|
<a href=/people/j/jan-hajic/>Jan Hajic</a>
|
<a href=/people/d/dag-haug/>Dag Haug</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/p/per-erik-solberg/>Per Erik Solberg</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6003><div class="card-body p-3 small">Although treebanks annotated according to the guidelines of Universal Dependencies (UD) now exist for many languages, the goal of annotating the same phenomena in a cross-linguistically consistent fashion is not always met. In this paper, we investigate one phenomenon where we believe such consistency is lacking, namely expletive elements. Such elements occupy a position that is structurally associated with a <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>core argument</a> (or sometimes an oblique dependent), yet are non-referential and semantically void. Many UD treebanks identify at least some elements as expletive, but the range of phenomena differs between treebanks, even for closely related languages, and sometimes even for different treebanks for the same language. In this paper, we present criteria for identifying <a href=https://en.wikipedia.org/wiki/Profanity>expletives</a> that are applicable across languages and compatible with the goals of UD, give an overview of <a href=https://en.wikipedia.org/wiki/Profanity>expletives</a> as found in current UD treebanks, and present recommendations for the annotation of expletives so that more consistent annotation can be achieved in future releases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6007/>Integration complexity and the order of cosisters</a></strong><br><a href=/people/w/william-dyer/>William Dyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6007><div class="card-body p-3 small">The cost of integrating dependent constituents to their heads is thought to involve the distance between dependent and head and the complexity of the integration (Gibson, 1998). The former has been convincingly addressed by Dependency Distance Minimization (DDM) (cf. Liu et al., 2017). The current study addresses the latter by proposing a novel theory of integration complexity derived from the entropy of the probability distribution of a dependent&#8217;s heads. An analysis of Universal Dependency corpora provides empirical evidence regarding the preferred order of isomorphic cosisterssister constituents of the same syntactic form on the same side of their headsuch as the adjectives in pretty blue fish. Integration complexity, alongside DDM, allows for a general theory of <a href=https://en.wikipedia.org/wiki/Constituent_order>constituent order</a> based on integration cost.<i>pretty blue fish</i>. Integration complexity, alongside DDM, allows for a general theory of constituent order based on integration cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6008/>SUD or Surface-Syntactic Universal Dependencies : An annotation scheme near-isomorphic to UD<span class=acl-fixed-case>SUD</span> or Surface-Syntactic <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies: An annotation scheme near-isomorphic to <span class=acl-fixed-case>UD</span></a></strong><br><a href=/people/k/kim-gerdes/>Kim Gerdes</a>
|
<a href=/people/b/bruno-guillaume/>Bruno Guillaume</a>
|
<a href=/people/s/sylvain-kahane/>Sylvain Kahane</a>
|
<a href=/people/g/guy-perrier/>Guy Perrier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6008><div class="card-body p-3 small">This article proposes a surface-syntactic annotation scheme called SUD that is near-isomorphic to the Universal Dependencies (UD) annotation scheme while following distributional criteria for defining the dependency tree structure and the naming of the syntactic functions. Rule-based graph transformation grammars allow for a bi-directional transformation of UD into SUD. The back-and-forth transformation can serve as an error-mining tool to assure the intra-language and inter-language coherence of the UD treebanks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6011 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6011/>Marrying Universal Dependencies and Universal Morphology<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies and <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>M</span>orphology</a></strong><br><a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a>
|
<a href=/people/d/david-yarowsky/>David Yarowsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6011><div class="card-body p-3 small">The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects each present schemata for annotating the morphosyntactic details of language. Each project also provides corpora of annotated text in many languagesUD at the token level and UniMorph at the type level. As each <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is built by different annotators, language-specific decisions hinder the goal of universal schemata. With compatibility of tags, each project&#8217;s annotations could be used to validate the other&#8217;s. Additionally, the availability of both type- and token-level resources would be a boon to tasks such as <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and homograph disambiguation. To ease this interoperability, we present a deterministic mapping from Universal Dependencies v2 features into the UniMorph schema. We validate our approach by lookup in the UniMorph corpora and find a macro-average of 64.13 % <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>. We also note incompatibilities due to paucity of data on either side. Finally, we present a critical evaluation of the foundations, strengths, and weaknesses of the two annotation projects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6012/>Enhancing Universal Dependency Treebanks : A Case Study<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependency Treebanks: A Case Study</a></strong><br><a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/p/paola-marongiu/>Paola Marongiu</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/j/jenna-kanerva/>Jenna Kanerva</a>
|
<a href=/people/s/simonetta-montemagni/>Simonetta Montemagni</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a>
|
<a href=/people/m/maria-simi/>Maria Simi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6012><div class="card-body p-3 small">We evaluate two cross-lingual techniques for adding enhanced dependencies to existing <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> in Universal Dependencies. We apply a rule-based system developed for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and a data-driven system trained on <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> to <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> and <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>. We find that both systems are accurate enough to bootstrap enhanced dependencies in existing UD treebanks. In the case of <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, results are even on par with those of a prototype language-specific system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6017 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6017/>Multi-source synthetic treebank creation for improved cross-lingual dependency parsing</a></strong><br><a href=/people/f/francis-tyers/>Francis Tyers</a>
|
<a href=/people/m/mariya-sheyanova/>Mariya Sheyanova</a>
|
<a href=/people/a/aleksandra-martynova/>Aleksandra Martynova</a>
|
<a href=/people/p/pavel-stepachev/>Pavel Stepachev</a>
|
<a href=/people/k/konstantin-vinogorodskiy/>Konstantin Vinogorodskiy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6017><div class="card-body p-3 small">This paper describes a method of creating synthetic treebanks for cross-lingual dependency parsing using a combination of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> (including pivot translation), annotation projection and the spanning tree algorithm. Sentences are first automatically translated from a lesser-resourced language to a number of related highly-resourced languages, parsed and then the annotations are projected back to the lesser-resourced language, leading to multiple <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a> for each sentence from the lesser-resourced language. The final treebank is created by merging the possible <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>trees</a> into a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> and running the spanning tree algorithm to vote for the best <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> for each sentence. We present experiments aimed at parsing <a href=https://en.wikipedia.org/wiki/Faroese_language>Faroese</a> using a combination of <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a>, <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> and <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian</a>. In a similar experimental setup to the CoNLL 2018 shared task on dependency parsing we report state-of-the-art results on dependency parsing for <a href=https://en.wikipedia.org/wiki/Faroese_language>Faroese</a> using an off-the-shelf parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6018/>Toward <a href=https://en.wikipedia.org/wiki/United_Nations_geoscheme>Universal Dependencies</a> for Shipibo-Konibo<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies for <span class=acl-fixed-case>S</span>hipibo-Konibo</a></strong><br><a href=/people/a/alonso-vasquez/>Alonso Vasquez</a>
|
<a href=/people/r/renzo-ego-aguirre/>Renzo Ego Aguirre</a>
|
<a href=/people/c/candy-angulo/>Candy Angulo</a>
|
<a href=/people/j/john-miller/>John Miller</a>
|
<a href=/people/c/claudia-villanueva/>Claudia Villanueva</a>
|
<a href=/people/z/zeljko-agic/>Željko Agić</a>
|
<a href=/people/r/roberto-zariquiey/>Roberto Zariquiey</a>
|
<a href=/people/a/arturo-oncevay/>Arturo Oncevay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6018><div class="card-body p-3 small">We present an initial version of the Universal Dependencies (UD) treebank for <a href=https://en.wikipedia.org/wiki/Shipibo-Konibo_language>Shipibo-Konibo</a>, the first South American, Amazonian, Panoan and Peruvian language with a resource built under UD. We describe the linguistic aspects of how the tagset was defined and the <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> was annotated ; in addition we present our specific treatment of linguistic units called <a href=https://en.wikipedia.org/wiki/Clitic>clitics</a>. Although the <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> is still under development, it allowed us to perform a typological comparison against <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, the predominant language in <a href=https://en.wikipedia.org/wiki/Peru>Peru</a>, and dependency syntax parsing experiments in both monolingual and cross-lingual approaches.<i>clitics</i>. Although the treebank is still under development, it allowed us to perform a typological comparison against Spanish, the predominant language in Peru, and dependency syntax parsing experiments in both monolingual and cross-lingual approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6019 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6019/>Transition-based Parsing with Lighter Feed-Forward Networks</a></strong><br><a href=/people/d/david-vilares/>David Vilares</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6019><div class="card-body p-3 small">We explore whether it is possible to build lighter <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>, that are statistically equivalent to their corresponding standard version, for a wide set of <a href=https://en.wikipedia.org/wiki/Language>languages</a> showing different structures and morphologies. As testbed, we use the Universal Dependencies and transition-based dependency parsers trained on feed-forward networks. For these, most existing research assumes de facto standard embedded features and relies on pre-computation tricks to obtain speed-ups. We explore how these <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> and their size can be reduced and whether this translates into <a href=https://en.wikipedia.org/wiki/Speedup>speed-ups</a> with a negligible impact on <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. The experiments show that grand-daughter features can be removed for the majority of treebanks without a significant (negative or positive) LAS difference. They also show how the size of the <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> can be notably reduced.<i>de facto standard</i> embedded features and relies on pre-computation tricks to obtain speed-ups. We explore how these features and their size can be reduced and whether this translates into speed-ups with a negligible impact on accuracy. The experiments show that <i>grand-daughter</i> features can be removed for the majority of treebanks without a significant (negative or positive) LAS difference. They also show how the size of the embeddings can be notably reduced.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6020 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6020/>Extended and Enhanced Polish Dependency Bank in Universal Dependencies Format<span class=acl-fixed-case>P</span>olish Dependency Bank in <span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies Format</a></strong><br><a href=/people/a/alina-wroblewska/>Alina Wróblewska</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6020><div class="card-body p-3 small">The paper presents the largest Polish Dependency Bank in Universal Dependencies format PDBUD with 22 K trees and 352 K tokens. PDBUD builds on its previous version, i.e. the Polish UD treebank (PL-SZ), and contains all 8 K PL-SZ trees. The PL-SZ trees are checked and possibly corrected in the current edition of PDBUD. Further 14 K trees are automatically converted from a new version of Polish Dependency Bank. The PDBUD trees are expanded with the enhanced edges encoding the shared dependents and the shared governors of the coordinated conjuncts and with the semantic roles of some dependents. The conducted evaluation experiments show that PDBUD is large enough for training a high-quality graph-based dependency parser for <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6021/>Approximate Dynamic Oracle for Dependency Parsing with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a></a></strong><br><a href=/people/x/xiang-yu/>Xiang Yu</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a>
|
<a href=/people/j/jonas-kuhn/>Jonas Kuhn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6021><div class="card-body p-3 small">We present a general approach with <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning (RL)</a> to approximate dynamic oracles for transition systems where exact dynamic oracles are difficult to derive. We treat <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle parsing</a> as a reinforcement learning problem, design the reward function inspired by the classical dynamic oracle, and use Deep Q-Learning (DQN) techniques to train the <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle</a> with gold trees as features. The combination of a priori knowledge and <a href=https://en.wikipedia.org/wiki/Data-driven_programming>data-driven methods</a> enables an efficient dynamic oracle, which improves the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> performance over static oracles in several <a href=https://en.wikipedia.org/wiki/Transition_system>transition systems</a>.</div></div></div><hr><div id=w18-61><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-61.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-61/>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6100/>Proceedings of the 2018 <span class=acl-fixed-case>EMNLP</span> Workshop W-<span class=acl-fixed-case>NUT</span>: The 4th Workshop on Noisy User-generated Text</a></strong><br><a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/t/timothy-baldwin/>Tim Baldwin</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6101" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6101/>Inducing a lexicon of sociolinguistic variables from code-mixed text</a></strong><br><a href=/people/p/philippa-shoemark/>Philippa Shoemark</a>
|
<a href=/people/j/james-kirby/>James Kirby</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6101><div class="card-body p-3 small">Sociolinguistics is often concerned with how variants of a linguistic item (e.g., nothing vs. nothin&#8217;) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text : that is, identifying equivalence pairs such as (football, fitba) along with their linguistic code (footballBritish, fitbaScottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of <a href=https://en.wikipedia.org/wiki/List_of_dialects_of_English>English dialects</a>, using <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> as the code-mixed text. Our system achieves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of over 70 % for two of these three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, and produces useful results even without extensive parameter tuning. Our success in adapting this <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> from <a href=https://en.wikipedia.org/wiki/Gender>gender</a> to <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language variety</a> suggests that it could be used to discover other types of analogous pairs as well.<i>nothing</i> vs. <i>nothin&#8217;</i>) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text: that is, identifying equivalence pairs such as (<i>football</i>, <i>fitba</i>) along with their linguistic code (<i>football</i>&#8594;British, <i>fitba</i>&#8594;Scottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of English dialects, using tweets as the code-mixed text. Our system achieves precision of over 70% for two of these three datasets, and produces useful results even without extensive parameter tuning. Our success in adapting this framework from gender to language variety suggests that it could be used to discover other types of analogous pairs as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6102/>Twitter Geolocation using Knowledge-Based Methods<span class=acl-fixed-case>T</span>witter Geolocation using Knowledge-Based Methods</a></strong><br><a href=/people/t/taro-miyazaki/>Taro Miyazaki</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6102><div class="card-body p-3 small">Automatic geolocation of microblog posts from their text content is particularly difficult because many location-indicative terms are rare terms, notably entity names such as locations, people or local organisations. Their low frequency means that key terms observed in testing are often unseen in training, such that standard <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> are unable to learn weights for them. We propose a method for reasoning over such terms using a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, through exploiting their relations with other entities. Our technique uses a <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embedding</a> over the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, which we couple with a text representation to learn a geolocation classifier, trained end-to-end. We show that our method improves over purely text-based methods, which we ascribe to more robust treatment of low-count and out-of-vocabulary entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6104/>Assigning people to tasks identified in email : The EPA dataset for addressee tagging for detected task intent<span class=acl-fixed-case>EPA</span> dataset for addressee tagging for detected task intent</a></strong><br><a href=/people/r/revanth-rameshkumar/>Revanth Rameshkumar</a>
|
<a href=/people/p/peter-bailey/>Peter Bailey</a>
|
<a href=/people/a/abhishek-jha/>Abhishek Jha</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6104><div class="card-body p-3 small">We describe the Enron People Assignment (EPA) dataset, in which <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that are described in emails are associated with the person(s) responsible for carrying out these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. We identify <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and the responsible people in the Enron email dataset. We define evaluation methods for this challenge and report scores for our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and nave baselines. The resulting model enables a <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a> operating within a commercial email service : given a person and a task, it determines if the person should be notified of the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6109/>Paraphrase Detection on Noisy Subtitles in Six Languages</a></strong><br><a href=/people/e/eetu-sjoblom/>Eetu Sjöblom</a>
|
<a href=/people/m/mathias-creutz/>Mathias Creutz</a>
|
<a href=/people/m/mikko-aulamo/>Mikko Aulamo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6109><div class="card-body p-3 small">We perform automatic paraphrase detection on subtitle data from the Opusparcus corpus comprising six European languages : <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>. We train two types of supervised sentence embedding models : a word-averaging (WA) model and a gated recurrent averaging network (GRAN) model. We find out that <a href=https://en.wikipedia.org/wiki/GRAN>GRAN</a> outperforms WA and is more robust to noisy training data. Better results are obtained with more and noisier data than less and cleaner data. Additionally, we experiment on other <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, without reaching the same level of performance, because of domain mismatch between training and test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6110/>Distantly Supervised Attribute Detection from Reviews</a></strong><br><a href=/people/l/lisheng-fu/>Lisheng Fu</a>
|
<a href=/people/p/pablo-barrio/>Pablo Barrio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6110><div class="card-body p-3 small">This work aims to detect specific <a href=https://en.wikipedia.org/wiki/Attribute_(philosophy)>attributes</a> of a place (e.g., if it has a romantic atmosphere, or if it offers outdoor seating) from its user reviews via distant supervision : without direct annotation of the review text, we use the crowdsourced attribute labels of the place as labels of the review text. We then use review-level attention to pay more attention to those reviews related to the <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attributes</a>. The experimental results show that our attention-based model predicts attributes for places from reviews with over 98 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. The attention weights assigned to each review provide explanation of capturing relevant reviews.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6111 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6111/>Using Wikipedia Edits in Low Resource Grammatical Error Correction<span class=acl-fixed-case>W</span>ikipedia Edits in Low Resource Grammatical Error Correction</a></strong><br><a href=/people/a/adriane-boyd/>Adriane Boyd</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6111><div class="card-body p-3 small">We develop a grammatical error correction (GEC) system for <a href=https://en.wikipedia.org/wiki/German_language>German</a> using a small gold GEC corpus augmented with edits extracted from Wikipedia revision history. We extend the automatic error annotation tool ERRANT (Bryant et al., 2017) for German and use it to analyze both gold GEC corrections and Wikipedia edits (Grundkiewicz and Junczys-Dowmunt, 2014) in order to select as additional training data Wikipedia edits containing grammatical corrections similar to those in the gold corpus. Using a multilayer convolutional encoder-decoder neural network GEC approach (Chollampatt and Ng, 2018), we evaluate the contribution of Wikipedia edits and find that carefully selected Wikipedia edits increase performance by over 5 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6112/>Empirical Evaluation of Character-Based Model on Neural Named-Entity Recognition in Indonesian Conversational Texts<span class=acl-fixed-case>I</span>ndonesian Conversational Texts</a></strong><br><a href=/people/k/kemal-kurniawan/>Kemal Kurniawan</a>
|
<a href=/people/s/samuel-louvan/>Samuel Louvan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6112><div class="card-body p-3 small">Despite the long history of named-entity recognition (NER) task in the natural language processing community, previous work rarely studied the task on conversational texts. Such <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>texts</a> are challenging because they contain a lot of <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>word variations</a> which increase the number of out-of-vocabulary (OOV) words. The high number of OOV words poses a difficulty for word-based neural models. Meanwhile, there is plenty of evidence to the effectiveness of character-based neural models in mitigating this OOV problem. We report an empirical evaluation of neural sequence labeling models with character embedding to tackle NER task in Indonesian conversational texts. Our experiments show that (1) character models outperform word embedding-only models by up to 4 F1 points, (2) character models perform better in OOV cases with an improvement of as high as 15 F1 points, and (3) character models are robust against a very high OOV rate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6113 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6113" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6113/>Orthogonal Matching Pursuit for Text Classification</a></strong><br><a href=/people/k/konstantinos-skianis/>Konstantinos Skianis</a>
|
<a href=/people/n/nikolaos-tziortziotis/>Nikolaos Tziortziotis</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6113><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>, the problem of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> arises due to the high dimensionality, making <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> essential. Although classic <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizers</a> provide sparsity, they fail to return highly accurate models. On the contrary, state-of-the-art group-lasso regularizers provide better results at the expense of low sparsity. In this paper, we apply a greedy variable selection algorithm, called Orthogonal Matching Pursuit, for the text classification task. We also extend standard group OMP by introducing overlapping Group OMP to handle overlapping groups of features. Empirical analysis verifies that both OMP and overlapping GOMP constitute powerful <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizers</a>, able to produce effective and very sparse models. Code and data are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6114 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6114/>Training and Prediction Data Discrepancies : Challenges of Text Classification with Noisy, Historical Data</a></strong><br><a href=/people/r/r-andrew-kreek/>R. Andrew Kreek</a>
|
<a href=/people/e/emilia-apostolova/>Emilia Apostolova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6114><div class="card-body p-3 small">Industry datasets used for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> are rarely created for that purpose. In most cases, the data and target predictions are a by-product of accumulated historical data, typically fraught with <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>, present in both the text-based document, as well as in the targeted labels. In this work, we address the question of how well performance metrics computed on noisy, historical data reflect the performance on the intended future machine learning model input. The results demonstrate the utility of dirty training datasets used to build prediction models for cleaner (and different) prediction inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6115/>Detecting Code-Switching between Turkish-English Language Pair<span class=acl-fixed-case>T</span>urkish-<span class=acl-fixed-case>E</span>nglish Language Pair</a></strong><br><a href=/people/z/zeynep-yirmibesoglu/>Zeynep Yirmibeşoğlu</a>
|
<a href=/people/g/gulsen-eryigit/>Gülşen Eryiğit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6115><div class="card-body p-3 small">Code-switching (usage of different languages within a single conversation context in an alternative manner) is a highly increasing phenomenon in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and <a href=https://en.wikipedia.org/wiki/Colloquialism>colloquial usage</a> which poses different challenges for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. This paper introduces the first study for the detection of Turkish-English code-switching and also a small test data collected from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in order to smooth the way for further studies. The proposed system using character level n-grams and conditional random fields (CRFs) obtains 95.6 % micro-averaged F1-score on the introduced test data set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6116 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6116/>Language Identification in Code-Mixed Data using Multichannel Neural Networks and Context Capture</a></strong><br><a href=/people/s/soumil-mandal/>Soumil Mandal</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6116><div class="card-body p-3 small">An accurate language identification tool is an absolute necessity for building complex NLP systems to be used on code-mixed data. Lot of work has been recently done on the same, but there&#8217;s still room for improvement. Inspired from the recent advancements in neural network architectures for computer vision tasks, we have implemented multichannel neural networks combining CNN and LSTM for word level language identification of code-mixed data. Combining this with a Bi-LSTM-CRF context capture module, accuracies of 93.28 % and 93.32 % is achieved on our two testing sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6117 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6117/>Modeling Student Response Times : Towards Efficient One-on-one Tutoring Dialogues</a></strong><br><a href=/people/l/luciana-benotti/>Luciana Benotti</a>
|
<a href=/people/j/jayadev-bhaskaran/>Jayadev Bhaskaran</a>
|
<a href=/people/s/sigtryggur-kjartansson/>Sigtryggur Kjartansson</a>
|
<a href=/people/d/david-lang/>David Lang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6117><div class="card-body p-3 small">In this paper we investigate the task of modeling how long it would take a student to respond to a tutor question during a tutoring dialogue. Solving such a task has applications in educational settings such as <a href=https://en.wikipedia.org/wiki/Intelligent_tutoring_system>intelligent tutoring systems</a>, as well as in platforms that help busy human tutors to keep students engaged. Knowing how long it would normally take a student to respond to different types of questions could help tutors optimize their own time while answering multiple dialogues concurrently, as well as deciding when to prompt a student again. We study this problem using data from a service that offers tutor support for <a href=https://en.wikipedia.org/wiki/Mathematics>math</a>, <a href=https://en.wikipedia.org/wiki/Chemistry>chemistry</a> and <a href=https://en.wikipedia.org/wiki/Physics>physics</a> through an instant messaging platform. We create a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 240 K questions. We explore several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and compare them with human performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6118 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6118/>Content Extraction and <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Analysis</a> from Customer-Agent Interactions</a></strong><br><a href=/people/s/sergiu-nisioi/>Sergiu Nisioi</a>
|
<a href=/people/a/anca-bucur/>Anca Bucur</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6118><div class="card-body p-3 small">In this paper, we provide a lexical comparative analysis of the <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabulary</a> used by customers and agents in an Enterprise Resource Planning (ERP) environment and a potential solution to clean the data and extract relevant content for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. As a result, we demonstrate that the actual vocabulary for the <a href=https://en.wikipedia.org/wiki/Language>language</a> that prevails in the ERP conversations is highly divergent from the standardized dictionary and further different from general language usage as extracted from the Common Crawl corpus. Moreover, in specific business communication circumstances, where it is expected to observe a high usage of <a href=https://en.wikipedia.org/wiki/Standard_language>standardized language</a>, <a href=https://en.wikipedia.org/wiki/Code_switching>code switching</a> and non-standard expression are predominant, emphasizing once more the discrepancy between the day-to-day use of language and the standardized one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6120 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6120" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6120/>Word-like character n-gram embedding</a></strong><br><a href=/people/g/geewook-kim/>Geewook Kim</a>
|
<a href=/people/k/kazuki-fukui/>Kazuki Fukui</a>
|
<a href=/people/h/hidetoshi-shimodaira/>Hidetoshi Shimodaira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6120><div class="card-body p-3 small">We propose a new word embedding method called word-like character n-gram embedding, which learns distributed representations of words by embedding word-like character n-grams. Our method is an extension of recently proposed segmentation-free word embedding, which directly embeds frequent character n-grams from a raw corpus. However, its <a href=https://en.wikipedia.org/wiki/N-gram>n-gram vocabulary</a> tends to contain too many non-word n-grams. We solved this problem by introducing an idea of expected word frequency. Compared to the previously proposed methods, our method can embed more words, along with the words that are not included in a given basic word dictionary. Since our method does not rely on <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> with rich word dictionaries, it is especially effective when the text in the corpus is in unsegmented language and contains many <a href=https://en.wikipedia.org/wiki/Neologism>neologisms</a> and informal words (e.g., Chinese SNS dataset). Our experimental results on <a href=https://en.wikipedia.org/wiki/Sina_Weibo>Sina Weibo</a> (a Chinese microblog service) and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> show that the proposed method can embed more words and improve the performance of downstream tasks.<i>word-like character</i> n<i>-gram embedding</i>, which learns distributed representations of words by embedding word-like character n-grams. Our method is an extension of recently proposed <i>segmentation-free word embedding</i>, which directly embeds frequent character n-grams from a raw corpus. However, its n-gram vocabulary tends to contain too many non-word n-grams. We solved this problem by introducing an idea of <i>expected word frequency</i>. Compared to the previously proposed methods, our method can embed more words, along with the words that are not included in a given basic word dictionary. Since our method does not rely on word segmentation with rich word dictionaries, it is especially effective when the text in the corpus is in unsegmented language and contains many neologisms and informal words (e.g., Chinese SNS dataset). Our experimental results on Sina Weibo (a Chinese microblog service) and Twitter show that the proposed method can embed more words and improve the performance of downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6121 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6121/>Classification of Tweets about Reported Events using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a></a></strong><br><a href=/people/k/kiminobu-makino/>Kiminobu Makino</a>
|
<a href=/people/y/yuka-takei/>Yuka Takei</a>
|
<a href=/people/t/taro-miyazaki/>Taro Miyazaki</a>
|
<a href=/people/j/jun-goto/>Jun Goto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6121><div class="card-body p-3 small">We developed a system that automatically extracts Event-describing Tweets which include incidents or accidents information for creating news reports. Event-describing Tweets can be classified into Reported-event Tweets and New-information Tweets. Reported-event Tweets cite <a href=https://en.wikipedia.org/wiki/News_agency>news agencies</a> or user generated content sites, and New-information Tweets are other Event-describing Tweets. A <a href=https://en.wikipedia.org/wiki/System>system</a> is needed to classify them so that creators of factual TV programs can use <a href=https://en.wikipedia.org/wiki/Them_(band)>them</a> in their productions. Proposing this Tweet classification task is one of the contributions of this paper, because no prior papers have used the same task even though program creators and other events information collectors have to do it to extract required information from <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking sites</a>. To classify Tweets in this task, this paper proposes a method to input and concatenate character and word sequences in Japanese Tweets by using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. This proposed method is another contribution of this paper. For comparison, character or word input methods and other <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> are also used. Results show that a <a href=https://en.wikipedia.org/wiki/System>system</a> using the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> and <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> can classify Tweets with an F1 score of 88 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6122 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6122/>Learning to Define Terms in the Software Domain</a></strong><br><a href=/people/v/vidhisha-balachandran/>Vidhisha Balachandran</a>
|
<a href=/people/d/dheeraj-rajagopal/>Dheeraj Rajagopal</a>
|
<a href=/people/r/rose-catherine-kanjirathinkal/>Rose Catherine Kanjirathinkal</a>
|
<a href=/people/w/william-cohen/>William Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6122><div class="card-body p-3 small">One way to test a person&#8217;s knowledge of a domain is to ask them to define domain-specific terms. Here, we investigate the task of automatically generating definitions of technical terms by reading text from the technical domain. Specifically, we learn definitions of software entities from a large corpus built from the user forum Stack Overflow. To model definitions, we train a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and incorporate additional domain-specific information like <a href=https://en.wikipedia.org/wiki/Co-occurrence>word co-occurrence</a>, and <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological category information</a>. Our approach improves previous <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by 2 <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>BLEU points</a> for the definition generation task. Our experiments also show the additional challenges associated with the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and the short-comings of language-model based architectures for definition generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6123 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6123/>FrameIt : <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>Ontology Discovery</a> for Noisy User-Generated Text<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>I</span>t: Ontology Discovery for Noisy User-Generated Text</a></strong><br><a href=/people/d/dan-iter/>Dan Iter</a>
|
<a href=/people/a/alon-halevy/>Alon Halevy</a>
|
<a href=/people/w/wang-chiew-tan/>Wang-Chiew Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6123><div class="card-body p-3 small">A common need of NLP applications is to extract <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> in order to perform <a href=https://en.wikipedia.org/wiki/Analytics>analytics</a> or trigger an appropriate action. The <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> defining the structure is typically application dependent and in many cases it is not known a priori. We describe the FrameIt System that provides a workflow for (1) quickly discovering an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> to model a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> and (2) learning an SRL model that extracts the instances of the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> from sentences in the corpus. FrameIt exploits data that is obtained in the ontology discovery phase as weak supervision data to bootstrap the SRL model and then enables the user to refine the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with active learning. We present empirical results and qualitative analysis of the performance of FrameIt on three corpora of noisy user-generated text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6125 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6125/>Low-resource named entity recognition via multi-source projection : Not quite there yet?</a></strong><br><a href=/people/j/jan-vium-enghoff/>Jan Vium Enghoff</a>
|
<a href=/people/s/soren-harrison/>Søren Harrison</a>
|
<a href=/people/z/zeljko-agic/>Željko Agić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6125><div class="card-body p-3 small">Projecting linguistic annotations through word alignments is one of the most prevalent approaches to cross-lingual transfer learning. Conventional wisdom suggests that annotation projection just works regardless of the task at hand. We carefully consider multi-source projection for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Our experiment with 17 languages shows that to detect <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> in true low-resource languages, annotation projection may not be the right way to move forward. On a more positive note, we also uncover the conditions that do favor named entity projection from multiple sources. We argue these are infeasible under noisy low-resource constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6126 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6126/>A Case Study on Learning a Unified Encoder of Relations</a></strong><br><a href=/people/l/lisheng-fu/>Lisheng Fu</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a>
|
<a href=/people/r/ralph-grishman/>Ralph Grishman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6126><div class="card-body p-3 small">Typical relation extraction models are trained on a single corpus annotated with a pre-defined relation schema. An individual corpus is often small, and the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> may often be biased or overfitted to the corpus. We hypothesize that we can learn a better <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> by combining multiple relation datasets. We attempt to use a shared encoder to learn the unified feature representation and to augment it with <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> by <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial training</a>. The additional corpora feeding the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> can help to learn a better feature representation layer even though the relation schemas are different. We use ACE05 and ERE datasets as our case study for experiments. The multi-task model obtains significant improvement on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6129 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6129/>Combining Human and Machine Transcriptions on the Zooniverse Platform</a></strong><br><a href=/people/d/daniel-hanson/>Daniel Hanson</a>
|
<a href=/people/a/andrea-simenstad/>Andrea Simenstad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6129><div class="card-body p-3 small">Transcribing handwritten documents to create fully searchable texts is an essential part of the <a href=https://en.wikipedia.org/wiki/Archival_science>archival process</a>. Traditional text recognition methods, such as <a href=https://en.wikipedia.org/wiki/Optical_character_recognition>optical character recognition (OCR)</a>, do not work on handwritten documents due to their frequent noisiness and OCR&#8217;s need for individually segmented letters. Crowdsourcing and improved machine models are two modern methods for transcribing handwritten documents.</div></div></div><hr><div id=w18-62><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-62.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-62/>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6200/>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></strong><br><a href=/people/a/alexandra-balahur/>Alexandra Balahur</a>
|
<a href=/people/s/saif-mohammad/>Saif M. Mohammad</a>
|
<a href=/people/v/veronique-hoste/>Veronique Hoste</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6201/>Identifying Affective Events and the Reasons for their Polarity</a></strong><br><a href=/people/e/ellen-riloff/>Ellen Riloff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6201><div class="card-body p-3 small">Many events have a positive or negative impact on our lives (e.g., I bought a house is typically good news, but My house burned down is bad news). Recognizing events that have affective polarity is essential for narrative text understanding, conversational dialogue, and applications such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> and sarcasm detection. We will discuss our recent work on identifying <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective events</a> and categorizing them based on the underlying reasons for their <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affective polarity</a>. First, we will describe a weakly supervised learning method to induce a large set of <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>affective events</a> from a <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a> by optimizing for semantic consistency. Second, we will present models to classify affective events based on Human Need Categories, which often explain people&#8217;s motivations and desires. Our best results use a co-training model that consists of event expression and event context classifiers and exploits both labeled and unlabeled texts. We will conclude with a discussion of interesting directions for future work in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6202" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6202/>Deep contextualized word representations for detecting sarcasm and irony</a></strong><br><a href=/people/s/suzana-ilic/>Suzana Ilić</a>
|
<a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/j/jorge-balazs/>Jorge Balazs</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6202><div class="card-body p-3 small">Predicting context-dependent and non-literal utterances like sarcastic and ironic expressions still remains a challenging task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, as it goes beyond linguistic patterns, encompassing <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a> and shared knowledge as crucial components. To capture complex morpho-syntactic features that can usually serve as indicators for <a href=https://en.wikipedia.org/wiki/Irony>irony</a> or <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> across dynamic contexts, we propose a model that uses character-level vector representations of words, based on ELMo. We test our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on 7 different datasets derived from 3 different data sources, providing state-of-the-art performance in 6 of them, and otherwise offering competitive results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6203/>Implicit Subjective and Sentimental Usages in Multi-sense Word Embeddings</a></strong><br><a href=/people/y/yuqi-sun/>Yuqi Sun</a>
|
<a href=/people/h/haoyue-shi/>Haoyue Shi</a>
|
<a href=/people/j/junfeng-hu/>Junfeng Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6203><div class="card-body p-3 small">In multi-sense word embeddings, contextual variations in corpus may cause a univocal word to be embedded into different sense vectors. Shi et al. (2016) show that this kind of pseudo multi-senses can be eliminated by <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformations</a>. In this paper, we show that pseudo multi-senses may come from a uniform and meaningful phenomenon such as subjective and sentimental usage, though they are seemingly redundant. In this paper, we present an unsupervised algorithm to find a <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a> which can minimize the transformed distance of a group of sense pairs. The major shrinking direction of this transformation is found to be related with subjective shift. Therefore, we can not only eliminate pseudo multi-senses in multisense embeddings, but also identify these subjective senses and tag the subjective and sentimental usage of words in the corpus automatically.<i>pseudo multi-senses</i> can be eliminated by linear transformations. In this paper, we show that <i>pseudo multi-senses</i> may come from a uniform and meaningful phenomenon such as subjective and sentimental usage, though they are seemingly redundant. In this paper, we present an unsupervised algorithm to find a linear transformation which can minimize the transformed distance of a group of sense pairs. The major shrinking direction of this transformation is found to be related with subjective shift. Therefore, we can not only eliminate <i>pseudo multi-senses</i> in multisense embeddings, but also identify these subjective senses and tag the subjective and sentimental usage of words in the corpus automatically.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6207/>Amobee at IEST 2018 : Transfer Learning from Language Models<span class=acl-fixed-case>A</span>mobee at <span class=acl-fixed-case>IEST</span> 2018: Transfer Learning from Language Models</a></strong><br><a href=/people/a/alon-rozental/>Alon Rozental</a>
|
<a href=/people/d/daniel-fleischer/>Daniel Fleischer</a>
|
<a href=/people/z/zohar-kelrich/>Zohar Kelrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6207><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>system</a> developed at Amobee for the WASSA 2018 implicit emotions shared task (IEST). The goal of this task was to predict the <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> expressed by missing words in tweets without an explicit mention of those words. We developed an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble system</a> consisting of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> together with LSTM-based networks containing a CNN attention mechanism. Our approach represents a novel use of language modelsspecifically trained on a large Twitter datasetto predict and classify <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> reached 1st place with a macro F1 score of 0.7145.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6208/>IIIDYT at IEST 2018 : Implicit Emotion Classification With Deep Contextualized Word Representations<span class=acl-fixed-case>IIIDYT</span> at <span class=acl-fixed-case>IEST</span> 2018: Implicit Emotion Classification With Deep Contextualized Word Representations</a></strong><br><a href=/people/j/jorge-balazs/>Jorge Balazs</a>
|
<a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6208><div class="card-body p-3 small">In this paper we describe our <a href=https://en.wikipedia.org/wiki/System>system</a> designed for the WASSA 2018 Implicit Emotion Shared Task (IEST), which obtained 2nd place out of 30 teams with a test macro F1 score of 0.710. The system is composed of a single pre-trained ELMo layer for encoding words, a Bidirectional Long-Short Memory Network BiLSTM for enriching word representations with context, a max-pooling operation for creating sentence representations from them, and a Dense Layer for projecting the sentence representations into label space. Our official submission was obtained by ensembling 6 of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> initialized with different <a href=https://en.wikipedia.org/wiki/Random_seed>random seeds</a>. The code for replicating this paper is available at.<url>https://github.com/jabalazs/implicit_emotion</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6211/>Not Just Depressed : Bipolar Disorder Prediction on Reddit<span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/i/ivan-sekulic/>Ivan Sekulic</a>
|
<a href=/people/m/matej-gjurkovic/>Matej Gjurković</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6211><div class="card-body p-3 small">Bipolar disorder, an illness characterized by <a href=https://en.wikipedia.org/wiki/Bipolar_disorder>manic and depressive episodes</a>, affects more than 60 million people worldwide. We present a preliminary study on bipolar disorder prediction from <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated text</a> on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>, which relies on users&#8217; self-reported labels. Our benchmark classifiers for bipolar disorder prediction outperform the baselines and reach <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and F1-scores of above 86 %. Feature analysis shows interesting differences in language use between users with <a href=https://en.wikipedia.org/wiki/Bipolar_disorder>bipolar disorders</a> and the <a href=https://en.wikipedia.org/wiki/Treatment_and_control_groups>control group</a>, including differences in the use of emotion-expressive words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6212/>Topic-Specific Sentiment Analysis Can Help Identify Political Ideology</a></strong><br><a href=/people/s/sumit-bhatia/>Sumit Bhatia</a>
|
<a href=/people/d/deepak-p/>Deepak P</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6212><div class="card-body p-3 small">Ideological leanings of an individual can often be gauged by the sentiment one expresses about different issues. We propose a simple <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> that represents a <a href=https://en.wikipedia.org/wiki/Ideology>political ideology</a> as a distribution of sentiment polarities towards a set of topics. This <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representation</a> can then be used to detect ideological leanings of documents (speeches, <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>, etc.) based on the sentiments expressed towards different topics. Experiments performed using a widely used dataset show the promise of our proposed approach that achieves comparable performance to other methods despite being much simpler and more interpretable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6213/>Saying no but meaning yes : negation and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in Basque<span class=acl-fixed-case>B</span>asque</a></strong><br><a href=/people/j/jon-alkorta/>Jon Alkorta</a>
|
<a href=/people/k/koldo-gojenola/>Koldo Gojenola</a>
|
<a href=/people/m/mikel-iruskieta/>Mikel Iruskieta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6213><div class="card-body p-3 small">In this work, we have analyzed the effects of <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> on the semantic orientation in <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>. The analysis shows that negation markers can strengthen, weaken or have no effect on sentiment orientation of a word or a group of words. Using the Constraint Grammar formalism, we have designed and evaluated a set of <a href=https://en.wikipedia.org/wiki/Rule_of_inference>linguistic rules</a> to formalize these three <a href=https://en.wikipedia.org/wiki/Phenomenon>phenomena</a>. The results show that two <a href=https://en.wikipedia.org/wiki/Phenomenon>phenomena</a>, strengthening and no change, have been identified accurately and the third one, weakening, with acceptable results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6214/>Leveraging Writing Systems Change for Deep Learning Based Chinese Emotion Analysis<span class=acl-fixed-case>C</span>hinese Emotion Analysis</a></strong><br><a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/d/dan-xiong/>Dan Xiong</a>
|
<a href=/people/i/i-hsuan-chen/>I-Hsuan Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6214><div class="card-body p-3 small">Social media text written in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese communities</a> contains mixed scripts including major text written in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, an ideograph-based writing system, and some minor text using <a href=https://en.wikipedia.org/wiki/Latin_script>Latin letters</a>, an alphabet-based writing system. This phenomenon is called writing systems changes (WSCs). Past studies have shown that WSCs can be used to express emotions, particularly where the social and political environment is more conservative. However, because WSCs can break the <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> of the major text, it poses more challenges in Natural Language Processing (NLP) tasks like <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>. In this work, we present a novel deep learning based method to include WSCs as an effective <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> for emotion analysis. The method first identifies all WSCs points. Then representation of the major text is learned through an LSTM model whereas the minor text is learned by a separate CNN model. Emotions in the minor text are further highlighted through an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> before <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification</a>. Performance evaluation shows that incorporating WSCs features using deep learning models can improve performance measured by F1-scores compared to the state-of-the-art model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6215/>Ternary Twitter Sentiment Classification with Distant Supervision and Sentiment-Specific Word Embeddings<span class=acl-fixed-case>T</span>witter Sentiment Classification with Distant Supervision and Sentiment-Specific Word Embeddings</a></strong><br><a href=/people/m/mats-byrkjeland/>Mats Byrkjeland</a>
|
<a href=/people/f/frederik-gorvell-de-lichtenberg/>Frederik Gørvell de Lichtenberg</a>
|
<a href=/people/b/bjorn-gamback/>Björn Gambäck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6215><div class="card-body p-3 small">The paper proposes the Ternary Sentiment Embedding Model, a new model for creating sentiment embeddings based on the Hybrid Ranking Model of Tang et al. (2016), but trained on ternary-labeled data instead of binary-labeled, utilizing sentiment embeddings from datasets made with different distant supervision methods. The model is used as part of a complete Twitter Sentiment Analysis system and empirically compared to existing systems, showing that it outperforms Hybrid Ranking and that the quality of the distant-supervised dataset has a great impact on the quality of the produced sentiment embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6216/>Linking News Sentiment to <a href=https://en.wikipedia.org/wiki/Microblogging>Microblogs</a> : A Distributional Semantics Approach to Enhance Microblog Sentiment Classification</a></strong><br><a href=/people/t/tobias-daudert/>Tobias Daudert</a>
|
<a href=/people/p/paul-buitelaar/>Paul Buitelaar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6216><div class="card-body p-3 small">Social media&#8217;s popularity in society and research is gaining momentum and simultaneously increasing the importance of short textual content such as <a href=https://en.wikipedia.org/wiki/Microblogging>microblogs</a>. Microblogs are affected by many factors including the <a href=https://en.wikipedia.org/wiki/News_media>news media</a>, therefore, we exploit sentiments conveyed from news to detect and classify sentiment in <a href=https://en.wikipedia.org/wiki/Microblogging>microblogs</a>. Given that texts can deal with the same entity but might not be vastly related when it comes to <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, it becomes necessary to introduce further measures ensuring the relatedness of texts while leveraging the contained sentiments. This paper describes ongoing research introducing <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a> to improve the exploitation of news-contained sentiment to enhance microblog sentiment classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6217/>Aspect Based Sentiment Analysis into the Wild</a></strong><br><a href=/people/c/caroline-brun/>Caroline Brun</a>
|
<a href=/people/v/vassilina-nikoulina/>Vassilina Nikoulina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6217><div class="card-body p-3 small">In this paper, we test state-of-the-art Aspect Based Sentiment Analysis (ABSA) systems trained on a widely used dataset on actual data. We created a new manually annotated dataset of user generated data from the same domain as the training dataset, but from other sources and analyse the differences between the new and the standard ABSA dataset. We then analyse the results in performance of different versions of the same <a href=https://en.wikipedia.org/wiki/System>system</a> on both datasets. We also propose light adaptation methods to increase system robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6219 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6219" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6219/>Self-Attention : A Better Building Block for Sentiment Analysis Neural Network Classifiers</a></strong><br><a href=/people/a/artaches-ambartsoumian/>Artaches Ambartsoumian</a>
|
<a href=/people/f/fred-popowich/>Fred Popowich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6219><div class="card-body p-3 small">Sentiment Analysis has seen much progress in the past two decades. For the past few years, neural network approaches, primarily RNNs and CNNs, have been the most successful for this task. Recently, a new category of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, self-attention networks (SANs), have been created which utilizes the <a href=https://en.wikipedia.org/wiki/Attention>attention mechanism</a> as the basic building block. Self-attention networks have been shown to be effective for sequence modeling tasks, while having no <a href=https://en.wikipedia.org/wiki/Recurrence_relation>recurrence</a> or <a href=https://en.wikipedia.org/wiki/Convolution>convolutions</a>. In this work we explore the effectiveness of the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>SANs</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. We demonstrate that SANs are superior in performance to their RNN and CNN counterparts by comparing their classification accuracy on six datasets as well as their model characteristics such as training speed and memory consumption. Finally, we explore the effects of various SAN modifications such as multi-head attention as well as two methods of incorporating sequence position information into SANs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6220 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6220/>Dual Memory Network Model for Biased Product Review Classification</a></strong><br><a href=/people/y/yunfei-long/>Yunfei Long</a>
|
<a href=/people/m/mingyu-ma/>Mingyu Ma</a>
|
<a href=/people/q/qin-lu/>Qin Lu</a>
|
<a href=/people/r/rong-xiang/>Rong Xiang</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6220><div class="card-body p-3 small">In sentiment analysis (SA) of product reviews, both user and product information are proven to be useful. Current tasks handle <a href=https://en.wikipedia.org/wiki/User_profile>user profile</a> and product information in a unified model which may not be able to learn salient features of users and products effectively. In this work, we propose a dual user and product memory network (DUPMN) model to learn user profiles and product reviews using separate memory networks. Then, the two <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> are used jointly for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment prediction</a>. The use of separate <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> aims to capture <a href=https://en.wikipedia.org/wiki/User_profile>user profiles</a> and <a href=https://en.wikipedia.org/wiki/Product_information>product information</a> more effectively. Compared to state-of-the-art unified prediction models, the evaluations on three benchmark datasets, <a href=https://en.wikipedia.org/wiki/Internet_Movie_Database>IMDB</a>, Yelp13, and Yelp14, show that our dual learning model gives performance gain of 0.6 %, 1.2 %, and 0.9 %, respectively. The improvements are also deemed very significant measured by <a href=https://en.wikipedia.org/wiki/P-value>p-values</a>.<i>p-values</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6221 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6221/>Measuring Issue Ownership using Word Embeddings</a></strong><br><a href=/people/a/amaru-cuba-gyllensten/>Amaru Cuba Gyllensten</a>
|
<a href=/people/m/magnus-sahlgren/>Magnus Sahlgren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6221><div class="card-body p-3 small">Sentiment and topic analysis are common methods used for <a href=https://en.wikipedia.org/wiki/Social_media_monitoring>social media monitoring</a>. Essentially, these methods answers questions such as, what is being talked about, regarding X, and what do people feel, regarding X. In this paper, we investigate another venue for <a href=https://en.wikipedia.org/wiki/Social_media_monitoring>social media monitoring</a>, namely issue ownership and <a href=https://en.wikipedia.org/wiki/Agenda_setting>agenda setting</a>, which are concepts from <a href=https://en.wikipedia.org/wiki/Political_science>political science</a> that have been used to explain voter choice and electoral outcomes. We argue that issue alignment and <a href=https://en.wikipedia.org/wiki/Agenda_setting>agenda setting</a> can be seen as a kind of semantic source similarity of the kind how similar is source A to issue owner P, when talking about issue X, and as such can be measured using word / document embedding techniques. We present work in progress towards measuring that kind of conditioned similarity, and introduce a new notion of <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> for predictive embeddings. We then test this method by measuring the similarity between politically aligned media and political parties, conditioned on bloc-specific issues.<i>X</i>&#8221;, and &#8220;what do people feel, regarding <i>X</i>&#8221;. In this paper, we investigate another venue for social media monitoring, namely <i>issue ownership</i> and <i>agenda setting</i>, which are concepts from political science that have been used to explain voter choice and electoral outcomes. We argue that issue alignment and agenda setting can be seen as a kind of semantic source similarity of the kind &#8220;how similar is source <i>A</i> to issue owner <i>P</i>, when talking about issue <i>X</i>&#8221;, and as such can be measured using word/document embedding techniques. We present work in progress towards measuring that kind of conditioned similarity, and introduce a new notion of similarity for predictive embeddings. We then test this method by measuring the similarity between politically aligned media and political parties, conditioned on bloc-specific issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6222 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6222/>Sentiment Expression Boundaries in Sentiment Polarity Classification</a></strong><br><a href=/people/r/rasoul-kaljahi/>Rasoul Kaljahi</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6222><div class="card-body p-3 small">We investigate the effect of using sentiment expression boundaries in predicting sentiment polarity in aspect-level sentiment analysis. We manually annotate a freely available English sentiment polarity dataset with these boundaries and carry out a series of experiments which demonstrate that high quality sentiment expressions can boost the performance of polarity classification. Our experiments with neural architectures also show that CNN networks outperform <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> on this task and <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6223 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6223/>Exploring and Learning Suicidal Ideation Connotations on Social Media with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a></a></strong><br><a href=/people/r/ramit-sawhney/>Ramit Sawhney</a>
|
<a href=/people/p/prachi-manchanda/>Prachi Manchanda</a>
|
<a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/r/rajiv-shah/>Rajiv Shah</a>
|
<a href=/people/r/raj-singh/>Raj Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6223><div class="card-body p-3 small">The increasing suicide rates amongst youth and its high correlation with suicidal ideation expression on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> warrants a deeper investigation into models for the detection of suicidal intent in text such as <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> to enable <a href=https://en.wikipedia.org/wiki/Suicide_prevention>prevention</a>. However, the complexity of the natural language constructs makes this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> very challenging. Deep Learning architectures such as <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a>, CNNs, and RNNs show promise in sentence level classification problems. This work investigates the ability of deep learning architectures to build an accurate and robust model for suicidal ideation detection and compares their performance with standard baselines in text classification problems. The experimental results reveal the merit in C-LSTM based models as compared to other deep learning and machine learning based classification models for suicidal ideation detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6226 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6226/>NLP at IEST 2018 : BiLSTM-Attention and LSTM-Attention via Soft Voting in Emotion Classification<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>IEST</span> 2018: <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-Attention and <span class=acl-fixed-case>LSTM</span>-Attention via Soft Voting in Emotion Classification</a></strong><br><a href=/people/q/qimin-zhou/>Qimin Zhou</a>
|
<a href=/people/h/hao-wu/>Hao Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6226><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> that competed at WASSA2018 Implicit Emotion Shared Task. The goal of this task is to classify the emotions of excluded words in tweets into six different classes : <a href=https://en.wikipedia.org/wiki/Sadness>sad</a>, <a href=https://en.wikipedia.org/wiki/Joy>joy</a>, <a href=https://en.wikipedia.org/wiki/Disgust>disgust</a>, <a href=https://en.wikipedia.org/wiki/Surprise_(emotion)>surprise</a>, <a href=https://en.wikipedia.org/wiki/Anger>anger</a> and fear. For this, we examine a BiLSTM architecture with attention mechanism (BiLSTM-Attention) and a LSTM architecture with attention mechanism (LSTM-Attention), and try different dropout rates based on these two models. We then exploit an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> of these methods to give the final prediction which improves the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance significantly compared with the baseline model. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves 7th position out of 30 teams and outperforms the baseline method by 12.5 % in terms of macro F1.<i>Implicit Emotion Shared Task</i>. The goal of this task is to classify the emotions of excluded words in tweets into six different classes: sad, joy, disgust, surprise, anger and fear. For this, we examine a BiLSTM architecture with attention mechanism (BiLSTM-Attention) and a LSTM architecture with attention mechanism (LSTM-Attention), and try different dropout rates based on these two models. We then exploit an ensemble of these methods to give the final prediction which improves the model performance significantly compared with the baseline model. The proposed method achieves 7th position out of 30 teams and outperforms the baseline method by 12.5% in terms of <i>macro F1</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6227 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6227/>SINAI at IEST 2018 : Neural Encoding of Emotional External Knowledge for Emotion Classification<span class=acl-fixed-case>SINAI</span> at <span class=acl-fixed-case>IEST</span> 2018: Neural Encoding of Emotional External Knowledge for Emotion Classification</a></strong><br><a href=/people/f/flor-miriam-plaza-del-arco/>Flor Miriam Plaza-del-Arco</a>
|
<a href=/people/e/eugenio-martinez-camara/>Eugenio Martínez-Cámara</a>
|
<a href=/people/m/m-teresa-martin-valdivia/>Maite Martin</a>
|
<a href=/people/l/l-alfonso-urena-lopez/>L. Alfonso Ureña- López</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6227><div class="card-body p-3 small">In this paper, we describe our participation in WASSA 2018 Implicit Emotion Shared Task (IEST 2018). We claim that the use of emotional external knowledge may enhance the performance and the capacity of <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> of an <a href=https://en.wikipedia.org/wiki/Emotion_classification>emotion classification system</a> based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Accordingly, we submitted four <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning systems</a> grounded in a sequence encoding layer. They mainly differ in the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature vector space</a> and the <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> used in the sequence encoding layer. The official results show that the <a href=https://en.wikipedia.org/wiki/System>systems</a> that used emotional external knowledge have a higher capacity of <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>, hence our claim holds.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6228 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6228/>EmoNLP at IEST 2018 : An Ensemble of Deep Learning Models and Gradient Boosting Regression Tree for Implicit Emotion Prediction in Tweets<span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>IEST</span> 2018: An Ensemble of Deep Learning Models and Gradient Boosting Regression Tree for Implicit Emotion Prediction in Tweets</a></strong><br><a href=/people/m/man-liu/>Man Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6228><div class="card-body p-3 small">This paper describes our system submitted to IEST 2018, a shared task (Klinger et al., 2018) to predict the emotion types. Six emotion types are involved : <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, <a href=https://en.wikipedia.org/wiki/Joy>joy</a>, <a href=https://en.wikipedia.org/wiki/Fear>fear</a>, <a href=https://en.wikipedia.org/wiki/Surprise_(emotion)>surprise</a>, <a href=https://en.wikipedia.org/wiki/Disgust>disgust</a> and <a href=https://en.wikipedia.org/wiki/Sadness>sad</a>. We perform three different approaches : feed forward neural network (FFNN), convolutional BLSTM (ConBLSTM) and Gradient Boosting Regression Tree Method (GBM). Word embeddings used in convolutional BLSTM are pre-trained on 470 million tweets which are filtered using the emotional words and emojis. In addition, broad sets of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> (i.e. syntactic features, lexicon features, cluster features) are adopted to train GBM and FFNN. The three approaches are finally ensembled by the <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted average of predicted probabilities</a> of each emotion label.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6229 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6229/>HGSGNLP at IEST 2018 : An Ensemble of Machine Learning and Deep Neural Architectures for Implicit Emotion Classification in Tweets<span class=acl-fixed-case>HGSGNLP</span> at <span class=acl-fixed-case>IEST</span> 2018: An Ensemble of Machine Learning and Deep Neural Architectures for Implicit Emotion Classification in Tweets</a></strong><br><a href=/people/w/wenting-wang/>Wenting Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6229><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> designed for the WASSA-2018 Implicit Emotion Shared Task (IEST). The task is to predict the emotion category expressed in a tweet by removing the terms angry, afraid, happy, sad, surprised, disgusted and their synonyms. Our final submission is an ensemble of one supervised learning model and three deep neural network based models, where each <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> approaches the problem from essentially different directions. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves the <a href=https://en.wikipedia.org/wiki/Macro_(computer_science)>macro F1 score</a> of 65.8 %, which is a 5.9 % performance improvement over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> and is ranked 12 out of 30 participating teams.<i>angry</i>, <i>afraid</i>, <i>happy</i>, <i>sad</i>, <i>surprised</i>, <i>disgusted</i> and their synonyms. Our final submission is an ensemble of one supervised learning model and three deep neural network based models, where each model approaches the problem from essentially different directions. Our system achieves the macro F1 score of 65.8%, which is a 5.9% performance improvement over the baseline and is ranked 12 out of 30 participating teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6232 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6232/>UWB at IEST 2018 : Emotion Prediction in Tweets with Bidirectional Long Short-Term Memory Neural Network<span class=acl-fixed-case>UWB</span> at <span class=acl-fixed-case>IEST</span> 2018: Emotion Prediction in Tweets with Bidirectional Long Short-Term Memory Neural Network</a></strong><br><a href=/people/p/pavel-priban/>Pavel Přibáň</a>
|
<a href=/people/j/jiri-martinek/>Jiří Martínek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6232><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> created for the WASSA 2018 Implicit Emotion Shared Task. The goal of this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is to predict the emotion of a given tweet, from which a certain emotion word is removed. The removed word can be sad, happy, disgusted, angry, afraid or a synonym of one of them. Our proposed <a href=https://en.wikipedia.org/wiki/System>system</a> is based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep-learning methods</a>. We use Bidirectional Long Short-Term Memory (BiLSTM) with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> as an input. Pre-trained DeepMoji model and pre-trained emoji2vec emoji embeddings are also used as additional inputs. Our <a href=https://en.wikipedia.org/wiki/System>System</a> achieves 0.657 macro F1 score and our rank is 13th out of 30.<i>sad</i>, <i>happy</i>, <i>disgusted</i>, <i>angry</i>, <i>afraid</i> or a synonym of one of them. Our proposed system is based on deep-learning methods. We use Bidirectional Long Short-Term Memory (BiLSTM) with word embeddings as an input. Pre-trained DeepMoji model and pre-trained emoji2vec emoji embeddings are also used as additional inputs. Our System achieves 0.657 macro F1 score and our rank is 13th out of 30.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6234 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6234" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6234/>EmotiKLUE at IEST 2018 : Topic-Informed Classification of Implicit Emotions<span class=acl-fixed-case>E</span>moti<span class=acl-fixed-case>KLUE</span> at <span class=acl-fixed-case>IEST</span> 2018: Topic-Informed Classification of Implicit Emotions</a></strong><br><a href=/people/t/thomas-proisl/>Thomas Proisl</a>
|
<a href=/people/p/philipp-heinrich/>Philipp Heinrich</a>
|
<a href=/people/b/besim-kabashi/>Besim Kabashi</a>
|
<a href=/people/s/stefan-evert/>Stefan Evert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6234><div class="card-body p-3 small">EmotiKLUE is a submission to the Implicit Emotion Shared Task. It is a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning system</a> that combines independent representations of the left and right contexts of the emotion word with the topic distribution of an LDA topic model. EmotiKLUE achieves a macro average Fscore of 67.13 %, significantly outperforming the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> produced by a simple ML classifier. Further enhancements after the evaluation period lead to an improved Fscore of 68.10 %.<i>F&#8321;</i>score of 67.13%, significantly outperforming the baseline produced by a simple ML classifier. Further enhancements after the evaluation period lead to an improved <i>F&#8321;</i>score of 68.10%.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6236 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6236/>Disney at IEST 2018 : Predicting Emotions using an Ensemble<span class=acl-fixed-case>IEST</span> 2018: Predicting Emotions using an Ensemble</a></strong><br><a href=/people/w/wojciech-witon/>Wojciech Witon</a>
|
<a href=/people/p/pierre-colombo/>Pierre Colombo</a>
|
<a href=/people/a/ashutosh-modi/>Ashutosh Modi</a>
|
<a href=/people/m/mubbasir-kapadia/>Mubbasir Kapadia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6236><div class="card-body p-3 small">This paper describes our participating <a href=https://en.wikipedia.org/wiki/System>system</a> in the WASSA 2018 shared task on emotion prediction. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> focuses on implicit emotion prediction in a tweet. In this task, keywords corresponding to the six emotion labels used (anger, fear, disgust, joy, sad, and surprise) have been removed from the tweet text, making emotion prediction implicit and the task challenging. We propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble of classifiers</a> for <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. Each <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> uses a sequence of Convolutional Neural Network (CNN) architecture blocks and uses ELMo (Embeddings from Language Model) as an input. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves a 66.2 % <a href=https://en.wikipedia.org/wiki/Grading_in_education>F1 score</a> on the test set. The best performing <a href=https://en.wikipedia.org/wiki/System>system</a> in the shared task has reported a 71.4 % <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6238 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6238/>Fast Approach to Build an Automatic Sentiment Annotator for Legal Domain using Transfer Learning</a></strong><br><a href=/people/v/viraj-salaka/>Viraj Salaka</a>
|
<a href=/people/m/menuka-warushavithana/>Menuka Warushavithana</a>
|
<a href=/people/n/nisansa-de-silva/>Nisansa de Silva</a>
|
<a href=/people/a/amal-shehan-perera/>Amal Shehan Perera</a>
|
<a href=/people/g/gathika-ratnayaka/>Gathika Ratnayaka</a>
|
<a href=/people/t/thejan-rupasinghe/>Thejan Rupasinghe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6238><div class="card-body p-3 small">This study proposes a novel way of identifying the sentiment of the phrases used in the <a href=https://en.wikipedia.org/wiki/Legal_term>legal domain</a>. The added <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of the language used in <a href=https://en.wikipedia.org/wiki/Law>law</a>, and the inability of the existing <a href=https://en.wikipedia.org/wiki/System>systems</a> to accurately predict the sentiments of words in law are the main motivations behind this study. This is a transfer learning approach which can be used for other domain adaptation tasks as well. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> achieves an improvement of over 6 % compared to the source model&#8217;s accuracy in the <a href=https://en.wikipedia.org/wiki/Legal_term>legal domain</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6239 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6239/>What Makes You Stressed? Finding Reasons From Tweets</a></strong><br><a href=/people/r/reshmi-gopalakrishna-pillai/>Reshmi Gopalakrishna Pillai</a>
|
<a href=/people/m/mike-thelwall/>Mike Thelwall</a>
|
<a href=/people/c/constantin-orasan/>Constantin Orasan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6239><div class="card-body p-3 small">Detecting stress from <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> gives a non-intrusive and inexpensive alternative to traditional tools such as <a href=https://en.wikipedia.org/wiki/Questionnaire>questionnaires</a> or physiological sensors for monitoring mental state of individuals. This paper introduces a novel framework for finding reasons for stress from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, analyzing multiple categories for the first time. Three word-vector based methods are evaluated on collections of tweets about <a href=https://en.wikipedia.org/wiki/Politics>politics</a> or <a href=https://en.wikipedia.org/wiki/Airline>airlines</a> and are found to be more accurate than standard <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6240.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6240 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6240 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6240/>EmojiGAN : learning emojis distributions with a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a><span class=acl-fixed-case>E</span>moji<span class=acl-fixed-case>GAN</span>: learning emojis distributions with a generative model</a></strong><br><a href=/people/b/bogdan-mazoure/>Bogdan Mazoure</a>
|
<a href=/people/t/thang-doan/>Thang Doan</a>
|
<a href=/people/s/saibal-ray/>Saibal Ray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6240><div class="card-body p-3 small">Generative models have recently experienced a surge in popularity due to the development of more efficient <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training algorithms</a> and increasing <a href=https://en.wikipedia.org/wiki/Computational_power>computational power</a>. Models such as adversarial generative networks (GANs) have been successfully used in various areas such as <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, <a href=https://en.wikipedia.org/wiki/Medical_imaging>medical imaging</a>, style transfer and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. Adversarial nets were recently shown to yield results in the image-to-text task, where given a set of images, one has to provide their corresponding text description. In this paper, we take a similar approach and propose a image-to-emoji architecture, which is trained on data from <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> and can be used to score a given picture using <a href=https://en.wikipedia.org/wiki/Ideogram>ideograms</a>. We show empirical results of our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> on data obtained from the most influential Instagram accounts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6242 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6242/>Homonym Detection For Humor Recognition In Short Text</a></strong><br><a href=/people/s/sven-van-den-beukel/>Sven van den Beukel</a>
|
<a href=/people/l/lora-aroyo/>Lora Aroyo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6242><div class="card-body p-3 small">In this paper, automatic homophone- and homograph detection are suggested as new useful <a href=https://en.wikipedia.org/wiki/Feature_detection_(computer_vision)>features</a> for humor recognition systems. The <a href=https://en.wikipedia.org/wiki/System>system</a> combines style-features from previous studies on humor recognition in short text with ambiguity-based features. The performance of two potentially useful homograph detection methods is evaluated using crowdsourced annotations as ground truth. Adding <a href=https://en.wikipedia.org/wiki/Homophone>homophones</a> and <a href=https://en.wikipedia.org/wiki/Homograph>homographs</a> as <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> to the classifier results in a small but significant improvement over the style-features alone. For the task of humor recognition, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> appears to be a more important quality measure than <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. Although the <a href=https://en.wikipedia.org/wiki/System>system</a> was designed for humor recognition in <a href=https://en.wikipedia.org/wiki/Oneliner>oneliners</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> also performs well at the classification of longer humorous texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6243 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6243" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6243/>Emo2Vec : Learning Generalized Emotion Representation by Multi-task Training<span class=acl-fixed-case>E</span>mo2<span class=acl-fixed-case>V</span>ec: Learning Generalized Emotion Representation by Multi-task Training</a></strong><br><a href=/people/p/peng-xu/>Peng Xu</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/j/ji-ho-park/>Ji Ho Park</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6243><div class="card-body p-3 small">In this paper, we propose Emo2Vec which encodes emotional semantics into vectors. We train Emo2Vec by multi-task learning six different emotion-related tasks, including emotion / sentiment analysis, sarcasm classification, stress detection, abusive language classification, insult detection, and personality recognition. Our evaluation of Emo2Vec shows that it outperforms existing affect-related representations, such as Sentiment-Specific Word Embedding and DeepMoji embeddings with much smaller training corpora. When concatenated with <a href=https://en.wikipedia.org/wiki/GloVe_(machine_learning)>GloVe</a>, Emo2Vec achieves competitive performances to state-of-the-art results on several tasks using a simple logistic regression classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6245.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6245 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6245 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6245/>Super Characters : A Conversion from Sentiment Classification to Image Classification</a></strong><br><a href=/people/b/baohua-sun/>Baohua Sun</a>
|
<a href=/people/l/lin-yang/>Lin Yang</a>
|
<a href=/people/p/patrick-dong/>Patrick Dong</a>
|
<a href=/people/w/wenhan-zhang/>Wenhan Zhang</a>
|
<a href=/people/j/jason-dong/>Jason Dong</a>
|
<a href=/people/c/charles-young/>Charles Young</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6245><div class="card-body p-3 small">We propose a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> named Super Characters for sentiment classification. This method converts the sentiment classification problem into image classification problem by projecting texts into images and then applying CNN models for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Text features are extracted automatically from the generated Super Characters images, hence there is no need of any explicit step of embedding the words or characters into numerical vector representations. Experimental results on large social media corpus show that the Super Characters method consistently outperforms other methods for sentiment classification and topic classification tasks on ten large social media datasets of millions of contents in four different languages, including <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6246 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6246/>Learning Comment Controversy Prediction in Web Discussions Using Incidentally Supervised Multi-Task CNNs<span class=acl-fixed-case>CNN</span>s</a></strong><br><a href=/people/n/nils-rethmeier/>Nils Rethmeier</a>
|
<a href=/people/m/marc-hubner/>Marc Hübner</a>
|
<a href=/people/l/leonhard-hennig/>Leonhard Hennig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6246><div class="card-body p-3 small">Comments on <a href=https://en.wikipedia.org/wiki/Online_newspaper>web news</a> contain <a href=https://en.wikipedia.org/wiki/Controversy>controversies</a> that manifest as inter-group agreement-conflicts. Tracking such rapidly evolving controversy could ease <a href=https://en.wikipedia.org/wiki/Conflict_resolution>conflict resolution</a> or journalist-user interaction. However, this presupposes controversy online-prediction that scales to diverse domains using incidental supervision signals instead of manual labeling. To more deeply interpret comment-controversy model decisions we frame prediction as <a href=https://en.wikipedia.org/wiki/Binary_classification>binary classification</a> and evaluate baselines and multi-task CNNs that use an auxiliary news-genre-encoder. Finally, we use ablation and interpretability methods to determine the impacts of topic, discourse and sentiment indicators, contextual vs. global word influence, as well as genre-keywords vs. per-genre-controversy keywords to find that the models learn plausible controversy features using only incidentally supervised signals.<i>rapidly evolving controversy</i> could ease conflict resolution or journalist-user interaction. However, this presupposes controversy online-prediction that scales to diverse domains using incidental supervision signals instead of manual labeling. To more deeply interpret comment-controversy model decisions we frame prediction as binary classification and evaluate baselines and multi-task CNNs that use an auxiliary news-genre-encoder. Finally, we use ablation and interpretability methods to determine the impacts of topic, discourse and sentiment indicators, contextual vs. global word influence, as well as genre-keywords vs. per-genre-controversy keywords &#8211; to find that the models learn plausible controversy features using only incidentally supervised signals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6248.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6248 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6248 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6248/>Predicting Adolescents’ Educational Track from Chat Messages on Dutch Social Media<span class=acl-fixed-case>D</span>utch Social Media</a></strong><br><a href=/people/l/lisa-hilte/>Lisa Hilte</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a>
|
<a href=/people/r/reinhild-vandekerckhove/>Reinhild Vandekerckhove</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6248><div class="card-body p-3 small">We aim to predict Flemish adolescents&#8217; educational track based on their Dutch social media writing. We distinguish between the three main types of Belgian secondary education : General (theory-oriented), Vocational (practice-oriented), and Technical Secondary Education (hybrid). The best results are obtained with a Naive Bayes model, i.e. an F-score of 0.68 (std. dev. 0.05) in <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>10-fold cross-validation</a> experiments on the training data and an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.60 on unseen data. Many of the most informative features are character n-grams containing specific occurrences of chatspeak phenomena such as <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a>. While the detection of the most theory- and practice-oriented educational tracks seems to be a relatively easy task, the hybrid Technical level appears to be much harder to capture based on online writing style, as expected.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6249.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6249 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6249 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6249/>Arabizi sentiment analysis based on <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> and automatic corpus annotation<span class=acl-fixed-case>A</span>rabizi sentiment analysis based on transliteration and automatic corpus annotation</a></strong><br><a href=/people/i/imane-guellil/>Imane Guellil</a>
|
<a href=/people/a/ahsan-adeel/>Ahsan Adeel</a>
|
<a href=/people/f/faical-azouaou/>Faical Azouaou</a>
|
<a href=/people/f/fodil-benali/>Fodil Benali</a>
|
<a href=/people/a/ala-eddine-hachani/>Ala-eddine Hachani</a>
|
<a href=/people/a/amir-hussain/>Amir Hussain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6249><div class="card-body p-3 small">Arabizi is a form of writing Arabic text which relies on <a href=https://en.wikipedia.org/wiki/Latin_script>Latin letters</a>, <a href=https://en.wikipedia.org/wiki/Arabic_numerals>numerals</a> and <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a> rather than <a href=https://en.wikipedia.org/wiki/Arabic_alphabet>Arabic letters</a>. In the literature, the difficulties associated with <a href=https://en.wikipedia.org/wiki/Arabizi>Arabizi sentiment analysis</a> have been underestimated, principally due to the complexity of <a href=https://en.wikipedia.org/wiki/Arabizi>Arabizi</a>. In this paper, we present an approach to automatically classify <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiments</a> of Arabizi messages into positives or negatives. In the proposed approach, Arabizi messages are first transliterated into <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. Afterwards, we automatically classify the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> of the transliterated corpus using an automatically annotated corpus. For <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus validation</a>, shallow machine learning algorithms such as Support Vectors Machine (SVM) and Naive Bays (NB) are used. Simulations results demonstrate the outperformance of NB algorithm over all others. The highest achieved F1-score is up to 78 % and 76 % for manually and automatically transliterated dataset respectively. Ongoing work is aimed at improving the transliterator module and annotated sentiment dataset.</div></div></div><hr><div id=w18-63><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-63.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-63/>Proceedings of the Third Conference on Machine Translation: Research Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6300/>Proceedings of the Third Conference on Machine Translation: Research Papers</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6301/>Scaling Neural Machine Translation</a></strong><br><a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/s/sergey-edunov/>Sergey Edunov</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6301><div class="card-body p-3 small">Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT&#8217;14 English-German translation, we match the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of Vaswani et al. (2017) in under 5 hours when training on 8 <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPUs</a> and we obtain a new state of the art of 29.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT&#8217;14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6302/>Character-level Chinese-English Translation through ASCII Encoding<span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish Translation through <span class=acl-fixed-case>ASCII</span> Encoding</a></strong><br><a href=/people/n/nikola-i-nikolov/>Nikola I. Nikolov</a>
|
<a href=/people/y/yuhuang-hu/>Yuhuang Hu</a>
|
<a href=/people/m/mi-xue-tan/>Mi Xue Tan</a>
|
<a href=/people/r/richard-h-r-hahnloser/>Richard H.R. Hahnloser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6302><div class="card-body p-3 small">Character-level Neural Machine Translation (NMT) models have recently achieved impressive results on many language pairs. They mainly do well for <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European language pairs</a>, where the languages share the same <a href=https://en.wikipedia.org/wiki/Writing_system>writing system</a>. However, for <a href=https://en.wikipedia.org/wiki/Translation>translating</a> between <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, the gap between the two different <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a> poses a major challenge because of a lack of systematic correspondence between the individual linguistic units. In this paper, we enable character-level NMT for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, by breaking down <a href=https://en.wikipedia.org/wiki/Chinese_characters>Chinese characters</a> into linguistic units similar to that of <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European languages</a>. We use the Wubi encoding scheme, which preserves the original shape and semantic information of the characters, while also being reversible. We show promising results from training Wubi-based models on the character- and subword-level with recurrent as well as convolutional models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6304/>An Analysis of Attention Mechanisms : The Case of <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>Word Sense Disambiguation</a> in Neural Machine Translation</a></strong><br><a href=/people/g/gongbo-tang/>Gongbo Tang</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6304><div class="card-body p-3 small">Recent work has shown that the encoder-decoder attention mechanisms in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> are different from the <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a> in <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> pay more attention to <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context tokens</a> when translating <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguous words</a>. We explore the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention distribution patterns</a> when translating ambiguous nouns. Counterintuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns. We conclude that <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is not the main mechanism used by NMT models to incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> for WSD. The experimental results suggest that NMT models learn to encode <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> necessary for WSD in the encoder hidden states. For the attention mechanism in Transformer models, we reveal that the first few layers gradually learn to align source and target tokens and the last few layers learn to extract <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> from the related but unaligned context tokens.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6305/>Discourse-Related Language Contrasts in English-Croatian Human and Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>C</span>roatian Human and Machine Translation</a></strong><br><a href=/people/m/margita-sostaric/>Margita Šoštarić</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6305><div class="card-body p-3 small">We present an analysis of a number of <a href=https://en.wikipedia.org/wiki/Coreference>coreference phenomena</a> in English-Croatian human and machine translations. The aim is to shed light on the differences in the way these structurally different languages make use of discourse information and provide insights for discourse-aware machine translation system development. The phenomena are automatically identified in parallel data using <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> produced by <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> and word alignment tools, enabling us to pinpoint patterns of interest in both languages. We make the <a href=https://en.wikipedia.org/wiki/Analysis>analysis</a> more fine-grained by including three corpora pertaining to three different registers. In a second step, we create a <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> with the challenging linguistic constructions and use it to evaluate the performance of three MT systems. We show that both SMT and NMT systems struggle with handling these discourse phenomena, even though NMT tends to perform somewhat better than SMT. By providing an overview of patterns frequently occurring in actual language use, as well as by pointing out the weaknesses of current MT systems that commonly mistranslate them, we hope to contribute to the effort of resolving the issue of discourse phenomena in MT applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6306/>Coreference and Coherence in Neural Machine Translation : A Study Using Oracle Experiments</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6306><div class="card-body p-3 small">Cross-sentence context can provide valuable information in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> and is critical for translation of anaphoric pronouns and for providing consistent translations. In this paper, we devise simple oracle experiments targeting <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and <a href=https://en.wikipedia.org/wiki/Coherence_(physics)>coherence</a>. Oracles are an easy way to evaluate the effect of different discourse-level phenomena in NMT using BLEU and eliminate the necessity to manually define challenge sets for this purpose. We propose two context-aware NMT models and compare them against <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> working on a concatenation of consecutive sentences. Concatenation models perform better, but are computationally expensive. We show that NMT models taking advantage of context oracle signals can achieve considerable gains in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, of up to 7.02 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> and 1.89 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> on subtitles translation. Access to strong signals allows us to make clear comparisons between context-aware models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6307" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6307/>A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/mathias-muller/>Mathias Müller</a>
|
<a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/e/elena-voita/>Elena Voita</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6307><div class="card-body p-3 small">The translation of pronouns presents a special challenge to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to this day, since <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> often requires context outside the current sentence. Recent work on <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. However, <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that quantify the overall translation quality are ill-equipped to measure gains from additional context. We argue that a different kind of <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is needed to assess how well <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> translate inter-sentential phenomena such as <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>. This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns. Furthermore, we perform experiments with several context-aware models. We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on our contrastive test set. Our experiments also show the effectiveness of parameter tying for multi-encoder architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6309/>A neural interlingua for multilingual machine translation</a></strong><br><a href=/people/y/yichao-lu/>Yichao Lu</a>
|
<a href=/people/p/phillip-keung/>Phillip Keung</a>
|
<a href=/people/f/faisal-ladhak/>Faisal Ladhak</a>
|
<a href=/people/v/vikas-bhardwaj/>Vikas Bhardwaj</a>
|
<a href=/people/s/shaonan-zhang/>Shaonan Zhang</a>
|
<a href=/people/j/jason-sun/>Jason Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6309><div class="card-body p-3 small">We incorporate an explicit neural interlingua into a multilingual encoder-decoder neural machine translation (NMT) architecture. We demonstrate that our model learns a language-independent representation by performing direct zero-shot translation (without using pivot translation), and by using the source sentence embeddings to create an English Yelp review classifier that, through the mediation of the neural interlingua, can also classify French and German reviews. Furthermore, we show that, despite using a smaller number of parameters than a pairwise collection of bilingual NMT models, our approach produces comparable BLEU scores for each language pair in WMT15.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6310/>Improving Neural Language Models with Weight Norm Initialization and Regularization</a></strong><br><a href=/people/c/christian-herold/>Christian Herold</a>
|
<a href=/people/y/yingbo-gao/>Yingbo Gao</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6310><div class="card-body p-3 small">Embedding and projection matrices are commonly used in neural language models (NLM) as well as in other sequence processing networks that operate on large vocabularies. We examine such <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrices</a> in fine-tuned language models and observe that a NLM learns word vectors whose norms are related to the word frequencies. We show that by initializing the weight norms with scaled log word counts, together with other techniques, lower perplexities can be obtained in early epochs of training. We also introduce a weight norm regularization loss term, whose hyperparameters are tuned via a <a href=https://en.wikipedia.org/wiki/Grid_search>grid search</a>. With this <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a>, we are able to significantly improve perplexities on two word-level language modeling tasks (without dynamic evaluation): from 54.44 to 53.16 on <a href=https://en.wikipedia.org/wiki/Penn_Treebank>Penn Treebank (PTB)</a> and from 61.45 to 60.13 on <a href=https://en.wikipedia.org/wiki/WikiText>WikiText-2 (WT2)</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6311" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6311/>Contextual Neural Model for Translating Bilingual Multi-Speaker Conversations</a></strong><br><a href=/people/s/sameen-maruf/>Sameen Maruf</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6311><div class="card-body p-3 small">Recent works in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> have begun to explore document translation. However, translating online multi-speaker conversations is still an open problem. In this work, we propose the task of translating Bilingual Multi-Speaker Conversations, and explore neural architectures which exploit both source and target-side conversation histories for this task. To initiate an evaluation for this task, we introduce <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> extracted from <a href=https://en.wikipedia.org/wiki/Europarl>Europarl v7</a> and OpenSubtitles2016. Our experiments on four language-pairs confirm the significance of leveraging conversation history, both in terms of <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6313" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6313/>Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation</a></strong><br><a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/a/arya-d-mccarthy/>Arya D. McCarthy</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/r/rebecca-marvin/>Rebecca Marvin</a>
|
<a href=/people/p/paul-mcnamee/>Paul McNamee</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/t/tim-anderson/>Tim Anderson</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6313><div class="card-body p-3 small">To better understand the effectiveness of continued training, we analyze the major components of a neural machine translation system (the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, <a href=https://en.wikipedia.org/wiki/Code>decoder</a>, and each embedding space) and consider each component&#8217;s contribution to, and capacity for, domain adaptation. We find that freezing any single component during continued training has minimal impact on performance, and that performance is surprisingly good when a single component is adapted while holding the rest of the model fixed. We also find that continued training does not move the model very far from the out-of-domain model, compared to a sensitivity analysis metric, suggesting that the out-of-domain model can provide a good generic initialization for the new domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6314/>Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection</a></strong><br><a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a>
|
<a href=/people/m/macduff-hughes/>Macduff Hughes</a>
|
<a href=/people/t/tetsuji-nakagawa/>Tetsuji Nakagawa</a>
|
<a href=/people/c/ciprian-chelba/>Ciprian Chelba</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6314><div class="card-body p-3 small">Measuring domain relevance of data and identifying or selecting well-fit domain data for machine translation (MT) is a well-studied topic, but denoising is not yet. Denoising is concerned with a different type of <a href=https://en.wikipedia.org/wiki/Data_quality>data quality</a> and tries to reduce the negative impact of data noise on MT training, in particular, neural MT (NMT) training. This paper generalizes methods for measuring and selecting data for domain MT and applies them to denoising NMT training. The proposed approach uses trusted data and a denoising curriculum realized by online data selection. Intrinsic and extrinsic evaluations of the approach show its significant effectiveness for NMT to train on data with severe noise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6315/>Using Monolingual Data in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> : a Systematic Study</a></strong><br><a href=/people/f/franck-burlot/>Franck Burlot</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6315><div class="card-body p-3 small">Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (Phrase-Based MT) is the way monolingual target data, which often abounds, is used in these two paradigms. While Phrase-Based MT can seamlessly integrate very large language models trained on billions of sentences, the best option for Neural MT developers seems to be the generation of artificial parallel data through back-translation-a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6319 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W18-6319.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W18-6319/>A Call for Clarity in Reporting BLEU Scores<span class=acl-fixed-case>BLEU</span> Scores</a></strong><br><a href=/people/m/matt-post/>Matt Post</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6319><div class="card-body p-3 small">The field of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to the <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a>, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These <a href=https://en.wikipedia.org/wiki/Parameter>parameters</a> are often not reported or are hard to find, and consequently, BLEU scores between papers can not be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6320/>Exploring gap filling as a cheaper alternative to reading comprehension questionnaires when evaluating <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> for gisting</a></strong><br><a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6320><div class="card-body p-3 small">A popular application of machine translation (MT) is gisting : MT is consumed as is to make sense of text in a foreign language. Evaluation of the usefulness of MT for gisting is surprisingly uncommon. The classical method uses reading comprehension questionnaires (RCQ), in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. Recently, gap-filling (GF), a form of cloze testing, has been proposed as a cheaper alternative to RCQ. In GF, certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. This paper reports, for the first time, a comparative evaluation, using both RCQ and GF, of translations from multiple MT systems for the same foreign texts, and a systematic study on the effect of variables such as gap density, gap-selection strategies, and document context in GF. The main findings of the study are : (a) both <a href=https://en.wikipedia.org/wiki/Questionnaire_construction>RCQ</a> and GF clearly identify MT to be useful ; (b) global RCQ and GF rankings for the MT systems are mostly in agreement ; (c) GF scores vary very widely across informants, making comparisons among MT systems hard, and (d) unlike <a href=https://en.wikipedia.org/wiki/Questionnaire_construction>RCQ</a>, which is framed around documents, GF evaluation can be framed at the sentence level. These findings support the use of <a href=https://en.wikipedia.org/wiki/Glucosamine>GF</a> as a cheaper alternative to <a href=https://en.wikipedia.org/wiki/Carboxylic_acid>RCQ</a>.<i>gisting</i>: MT is consumed <i>as is</i> to make sense of text in a foreign language. Evaluation of the usefulness of MT for gisting is surprisingly uncommon. The classical method uses <i>reading comprehension questionnaires</i> (RCQ), in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. Recently, <i>gap-filling</i> (GF), a form of <i>cloze</i> testing, has been proposed as a cheaper alternative to RCQ. In GF, certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. This paper reports, for the first time, a comparative evaluation, using both RCQ and GF, of translations from multiple MT systems for the same foreign texts, and a systematic study on the effect of variables such as gap density, gap-selection strategies, and document context in GF. The main findings of the study are: (a) both RCQ and GF clearly identify MT to be useful; (b) global RCQ and GF rankings for the MT systems are mostly in agreement; (c) GF scores vary very widely across informants, making comparisons among MT systems hard, and (d) unlike RCQ, which is framed around documents, GF evaluation can be framed at the sentence level. These findings support the use of GF as a cheaper alternative to RCQ.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6321/>Simple Fusion : Return of the <a href=https://en.wikipedia.org/wiki/Language_model>Language Model</a></a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/j/james-cross/>James Cross</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6321><div class="card-body p-3 small">Neural Machine Translation (NMT) typically leverages <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> in training through backtranslation. We investigate an alternative simple method to use monolingual data for NMT training : We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch. To achieve that, we train the translation model to predict the <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>residual probability</a> of the training data added to the prediction of the LM. This enables the TM to focus its capacity on modeling the source sentence since it can rely on the LM for <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. We show that our method outperforms previous approaches to integrate LMs into NMT while the architecture is simpler as it does not require gating networks to balance TM and LM. We observe gains of between +0.24 and +2.36 BLEU on all four test sets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top of ensembles without LM. We compare our method with alternative ways to utilize monolingual data such as backtranslation, shallow fusion, and <a href=https://en.wikipedia.org/wiki/Cold_fusion>cold fusion</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6324/>Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation</a></strong><br><a href=/people/z/zhong-zhou/>Zhong Zhou</a>
|
<a href=/people/m/matthias-sperber/>Matthias Sperber</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6324><div class="card-body p-3 small">We work on <a href=https://en.wikipedia.org/wiki/Translation>translation</a> from rich-resource languages to low-resource languages. The main challenges we identify are the lack of low-resource language data, effective methods for cross-lingual transfer, and the variable-binding problem that is common in neural systems. We build a <a href=https://en.wikipedia.org/wiki/Machine_translation>translation system</a> that addresses these challenges using eight <a href=https://en.wikipedia.org/wiki/Languages_of_Europe>European language families</a> as our test ground. Firstly, we add the source and the target family labels and study intra-family and inter-family influences for effective cross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for English-Swedish translation using eight families compared to the single-family multi-source multi-target baseline. Moreover, we find that training on two neighboring families closest to the low-resource language is often enough. Secondly, we construct an ablation study and find that reasonably good results can be achieved even with considerably less target data. Thirdly, we address the variable-binding problem by building an order-preserving named entity translation model. We obtain 60.6 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in qualitative evaluation where our translations are akin to human translations in a preliminary study.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6325 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6325/>Trivial Transfer Learning for Low-Resource Neural Machine Translation</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6325><div class="card-body p-3 small">Transfer learning has been proven as an effective technique for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> under low-resource conditions. Existing methods require a common target language, <a href=https://en.wikipedia.org/wiki/Language_family>language relatedness</a>, or specific training tricks and regimes. We present a simple transfer learning method, where we first train a parent model for a high-resource language pair and then continue the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> on a low-resource pair only by replacing the training corpus. This <a href=https://en.wikipedia.org/wiki/Child_model>child model</a> performs significantly better than the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> trained for low-resource pair only. We are the first to show this for targeting different languages, and we observe the improvements even for unrelated languages with different alphabets.</div></div></div><hr><div id=w18-64><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-64.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-64/>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6400/>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6403/>Findings of the WMT 2018 Biomedical Translation Shared Task : Evaluation on Medline test sets<span class=acl-fixed-case>WMT</span> 2018 Biomedical Translation Shared Task: Evaluation on <span class=acl-fixed-case>M</span>edline test sets</a></strong><br><a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/c/cristian-grozea/>Cristian Grozea</a>
|
<a href=/people/a/amy-siu/>Amy Siu</a>
|
<a href=/people/m/madeleine-kittner/>Madeleine Kittner</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6403><div class="card-body p-3 small">Machine translation enables the <a href=https://en.wikipedia.org/wiki/Machine_translation>automatic translation</a> of textual documents between languages and can facilitate access to information only available in a given language for non-speakers of this language, e.g. research results presented in scientific publications. In this paper, we provide an overview of the Biomedical Translation shared task in the Workshop on Machine Translation (WMT) 2018, which specifically examined the performance of machine translation systems for biomedical texts. This year, we provided test sets of scientific publications from two sources (EDP and Medline) and for six language pairs (English with each of Chinese, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a> and Spanish). We describe the development of the various <a href=https://en.wikipedia.org/wiki/Test_(assessment)>test sets</a>, the submissions that we received and the evaluations that we carried out. We obtained a total of 39 runs from six teams and some of this year&#8217;s BLEU scores were somewhat higher that last year&#8217;s, especially for teams that made use of biomedical resources or state-of-the-art MT algorithms (e.g. Transformer). Finally, our manual evaluation scored automatic translations higher than the reference translations for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6404/>An Empirical Study of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> for the Shared Task of WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/c/chao-bei/>Chao Bei</a>
|
<a href=/people/h/hao-zong/>Hao Zong</a>
|
<a href=/people/y/yiming-wang/>Yiming Wang</a>
|
<a href=/people/b/baoyong-fan/>Baoyong Fan</a>
|
<a href=/people/s/shiqi-li/>Shiqi Li</a>
|
<a href=/people/c/conghu-yuan/>Conghu Yuan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6404><div class="card-body p-3 small">This paper describes the Global Tone Communication Co., Ltd.&#8217;s submission of the WMT18 shared news translation task. We participated in the English-to-Chinese direction and get the best BLEU (43.8) scores among all the participants. The submitted <a href=https://en.wikipedia.org/wiki/System>system</a> focus on data clearing and techniques to build a competitive <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Unlike other participants, the submitted system are mainly relied on the data filtering to obtain the best BLEU score. We do data filtering not only for provided sentences but also for the back translated sentences. The techniques we apply for data filtering include filtering by rules, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and translation models. We also conduct several experiments to validate the effectiveness of training techniques. According to our experiments, the Annealing Adam optimizing function and ensemble decoding are the most effective techniques for the model training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6406/>The TALP-UPC Machine Translation Systems for WMT18 News Shared Translation Task<span class=acl-fixed-case>TALP</span>-<span class=acl-fixed-case>UPC</span> Machine Translation Systems for <span class=acl-fixed-case>WMT</span>18 News Shared Translation Task</a></strong><br><a href=/people/n/noe-casas/>Noe Casas</a>
|
<a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6406><div class="card-body p-3 small">In this article we describe the TALP-UPC research group participation in the WMT18 news shared translation task for Finnish-English and Estonian-English within the multi-lingual subtrack. All of our primary submissions implement an attention-based Neural Machine Translation architecture. Given that <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> and Estonian belong to the same language family and are similar, we use as training data the combination of the datasets of both language pairs to paliate the data scarceness of each individual pair. We also report the translation quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>systems</a> trained on individual language pair data to serve as baseline and comparison reference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6411/>The AFRL WMT18 Systems : Ensembling, Continuation and Combination<span class=acl-fixed-case>AFRL</span> <span class=acl-fixed-case>WMT</span>18 Systems: Ensembling, Continuation and Combination</a></strong><br><a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/t/tim-anderson/>Tim Anderson</a>
|
<a href=/people/g/grant-erdmann/>Grant Erdmann</a>
|
<a href=/people/k/katherine-young/>Katherine Young</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6411><div class="card-body p-3 small">This paper describes the Air Force Research Laboratory (AFRL) machine translation systems and the improvements that were developed during the WMT18 evaluation campaign. This year, we examined the developments and additions to popular neural machine translation toolkits and measure improvements in performance on the RussianEnglish language pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6412/>The University of Edinburgh’s Submissions to the WMT18 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s Submissions to the <span class=acl-fixed-case>WMT</span>18 News Translation Task</a></strong><br><a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/a/antonio-valerio-miceli-barone/>Antonio Valerio Miceli Barone</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6412><div class="card-body p-3 small">The University of Edinburgh made submissions to all 14 language pairs in the news translation task, with strong performances in most pairs. We introduce new RNN-variant, mixed RNN / Transformer ensembles, data selection and weighting, and extensions to back-translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6416/>CUNI Submissions in WMT18<span class=acl-fixed-case>CUNI</span> Submissions in <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/r/roman-sudarikov/>Roman Sudarikov</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6416><div class="card-body p-3 small">We participated in the WMT 2018 shared news translation task in three language pairs : <a href=https://en.wikipedia.org/wiki/Estonian_language>English-Estonian</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>English-Finnish</a>, and <a href=https://en.wikipedia.org/wiki/Czech_language>English-Czech</a>. Our main focus was the low-resource language pair of Estonian and English for which we utilized Finnish parallel data in a simple method. We first train a parent model for the high-resource language pair followed by <a href=https://en.wikipedia.org/wiki/Adaptation>adaptation</a> on the related low-resource language pair. This approach brings a substantial performance boost over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a> trained only on Estonian-English parallel data. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> are based on the Transformer architecture. For the English to Czech translation, we have evaluated our last year models of hybrid phrase-based approach and <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> mainly for comparison purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6418/>JUCBNMT at WMT2018 News Translation Task : Character Based Neural Machine Translation of Finnish to English<span class=acl-fixed-case>JUCBNMT</span> at <span class=acl-fixed-case>WMT</span>2018 News Translation Task: Character Based Neural Machine Translation of <span class=acl-fixed-case>F</span>innish to <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/s/sainik-mahata/>Sainik Kumar Mahata</a>
|
<a href=/people/d/dipankar-das/>Dipankar Das</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6418><div class="card-body p-3 small">In the current work, we present a description of the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted to WMT 2018 News Translation Shared task. The <a href=https://en.wikipedia.org/wiki/System>system</a> was created to translate news text from <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> used a Character Based Neural Machine Translation model to accomplish the given <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The current paper documents the preprocessing steps, the description of the submitted <a href=https://en.wikipedia.org/wiki/System>system</a> and the results produced using the same. Our <a href=https://en.wikipedia.org/wiki/System>system</a> garnered a BLEU score of 12.9.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6420/>PROMT Systems for WMT 2018 Shared Translation Task<span class=acl-fixed-case>PROMT</span> Systems for <span class=acl-fixed-case>WMT</span> 2018 Shared Translation Task</a></strong><br><a href=/people/a/alexander-molchanov/>Alexander Molchanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6420><div class="card-body p-3 small">This paper describes the PROMT submissions for the WMT 2018 Shared News Translation Task. This year we participated only in the English-Russian language pair. We built two primary neural networks-based systems : 1) a pure Marian-based neural system and 2) a hybrid system which incorporates OpenNMT-based neural post-editing component into our RBMT engine. We also submitted pure rule-based translation (RBMT) for <a href=https://en.wikipedia.org/wiki/Contrast_(linguistics)>contrast</a>. We show competitive results with both primary submissions which significantly outperform the <a href=https://en.wikipedia.org/wiki/Randomized_controlled_trial>RBMT baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6422/>The Karlsruhe Institute of Technology Systems for the News Translation Task in WMT 2018<span class=acl-fixed-case>WMT</span> 2018</a></strong><br><a href=/people/n/ngoc-quan-pham/>Ngoc-Quan Pham</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/a/alex-waibel/>Alexander Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6422><div class="card-body p-3 small">We present our experiments in the scope of the news translation task in WMT 2018, in directions : EnglishGerman. The core of our systems is the encoder-decoder based neural machine translation models using the transformer architecture. We enhanced the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with a deeper architecture. By using techniques to limit the memory consumption, we were able to train <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> that are 4 times larger on one <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a> and improve the performance by 1.2 BLEU points. Furthermore, we performed sentence selection for the newly available ParaCrawl corpus. Thereby, we could improve the effectiveness of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> by 0.5 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6424 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6424/>CUNI Transformer Neural MT System for WMT18<span class=acl-fixed-case>CUNI</span> Transformer Neural <span class=acl-fixed-case>MT</span> System for <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/m/martin-popel/>Martin Popel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6424><div class="card-body p-3 small">We describe our NMT system submitted to the WMT2018 shared task in news translation. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is based on the Transformer model (Vaswani et al., 2017). We use an improved technique of backtranslation, where we iterate the process of translating monolingual data in one direction and training an NMT model for the opposite direction using synthetic parallel data. We apply a simple but effective <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filtering of the synthetic data</a>. We pre-process the input sentences using <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> in order to disambiguate the gender of pro-dropped personal pronouns. Finally, we apply two simple post-processing substitutions on the translated output. Our system is significantly (p 0.05) better than all other English-Czech and Czech-English systems in WMT2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6426 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6426" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6426/>The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018<span class=acl-fixed-case>RWTH</span> <span class=acl-fixed-case>A</span>achen <span class=acl-fixed-case>U</span>niversity Supervised Machine Translation Systems for <span class=acl-fixed-case>WMT</span> 2018</a></strong><br><a href=/people/j/julian-schamper/>Julian Schamper</a>
|
<a href=/people/j/jan-rosendahl/>Jan Rosendahl</a>
|
<a href=/people/p/parnia-bahar/>Parnia Bahar</a>
|
<a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/a/arne-nix/>Arne Nix</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6426><div class="card-body p-3 small">This paper describes the statistical machine translation systems developed at RWTH Aachen University for the GermanEnglish, EnglishTurkish and ChineseEnglish translation tasks of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). We use ensembles of neural machine translation systems based on the Transformer architecture. Our main focus is on the GermanEnglish task where we to all automatic scored first with respect metrics provided by the organizers. We identify data selection, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, <a href=https://en.wikipedia.org/wiki/Batch_processing>batch size</a> and model dimension as important hyperparameters. In total we improve by 6.8 % <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> over our last year&#8217;s submission and by 4.8 % <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> over the winning system of the 2017 GermanEnglish task. In EnglishTurkish task, we show 3.6 % <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> improvement over the last year&#8217;s winning system. We further report results on the <a href=https://en.wikipedia.org/wiki/English_language>ChineseEnglish task</a> where we improve 2.2 % BLEU on average over our baseline systems but stay behind the 2018 winning systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6427 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6427/>The University of Cambridge’s Machine Translation Systems for WMT18<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>C</span>ambridge’s Machine Translation Systems for <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/a/adria-de-gispert/>Adrià de Gispert</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6427><div class="card-body p-3 small">The University of Cambridge submission to the WMT18 news translation task focuses on the combination of diverse models of translation. We compare recurrent, convolutional, and self-attention-based neural models on German-English, English-German, and Chinese-English. Our final <a href=https://en.wikipedia.org/wiki/System>system</a> combines all neural models together with a phrase-based SMT system in an MBR-based scheme. We report small but consistent gains on top of strong Transformer ensembles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6428.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6428 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6428 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6428/>The LMU Munich Unsupervised Machine Translation Systems<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Unsupervised Machine Translation Systems</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6428><div class="card-body p-3 small">We describe LMU Munich&#8217;s unsupervised machine translation systems for EnglishGerman translation. These systems were used to participate in the WMT18 news translation shared task and more specifically, for the unsupervised learning sub-track. The systems are trained on English and German monolingual data only and exploit and combine previously proposed techniques such as using word-by-word translated data based on bilingual word embeddings, denoising and on-the-fly backtranslation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6429/>Tencent Neural Machine Translation Systems for WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/l/li-gong/>Li Gong</a>
|
<a href=/people/w/wenhuan-zhu/>Wenhuan Zhu</a>
|
<a href=/people/j/jun-xie/>Jun Xie</a>
|
<a href=/people/c/chao-bian/>Chao Bian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6429><div class="card-body p-3 small">We participated in the WMT 2018 shared news translation task on EnglishChinese language pair. Our systems are based on attentional sequence-to-sequence models with some form of <a href=https://en.wikipedia.org/wiki/Recursion>recursion</a> and self-attention. Some data augmentation methods are also introduced to improve the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. The best <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translation</a> result is obtained with ensemble and reranking techniques. Our ChineseEnglish system achieved the highest cased BLEU score among all 16 submitted systems, and our EnglishChinese system ranked the third out of 18 submitted systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6431/>The University of Maryland’s Chinese-English Neural Machine Translation Systems at WMT18<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>M</span>aryland’s <span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish Neural Machine Translation Systems at <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/w/weijia-xu/>Weijia Xu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6431><div class="card-body p-3 small">This paper describes the University of Maryland&#8217;s submission to the WMT 2018 ChineseEnglish news translation tasks. Our systems are BPE-based self-attentional Transformer networks with parallel and backtranslated monolingual training data. Using ensembling and <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a>, we improve over the Transformer baseline by +1.4 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for ChineseEnglish and +3.97 BLEU for EnglishChinese on newstest2017. Our best systems reach <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> of 24.4 for <a href=https://en.wikipedia.org/wiki/Chinese_English>ChineseEnglish</a> and 39.0 for <a href=https://en.wikipedia.org/wiki/Chinese_English>EnglishChinese</a> on newstest2018.<i>newstest2017</i>. Our best systems reach BLEU scores of 24.4 for Chinese&#8594;English and 39.0 for English&#8594;Chinese on <i>newstest2018</i>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6432.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6432 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6432 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6432/>EvalD Reference-Less Discourse Evaluation for WMT18<span class=acl-fixed-case>E</span>val<span class=acl-fixed-case>D</span> Reference-Less Discourse Evaluation for <span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/j/jiri-mirovsky/>Jiří Mírovský</a>
|
<a href=/people/k/katerina-rysova/>Kateřina Rysová</a>
|
<a href=/people/m/magdalena-rysova/>Magdaléna Rysová</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6432><div class="card-body p-3 small">We present the results of automatic evaluation of <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> in machine translation (MT) outputs using the EVALD tool. EVALD was originally designed and trained to assess the quality of <a href=https://en.wikipedia.org/wiki/Writing>human writing</a>, for <a href=https://en.wikipedia.org/wiki/First_language>native speakers</a> and foreign-language learners. MT has seen a tremendous leap in translation quality at the level of sentences and it is thus interesting to see if the human-level evaluation is becoming relevant.<i>human</i> writing, for native speakers and foreign-language learners. MT has seen a tremendous leap in translation quality at the level of sentences and it is thus interesting to see if the human-level evaluation is becoming relevant.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6436 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6436/>Fine-grained evaluation of German-English Machine Translation based on a Test Suite<span class=acl-fixed-case>G</span>erman-<span class=acl-fixed-case>E</span>nglish Machine Translation based on a Test Suite</a></strong><br><a href=/people/v/vivien-macketanz/>Vivien Macketanz</a>
|
<a href=/people/e/eleftherios-avramidis/>Eleftherios Avramidis</a>
|
<a href=/people/a/aljoscha-burchardt/>Aljoscha Burchardt</a>
|
<a href=/people/h/hans-uszkoreit/>Hans Uszkoreit</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6436><div class="card-body p-3 small">We present an analysis of 16 state-of-the-art MT systems on German-English based on a linguistically-motivated test suite. The <a href=https://en.wikipedia.org/wiki/Test_suite>test suite</a> has been devised manually by a team of language professionals in order to cover a broad variety of linguistic phenomena that MT often fails to translate properly. It contains 5,000 test sentences covering 106 linguistic phenomena in 14 categories, with an increased focus on <a href=https://en.wikipedia.org/wiki/Grammatical_tense>verb tenses</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_mood>moods</a>. The MT outputs are evaluated in a semi-automatic way through <a href=https://en.wikipedia.org/wiki/Regular_expression>regular expressions</a> that focus only on the part of the sentence that is relevant to each phenomenon. Through our analysis, we are able to compare <a href=https://en.wikipedia.org/wiki/System>systems</a> based on their performance on these categories. Additionally, we reveal strengths and weaknesses of particular <a href=https://en.wikipedia.org/wiki/Linguistic_system>systems</a> and we identify grammatical phenomena where the overall performance of MT is relatively low.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6437 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6437/>The Word Sense Disambiguation Test Suite at WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/m/mathias-muller/>Mathias Müller</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6437><div class="card-body p-3 small">We present a task to measure an MT system&#8217;s capability to translate ambiguous words with their correct sense according to the given context. The task is based on the GermanEnglish Word Sense Disambiguation (WSD) test set ContraWSD (Rios Gonzales et al., 2017), but it has been filtered to reduce noise, and the evaluation has been adapted to assess MT output directly rather than scoring existing translations. We evaluate all GermanEnglish submissions to the WMT&#8217;18 shared translation task, plus a number of submissions from previous years, and find that performance on the task has markedly improved compared to the 2016 WMT submissions (81%93 % accuracy on the WSD task). We also find that the unsupervised submissions to the task have a low WSD capability, and predominantly translate ambiguous source words with the same sense.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6440.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6440 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6440 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6440/>The AFRL-Ohio State WMT18 Multimodal System : Combining Visual with Traditional<span class=acl-fixed-case>AFRL</span>-<span class=acl-fixed-case>O</span>hio <span class=acl-fixed-case>S</span>tate <span class=acl-fixed-case>WMT</span>18 Multimodal System: Combining Visual with Traditional</a></strong><br><a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a>
|
<a href=/people/j/joshua-sandvick/>Joshua Sandvick</a>
|
<a href=/people/m/michael-hutt/>Michael Hutt</a>
|
<a href=/people/g/grant-erdmann/>Grant Erdmann</a>
|
<a href=/people/j/john-duselis/>John Duselis</a>
|
<a href=/people/j/james-davis/>James Davis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6440><div class="card-body p-3 small">AFRL-Ohio State extends its usage of visual domain-driven machine translation for use as a peer with traditional <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>. As a peer, it is enveloped into a system combination of neural and statistical MT systems to present a composite translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6441 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6441/>CUNI System for the WMT18 Multimodal Translation Task<span class=acl-fixed-case>CUNI</span> System for the <span class=acl-fixed-case>WMT</span>18 Multimodal Translation Task</a></strong><br><a href=/people/j/jindrich-helcl/>Jindřich Helcl</a>
|
<a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/d/dusan-varis/>Dušan Variš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6441><div class="card-body p-3 small">We present our submission to the WMT18 Multimodal Translation Task. The main feature of our submission is applying a self-attentive network instead of a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>. We evaluate two methods of incorporating the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>visual features</a> in the <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> : first, we include the image representation as another input to the network ; second, we train the <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> to predict the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>visual features</a> and use it as an auxiliary objective. For our submission, we acquired both textual and multimodal additional data. Both of the proposed methods yield significant improvements over <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent networks</a> and self-attentive textual baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6442 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6442/>Sheffield Submissions for WMT18 Multimodal Translation Shared Task<span class=acl-fixed-case>S</span>heffield Submissions for <span class=acl-fixed-case>WMT</span>18 Multimodal Translation Shared Task</a></strong><br><a href=/people/c/chiraag-lala/>Chiraag Lala</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Swaroop Madhyastha</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6442><div class="card-body p-3 small">This paper describes the University of Sheffield&#8217;s submissions to the WMT18 Multimodal Machine Translation shared task. We participated in both tasks 1 and 1b. For task 1, we build on a standard sequence to sequence attention-based neural machine translation system (NMT) and investigate the utility of multimodal re-ranking approaches. More specifically, n-best translation candidates from this system are re-ranked using novel multimodal cross-lingual word sense disambiguation models. For task 1b, we explore three approaches : (i) re-ranking based on cross-lingual word sense disambiguation (as for task 1), (ii) re-ranking based on consensus of NMT n-best lists from German-Czech, French-Czech and English-Czech systems, and (iii) data augmentation by generating English source data through <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> from French to English and from German to English followed by hypothesis selection using a multimodal-reranker.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6444.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6444 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6444 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6444/>Translation of Biomedical Documents with Focus on Spanish-English<span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/m/mirela-stefania-duma/>Mirela-Stefania Duma</a>
|
<a href=/people/w/wolfgang-menzel/>Wolfgang Menzel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6444><div class="card-body p-3 small">For the WMT 2018 shared task of translating documents pertaining to the Biomedical domain, we developed a scoring formula that uses an unsophisticated and effective method of weighting term frequencies and was integrated in a data selection pipeline. The method was applied on five language pairs and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> performed best on <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese-English</a>, where a BLEU score of 41.84 placed <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> third out of seven runs submitted by three institutions. In this paper, we describe our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> and results with a special focus on <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish-English</a> where we compare it against a state-of-the-art method. Our contribution to the task lies in introducing a fast, <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> for selecting domain-specific data for training models which obtain good results using only 10 % of the general domain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6445 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6445/>Ensemble of Translators with Automatic Selection of the Best Translation the submission of FOKUS to the WMT 18 biomedical translation task<span class=acl-fixed-case>FOKUS</span> to the <span class=acl-fixed-case>WMT</span> 18 biomedical translation task –</a></strong><br><a href=/people/c/cristian-grozea/>Cristian Grozea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6445><div class="card-body p-3 small">This paper describes the system of Fraunhofer FOKUS for the WMT 2018 biomedical translation task. Our approach, described here, was to automatically select the most promising <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> from a set of candidates produced with NMT (Transformer) models. We selected the highest fidelity translation of each sentence by using a <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a>, <a href=https://en.wikipedia.org/wiki/Stemming>stemming</a> and a set of <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>. Our method is simple, can use any machine translators, and requires no further training in addition to that already employed to build the NMT models. The downside is that the score did not increase over the best in <a href=https://en.wikipedia.org/wiki/Musical_ensemble>ensemble</a>, but was quite close to it (difference about 0.5 BLEU).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6446.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6446 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6446 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6446/>LMU Munich’s Neural Machine Translation Systems at WMT 2018<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich’s Neural Machine Translation Systems at <span class=acl-fixed-case>WMT</span> 2018</a></strong><br><a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6446><div class="card-body p-3 small">We present the LMU Munich machine translation systems for the EnglishGerman language pair. We have built neural machine translation systems for both translation directions (EnglishGerman and GermanEnglish) and for two different domains (the biomedical domain and the news domain). The systems were used for our participation in the WMT18 biomedical translation task and in the shared task on machine translation of news. The main focus of our recent system development efforts has been on achieving improvements in the biomedical domain over last year&#8217;s strong biomedical translation engine for EnglishGerman (Huck et al., 2017a). Considerable progress has been made in the latter <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, which we report on in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6447 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6447/>Hunter NMT System for WMT18 Biomedical Translation Task : Transfer Learning in Neural Machine Translation<span class=acl-fixed-case>NMT</span> System for <span class=acl-fixed-case>WMT</span>18 Biomedical Translation Task: Transfer Learning in Neural Machine Translation</a></strong><br><a href=/people/a/abdul-khan/>Abdul Khan</a>
|
<a href=/people/s/subhadarshi-panda/>Subhadarshi Panda</a>
|
<a href=/people/j/jia-xu/>Jia Xu</a>
|
<a href=/people/l/lampros-flokas/>Lampros Flokas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6447><div class="card-body p-3 small">This paper describes the submission of Hunter Neural Machine Translation (NMT) to the WMT&#8217;18 Biomedical translation task from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/French_language>French</a>. The discrepancy between training and test data distribution brings a challenge to translate text in new domains. Beyond the previous work of combining in-domain with out-of-domain models, we found accuracy and efficiency gain in combining different in-domain models. We conduct extensive experiments on NMT with <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We train on different in-domain Biomedical datasets one after another. That means parameters of the previous training serve as the initialization of the next one. Together with a pre-trained out-of-domain News model, we enhanced translation quality with 3.73 BLEU points over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Furthermore, we applied <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble learning</a> on training models of intermediate epochs and achieved an improvement of 4.02 BLEU points over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Overall, our system is 11.29 BLEU points above the best system of last year on the EDP 2017 test set.<i>transfer learning</i>. We train on different in-domain Biomedical datasets one after another. That means parameters of the previous training serve as the initialization of the next one. Together with a pre-trained out-of-domain News model, we enhanced translation quality with 3.73 BLEU points over the baseline. Furthermore, we applied ensemble learning on training models of intermediate epochs and achieved an improvement of 4.02 BLEU points over the baseline. Overall, our system is 11.29 BLEU points above the best system of last year on the EDP 2017 test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6448.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6448 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6448 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6448/>UFRGS Participation on the WMT Biomedical Translation Shared Task<span class=acl-fixed-case>UFRGS</span> Participation on the <span class=acl-fixed-case>WMT</span> Biomedical Translation Shared Task</a></strong><br><a href=/people/f/felipe-soares/>Felipe Soares</a>
|
<a href=/people/k/karin-becker/>Karin Becker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6448><div class="card-body p-3 small">This paper describes the machine translation systems developed by the Universidade Federal do Rio Grande do Sul (UFRGS) team for the biomedical translation shared task. Our systems are based on <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> and <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, using the Moses and OpenNMT toolkits, respectively. We participated in four translation directions for the English / Spanish and English / Portuguese language pairs. To create our training data, we concatenated several parallel corpora, both from in-domain and out-of-domain sources, as well as terminological resources from <a href=https://en.wikipedia.org/wiki/Unified_Modeling_Language>UMLS</a>. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved the best <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a> according to the official shared task evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6449 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6449/>Neural Machine Translation with the Transformer and Multi-Source Romance Languages for the Biomedical WMT 2018 task<span class=acl-fixed-case>R</span>omance Languages for the Biomedical <span class=acl-fixed-case>WMT</span> 2018 task</a></strong><br><a href=/people/b/brian-tubay/>Brian Tubay</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6449><div class="card-body p-3 small">The Transformer architecture has become the state-of-the-art in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>. This <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, which relies on <a href=https://en.wikipedia.org/wiki/Attentional_control>attention-based mechanisms</a>, has outperformed previous neural machine translation architectures in several tasks. In this system description paper, we report details of training <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> with multi-source Romance languages with the Transformer model and in the evaluation frame of the biomedical WMT 2018 task. Using <a href=https://en.wikipedia.org/wiki/List_of_programming_languages_by_type>multi-source languages</a> from the same family allows improvements of over 6 BLEU points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6455.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6455 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6455 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6455/>ITER : Improving Translation Edit Rate through Optimizable Edit Costs<span class=acl-fixed-case>ITER</span>: Improving Translation Edit Rate through Optimizable Edit Costs</a></strong><br><a href=/people/j/joybrata-panja/>Joybrata Panja</a>
|
<a href=/people/s/sudip-kumar-naskar/>Sudip Kumar Naskar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6455><div class="card-body p-3 small">The paper presents our participation in the WMT 2018 Metrics Shared Task. We propose an improved version of Translation Edit / Error Rate (TER). In addition to including the basic edit operations in TER, namely-insertion, deletion, substitution and shift, our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> also allows stem matching, optimizable edit costs and better <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> so as to correlate better with human judgement scores. The proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> shows much higher correlation with <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a> than TER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6456.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6456 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6456 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6456" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6456/>RUSE : Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation<span class=acl-fixed-case>RUSE</span>: Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation</a></strong><br><a href=/people/h/hiroki-shimanaka/>Hiroki Shimanaka</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6456><div class="card-body p-3 small">We introduce the RUSE metric for the WMT18 metrics shared task. Sentence embeddings can capture global information that can not be captured by local features based on character or word N-grams. Although training <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> using small-scale translation datasets with manual evaluation is difficult, <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> trained from large-scale data in other tasks can improve the automatic evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We use a multi-layer perceptron regressor based on three types of <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a>. The experimental results of the WMT16 and WMT17 datasets show that the RUSE metric achieves a state-of-the-art performance in both segment- and system-level metrics tasks with embedding features only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6459.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6459 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6459 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6459/>Neural Machine Translation for English-Tamil<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>T</span>amil</a></strong><br><a href=/people/h/himanshu-choudhary/>Himanshu Choudhary</a>
|
<a href=/people/a/aditya-kumar-pathak/>Aditya Kumar Pathak</a>
|
<a href=/people/r/rajiv-ratan-saha/>Rajiv Ratan Saha</a>
|
<a href=/people/p/ponnurangam-kumaraguru/>Ponnurangam Kumaraguru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6459><div class="card-body p-3 small">A huge amount of valuable resources is available on the web in English, which are often translated into local languages to facilitate knowledge sharing among local people who are not much familiar with <a href=https://en.wikipedia.org/wiki/English_language>English</a>. However, translating such content manually is very tedious, costly, and time-consuming process. To this end, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> is an efficient approach to translate text without any human involvement. Neural machine translation (NMT) is one of the most recent and effective <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation technique</a> amongst all existing <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>. In this paper, we apply NMT for English-Tamil language pair. We propose a novel neural machine translation technique using word-embedding along with Byte-Pair-Encoding (BPE) to develop an efficient translation system that overcomes the OOV (Out Of Vocabulary) problem for languages which do not have much translations available online. We use the BLEU score for evaluating the <a href=https://en.wikipedia.org/wiki/System>system</a> performance. Experimental results confirm that our proposed MIDAS translator (8.33 BLEU score) outperforms <a href=https://en.wikipedia.org/wiki/Google_Translate>Google translator</a> (3.75 BLEU score).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6460.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6460 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6460 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6460/>The Benefit of Pseudo-Reference Translations in Quality Estimation of MT Output<span class=acl-fixed-case>MT</span> Output</a></strong><br><a href=/people/m/melania-duma/>Melania Duma</a>
|
<a href=/people/w/wolfgang-menzel/>Wolfgang Menzel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6460><div class="card-body p-3 small">In this paper, a novel approach to Quality Estimation is introduced, which extends the method in (Duma and Menzel, 2017) by also considering pseudo-reference translations as data sources to the tree and sequence kernels used before. Two variants of the system were submitted to the sentence level WMT18 Quality Estimation Task for the English-German language pair. They have been ranked 4th and 6th out of 13 systems in the SMT track, while in the NMT track ranks 4 and 5 out of 11 submissions have been reached.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6461.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6461 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6461 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6461/>Supervised and Unsupervised Minimalist Quality Estimators : Vicomtech’s Participation in the WMT 2018 Quality Estimation Task<span class=acl-fixed-case>WMT</span> 2018 Quality Estimation Task</a></strong><br><a href=/people/t/thierry-etchegoyhen/>Thierry Etchegoyhen</a>
|
<a href=/people/e/eva-martinez-garcia/>Eva Martínez Garcia</a>
|
<a href=/people/a/andoni-azpeitia/>Andoni Azpeitia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6461><div class="card-body p-3 small">We describe Vicomtech&#8217;s participation in the WMT 2018 shared task on quality estimation, for which we submitted minimalist quality estimators. The core of our approach is based on two simple features : <a href=https://en.wikipedia.org/wiki/Lexical_overlap>lexical translation overlaps</a> and language model cross-entropy scores. These <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are exploited in two system variants : uMQE is an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised system</a>, where the final quality score is obtained by averaging individual feature scores ; sMQE is a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised variant</a>, where the final score is estimated by a Support Vector Regressor trained on the available annotated datasets. The main goal of our minimalist approach to quality estimation is to provide reliable estimators that require minimal deployment effort, few resources, and, in the case of uMQE, do not depend on costly data annotation or <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>. Our approach was applied to all language pairs in sentence quality estimation, obtaining competitive results across the board.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6463.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6463 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6463 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6463/>Sheffield Submissions for the WMT18 Quality Estimation Shared Task<span class=acl-fixed-case>S</span>heffield Submissions for the <span class=acl-fixed-case>WMT</span>18 Quality Estimation Shared Task</a></strong><br><a href=/people/j/julia-ive/>Julia Ive</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/f/frederic-blain/>Frédéric Blain</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6463><div class="card-body p-3 small">In this paper we present the University of Sheffield submissions for the WMT18 Quality Estimation shared task. We discuss our submissions to all four sub-tasks, where ours is the only team to participate in all language pairs and variations (37 combinations). Our <a href=https://en.wikipedia.org/wiki/System>systems</a> show competitive results and outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> in nearly all cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6464 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6464/>UAlacant machine translation quality estimation at WMT 2018 : a simple approach using phrase tables and feed-forward neural networks<span class=acl-fixed-case>UA</span>lacant machine translation quality estimation at <span class=acl-fixed-case>WMT</span> 2018: a simple approach using phrase tables and feed-forward neural networks</a></strong><br><a href=/people/f/felipe-sanchez-martinez/>Felipe Sánchez-Martínez</a>
|
<a href=/people/m/miquel-espla-gomis/>Miquel Esplà-Gomis</a>
|
<a href=/people/m/mikel-l-forcada/>Mikel L. Forcada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6464><div class="card-body p-3 small">We describe the Universitat d&#8217;Alacant submissions to the word- and sentence-level machine translation (MT) quality estimation (QE) shared task at WMT 2018. Our approach to word-level MT QE builds on previous work to mark the words in the machine-translated sentence as OK or BAD, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word. Our sentence-level submission simply uses the edit operations predicted by the word-level approach to approximate TER. The method presented ranked first in the sub-task of identifying insertions in gaps for three out of the six datasets, and second in the rest of them.<i>OK</i> or <i>BAD</i>, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word. Our sentence-level submission simply uses the edit operations predicted by the word-level approach to approximate TER. The method presented ranked first in the sub-task of identifying insertions in gaps for three out of the six datasets, and second in the rest of them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6465.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6465 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6465 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6465/>Alibaba Submission for WMT18 Quality Estimation Task<span class=acl-fixed-case>A</span>libaba Submission for <span class=acl-fixed-case>WMT</span>18 Quality Estimation Task</a></strong><br><a href=/people/j/jiayi-wang/>Jiayi Wang</a>
|
<a href=/people/k/kai-fan/>Kai Fan</a>
|
<a href=/people/b/bo-li/>Bo Li</a>
|
<a href=/people/f/fengming-zhou/>Fengming Zhou</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/y/yangbin-shi/>Yangbin Shi</a>
|
<a href=/people/l/luo-si/>Luo Si</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6465><div class="card-body p-3 small">The goal of WMT 2018 Shared Task on Translation Quality Estimation is to investigate automatic methods for estimating the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> results without reference translations. This paper presents the QE Brain system, which proposes the neural Bilingual Expert model as a feature extractor based on conditional target language model with a bidirectional transformer and then processes the semantic representations of source and the translation output with a Bi-LSTM predictive model for automatic quality estimation. The <a href=https://en.wikipedia.org/wiki/System>system</a> has been applied to the sentence-level scoring and ranking tasks as well as the word-level tasks for finding errors for each word in translations. An extensive set of experimental results have shown that our <a href=https://en.wikipedia.org/wiki/System>system</a> outperformed the best results in WMT 2017 Quality Estimation tasks and obtained top results in WMT 2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6466.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6466 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6466 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6466/>Quality Estimation with Force-Decoded Attention and Cross-lingual Embeddings</a></strong><br><a href=/people/e/elizaveta-yankovskaya/>Elizaveta Yankovskaya</a>
|
<a href=/people/a/andre-tattar/>Andre Tättar</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6466><div class="card-body p-3 small">This paper describes the submissions of the team from the University of Tartu for the sentence-level Quality Estimation shared task of WMT18. The proposed models use features based on attention weights of a neural machine translation system and cross-lingual phrase embeddings as input features of a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a>. Two of the proposed models require only a neural machine translation system with an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> with no additional resources. Results show that combining <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> and baseline features leads to significant improvements over the baseline features alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6467.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6467 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6467 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6467/>MS-UEdin Submission to the WMT2018 APE Shared Task : Dual-Source Transformer for Automatic Post-Editing<span class=acl-fixed-case>MS</span>-<span class=acl-fixed-case>UE</span>din Submission to the <span class=acl-fixed-case>WMT</span>2018 <span class=acl-fixed-case>APE</span> Shared Task: Dual-Source Transformer for Automatic Post-Editing</a></strong><br><a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6467><div class="card-body p-3 small">This paper describes the Microsoft and University of Edinburgh submission to the Automatic Post-editing shared task at WMT2018. Based on training data and systems from the WMT2017 shared task, we re-implement our own models from the last shared task and introduce improvements based on extensive parameter sharing. Next we experiment with our implementation of dual-source transformer models and data selection for the IT domain. Our submissions decisively wins the SMT post-editing sub-task establishing the new state-of-the-art and is a very close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task. Based on the rather weak results in the NMT sub-task, we hypothesize that neural-on-neural APE might not be actually useful.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6468.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6468 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6468 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6468/>A Transformer-Based Multi-Source Automatic Post-Editing System</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/n/nico-herbig/>Nico Herbig</a>
|
<a href=/people/a/antonio-kruger/>Antonio Krüger</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6468><div class="card-body p-3 small">This paper presents our EnglishGerman Automatic Post-Editing (APE) system submitted to the APE Task organized at WMT 2018 (Chatterjee et al., 2018). The proposed model is an extension of the transformer architecture : two separate self-attention-based encoders encode the machine translation output (mt) and the source (src), followed by a joint encoder that attends over a combination of these two encoded sequences (encsrc and encmt) for generating the post-edited sentence. We compare this multi-source architecture (i.e, <a href=https://en.wikipedia.org/wiki/Source_code>src</a>, mt pe) to a monolingual transformer (i.e., mt pe) model and an ensemble combining the multi-source src, mt pe and single-source mt pe models. For both the PBSMT and the NMT task, the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> yields the best results, followed by the multi-source model and last the single-source approach. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, the <a href=https://en.wikipedia.org/wiki/Ensemble_cast>ensemble</a>, achieves a BLEU score of 66.16 and 74.22 for the PBSMT and NMT task, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6469.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6469 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6469 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6469/>DFKI-MLT System Description for the WMT18 Automatic Post-editing Task<span class=acl-fixed-case>DFKI</span>-<span class=acl-fixed-case>MLT</span> System Description for the <span class=acl-fixed-case>WMT</span>18 Automatic Post-editing Task</a></strong><br><a href=/people/d/daria-pylypenko/>Daria Pylypenko</a>
|
<a href=/people/r/raphael-rubino/>Raphael Rubino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6469><div class="card-body p-3 small">This paper presents the Automatic Post-editing (APE) systems submitted by the DFKI-MLT group to the WMT&#8217;18 APE shared task. Three monolingual neural sequence-to-sequence APE systems were trained using target-language data only : one using an attentional recurrent neural network architecture and two using the attention-only (transformer) architecture. The <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> was composed of machine translated (MT) output used as source to the APE model aligned with their manually post-edited version or reference translation as target. We made use of the provided training sets only and trained APE models applicable to phrase-based and neural MT outputs. Results show better performances reached by the attention-only model over the recurrent one, significant improvement over the baseline when post-editing phrase-based MT output but degradation when applied to neural MT output.<i>transformer</i>) architecture. The training data was composed of machine translated (MT) output used as source to the APE model aligned with their manually post-edited version or reference translation as target. We made use of the provided training sets only and trained APE models applicable to phrase-based and neural MT outputs. Results show better performances reached by the attention-only model over the recurrent one, significant improvement over the baseline when post-editing phrase-based MT output but degradation when applied to neural MT output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6472.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6472 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6472 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6472/>The Speechmatics Parallel Corpus Filtering System for WMT18<span class=acl-fixed-case>WMT</span>18</a></strong><br><a href=/people/t/tom-ash/>Tom Ash</a>
|
<a href=/people/r/remi-francis/>Remi Francis</a>
|
<a href=/people/w/will-williams/>Will Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6472><div class="card-body p-3 small">Our entry to the parallel corpus filtering task uses a two-step strategy. The first step uses a series of pragmatic hard &#8216;rules&#8217; to remove the worst example sentences. This first step reduces the effective corpus size down from the initial 1 billion to 160 million tokens. The second step uses four different <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>heuristics</a> weighted to produce a score that is then used for further filtering down to 100 or 10 million tokens. Our final <a href=https://en.wikipedia.org/wiki/System>system</a> produces competitive results without requiring excessive fine tuning to the exact task or language pair. The first step in isolation provides a very fast filter that gives most of the gains of the final <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6477.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6477 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6477 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6477/>An Unsupervised System for Parallel Corpus Filtering</a></strong><br><a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6477><div class="card-body p-3 small">In this paper we describe LMU Munich&#8217;s submission for the WMT 2018 Parallel Corpus Filtering shared task which addresses the problem of cleaning noisy parallel corpora. The task of mining and cleaning parallel sentences is important for improving the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>, especially for low-resource languages. We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal. After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing monolingual similarity as well. Our <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised system</a> achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems, while not requiring any parallel training data.<i>WMT 2018 Parallel Corpus Filtering</i> shared task which addresses the problem of cleaning noisy parallel corpora. The task of mining and cleaning parallel sentences is important for improving the quality of machine translation systems, especially for low-resource languages. We tackle this problem in a fully unsupervised fashion relying on bilingual word embeddings created without any bilingual signal. After pre-filtering noisy data we rank sentence pairs by calculating bilingual sentence-level similarities and then remove redundant data by employing monolingual similarity as well. Our unsupervised system achieved good performance during the official evaluation of the shared task, scoring only a few BLEU points behind the best systems, while not requiring any parallel training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6478.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6478 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6478 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6478/>Dual Conditional Cross-Entropy Filtering of Noisy Parallel Corpora</a></strong><br><a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6478><div class="card-body p-3 small">In this work we introduce dual conditional cross-entropy filtering for noisy parallel data. For each sentence pair of the noisy parallel corpus we compute cross-entropy scores according to two inverse translation models trained on clean data. We penalize divergent cross-entropies and weigh the penalty by the cross-entropy average of both models. Sorting or thresholding according to these scores results in better subsets of parallel data. We achieve higher BLEU scores with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on parallel data filtered only from Paracrawl than with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on clean WMT data. We further evaluate our method in the context of the WMT2018 shared task on parallel corpus filtering and achieve the overall highest ranking scores of the shared task, scoring top in three out of four subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6480.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6480 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6480 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6480/>Measuring sentence parallelism using Mahalanobis distances : The NRC unsupervised submissions to the WMT18 Parallel Corpus Filtering shared task<span class=acl-fixed-case>NRC</span> unsupervised submissions to the <span class=acl-fixed-case>WMT</span>18 Parallel Corpus Filtering shared task</a></strong><br><a href=/people/p/patrick-littell/>Patrick Littell</a>
|
<a href=/people/s/samuel-larkin/>Samuel Larkin</a>
|
<a href=/people/d/darlene-stewart/>Darlene Stewart</a>
|
<a href=/people/m/michel-simard/>Michel Simard</a>
|
<a href=/people/c/cyril-goutte/>Cyril Goutte</a>
|
<a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6480><div class="card-body p-3 small">The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to score sentence pairs from a large high-recall, low-precision web-scraped parallel corpus (Koehn et al., 2018a). Participants could use existing sample corpora (e.g. past WMT data) as a supervisory signal to learn what a clean corpus looks like. However, in lower-resource situations it often happens that the target corpus of the language is the only sample of <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel text</a> in that language. We therefore made several unsupervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one tasktranslating the European Medicines Agency corpus (Tiedemann, 2009)scored among the best systems even in the 10M-word conditions.<i>only</i> sample of parallel text in that language. We therefore made several unsupervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one task&#8212;translating the European Medicines Agency corpus (Tiedemann, 2009)&#8212;scored among the best systems even in the 10M-word conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6481.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6481 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6481 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6481/>Accurate semantic textual similarity for cleaning noisy parallel corpora using semantic machine translation evaluation metric : The NRC supervised submissions to the Parallel Corpus Filtering task<span class=acl-fixed-case>NRC</span> supervised submissions to the Parallel Corpus Filtering task</a></strong><br><a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a>
|
<a href=/people/m/michel-simard/>Michel Simard</a>
|
<a href=/people/d/darlene-stewart/>Darlene Stewart</a>
|
<a href=/people/s/samuel-larkin/>Samuel Larkin</a>
|
<a href=/people/c/cyril-goutte/>Cyril Goutte</a>
|
<a href=/people/p/patrick-littell/>Patrick Littell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6481><div class="card-body p-3 small">We present our semantic textual similarity approach in filtering a noisy web crawled parallel corpus using YiSia novel semantic machine translation evaluation metric. The systems mainly based on this supervised approach perform well in the WMT18 Parallel Corpus Filtering shared task (4th place in 100-million-word evaluation, 8th place in 10-million-word evaluation, and 6th place overall, out of 48 submissions). In fact, our best performing systemNRC-yisi-bicov is one of the only four submissions ranked top 10 in both evaluations. Our submitted systems also include some initial filtering steps for scaling down the size of the test corpus and a final redundancy removal step for better semantic and token coverage of the filtered corpus. In this paper, we also describe our unsuccessful attempt in automatically synthesizing a noisy parallel development corpus for tuning the weights to combine different parallelism and fluency features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6482.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6482 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6482 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6482/>Alibaba Submission to the WMT18 Parallel Corpus Filtering Task<span class=acl-fixed-case>A</span>libaba Submission to the <span class=acl-fixed-case>WMT</span>18 Parallel Corpus Filtering Task</a></strong><br><a href=/people/j/jun-lu/>Jun Lu</a>
|
<a href=/people/x/xiaoyu-lv/>Xiaoyu Lv</a>
|
<a href=/people/y/yangbin-shi/>Yangbin Shi</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6482><div class="card-body p-3 small">This paper describes the Alibaba Machine Translation Group submissions to the WMT 2018 Shared Task on Parallel Corpus Filtering. While evaluating the quality of the <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a>, the three characteristics of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> are investigated, i.e. 1) the <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual / translation quality</a>, 2) the <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual quality</a> and 3) the <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus diversity</a>. Both rule-based and model-based methods are adapted to score the parallel sentence pairs. The final parallel corpus filtering system is reliable, easy to build and adapt to other language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6483.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6483 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6483 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6483/>UTFPR at WMT 2018 : Minimalistic Supervised Corpora Filtering for Machine Translation<span class=acl-fixed-case>UTFPR</span> at <span class=acl-fixed-case>WMT</span> 2018: Minimalistic Supervised Corpora Filtering for Machine Translation</a></strong><br><a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6483><div class="card-body p-3 small">We present the UTFPR systems at the WMT 2018 parallel corpus filtering task. Our supervised approach discerns between good and bad translations by training classic binary classification models over an artificially produced binary classification dataset derived from a high-quality translation set, and a minimalistic set of 6 semantic distance features that rely only on easy-to-gather resources. We rank translations by their probability for the good label. Our results show that <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> pairs best with our approach, yielding more consistent results throughout the different settings evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6484.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6484 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6484 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6484/>The ILSP / ARC submission to the WMT 2018 Parallel Corpus Filtering Shared Task<span class=acl-fixed-case>ILSP</span>/<span class=acl-fixed-case>ARC</span> submission to the <span class=acl-fixed-case>WMT</span> 2018 Parallel Corpus Filtering Shared Task</a></strong><br><a href=/people/v/vassilis-papavassiliou/>Vassilis Papavassiliou</a>
|
<a href=/people/s/sokratis-sofianopoulos/>Sokratis Sofianopoulos</a>
|
<a href=/people/p/prokopis-prokopidis/>Prokopis Prokopidis</a>
|
<a href=/people/s/stelios-piperidis/>Stelios Piperidis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6484><div class="card-body p-3 small">This paper describes the submission of the Institute for Language and Speech Processing / Athena Research and Innovation Center (ILSP / ARC) for the WMT 2018 Parallel Corpus Filtering shared task. We explore several properties of sentences and sentence pairs that our system explored in the context of the task with the purpose of clustering sentence pairs according to their appropriateness in training MT systems. We also discuss alternative methods for ranking the sentence pairs of the most appropriate clusters with the aim of generating the two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> (of 10 and 100 million words as required in the task) that were evaluated. By summarizing the results of several experiments that were carried out by the organizers during the evaluation phase, our submission achieved an average BLEU score of 26.41, even though it does not make use of any language-specific resources like <a href=https://en.wikipedia.org/wiki/Bilingual_dictionary>bilingual lexica</a>, monolingual corpora, or MT output, while the average score of the best participant system was 27.91.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6485.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6485 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6485 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6485/>SYSTRAN Participation to the WMT2018 Shared Task on Parallel Corpus Filtering<span class=acl-fixed-case>SYSTRAN</span> Participation to the <span class=acl-fixed-case>WMT</span>2018 Shared Task on Parallel Corpus Filtering</a></strong><br><a href=/people/m/minh-quang-pham/>MinhQuang Pham</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6485><div class="card-body p-3 small">This paper describes the participation of <a href=https://en.wikipedia.org/wiki/SYSTRAN>SYSTRAN</a> to the shared task on parallel corpus filtering at the Third Conference on Machine Translation (WMT 2018). We participate for the first time using a neural sentence similarity classifier which aims at predicting the relatedness of sentence pairs in a multilingual context. The paper describes the main characteristics of our approach and discusses the results obtained on the data sets published for the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6487.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6487 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6487 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-6487/>The RWTH Aachen University Filtering System for the WMT 2018 Parallel Corpus Filtering Task<span class=acl-fixed-case>RWTH</span> <span class=acl-fixed-case>A</span>achen <span class=acl-fixed-case>U</span>niversity Filtering System for the <span class=acl-fixed-case>WMT</span> 2018 Parallel Corpus Filtering Task</a></strong><br><a href=/people/n/nick-rossenbach/>Nick Rossenbach</a>
|
<a href=/people/j/jan-rosendahl/>Jan Rosendahl</a>
|
<a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/m/miguel-graca/>Miguel Graça</a>
|
<a href=/people/a/aman-gokrani/>Aman Gokrani</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6487><div class="card-body p-3 small">This paper describes the submission of RWTH Aachen University for the DeEn parallel corpus filtering task of the EMNLP 2018 Third Conference on Machine Translation (WMT 2018). We use several rule-based, heuristic methods to preselect sentence pairs. These <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence pairs</a> are scored with count-based and neural systems as language and translation models. In addition to single sentence-pair scoring, we further implement a simple redundancy removing heuristic. Our best performing corpus filtering system relies on <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural language models</a> and <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation models</a> based on the transformer architecture. A <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on 10 M randomly sampled tokens reaches a performance of 9.2 % BLEU on newstest2018. Using our filtering and ranking techniques we achieve 34.8 % BLEU.<i>EMNLP 2018 Third Conference on Machine Translation</i> (WMT 2018). We use several rule-based, heuristic methods to preselect sentence pairs. These sentence pairs are scored with count-based and neural systems as language and translation models. In addition to single sentence-pair scoring, we further implement a simple redundancy removing heuristic. Our best performing corpus filtering system relies on recurrent neural language models and translation models based on the transformer architecture. A model trained on 10M randomly sampled tokens reaches a performance of 9.2% BLEU on newstest2018. Using our filtering and ranking techniques we achieve 34.8% BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-6488.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-6488 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-6488 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-6488" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-6488/>Prompsit’s submission to WMT 2018 Parallel Corpus Filtering shared task<span class=acl-fixed-case>WMT</span> 2018 Parallel Corpus Filtering shared task</a></strong><br><a href=/people/v/victor-m-sanchez-cartagena/>Víctor M. Sánchez-Cartagena</a>
|
<a href=/people/m/marta-banon/>Marta Bañón</a>
|
<a href=/people/s/sergio-ortiz-rojas/>Sergio Ortiz-Rojas</a>
|
<a href=/people/g/gema-ramirez-sanchez/>Gema Ramírez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-6488><div class="card-body p-3 small">This paper describes Prompsit Language Engineering&#8217;s submissions to the WMT 2018 parallel corpus filtering shared task. Our four submissions were based on an automatic classifier for identifying pairs of sentences that are mutual translations. A set of hand-crafted hard rules for discarding sentences with evident flaws were applied before the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a>. We explored different strategies for achieving a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training corpus</a> with diverse vocabulary and fluent sentences : language model scoring, an active-learning-inspired data selection algorithm and n-gram saturation. Our submissions were very competitive in comparison with other participants on the 100 million word training corpus.</div></div></div><hr><div id=2018iwslt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2018.iwslt-1/>Proceedings of the 15th International Conference on Spoken Language Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.0/>Proceedings of the 15th International Conference on Spoken Language Translation</a></strong><br><a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/m/marcello-frederico/>Marcello Frederico</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.2/>Unsupervised Parallel Sentence Extraction from Comparable Corpora</a></strong><br><a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/f/fabienne-braune/>Fabienne Braune</a>
|
<a href=/people/y/yuliya-kalasouskaya/>Yuliya Kalasouskaya</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--2><div class="card-body p-3 small">Mining parallel sentences from comparable corpora is of great interest for many downstream tasks. In the BUCC 2017 shared task, <a href=https://en.wikipedia.org/wiki/System>systems</a> performed well by training on gold standard parallel sentences. However, we often want to mine <a href=https://en.wikipedia.org/wiki/Parallelism_(grammar)>parallel sentences</a> without bilingual supervision. We present a simple approach relying on bilingual word embeddings trained in an unsupervised fashion. We incorporate orthographic similarity in order to handle words with similar surface forms. In addition, we propose a dynamic threshold method to decide if a candidate sentence-pair is parallel which eliminates the need to fine tune a static value for different datasets. Since we do not employ any language specific engineering our approach is highly generic. We show that our approach is effective, on three language-pairs, without the use of any bilingual signal which is important because parallel sentence mining is most useful in low resource scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.4/>Analyzing Knowledge Distillation in Neural Machine Translation</a></strong><br><a href=/people/d/dakun-zhang/>Dakun Zhang</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--4><div class="card-body p-3 small">Knowledge distillation has recently been successfully applied to <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. It allows for building shrunk networks while the resulting <a href=https://en.wikipedia.org/wiki/System>systems</a> retain most of the quality of the original <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. Despite the fact that many authors report on the benefits of knowledge distillation, few have discussed the actual reasons why it works, especially in the context of neural MT. In this paper, we conduct several experiments aimed at understanding why and how <a href=https://en.wikipedia.org/wiki/Distillation>distillation</a> impacts <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on an English-German translation task. We show that translation complexity is actually reduced when building a distilled / synthesised bi-text when compared to the reference bi-text. We further remove noisy data from synthesised translations and merge filtered synthesised data together with original reference, thus achieving additional gains in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Student Paper"><i class="fas fa-award"></i></span></span>
<span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.7/>Multi-Source Neural Machine Translation with Data Augmentation</a></strong><br><a href=/people/y/yuta-nishimura/>Yuta Nishimura</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--7><div class="card-body p-3 small">Multi-source translation systems translate from multiple languages to a single target language. By using information from these multiple sources, these <a href=https://en.wikipedia.org/wiki/System>systems</a> achieve large gains in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. To train these systems, it is necessary to have corpora with parallel text in multiple sources and the target language. However, these <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> are rarely complete in practice due to the difficulty of providing human translations in all of the relevant languages. In this paper, we propose a data augmentation approach to fill such incomplete parts using multi-source neural machine translation (NMT). In our experiments, results varied over different language combinations but significant gains were observed when using a source language similar to the target language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.10/>The USTC-NEL Speech Translation system at IWSLT 2018<span class=acl-fixed-case>USTC</span>-<span class=acl-fixed-case>NEL</span> Speech Translation system at <span class=acl-fixed-case>IWSLT</span> 2018</a></strong><br><a href=/people/d/dan-liu/>Dan Liu</a>
|
<a href=/people/j/junhua-liu/>Junhua Liu</a>
|
<a href=/people/w/wu-guo/>Wu Guo</a>
|
<a href=/people/s/shifu-xiong/>Shifu Xiong</a>
|
<a href=/people/z/zhiqiang-ma/>Zhiqiang Ma</a>
|
<a href=/people/r/rui-song/>Rui Song</a>
|
<a href=/people/c/chongliang-wu/>Chongliang Wu</a>
|
<a href=/people/q/quan-liu/>Quan Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--10><div class="card-body p-3 small">This paper describes the USTC-NEL (short for National Engineering Laboratory for Speech and Language Information Processing University of science and technology of china) system to the speech translation task of the IWSLT Evaluation 2018. The system is a conventional <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline system</a> which contains 3 modules : <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Video_post-processing>post-processing</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We train a group of hybrid-HMM models for our <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, and for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> we train transformer based neural machine translation models with speech recognition output style text as input. Experiments conducted on the IWSLT 2018 task indicate that, compared to baseline system from KIT, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved 14.9 BLEU improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.11/>The ADAPT System Description for the IWSLT 2018 Basque to English Translation Task<span class=acl-fixed-case>ADAPT</span> System Description for the <span class=acl-fixed-case>IWSLT</span> 2018 <span class=acl-fixed-case>B</span>asque to <span class=acl-fixed-case>E</span>nglish Translation Task</a></strong><br><a href=/people/a/alberto-poncelas/>Alberto Poncelas</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/k/kepa-sarasola/>Kepa Sarasola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--11><div class="card-body p-3 small">In this paper we present the ADAPT system built for the Basque to English Low Resource MT Evaluation Campaign. Basque is a low-resourced, morphologically-rich language. This poses a challenge for Neural Machine Translation models which usually achieve better performance when trained with large sets of data. Accordingly, we used <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a> to improve the translation quality produced by a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> built using only authentic data. Our proposal uses back-translated data to : (a) create new sentences, so the system can be trained with more data ; and (b) translate sentences that are close to the test set, so the model can be fine-tuned to the document to be translated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.13/>The MeMAD Submission to the IWSLT 2018 Speech Translation Task<span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>MAD</span> Submission to the <span class=acl-fixed-case>IWSLT</span> 2018 Speech Translation Task</a></strong><br><a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/a/aku-rouhe/>Aku Rouhe</a>
|
<a href=/people/s/stig-arnegronroos/>Stig-ArneGrönroos</a>
|
<a href=/people/m/mikko-kurimo/>Mikko Kurimo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--13><div class="card-body p-3 small">This paper describes the MeMAD project entry to the IWSLT Speech Translation Shared Task, addressing the translation of English audio into German text. Between the <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> and end-to-end model tracks, we participated only in the former, with three contrastive systems. We tried also the latter, but were not able to finish our end-to-end model in time. All of our systems start by transcribing the audio into text through an automatic speech recognition (ASR) model trained on the TED-LIUM English Speech Recognition Corpus (TED-LIUM). Afterwards, we feed the transcripts into English-German text-based neural machine translation (NMT) models. Our systems employ three different translation models trained on separate training sets compiled from the English-German part of the TED Speech Translation Corpus (TED-TRANS) and the OPENSUBTITLES2018 section of the OPUS collection. In this paper, we also describe the experiments leading up to our final <a href=https://en.wikipedia.org/wiki/Thermodynamic_system>systems</a>. Our experiments indicate that using OPENSUBTITLES2018 in training significantly improves <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance. We also experimented with various preand postprocessing routines for the NMT module, but we did not have much success with these. Our best-scoring system attains a BLEU score of 16.45 on the test set for this year&#8217;s task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.17/>Samsung and University of Edinburgh’s System for the IWSLT 2018 Low Resource MT Task<span class=acl-fixed-case>S</span>amsung and <span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s System for the <span class=acl-fixed-case>IWSLT</span> 2018 Low Resource <span class=acl-fixed-case>MT</span> Task</a></strong><br><a href=/people/p/philip-williams/>Philip Williams</a>
|
<a href=/people/m/marcin-chochowski/>Marcin Chochowski</a>
|
<a href=/people/p/pawel-przybysz/>Pawel Przybysz</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--17><div class="card-body p-3 small">This paper describes the joint submission to the IWSLT 2018 Low Resource MT task by Samsung R&D Institute, Poland, and the University of Edinburgh. We focused on supplementing the very limited in-domain Basque-English training data with out-of-domain data, with <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a>, and with data for other language pairs. We also experimented with a variety of model architectures and features, which included the development of extensions to the Nematus toolkit. Our submission was ultimately produced by a system combination in which we reranked translations from our strongest individual system using multiple weaker systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.18/>The AFRL IWSLT 2018 Systems : What Worked, What Did n’t<span class=acl-fixed-case>AFRL</span> <span class=acl-fixed-case>IWSLT</span> 2018 Systems: What Worked, What Didn’t</a></strong><br><a href=/people/b/brian-ore/>Brian Ore</a>
|
<a href=/people/e/eric-hansen/>Eric Hansen</a>
|
<a href=/people/k/katherine-young/>Katherine Young</a>
|
<a href=/people/g/grant-erdmann/>Grant Erdmann</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--18><div class="card-body p-3 small">This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) and automatic speech recognition (ASR) systems submitted to the spoken language translation (SLT) and low-resource MT tasks as part of the IWSLT18 evaluation campaign.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.21/>CUNI Basque-to-English Submission in IWSLT18<span class=acl-fixed-case>CUNI</span> <span class=acl-fixed-case>B</span>asque-to-<span class=acl-fixed-case>E</span>nglish Submission in <span class=acl-fixed-case>IWSLT</span>18</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/d/dusan-varis/>Dušan Variš</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--21><div class="card-body p-3 small">We present our submission to the IWSLT18 Low Resource task focused on the translation from Basque-to-English. Our submission is based on the current state-of-the-art self-attentive neural network architecture, Transformer. We further improve this strong <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> by exploiting available <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> using the back-translation technique. We also present further improvements gained by a <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, a technique that trains a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using a high-resource language pair (Czech-English) and then fine-tunes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using the target low-resource language pair (Basque-English).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.26/>Data Selection with Feature Decay Algorithms Using an Approximated Target Side</a></strong><br><a href=/people/a/alberto-poncelas/>Alberto Poncelas</a>
|
<a href=/people/g/gideon-maillette-de-buy-wenniger/>Gideon Maillette de Buy Wenniger</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--26><div class="card-body p-3 small">Data selection techniques applied to neural machine translation (NMT) aim to increase the performance of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> by retrieving a subset of sentences for use as training data. One of the possible data selection techniques are <a href=https://en.wikipedia.org/wiki/Transduction_(machine_learning)>transductive learning methods</a>, which select the data based on the test set, i.e. the document to be translated. A limitation of these methods to date is that using the source-side test set does not by itself guarantee that sentences are selected with correct translations, or translations that are suitable given the test-set domain. Some <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, such as subtitle corpora, may contain parallel sentences with inaccurate translations caused by <a href=https://en.wikipedia.org/wiki/Internationalization_and_localization>localization</a> or length restrictions. In order to try to fix this problem, in this paper we propose to use an approximated target-side in addition to the source-side when selecting suitable sentence-pairs for training a model. This approximated target-side is built by pre-translating the source-side. In this work, we explore the performance of this general idea for one specific data selection approach called Feature Decay Algorithms (FDA). We train German-English NMT models on data selected by using the test set (source), the approximated target side, and a mixture of both. Our findings reveal that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> built using a combination of outputs of <a href=https://en.wikipedia.org/wiki/Food_and_Drug_Administration>FDA</a> (using the <a href=https://en.wikipedia.org/wiki/Test_set>test set</a> and an approximated target side) perform better than those solely using the <a href=https://en.wikipedia.org/wiki/Test_set>test set</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2018.iwslt-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2018--iwslt-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2018.iwslt-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2018.iwslt-1.27/>Multi-paraphrase Augmentation to Leverage Neural Caption Translation</a></strong><br><a href=/people/j/johanes-effendi/>Johanes Effendi</a>
|
<a href=/people/s/sakriani-sakti/>Sakriani Sakti</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2018--iwslt-1--27><div class="card-body p-3 small">Paraphrasing has been proven to improve translation quality in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> and has been widely studied alongside with the development of <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical MT (SMT)</a>. In this paper, we investigate and utilize neural paraphrasing to improve <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation quality</a> in neural MT (NMT), which has not yet been much explored. Our first contribution is to propose a new way of creating a multi-paraphrase corpus through visual description. After that, we also proposed to construct neural paraphrase models which initiate expert models and utilize them to leverage NMT. Here, we diffuse the image information by using image-based paraphrasing without using the image itself. Our proposed image-based multi-paraphrase augmentation strategies showed improvement against a vanilla NMT baseline.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>