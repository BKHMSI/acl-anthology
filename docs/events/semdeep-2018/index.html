<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Semantic Deep Learning (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Semantic Deep Learning (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w18-40>Proceedings of the Third Workshop on Semantic Deep Learning</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li></ul></div></div><div id=w18-40><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-40.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-40/>Proceedings of the Third Workshop on Semantic Deep Learning</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4000/>Proceedings of the Third Workshop on Semantic Deep Learning</a></strong><br><a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/d/dagmar-gromann/>Dagmar Gromann</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4002/>Word-Embedding based Content Features for Automated Oral Proficiency Scoring</a></strong><br><a href=/people/s/su-youn-yoon/>Su-Youn Yoon</a>
|
<a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/c/chungmin-lee/>Chong Min Lee</a>
|
<a href=/people/m/matthew-mulholland/>Matthew Mulholland</a>
|
<a href=/people/x/xinhao-wang/>Xinhao Wang</a>
|
<a href=/people/i/ikkyu-choi/>Ikkyu Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4002><div class="card-body p-3 small">In this study, we develop content features for an <a href=https://en.wikipedia.org/wiki/Score_(statistics)>automated scoring system</a> of non-native English speakers&#8217; spontaneous speech. The <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> calculate the <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical similarity</a> between the question text and the ASR word hypothesis of the spoken response, based on traditional word vector models or <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. The proposed <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> do not require any sample training responses for each question, and this is a strong advantage since collecting question-specific data is an expensive task, and sometimes even impossible due to concerns about question exposure. We explore the impact of these new features on the automated scoring of two different question types : (a) providing opinions on familiar topics and (b) answering a question about a stimulus material. The proposed <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> showed statistically significant correlations with the oral proficiency scores, and the combination of new <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> with the speech-driven features achieved a small but significant further improvement for the latter question type. Further analyses suggested that the new <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> were effective in assigning more accurate scores for responses with serious content issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4003/>Automatically Linking Lexical Resources with Word Sense Embedding Models</a></strong><br><a href=/people/l/luis-nieto-pina/>Luis Nieto-Piña</a>
|
<a href=/people/r/richard-johansson/>Richard Johansson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4003><div class="card-body p-3 small">Automatically learnt word sense embeddings are developed as an attempt to refine the capabilities of coarse word embeddings. The word sense representations obtained this way are, however, sensitive to underlying corpora and parameterizations, and they might be difficult to relate to formally defined <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a>. We propose to tackle this problem by devising a mechanism to establish links between word sense embeddings and lexical resources created by experts. We evaluate the applicability of these <a href=https://en.wikipedia.org/wiki/Hyperlink>links</a> in a task to retrieve instances of word sense unlisted in the lexicon.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4004/>Transferred Embeddings for Igbo Similarity, <a href=https://en.wikipedia.org/wiki/Analogy>Analogy</a>, and Diacritic Restoration Tasks<span class=acl-fixed-case>I</span>gbo Similarity, Analogy, and Diacritic Restoration Tasks</a></strong><br><a href=/people/i/ignatius-ezeani/>Ignatius Ezeani</a>
|
<a href=/people/i/ikechukwu-onyenwe/>Ikechukwu Onyenwe</a>
|
<a href=/people/m/mark-hepple/>Mark Hepple</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4004><div class="card-body p-3 small">Existing NLP models are mostly trained with data from well-resourced languages. Most <a href=https://en.wikipedia.org/wiki/Minority_language>minority languages</a> face the challenge of lack of resources-data and technologies-for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP research</a>. Building these resources from scratch for each <a href=https://en.wikipedia.org/wiki/Minority_language>minority language</a> will be very expensive, time-consuming and amount largely to unnecessarily re-inventing the wheel. In this paper, we applied transfer learning techniques to create Igbo word embeddings from a variety of existing English trained embeddings. Transfer learning methods were also used to build standard datasets for Igbo word similarity and analogy tasks for intrinsic evaluation of embeddings. These projected embeddings were also applied to diacritic restoration task. Our results indicate that the projected models not only outperform the trained ones on the semantic-based tasks of <a href=https://en.wikipedia.org/wiki/Analogy>analogy</a>, word-similarity, and odd-word identifying, but they also achieve enhanced performance on the diacritic restoration with learned diacritic embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4007/>Knowledge Representation and Extraction at Scale</a></strong><br><a href=/people/c/christos-christodoulopoulos/>Christos Christodoulopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4007><div class="card-body p-3 small">These days, most general knowledge question-answering systems rely on large-scale knowledge bases comprising billions of facts about millions of entities. Having a structured source of semantic knowledge means that we can answer questions involving single static facts (e.g. Who was the 8th president of the US?) or dynamically generated ones (e.g. How old is Donald Trump?). More importantly, we can answer questions involving multiple inference steps (Is the queen older than the president of the US?). In this talk, I&#8217;m going to be discussing some of the unique challenges that are involved with building and maintaining a consistent <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> for <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a>, extending <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> with new facts and using <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to serve answers in multiple languages. I will focus on three recent projects from our group. First, a way of measuring the completeness of a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>, that is based on usage patterns. The definition of the usage of the <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> is done in terms of the relation distribution of entities seen in question-answer logs. Instead of directly estimating the <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation distribution</a> of individual entities, it is generalized to the class signature of each entity. For example, users ask for baseball players&#8217; height, age, and batting average, so a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> is complete (with respect to baseball players) if every entity has facts for those three relations. Second, an investigation into fact extraction from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a>. I will present a method for creating distant (weak) supervision labels for training a large-scale relation extraction system. I will also discuss the effectiveness of neural network approaches by decoupling the model architecture from the feature design of a state-of-the-art neural network system. Surprisingly, a much simpler <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> trained on similar <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> performs on par with the highly complex neural network system (at 75x reduction to the training time), suggesting that the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> are a bigger contributor to the final performance. Finally, I will present the Fact Extraction and VERification (FEVER) dataset and challenge. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> comprises more than 185,000 human-generated claims extracted from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia pages</a>. False claims were generated by mutating true claims in a variety of ways, some of which were meaningaltering. During the verification step, annotators were required to label a claim for its validity and also supply full-sentence textual evidence from (potentially multiple) Wikipedia articles for the label. With <a href=https://en.wikipedia.org/wiki/FEVER>FEVER</a>, we aim to help create a new generation of transparent and interprable knowledge extraction systems.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>