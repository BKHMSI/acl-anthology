<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Statistical Machine Translation (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Statistical Machine Translation (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020wmt-1>Proceedings of the Fifth Conference on Machine Translation</a>
<span class="badge badge-info align-middle ml-1">50&nbsp;papers</span></li></ul></div></div><div id=2020wmt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.wmt-1/>Proceedings of the Fifth Conference on Machine Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wmt-1.0/>Proceedings of the Fifth Conference on Machine Translation</a></strong><br><a href=/people/l/loic-barrault/>Loïc Barrault</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/p/paco-guzman/>Paco Guzman</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939545 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.12/>Tohoku-AIP-NTT at WMT 2020 News Translation Task<span class=acl-fixed-case>AIP</span>-<span class=acl-fixed-case>NTT</span> at <span class=acl-fixed-case>WMT</span> 2020 News Translation Task</a></strong><br><a href=/people/s/shun-kiyono/>Shun Kiyono</a>
|
<a href=/people/t/takumi-ito/>Takumi Ito</a>
|
<a href=/people/r/ryuto-konno/>Ryuto Konno</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--12><div class="card-body p-3 small">In this paper, we describe the submission of Tohoku-AIP-NTT to the WMT&#8217;20 news translation task. We participated in this task in two language pairs and four language directions : <a href=https://en.wikipedia.org/wiki/German_language>English German</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>English Japanese</a>. Our system consists of techniques such as <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, which are already widely adopted in translation tasks. We attempted to develop new <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for both synthetic data filtering and <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a>. However, the <a href=https://en.wikipedia.org/wiki/Scientific_method>methods</a> turned out to be ineffective, and they provided us with no significant improvement over the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a>. We analyze these negative results to provide insights for future studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939639 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.13/>NRC Systems for the 2020 Inuktitut-English News Translation Task<span class=acl-fixed-case>NRC</span> Systems for the 2020 <span class=acl-fixed-case>I</span>nuktitut-<span class=acl-fixed-case>E</span>nglish News Translation Task</a></strong><br><a href=/people/r/rebecca-knowles/>Rebecca Knowles</a>
|
<a href=/people/d/darlene-stewart/>Darlene Stewart</a>
|
<a href=/people/s/samuel-larkin/>Samuel Larkin</a>
|
<a href=/people/p/patrick-littell/>Patrick Littell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--13><div class="card-body p-3 small">We describe the National Research Council of Canada (NRC) submissions for the 2020 Inuktitut-English shared task on news translation at the Fifth Conference on Machine Translation (WMT20). Our submissions consist of ensembled domain-specific finetuned transformer models, trained using the Nunavut Hansard and news data and, in the case of Inuktitut-English, backtranslated news and parliamentary data. In this work we explore challenges related to the relatively small amount of parallel data, morphological complexity, and domain shifts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939666 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.14/>CUNI Submission for the Inuktitut Language in WMT News 2020<span class=acl-fixed-case>CUNI</span> Submission for the <span class=acl-fixed-case>I</span>nuktitut Language in <span class=acl-fixed-case>WMT</span> News 2020</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--14><div class="card-body p-3 small">This paper describes CUNI submission to the WMT 2020 News Translation Shared Task for the low-resource scenario InuktitutEnglish in both translation directions. Our system combines <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from a CzechEnglish high-resource language pair and backtranslation. We notice surprising behaviour when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. We are using the Transformer model in a constrained submission.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939661 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.17/>Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model : the UEDIN-CUNI Submission to the WMT 2020 News Translation Task<span class=acl-fixed-case>UEDIN</span>-<span class=acl-fixed-case>CUNI</span> Submission to the <span class=acl-fixed-case>WMT</span> 2020 News Translation Task</a></strong><br><a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/m/martin-popel/>Martin Popel</a>
|
<a href=/people/r/radina-dobreva/>Radina Dobreva</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--17><div class="card-body p-3 small">We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech / English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference efficiency</a>. On the WMT 2020 Czech English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single <a href=https://en.wikipedia.org/wiki/Thread_(computing)>CPU thread</a>, thus making neural translation feasible on consumer hardware without a <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939663 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.18/>The University of Edinburgh’s submission to the German-to-English and English-to-German Tracks in the WMT 2020 News Translation and Zero-shot Translation Robustness Tasks<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s submission to the <span class=acl-fixed-case>G</span>erman-to-<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>E</span>nglish-to-<span class=acl-fixed-case>G</span>erman Tracks in the <span class=acl-fixed-case>WMT</span> 2020 News Translation and Zero-shot Translation Robustness Tasks</a></strong><br><a href=/people/u/ulrich-germann/>Ulrich Germann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--18><div class="card-body p-3 small">This paper describes the University of Edinburgh&#8217;s submission of German-English systems to the WMT2020 Shared Tasks on News Translation and Zero-shot Robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939657 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.22/>SJTU-NICT’s Supervised and Unsupervised Neural Machine Translation Systems for the WMT20 News Translation Task<span class=acl-fixed-case>SJTU</span>-<span class=acl-fixed-case>NICT</span>’s Supervised and Unsupervised Neural Machine Translation Systems for the <span class=acl-fixed-case>WMT</span>20 News Translation Task</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--22><div class="card-body p-3 small">In this paper, we introduced our joint team SJTU-NICT &#8216;s participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs : English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation (NMT) techniques : document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT, data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>. In our submissions, the primary systems won the first place on <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and German to Upper Sorbian translation directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939668 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.28/>CUNI English-Czech and English-Polish Systems in WMT20 : Robust Document-Level Training<span class=acl-fixed-case>CUNI</span> <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>C</span>zech and <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>P</span>olish Systems in <span class=acl-fixed-case>WMT</span>20: Robust Document-Level Training</a></strong><br><a href=/people/m/martin-popel/>Martin Popel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--28><div class="card-body p-3 small">We describe our two NMT systems submitted to the WMT 2020 shared task in English-Czech and English-Polish news translation. One <a href=https://en.wikipedia.org/wiki/System>system</a> is sentence level, translating each sentence independently. The second system is document level, translating multiple sentences, trained on <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>multi-sentence sequences</a> up to 3000 characters long.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939558 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.30/>OPPO’s Machine Translation Systems for WMT20<span class=acl-fixed-case>OPPO</span>’s Machine Translation Systems for <span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/t/tingxun-shi/>Tingxun Shi</a>
|
<a href=/people/s/shiyu-zhao/>Shiyu Zhao</a>
|
<a href=/people/x/xiaopu-li/>Xiaopu Li</a>
|
<a href=/people/x/xiaoxue-wang/>Xiaoxue Wang</a>
|
<a href=/people/q/qian-zhang/>Qian Zhang</a>
|
<a href=/people/d/di-ai/>Di Ai</a>
|
<a href=/people/d/dawei-dang/>Dawei Dang</a>
|
<a href=/people/x/xue-zhengshan/>Xue Zhengshan</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--30><div class="card-body p-3 small">In this paper we demonstrate our (OPPO&#8217;s) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts : the data preprocessing part will show how the data are preprocessed and filtered, and the system part will show our models architecture and the techniques we followed. Detailed information, such as training hyperparameters and the results generated by each technique will be depicted in the corresponding subsections. Our final submissions ranked top in 6 directions (English Czech, English Russian, French German and Tamil English), third in 2 directions (English German, English Japanese), and fourth in 2 directions (English Pashto and and English Tamil).<tex-math>\\leftrightarrow</tex-math> Czech, English <tex-math>\\leftrightarrow</tex-math> Russian, French <tex-math>\\rightarrow</tex-math> German and Tamil <tex-math>\\rightarrow</tex-math> English), third in 2 directions (English <tex-math>\\rightarrow</tex-math> German, English <tex-math>\\rightarrow</tex-math> Japanese), and fourth in 2 directions (English <tex-math>\\rightarrow</tex-math> Pashto and and English <tex-math>\\rightarrow</tex-math> Tamil).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939573 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.31/>HW-TSC’s Participation in the WMT 2020 News Translation Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2020 News Translation Shared Task</a></strong><br><a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a>
|
<a href=/people/s/shiliang-sun/>Shiliang Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--31><div class="card-body p-3 small">This paper presents our work in the WMT 2020 News Translation Shared Task. We participate in 3 language pairs including Zh / En, Km / En, and Ps / En and in both directions under the constrained condition. We use the standard Transformer-Big model as the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> and obtain the best performance via two variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual dataset. Several commonly used strategies are used to train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> such as <a href=https://en.wikipedia.org/wiki/Back_translation>Back Translation</a>, Ensemble Knowledge Distillation, etc. We also conduct experiment with similar language augmentation, which lead to positive results, although not used in our submission. Our submission obtains remarkable results in the final evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939581 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.33/>The Volctrans Machine Translation System for WMT20<span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/l/liwei-wu/>Liwei Wu</a>
|
<a href=/people/x/xiao-pan/>Xiao Pan</a>
|
<a href=/people/z/zehui-lin/>Zehui Lin</a>
|
<a href=/people/y/yaoming-zhu/>Yaoming Zhu</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--33><div class="card-body p-3 small">This paper describes our submission systems for VolcTrans for WMT20 shared news translation task. We participated in 8 translation directions. Our basic systems are based on Transformer (CITATION), into which we also employed new architectures (bigger or deeper Transformers, dynamic convolution). The final systems include text pre-process, subword(a.k.a. BPE(CITATION)), baseline model training, iterative back-translation, model ensemble, knowledge distillation and multilingual pre-training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939572 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.37/>The NiuTrans Machine Translation Systems for WMT20<span class=acl-fixed-case>N</span>iu<span class=acl-fixed-case>T</span>rans Machine Translation Systems for <span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/z/ziyang-wang/>Ziyang Wang</a>
|
<a href=/people/r/runzhe-cao/>Runzhe Cao</a>
|
<a href=/people/b/binghao-wei/>Binghao Wei</a>
|
<a href=/people/w/weiqiao-shan/>Weiqiao Shan</a>
|
<a href=/people/s/shuhan-zhou/>Shuhan Zhou</a>
|
<a href=/people/a/abudurexiti-reheman/>Abudurexiti Reheman</a>
|
<a href=/people/t/tao-zhou/>Tao Zhou</a>
|
<a href=/people/x/xin-zeng/>Xin Zeng</a>
|
<a href=/people/l/laohu-wang/>Laohu Wang</a>
|
<a href=/people/y/yongyu-mu/>Yongyu Mu</a>
|
<a href=/people/j/jingnan-zhang/>Jingnan Zhang</a>
|
<a href=/people/x/xiaoqian-liu/>Xiaoqian Liu</a>
|
<a href=/people/x/xuanjun-zhou/>Xuanjun Zhou</a>
|
<a href=/people/y/yinqiao-li/>Yinqiao Li</a>
|
<a href=/people/b/bei-li/>Bei Li</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--37><div class="card-body p-3 small">This paper describes NiuTrans neural machine translation systems of the WMT20 news translation tasks. We participated in Japanese-English, English-Chinese, Inuktitut-English and Tamil-English total five tasks and rank first in Japanese-English both sides. We mainly utilized iterative back-translation, different depth and widen model architectures, iterative knowledge distillation and iterative fine-tuning. And we find that adequately widened and deepened the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> simultaneously, the performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut-English and Tamil-English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939659 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.39/>Gender Coreference and Bias Evaluation at WMT 2020<span class=acl-fixed-case>WMT</span> 2020</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/t/tomasz-limisiewicz/>Tomasz Limisiewicz</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--39><div class="card-body p-3 small">Gender bias in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> become more popular and deployed within commercial systems. Our work presents the largest evidence for the phenomenon in more than 19 systems submitted to the WMT over four diverse target languages : <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to languages with <a href=https://en.wikipedia.org/wiki/Grammatical_gender>grammatical gender</a>. We extend WinoMT to handle two new <a href=https://en.wikipedia.org/wiki/Language>languages</a> tested in WMT : <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. We find that all <a href=https://en.wikipedia.org/wiki/System>systems</a> consistently use spurious correlations in the data rather than meaningful <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939640 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.42/>Translating Similar Languages : Role of <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>Mutual Intelligibility</a> in Multilingual Transformers</a></strong><br><a href=/people/i/ife-adebara/>Ife Adebara</a>
|
<a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--42><div class="card-body p-3 small">In this work we investigate different approaches to translate between similar languages despite low resource limitations. This work is done as the participation of the UBC NLP research group in the WMT 2019 Similar Languages Translation Shared Task. We participated in all language pairs and performed various experiments. We used a transformer architecture for all the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and used <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> for one of the language pairs. We explore both bilingual and multi-lingual approaches. We describe the pre-processing, training, translation and results for each <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. We also investigate the role of <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>mutual intelligibility</a> in <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.47.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--47 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.47 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939595 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.47/>The IPN-CIC team system submission for the WMT 2020 similar language task<span class=acl-fixed-case>IPN</span>-<span class=acl-fixed-case>CIC</span> team system submission for the <span class=acl-fixed-case>WMT</span> 2020 similar language task</a></strong><br><a href=/people/l/luis-a-menendez-salazar/>Luis A. Menéndez-Salazar</a>
|
<a href=/people/g/grigori-sidorov/>Grigori Sidorov</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-Jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--47><div class="card-body p-3 small">This paper describes the participation of the NLP research team of the IPN Computer Research center in the WMT 2020 Similar Language Translation Task. We have submitted <a href=https://en.wikipedia.org/wiki/Linguistic_system>systems</a> for the Spanish-Portuguese language pair (in both directions). The three submitted systems are based on the Transformer architecture and used fine tuning for <a href=https://en.wikipedia.org/wiki/Domain_Adaptation>domain Adaptation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939638 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.49/>NUIG-Panlingua-KMI Hindi-Marathi MT Systems for Similar Language Translation Task @ WMT 2020<span class=acl-fixed-case>NUIG</span>-Panlingua-<span class=acl-fixed-case>KMI</span> <span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>M</span>arathi <span class=acl-fixed-case>MT</span> Systems for Similar Language Translation Task @ <span class=acl-fixed-case>WMT</span> 2020</a></strong><br><a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/p/priya-rani/>Priya Rani</a>
|
<a href=/people/a/akanksha-bansal/>Akanksha Bansal</a>
|
<a href=/people/b/bharathi-raja-chakravarthi/>Bharathi Raja Chakravarthi</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/j/john-philip-mccrae/>John P. McCrae</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--49><div class="card-body p-3 small">NUIG-Panlingua-KMI submission to WMT 2020 seeks to push the state-of-the-art in Similar Language Translation Task for HindiMarathi language pair. As part of these efforts, we conducteda series of experiments to address the challenges for translation between similar languages. Among the 4 MT systems prepared under this task, 1 PBSMT systems were prepared for HindiMarathi each and 1 NMT systems were developed for <a href=https://en.wikipedia.org/wiki/Marathi_language>HindiMarathi</a> using Byte PairEn-coding (BPE) into subwords. The results show that different architectures NMT could be an effective method for developing MT systems for closely related languages. Our Hindi-Marathi NMT system was ranked 8th among the 14 teams that participated and our Marathi-Hindi NMT system was ranked 8th among the 11 teams participated for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939608 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.53/>Document Level NMT of Low-Resource Languages with Backtranslation<span class=acl-fixed-case>NMT</span> of Low-Resource Languages with Backtranslation</a></strong><br><a href=/people/s/sami-ul-haq/>Sami Ul Haq</a>
|
<a href=/people/s/sadaf-abdul-rauf/>Sadaf Abdul Rauf</a>
|
<a href=/people/a/arsalan-shaukat/>Arsalan Shaukat</a>
|
<a href=/people/a/abdullah-saeed/>Abdullah Saeed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--53><div class="card-body p-3 small">This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair MarathiHindi. Our system is an extension of state-of-the-art Transformer architecture with hierarchical attention networks to incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>. Since, NMT requires large amount of parallel data which is not available for this task, our approach is focused on utilizing <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> with <a href=https://en.wikipedia.org/wiki/Back_translation>back translation</a> to train our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>. Our experiments reveal that document-level NMT can be a reasonable alternative to sentence-level NMT for improving translation quality of low resourced languages even when used with synthetic data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939647 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.56/>The University of Maryland’s Submissions to the WMT20 Chat Translation Task : Searching for More Data to Adapt Discourse-Aware Neural Machine Translation<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>M</span>aryland’s Submissions to the <span class=acl-fixed-case>WMT</span>20 Chat Translation Task: Searching for More Data to Adapt Discourse-Aware Neural Machine Translation</a></strong><br><a href=/people/c/calvin-bao/>Calvin Bao</a>
|
<a href=/people/y/yow-ting-shiue/>Yow-Ting Shiue</a>
|
<a href=/people/c/chujun-song/>Chujun Song</a>
|
<a href=/people/j/jie-li/>Jie Li</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--56><div class="card-body p-3 small">This paper describes the University of Maryland&#8217;s submissions to the WMT20 Shared Task on Chat Translation. We focus on translating agent-side utterances from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We started from an off-the-shelf BPE-based standard transformer model trained with WMT17 news and fine-tuned it with the provided in-domain training data. In addition, we augment the training set with its best matches in the WMT19 news dataset. Our primary submission uses a standard <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformer</a>, while our contrastive submissions use multi-encoder Transformers to attend to previous utterances. Our primary submission achieves 56.7 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on the agent side (ende), outperforming a baseline system provided by the task organizers by more than 13 BLEU points. Moreover, according to an evaluation on a set of carefully-designed examples, the multi-encoder architecture is able to generate more coherent translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939588 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.62" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.62/>Fast Interleaved Bidirectional Sequence Generation</a></strong><br><a href=/people/b/biao-zhang/>Biao Zhang</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--62><div class="card-body p-3 small">Independence assumptions during sequence generation can speed up <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-to-left directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and selfattention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of ~2x compared to autoregressive decoding with comparable quality. Notably, it outperforms left-to-right SA because the <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>independence assumptions</a> in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4x11x across different <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> at the cost of 1 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> or 0.5 ROUGE (on average)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939559 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.70" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.70/>Towards Multimodal Simultaneous Neural Machine Translation</a></strong><br><a href=/people/a/aizhan-imankulova/>Aizhan Imankulova</a>
|
<a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/t/tosho-hirasawa/>Tosho Hirasawa</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--70><div class="card-body p-3 small">Simultaneous translation involves translating a sentence before the speaker&#8217;s utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT), which leverages visual information as an additional modality. Our experiments with the Multi30k dataset showed that MSNMT significantly outperforms its text-only counterpart in more timely translation situations with low latency. Furthermore, we verified the importance of visual information during decoding by performing an adversarial evaluation of MSNMT, where we studied how models behaved with incongruent input modality and analyzed the effect of different word order between source and target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wmt-1.74.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939560 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.74" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.74/>Document-aligned Japanese-English Conversation Parallel Corpus<span class=acl-fixed-case>J</span>apanese-<span class=acl-fixed-case>E</span>nglish Conversation Parallel Corpus</a></strong><br><a href=/people/m/matiss-rikters/>Matīss Rikters</a>
|
<a href=/people/r/ryokan-ri/>Ryokan Ri</a>
|
<a href=/people/t/tong-li/>Tong Li</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--74><div class="card-body p-3 small">Sentence-level (SL) machine translation (MT) has reached acceptable quality for many high-resourced languages, but not document-level (DL) MT, which is difficult to 1) train with little amount of DL data ; and 2) evaluate, as the main methods and data sets focus on SL evaluation. To address the first issue, we present a document-aligned Japanese-English conversation corpus, including balanced, high-quality business conversation data for tuning and testing. As for the second issue, we manually identify the main areas where SL MT fails to produce adequate translations in lack of context. We then create an evaluation set where these phenomena are annotated to alleviate automatic evaluation of DL systems. We train MT models using our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to demonstrate how using <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> leads to improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939672 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.75/>Findings of the WMT 2020 Shared Task on Automatic Post-Editing<span class=acl-fixed-case>WMT</span> 2020 Shared Task on Automatic Post-Editing</a></strong><br><a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--75><div class="card-body p-3 small">We present the results of the 6th round of the WMT task on MT Automatic Post-Editing. The task consists in automatically correcting the output of a black-box machine translation system by learning from existing human corrections of different sentences. This year, the challenge consisted of fixing the errors present in English Wikipedia pages translated into <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> by state-ofthe-art, not domain-adapted neural MT (NMT) systems unknown to participants. Six teams participated in the English-German task, submitting a total of 11 runs. Two teams participated in the English-Chinese task submitting 2 runs each. Due to i) the different source / domain of data compared to the past (Wikipedia vs Information Technology), ii) the different quality of the initial translations to be corrected and iii) the introduction of a new language pair (English-Chinese), this year&#8217;s results are not directly comparable with last year&#8217;s round. However, on both language directions, participants&#8217; submissions show considerable improvements over the baseline results. On <a href=https://en.wikipedia.org/wiki/German_language>English-German</a>, the top ranked system improves over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by -11.35 TER and +16.68 BLEU points, while on <a href=https://en.wikipedia.org/wiki/Chinese_language>EnglishChinese</a> the improvements are respectively up to -12.13 TER and +14.57 BLEU points. Overall, coherent gains are also highlighted by the outcomes of human evaluation, which confirms the effectiveness of APE to improve MT quality, especially in the new generic domain selected for this year&#8217;s round.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wmt-1.77/>Results of the WMT20 Metrics Shared Task<span class=acl-fixed-case>WMT</span>20 Metrics Shared Task</a></strong><br><a href=/people/n/nitika-mathur/>Nitika Mathur</a>
|
<a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--77><div class="card-body p-3 small">This paper presents the results of the WMT20 Metrics Shared Task. Participants were asked to score the outputs of the translation systems competing in the WMT20 News Translation Task with <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>. Ten research groups submitted 27 metrics, four of which are <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>reference-less metrics</a>. In addition, we computed five baseline metrics, including sentBLEU, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, TER and using the SacreBLEU scorer. All <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> were evaluated on how well they correlate at the system-, document- and segment-level with the WMT20 official human scores. We present an extensive analysis on influence of different reference translations on <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric reliability</a>, how well automatic metrics score human translations, and we also flag major discrepancies between <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> and human scores when evaluating MT systems. Finally, we investigate whether we can use <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a> to flag incorrect human ratings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939547 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.81/>Cross-Lingual Transformers for Neural Automatic Post-Editing</a></strong><br><a href=/people/d/dongjun-lee/>Dongjun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--81><div class="card-body p-3 small">In this paper, we describe the Bering Lab&#8217;s submission to the WMT 2020 Shared Task on Automatic Post-Editing (APE). First, we propose a cross-lingual Transformer architecture that takes a concatenation of a source sentence and a machine-translated (MT) sentence as an input to generate the post-edited (PE) output. For further improvement, we mask incorrect or missing words in the PE output based on word-level quality estimation and then predict the actual word for each mask based on the fine-tuned cross-lingual language model (XLM-RoBERTa). Finally, to address the over-correction problem, we select the final output among the PE outputs and the original MT sentence based on a sentence-level quality estimation. When evaluated on the WMT 2020 English-German APE test dataset, our <a href=https://en.wikipedia.org/wiki/System>system</a> improves the NMT output by -3.95 and +4.50 in terms of <a href=https://en.wikipedia.org/wiki/Terminology_of_the_Low_Countries>TER</a> and BLEU, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939561 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.82/>POSTECH-ETRI’s Submission to the WMT2020 APE Shared Task : Automatic Post-Editing with Cross-lingual Language Model<span class=acl-fixed-case>POSTECH</span>-<span class=acl-fixed-case>ETRI</span>’s Submission to the <span class=acl-fixed-case>WMT</span>2020 <span class=acl-fixed-case>APE</span> Shared Task: Automatic Post-Editing with Cross-lingual Language Model</a></strong><br><a href=/people/j/jihyung-lee/>Jihyung Lee</a>
|
<a href=/people/w/wonkee-lee/>WonKee Lee</a>
|
<a href=/people/j/jaehun-shin/>Jaehun Shin</a>
|
<a href=/people/b/baikjin-jung/>Baikjin Jung</a>
|
<a href=/people/y/young-gil-kim/>Young-Kil Kim</a>
|
<a href=/people/j/jong-hyeok-lee/>Jong-Hyeok Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--82><div class="card-body p-3 small">This paper describes POSTECH-ETRI&#8217;s submission to WMT2020 for the shared task on automatic post-editing (APE) for 2 language pairs : English-German (En-De) and English-Chinese (En-Zh). We propose APE systems based on a cross-lingual language model, which jointly adopts translation language modeling (TLM) and masked language modeling (MLM) training objectives in the pre-training stage ; the APE models then utilize jointly learned language representations between the source language and the target language. In addition, we created 19 million new sythetic triplets as additional training data for our final ensemble model. According to experimental results on the WMT2020 APE development data set, our models showed an improvement over the baseline by <a href=https://en.wikipedia.org/wiki/Terminology>TER</a> of -3.58 and a BLEU score of +5.3 for the <a href=https://en.wikipedia.org/wiki/Terminology>En-De subtask</a> ; and <a href=https://en.wikipedia.org/wiki/Terminology>TER</a> of -5.29 and a BLEU score of +7.32 for the <a href=https://en.wikipedia.org/wiki/Terminology>En-Zh subtask</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939622 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.84/>Alibaba’s Submission for the WMT 2020 APE Shared Task : Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual BERT<span class=acl-fixed-case>A</span>libaba’s Submission for the <span class=acl-fixed-case>WMT</span> 2020 <span class=acl-fixed-case>APE</span> Shared Task: Improving Automatic Post-Editing with Pre-trained Conditional Cross-Lingual <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/j/jiayi-wang/>Jiayi Wang</a>
|
<a href=/people/k/ke-wang/>Ke Wang</a>
|
<a href=/people/k/kai-fan/>Kai Fan</a>
|
<a href=/people/y/yuqi-zhang/>Yuqi Zhang</a>
|
<a href=/people/j/jun-lu/>Jun Lu</a>
|
<a href=/people/x/xin-ge/>Xin Ge</a>
|
<a href=/people/y/yangbin-shi/>Yangbin Shi</a>
|
<a href=/people/y/yu-zhao/>Yu Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--84><div class="card-body p-3 small">The goal of Automatic Post-Editing (APE) is basically to examine the automatic methods for correcting translation errors generated by an unknown machine translation (MT) system. This paper describes Alibaba&#8217;s submissions to the WMT 2020 APE Shared Task for the English-German language pair. We design a two-stage training pipeline. First, a BERT-like cross-lingual language model is pre-trained by randomly masking target sentences alone. Then, an additional neural decoder on the top of the pre-trained model is jointly fine-tuned for the APE task. We also apply an imitation learning strategy to augment a reasonable amount of pseudo APE training data, potentially preventing the model to overfit on the limited real training data and boosting the performance on held-out data. To verify our proposed model and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, we examine our approach with the well-known benchmarking English-German dataset from the WMT 2017 APE task. The experiment results demonstrate that our <a href=https://en.wikipedia.org/wiki/System>system</a> significantly outperforms all other <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> and achieves the state-of-the-art performance. The final results on the WMT 2020 test dataset show that our <a href=https://en.wikipedia.org/wiki/Subscription_business_model>submission</a> can achieve +5.56 <a href=https://en.wikipedia.org/wiki/British_thermal_unit>BLEU</a> and -4.57 TER with respect to the official MT baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939618 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.86/>LIMSI @ WMT 2020<span class=acl-fixed-case>LIMSI</span> @ <span class=acl-fixed-case>WMT</span> 2020</a></strong><br><a href=/people/s/sadaf-abdul-rauf/>Sadaf Abdul Rauf</a>
|
<a href=/people/j/jose-carlos-rosales-nunez/>José Carlos Rosales Núñez</a>
|
<a href=/people/m/minh-quang-pham/>Minh Quang Pham</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--86><div class="card-body p-3 small">This paper describes LIMSI&#8217;s submissions to the translation shared tasks at WMT&#8217;20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/French_language>French</a>, using back-translated texts, terminological resources as well as multiple pre-processing pipelines, including pre-trained representations. Systems were also prepared for the robustness task for translating from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/German_language>German</a> ; for this large-scale task we developed multi-domain, noise-robust, translation systems aim to handle the two test conditions : zero-shot and few-shot domain adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939591 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.87/>Elhuyar submission to the Biomedical Translation Task 2020 on terminology and abstracts translation</a></strong><br><a href=/people/a/ander-corral/>Ander Corral</a>
|
<a href=/people/x/xabier-saralegi/>Xabier Saralegi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--87><div class="card-body p-3 small">This article describes the systems submitted by Elhuyar to the 2020 Biomedical Translation Shared Task, specifically the systems presented in the subtasks of terminology translation for English-Basque and abstract translation for English-Basque and English-Spanish. In all cases a Transformer architecture was chosen and we studied different strategies to combine <a href=https://en.wikipedia.org/wiki/Open_data>open domain data</a> with biomedical domain data for building the training corpora. For the English-Basque pair, given the scarcity of parallel corpora in the biomedical domain, we set out to create domain training data in a synthetic way. The <a href=https://en.wikipedia.org/wiki/System>systems</a> presented in the terminology and abstract translation subtasks for the English-Basque language pair ranked first in their respective tasks among four participants, achieving 0.78 accuracy for terminology translation and a <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> of 0.1279 for the translation of abstracts. In the abstract translation task for the English-Spanish pair our team ranked second (BLEU=0.4498) in the case of OK sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wmt-1.88.OptionalSupplementaryMaterial.tgz data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939644 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.88/>YerevaNN’s Systems for WMT20 Biomedical Translation Task : The Effect of Fixing Misaligned Sentence Pairs<span class=acl-fixed-case>Y</span>ereva<span class=acl-fixed-case>NN</span>’s Systems for <span class=acl-fixed-case>WMT</span>20 Biomedical Translation Task: The Effect of Fixing Misaligned Sentence Pairs</a></strong><br><a href=/people/k/karen-hambardzumyan/>Karen Hambardzumyan</a>
|
<a href=/people/h/hovhannes-tamoyan/>Hovhannes Tamoyan</a>
|
<a href=/people/h/hrant-khachatrian/>Hrant Khachatrian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--88><div class="card-body p-3 small">This report describes YerevaNN&#8217;s neural machine translation systems and data processing pipelines developed for WMT20 biomedical translation task. We provide <a href=https://en.wikipedia.org/wiki/Language_planning>systems</a> for English-Russian and English-German language pairs. For the English-Russian pair, our submissions achieve the best BLEU scores, with enru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.<tex-math>\\rightarrow</tex-math>ru direction outperforming the other systems by a significant margin. We explain most of the improvements by our heavy data preprocessing pipeline which attempts to fix poorly aligned sentences in the parallel data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939562 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.89/>Pretrained Language Models and Backtranslation for English-Basque Biomedical Neural Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>B</span>asque Biomedical Neural Machine Translation</a></strong><br><a href=/people/i/inigo-jauregi-unanue/>Inigo Jauregi Unanue</a>
|
<a href=/people/m/massimo-piccardi/>Massimo Piccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--89><div class="card-body p-3 small">This paper describes the machine translation systems proposed by the University of Technology Sydney Natural Language Processing (UTS_NLP) team for the WMT20 English-Basque biomedical translation tasks. Due to the limited parallel corpora available, we have proposed to train a BERT-fused NMT model that leverages the use of pretrained language models. Furthermore, we have augmented the training corpus by backtranslating monolingual data. Our experiments show that NMT models in low-resource scenarios can benefit from combining these two <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training techniques</a>, with improvements of up to 6.16 BLEU percentual points in the case of biomedical abstract translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939645 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.90" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.90/>Lite Training Strategies for Portuguese-English and English-Portuguese Translation<span class=acl-fixed-case>P</span>ortuguese-<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>P</span>ortuguese Translation</a></strong><br><a href=/people/a/alexandre-lopes/>Alexandre Lopes</a>
|
<a href=/people/r/rodrigo-nogueira/>Rodrigo Nogueira</a>
|
<a href=/people/r/roberto-lotufo/>Roberto Lotufo</a>
|
<a href=/people/h/helio-pedrini/>Helio Pedrini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--90><div class="card-body p-3 small">Despite the widespread adoption of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5 for Portuguese-English and English-Portuguese translation tasks using low-cost hardware. We explore the use of Portuguese and English pre-trained language models and propose an adaptation of the English tokenizer to represent Portuguese characters, such as <a href=https://en.wikipedia.org/wiki/Diaeresis_(diacritic)>diaeresis</a>, acute and grave accents. We compare our models to the Google Translate API and MarianMT on a subset of the ParaCrawl dataset, as well as to the winning submission to the WMT19 Biomedical Translation Shared Task. We also describe our submission to the WMT20 Biomedical Translation Shared Task. Our results show that our <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> have a competitive performance to state-of-the-art <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> while being trained on modest hardware (a single 8 GB gaming GPU for nine days). Our <a href=https://en.wikipedia.org/wiki/Data>data</a>, models and code are available in our GitHub repository.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939583 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.94/>Addressing Exposure Bias With Document Minimum Risk Training : Cambridge at the WMT20 Biomedical Translation Task<span class=acl-fixed-case>C</span>ambridge at the <span class=acl-fixed-case>WMT</span>20 Biomedical Translation Task</a></strong><br><a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--94><div class="card-body p-3 small">The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such <a href=https://en.wikipedia.org/wiki/Data>data</a> are susceptible to <a href=https://en.wikipedia.org/wiki/Exposure_bias>exposure bias effects</a>, particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> if the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns to neglect the source sentence. The UNICAM entry addresses this problem during <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove &#8216;problem&#8217; training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result, despite using only single models with no ensembling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wmt-1.101.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939604 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.101/>Unbabel’s Participation in the WMT20 Metrics Shared Task<span class=acl-fixed-case>WMT</span>20 Metrics Shared Task</a></strong><br><a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/c/craig-stewart/>Craig Stewart</a>
|
<a href=/people/a/ana-c-farinha/>Ana C Farinha</a>
|
<a href=/people/a/alon-lavie/>Alon Lavie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--101><div class="card-body p-3 small">We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segmentlevel, document-level and system-level tracks on all language pairs, as well as the QE as a Metric track. Accordingly, we illustrate results of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in these <a href=https://en.wikipedia.org/wiki/Track_(rail_transport)>tracks</a> with reference to test sets from the previous year. Our submissions build upon the recently proposed COMET framework : we train several estimator models to regress on different humangenerated quality scores and a novel ranking model trained on relative ranks obtained from Direct Assessments. We also propose a simple technique for converting segment-level predictions into a document-level score. Overall, our <a href=https://en.wikipedia.org/wiki/System>systems</a> achieve strong results for all language pairs on previous test sets and in many cases set a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939565 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.104/>Incorporate Semantic Structures into Machine Translation Evaluation via <a href=https://en.wikipedia.org/wiki/UCCA>UCCA</a><span class=acl-fixed-case>UCCA</span></a></strong><br><a href=/people/j/jin-xu/>Jin Xu</a>
|
<a href=/people/y/yinuo-guo/>Yinuo Guo</a>
|
<a href=/people/j/junfeng-hu/>Junfeng Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--104><div class="card-body p-3 small">Copying mechanism has been commonly used in neural paraphrasing networks and other text generation tasks, in which some important words in the input sequence are preserved in the output sequence. Similarly, in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, we notice that there are certain words or phrases appearing in all good translations of one source text, and these words tend to convey important semantic information. Therefore, in this work, we define words carrying important semantic meanings in sentences as semantic core words. Moreover, we propose an MT evaluation approach named Semantically Weighted Sentence Similarity (SWSS). It leverages the power of UCCA to identify semantic core words, and then calculates sentence similarity scores on the overlap of semantic core words. Experimental results show that SWSS can consistently improve the performance of popular MT evaluation metrics which are based on <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical similarity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--105 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939606 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.105" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.105/>Filtering Noisy Parallel Corpus using Transformers with Proxy Task Learning</a></strong><br><a href=/people/h/haluk-acarcicek/>Haluk Açarçiçek</a>
|
<a href=/people/t/talha-colakoglu/>Talha Çolakoğlu</a>
|
<a href=/people/p/pinar-ece-aktan-hatipoglu/>Pınar Ece Aktan Hatipoğlu</a>
|
<a href=/people/c/chong-hsuan-huang/>Chong Hsuan Huang</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--105><div class="card-body p-3 small">This paper illustrates Huawei&#8217;s submission to the WMT20 low-resource parallel corpus filtering shared task. Our approach focuses on developing a proxy task learner on top of a transformer-based multilingual pre-trained language model to boost the filtering capability for noisy parallel corpora. Such a supervised task also helps us to iterate much more quickly than using an existing <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation system</a> to perform the same <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. After performing empirical analyses of the finetuning task, we benchmark our approach by comparing the results with past years&#8217; state-of-theart records. This paper wraps up with a discussion of limitations and future work. The scripts for this study will be made publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--106 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939612 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.106/>Score Combination for Improved Parallel Corpus Filtering for Low Resource Conditions</a></strong><br><a href=/people/m/muhammad-elnokrashy/>Muhammad ElNokrashy</a>
|
<a href=/people/a/amr-hendy/>Amr Hendy</a>
|
<a href=/people/m/mohamed-abdelghaffar/>Mohamed Abdelghaffar</a>
|
<a href=/people/m/mohamed-afify/>Mohamed Afify</a>
|
<a href=/people/a/ahmed-tawfik/>Ahmed Tawfik</a>
|
<a href=/people/h/hany-hassan-awadalla/>Hany Hassan Awadalla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--106><div class="card-body p-3 small">This paper presents the description of our submission to WMT20 sentence filtering task. We combine scores from custom LASER built for each source language, a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> built to distinguish positive and negative pairs and the original scores provided with the task. For the mBART setup, provided by the organizers, our method shows 7 % and 5 % relative improvement, over the baseline, in sacreBLEU score on the test set for <a href=https://en.wikipedia.org/wiki/Pashto>Pashto</a> and <a href=https://en.wikipedia.org/wiki/Khmer_language>Khmer</a> respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939649 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.108/>An exploratory approach to the Parallel Corpus Filtering shared task WMT20<span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/a/ankur-kejriwal/>Ankur Kejriwal</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--108><div class="card-body p-3 small">In this document we describe our submission to the parallel corpus filtering task using multilingual word embedding, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and an ensemble of pre and post filtering rules. We use the norms of embedding and the perplexities of language models along with pre / post filtering rules to complement the LASER baseline scores and in the end get an improvement on the dev set in both language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--113 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939610 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.113/>PATQUEST : Papago Translation Quality Estimation<span class=acl-fixed-case>PATQUEST</span>: Papago Translation Quality Estimation</a></strong><br><a href=/people/y/yujin-baek/>Yujin Baek</a>
|
<a href=/people/z/zae-myung-kim/>Zae Myung Kim</a>
|
<a href=/people/j/jihyung-moon/>Jihyung Moon</a>
|
<a href=/people/h/hyunjoong-kim/>Hyunjoong Kim</a>
|
<a href=/people/e/eunjeong-park/>Eunjeong Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--113><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation : (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>errors</a> that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a>, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment ; EN-DE only), and Task 3 (Document-Level Score).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939546 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.118/>Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation</a></strong><br><a href=/people/d/dongjun-lee/>Dongjun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--118><div class="card-body p-3 small">In this paper, we describe the Bering Lab&#8217;s submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language model, with a few additional parameters. Model training consists of two phases. We first pre-train our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on a huge artificially generated QE dataset, and then we fine-tune the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with a human-labeled dataset. When evaluated on the WMT 2020 English-German QE test set, our systems achieve the best result on the target-side of word-level QE and the second best results on the source-side of word-level QE and sentence-level QE among all submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--119 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939643 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.119/>IST-Unbabel Participation in the WMT20 Quality Estimation Shared Task<span class=acl-fixed-case>IST</span>-Unbabel Participation in the <span class=acl-fixed-case>WMT</span>20 Quality Estimation Shared Task</a></strong><br><a href=/people/j/joao-moura/>João Moura</a>
|
<a href=/people/m/miguel-vera/>Miguel Vera</a>
|
<a href=/people/d/daan-van-stigt/>Daan van Stigt</a>
|
<a href=/people/f/fabio-kepler/>Fabio Kepler</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--119><div class="card-body p-3 small">We present the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitted systems were developed by extending the OpenKiwi framework to a transformer-based predictor-estimator architecture, and to cope with glass-box, uncertainty-based features coming from neural machine translation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--122 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939607 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.122" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.122/>TransQuest at WMT2020 : Sentence-Level Direct Assessment<span class=acl-fixed-case>T</span>rans<span class=acl-fixed-case>Q</span>uest at <span class=acl-fixed-case>WMT</span>2020: Sentence-Level Direct Assessment</a></strong><br><a href=/people/t/tharindu-ranasinghe/>Tharindu Ranasinghe</a>
|
<a href=/people/c/constantin-orasan/>Constantin Orasan</a>
|
<a href=/people/r/ruslan-mitkov/>Ruslan Mitkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--122><div class="card-body p-3 small">This paper presents the team TransQuest&#8217;s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. The proposed methods achieve state-of-the-art results surpassing the results obtained by OpenKiwi, the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> used in the shared task. We further fine tune the QE framework by performing ensemble and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. Our approach is the winning solution in all of the language pairs according to the WMT 2020 official results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939609 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.124/>Tencent submission for WMT20 Quality Estimation Shared Task<span class=acl-fixed-case>WMT</span>20 Quality Estimation Shared Task</a></strong><br><a href=/people/h/haijiang-wu/>Haijiang Wu</a>
|
<a href=/people/z/zixuan-wang/>Zixuan Wang</a>
|
<a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/x/xinjie-wen/>Xinjie Wen</a>
|
<a href=/people/r/ruichen-wang/>Ruichen Wang</a>
|
<a href=/people/x/xiaoli-wang/>Xiaoli Wang</a>
|
<a href=/people/y/yulin-zhang/>Yulin Zhang</a>
|
<a href=/people/z/zhipeng-yao/>Zhipeng Yao</a>
|
<a href=/people/s/siyao-peng/>Siyao Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--124><div class="card-body p-3 small">This paper presents Tencent&#8217;s submission to the WMT20 Quality Estimation (QE) Shared Task : Sentence-Level Post-editing Effort for English-Chinese in Task 2. Our system ensembles two <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>, XLM-based and Transformer-based Predictor-Estimator models. For the XLM-based Predictor-Estimator architecture, the predictor produces two types of contextualized token representations, i.e., masked XLM and non-masked XLM ; the LSTM-estimator and Transformer-estimator employ two effective strategies, top-K and multi-head attention, to enhance the sentence feature representation. For Transformer-based Predictor-Estimator architecture, we improve a top-performing model by conducting three modifications : using multi-decoding in machine translation module, creating a new model by replacing the transformer-based predictor with XLM-based predictor, and finally integrating two models by a weighted average. Our submission achieves a <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> of 0.664, ranking first (tied) on <a href=https://en.wikipedia.org/wiki/Standard_Chinese>English-Chinese</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.126.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--126 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.126 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939625 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.126/>NLPRL System for Very Low Resource Supervised Machine Translation<span class=acl-fixed-case>NLPRL</span> System for Very Low Resource Supervised Machine Translation</a></strong><br><a href=/people/r/rupjyoti-baruah/>Rupjyoti Baruah</a>
|
<a href=/people/r/rajesh-kumar-mundotiya/>Rajesh Kumar Mundotiya</a>
|
<a href=/people/a/amit-kumar/>Amit Kumar</a>
|
<a href=/people/a/anil-kumar-singh/>Anil kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--126><div class="card-body p-3 small">This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try to address the Out of Vocabulary (OOV) word problem by performing <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> so that segments correspond to morphological units. They are also reported to work across different languages, especially similar languages due to their sub-word nature. Based on BLEU cased score, our NLPRL systems ranked ninth for HSB to GER and tenth in GER to HSB translation scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939584 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.129/>UdS-DFKI@WMT20 : Unsupervised MT and Very Low Resource Supervised MT for German-Upper Sorbian<span class=acl-fixed-case>U</span>d<span class=acl-fixed-case>S</span>-<span class=acl-fixed-case>DFKI</span>@<span class=acl-fixed-case>WMT</span>20: Unsupervised <span class=acl-fixed-case>MT</span> and Very Low Resource Supervised <span class=acl-fixed-case>MT</span> for <span class=acl-fixed-case>G</span>erman-<span class=acl-fixed-case>U</span>pper <span class=acl-fixed-case>S</span>orbian</a></strong><br><a href=/people/s/sourav-dutta/>Sourav Dutta</a>
|
<a href=/people/j/jesujoba-alabi/>Jesujoba Alabi</a>
|
<a href=/people/s/saptarashmi-bandyopadhyay/>Saptarashmi Bandyopadhyay</a>
|
<a href=/people/d/dana-ruiter/>Dana Ruiter</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--129><div class="card-body p-3 small">This paper describes the UdS-DFKI submission to the shared task for unsupervised machine translation (MT) and very low-resource supervised MT between German (de) and Upper Sorbian (hsb) at the Fifth Conference of Machine Translation (WMT20). We submit <a href=https://en.wikipedia.org/wiki/System>systems</a> for both the supervised and unsupervised tracks. Apart from various experimental approaches like bitext mining, model pre-training, and iterative back-translation, we employ a factored machine translation approach on a small BPE vocabulary.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939641 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.133/>CUNI Systems for the Unsupervised and Very Low Resource Translation Task in WMT20<span class=acl-fixed-case>CUNI</span> Systems for the Unsupervised and Very Low Resource Translation Task in <span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/i/ivana-kvapilikova/>Ivana Kvapilíková</a>
|
<a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--133><div class="card-body p-3 small">This paper presents a description of CUNI systems submitted to the WMT20 task on unsupervised and very low-resource supervised machine translation between <a href=https://en.wikipedia.org/wiki/German_language>German</a> and Upper Sorbian. We experimented with training on <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a> and pre-training on a related language pair. In the fully unsupervised scenario, we achieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian, respectively. Our low-resource systems relied on <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from German-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an improvement of 10 BLEU points over the baseline trained only on the available small German-Upper Sorbian parallel corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939589 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.134/>The University of Helsinki and Aalto University submissions to the WMT 2020 news and low-resource translation tasks<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki and Aalto University submissions to the <span class=acl-fixed-case>WMT</span> 2020 news and low-resource translation tasks</a></strong><br><a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/s/stig-arne-gronroos/>Stig-Arne Grönroos</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--134><div class="card-body p-3 small">This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020 : the news translation between Inuktitut and <a href=https://en.wikipedia.org/wiki/English_language>English</a> and the low-resource translation between <a href=https://en.wikipedia.org/wiki/German_language>German</a> and Upper Sorbian. For both tasks, our efforts concentrate on efficient use of monolingual and related bilingual corpora with scheduled multi-task learning as well as an optimized subword segmentation with <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling</a>. Our submission obtained the highest score for <a href=https://en.wikipedia.org/wiki/Upper_Sorbian>Upper Sorbian-German</a> and was ranked second for German-Upper Sorbian according to <a href=https://en.wikipedia.org/wiki/BLEU>BLEU scores</a>. For EnglishInuktitut, we reached ranks 8 and 10 out of 11 according to BLEU scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--135 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939575 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.135/>The NITS-CNLP System for the Unsupervised MT Task at WMT 2020<span class=acl-fixed-case>NITS</span>-<span class=acl-fixed-case>CNLP</span> System for the Unsupervised <span class=acl-fixed-case>MT</span> Task at <span class=acl-fixed-case>WMT</span> 2020</a></strong><br><a href=/people/s/salam-michael-singh/>Salam Michael Singh</a>
|
<a href=/people/t/thoudam-doren-singh/>Thoudam Doren Singh</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--135><div class="card-body p-3 small">We describe NITS-CNLP&#8217;s submission to WMT 2020 unsupervised machine translation shared task for German language (de) to Upper Sorbian (hsb) in a constrained setting i.e, using only the data provided by the organizers. We train our <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised model</a> using monolingual data from both the languages by jointly pre-training the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Code>decoder</a> and fine-tune using backtranslation loss. The final model uses the source side (de) monolingual data and the target side (hsb) synthetic data as a pseudo-parallel data to train a pseudo-supervised system which is tuned using the provided development set(dev set).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939621 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.136/>Adobe AMPS’s Submission for Very Low Resource Supervised Translation Task at WMT20<span class=acl-fixed-case>AMPS</span>’s Submission for Very Low Resource Supervised Translation Task at <span class=acl-fixed-case>WMT</span>20</a></strong><br><a href=/people/k/keshaw-singh/>Keshaw Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--136><div class="card-body p-3 small">In this paper, we describe our <a href=https://en.wikipedia.org/wiki/System>systems</a> submitted to the very low resource supervised translation task at WMT20. We participate in both translation directions for Upper Sorbian-German language pair. Our primary submission is a subword-level Transformer-based neural machine translation model trained on original training bitext. We also conduct several experiments with backtranslation using limited monolingual data in our post-submission work and include our results for the same. In one such experiment, we observe jumps of up to 2.6 BLEU points over the primary system by <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pretraining</a> on a synthetic, backtranslated corpus followed by <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> on the original parallel training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939593 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.140/>Human-Paraphrased References Improve Neural Machine Translation</a></strong><br><a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--140><div class="card-body p-3 small">Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by freitag2020bleu. When used in place of original references, the paraphrased versions produce <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric scores</a> that correlate better with <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a>. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal (translationese) ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is ignificantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wmt-1.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wmt-1--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wmt-1.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939650 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wmt-1.141" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wmt-1.141/>Incorporating Terminology Constraints in Automatic Post-Editing</a></strong><br><a href=/people/d/david-wan/>David Wan</a>
|
<a href=/people/c/chris-kedzie/>Chris Kedzie</a>
|
<a href=/people/f/faisal-ladhak/>Faisal Ladhak</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wmt-1--141><div class="card-body p-3 small">Users of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> may want to ensure the use of specific <a href=https://en.wikipedia.org/wiki/Terminology>lexical terminologies</a>. While there exist techniques for incorporating terminology constraints during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> for MT, current APE approaches can not ensure that they will appear in the final translation. In this paper, we present both autoregressive and non-autoregressive models for lexically constrained APE, demonstrating that our approach enables preservation of 95 % of the terminologies and also improves translation quality on English-German benchmarks. Even when applied to lexically constrained MT output, our approach is able to improve preservation of the terminologies. However, we show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not learn to copy constraints systematically and suggest a simple data augmentation technique that leads to improved performance and <a href=https://en.wikipedia.org/wiki/Robust_statistics>robustness</a>.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>