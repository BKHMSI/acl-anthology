<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Computational Linguistics Journal (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Computational Linguistics Journal (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021cl-1>Computational Linguistics, Volume 47, Issue 1 - March 2021</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2021cl-2>Computational Linguistics, Volume 47, Issue 2 - June 2021</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2021cl-3>Computational Linguistics, Volume 47, Issue 3 - November 2021</a>
<span class="badge badge-info align-middle ml-1">2&nbsp;papers</span></li><li><a class=align-middle href=#2021cl-4>Computational Linguistics, Volume 47, Issue 4 - December 2021</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li></ul></div></div><div id=2021cl-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.cl-1/>Computational Linguistics, Volume 47, Issue 1 - March 2021</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-1.2/>Formal Basis of a Language Universal</a></strong><br><a href=/people/m/milos-stanojevic/>Miloš Stanojević</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-1--2><div class="card-body p-3 small">Abstract Steedman (2020) proposes as a formal universal of natural language grammar that grammatical permutations of the kind that have given rise to transformational rules are limited to a class known to mathematicians and computer scientists as the separable permutations. This <a href=https://en.wikipedia.org/wiki/Class_(set_theory)>class of permutations</a> is exactly the <a href=https://en.wikipedia.org/wiki/Class_(set_theory)>class</a> that can be expressed in combinatory categorial grammars (CCGs). The excluded non-separable permutations do in fact seem to be absent in a number of studies of crosslinguistic variation in <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> in nominal and verbal constructions. The number of <a href=https://en.wikipedia.org/wiki/Permutation>permutations</a> that are separable grows in the number n of lexical elements in the construction as the Large Schrder Number Sn1. Because that <a href=https://en.wikipedia.org/wiki/Number>number</a> grows much more slowly than the n ! number of all permutations, this <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> is also of considerable practical interest for computational applications such as <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The present article examines the mathematical and computational origins of this <a href=https://en.wikipedia.org/wiki/Restriction_(mathematics)>restriction</a>, and the reason it is exactly captured in CCG without the imposition of any further constraints.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-1.4/>Semantic Data Set Construction from Human Clustering and <a href=https://en.wikipedia.org/wiki/Spatial_analysis>Spatial Arrangement</a></a></strong><br><a href=/people/o/olga-majewska/>Olga Majewska</a>
|
<a href=/people/d/diana-mccarthy/>Diana McCarthy</a>
|
<a href=/people/j/jasper-j-f-van-den-bosch/>Jasper J. F. van den Bosch</a>
|
<a href=/people/n/nikolaus-kriegeskorte/>Nikolaus Kriegeskorte</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-1--4><div class="card-body p-3 small">Abstract Research into representation learning models of <a href=https://en.wikipedia.org/wiki/Lexical_semantics>lexical semantics</a> usually utilizes some form of intrinsic evaluation to ensure that the learned representations reflect human semantic judgments. Lexical semantic similarity estimation is a widely used evaluation method, but efforts have typically focused on pairwise judgments of words in isolation, or are limited to specific contexts and lexical stimuli. There are limitations with these approaches that either do not provide any context for judgments, and thereby ignore <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a>, or provide very specific sentential contexts that can not then be used to generate a larger lexical resource. Furthermore, <a href=https://en.wikipedia.org/wiki/Similarity_(psychology)>similarity</a> between more than two items is not considered. We provide a full description and analysis of our recently proposed methodology for large-scale data set construction that produces a semantic classification of a large sample of verbs in the first phase, as well as multi-way similarity judgments made within the resultant semantic classes in the second phase. The <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> uses a spatial multi-arrangement approach proposed in the field of <a href=https://en.wikipedia.org/wiki/Cognitive_neuroscience>cognitive neuroscience</a> for capturing multi-way similarity judgments of visual stimuli. We have adapted this <a href=https://en.wikipedia.org/wiki/Methodology>method</a> to handle polysemous linguistic stimuli and much larger samples than previous work. We specifically target verbs, but the method can equally be applied to other parts of speech. We perform <a href=https://en.wikipedia.org/wiki/Cluster_analysis>cluster analysis</a> on the data from the first phase and demonstrate how this might be useful in the construction of a comprehensive verb resource. We also analyze the semantic information captured by the second phase and discuss the potential of the spatially induced similarity judgments to better reflect human notions of word similarity. We demonstrate how the resultant data set can be used for fine-grained analyses and evaluation of representation learning models on the intrinsic tasks of semantic clustering and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. In particular, we find that stronger static word embedding methods still outperform lexical representations emerging from more recent pre-training methods, both on word-level similarity and <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>. Moreover, thanks to the data set&#8217;s vast coverage, we are able to compare the benefits of specializing vector representations for a particular type of external knowledge by evaluating FrameNet- and VerbNet-retrofitted models on specific semantic domains such as Heat or Motion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cl-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cl-1.6/>Supervised and Unsupervised Neural Approaches to Text Readability</a></strong><br><a href=/people/m/matej-martinc/>Matej Martinc</a>
|
<a href=/people/s/senja-pollak/>Senja Pollak</a>
|
<a href=/people/m/marko-robnik-sikonja/>Marko Robnik-Šikonja</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-1--6><div class="card-body p-3 small">Abstract We present a set of novel neural supervised and unsupervised approaches for determining the readability of documents. In the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised setting</a>, we leverage neural language models, whereas in the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised setting</a>, three different neural classification architectures are tested. We show that the proposed neural unsupervised approach is robust, transferable across languages, and allows adaptation to a specific readability task and <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>. By systematic comparison of several neural architectures on a number of benchmark and new labeled readability data sets in two languages, this study also offers a comprehensive analysis of different neural approaches to readability classification. We expose their strengths and weaknesses, compare their performance to current state-of-the-art classification approaches to <a href=https://en.wikipedia.org/wiki/Readability>readability</a>, which in most cases still rely on extensive <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>, and propose possibilities for improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-1.7/>Depth-Bounded Statistical PCFG Induction as a Model of Human Grammar Acquisition<span class=acl-fixed-case>PCFG</span> Induction as a Model of Human Grammar Acquisition</a></strong><br><a href=/people/l/lifeng-jin/>Lifeng Jin</a>
|
<a href=/people/l/lane-schwartz/>Lane Schwartz</a>
|
<a href=/people/f/finale-doshi-velez/>Finale Doshi-Velez</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/w/william-schuler/>William Schuler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-1--7><div class="card-body p-3 small">Abstract This article describes a simple PCFG induction model with a fixed category domain that predicts a large majority of attested constituent boundaries, and predicts labels consistent with nearly half of attested constituent labels on a standard evaluation data set of child-directed speech. The article then explores the idea that the difference between simple grammars exhibited by child learners and fully <a href=https://en.wikipedia.org/wiki/Recursive_grammar>recursive grammars</a> exhibited by adult learners may be an effect of increasing working memory capacity, where the shallow grammars are constrained images of the <a href=https://en.wikipedia.org/wiki/Recursive_grammar>recursive grammars</a>. An implementation of these memory bounds as limits on <a href=https://en.wikipedia.org/wiki/Center_embedding>center embedding</a> in a depth-specific transform of a <a href=https://en.wikipedia.org/wiki/Recursive_grammar>recursive grammar</a> yields a significant improvement over an equivalent but unbounded baseline, suggesting that this arrangement may indeed confer a learning advantage.</div></div></div><hr><div id=2021cl-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.cl-2/>Computational Linguistics, Volume 47, Issue 2 - June 2021</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-2.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-2--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-2.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-2.9/>Approximating Probabilistic Models as Weighted Finite Automata</a></strong><br><a href=/people/a/ananda-theertha-suresh/>Ananda Theertha Suresh</a>
|
<a href=/people/b/brian-roark/>Brian Roark</a>
|
<a href=/people/m/michael-riley/>Michael Riley</a>
|
<a href=/people/v/vlad-schogol/>Vlad Schogol</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-2--9><div class="card-body p-3 small">Abstract Weighted finite automata (WFAs) are often used to represent probabilistic models, such as n-gram language models, because among other things, they are efficient for recognition tasks in time and space. The probabilistic source to be represented as a WFA, however, may come in many forms. Given a generic probabilistic model over sequences, we propose an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to approximate it as a WFA such that the <a href=https://en.wikipedia.org/wiki/Kullback&#8211;Leibler_divergence>Kullback-Leibler divergence</a> between the source model and the WFA target model is minimized. The proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> involves a counting step and a difference of convex optimization step, both of which can be performed efficiently. We demonstrate the usefulness of our approach on various tasks, including distilling n-gram models from neural models, building compact language models, and building open-vocabulary character models. The <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> used for these experiments are available in an open-source software library.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-2.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-2--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-2.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cl-2.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cl-2.12/>RYANSQL : Recursively Applying Sketch-based Slot Fillings for Complex Text-to-SQL in Cross-Domain Databases<span class=acl-fixed-case>RYANSQL</span>: Recursively Applying Sketch-based Slot Fillings for Complex Text-to-<span class=acl-fixed-case>SQL</span> in Cross-Domain Databases</a></strong><br><a href=/people/d/donghyun-choi/>DongHyun Choi</a>
|
<a href=/people/m/myeong-cheol-shin/>Myeong Cheol Shin</a>
|
<a href=/people/e/eunggyun-kim/>EungGyun Kim</a>
|
<a href=/people/d/dong-ryeol-shin/>Dong Ryeol Shin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-2--12><div class="card-body p-3 small">Abstract Text-to-SQL is the problem of converting a user question into an <a href=https://en.wikipedia.org/wiki/SQL>SQL query</a>, when the question and database are given. In this article, we present a neural network approach called RYANSQL (Recursively Yielding Annotation Network for SQL) to solve complex Text-to-SQL tasks for cross-domain databases. Statement Position Code (SPC) is defined to transform a nested SQL query into a set of non-nested SELECT statements ; a sketch-based slot-filling approach is proposed to synthesize each SELECT statement for its corresponding SPC. Additionally, two input manipulation methods are presented to improve generation performance further. RYANSQL achieved competitive result of 58.2 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the challenging Spider benchmark. At the time of submission (April 2020), RYANSQL v2, a variant of original RYANSQL, is positioned at 3rd place among all systems and 1st place among the systems not using database content with 60.6 % exact matching accuracy. The source code is available at https://github.com/kakaoenterprise/RYANSQL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-2.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-2--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-2.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-2.13/>CausaLM : Causal Model Explanation Through Counterfactual Language Models<span class=acl-fixed-case>C</span>ausa<span class=acl-fixed-case>LM</span>: Causal Model Explanation Through Counterfactual Language Models</a></strong><br><a href=/people/a/amir-feder/>Amir Feder</a>
|
<a href=/people/n/nadav-oved/>Nadav Oved</a>
|
<a href=/people/u/uri-shalit/>Uri Shalit</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-2--13><div class="card-body p-3 small">Abstract Understanding predictions made by <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> is notoriously difficult, but also crucial to their dissemination. As all machine learningbased methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation</a> and <a href=https://en.wikipedia.org/wiki/Causality>causation</a>, and might be ill-suited for text-based models and for reasoning about high-level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the <a href=https://en.wikipedia.org/wiki/Causal_graph>causal graph</a> of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.1</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-2.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-2--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-2.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cl-2.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cl-2.14/>Analysis and Evaluation of <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> for Word Sense Disambiguation</a></strong><br><a href=/people/d/daniel-loureiro/>Daniel Loureiro</a>
|
<a href=/people/k/kiamehr-rezaee/>Kiamehr Rezaee</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-2--14><div class="card-body p-3 small">Abstract Transformer-based language models have taken many fields in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> by storm. BERT and its derivatives dominate most of the existing evaluation benchmarks, including those for Word Sense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic nuances. However, there is still little knowledge about their capabilities and potential limitations in encoding and recovering word senses. In this article, we provide an in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to <a href=https://en.wikipedia.org/wiki/Ambiguity>lexical ambiguity</a>. One of the main conclusions of our analysis is that BERT can accurately capture high-level sense distinctions, even when a limited number of examples is available for each word sense. Our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. However, this scenario rarely occurs in real-world settings and, hence, many practical challenges remain even in the coarse-grained setting. We also perform an in-depth comparison of the two main language model-based WSD strategies, namely, <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> and <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction</a>, finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data. In fact, the simple feature extraction strategy of averaging contextualized embeddings proves robust even using only three training sentences per word sense, with minimal improvements obtained by increasing the size of this training data.</div></div></div><hr><div id=2021cl-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.cl-3/>Computational Linguistics, Volume 47, Issue 3 - November 2021</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-3.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-3--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-3.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-3.16/>The Taxonomy of Writing Systems : How to Measure How Logographic a System Is</a></strong><br><a href=/people/r/richard-sproat/>Richard Sproat</a>
|
<a href=/people/a/alexander-gutkin/>Alexander Gutkin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-3--16><div class="card-body p-3 small">Taxonomies of writing systems since Gelb (1952) have classified systems based on what the written symbols represent : if they represent words or morphemes, they are logographic ; if syllables, syllabic ; if segments, alphabetic ; and so forth. Sproat (2000) and Rogers (2005) broke with tradition by splitting the logographic and phonographic aspects into two dimensions, with <a href=https://en.wikipedia.org/wiki/Logography>logography</a> being graded rather than a categorical distinction. A system could be syllabic, and highly logographic ; or alphabetic, and mostly non-logographic. This accords better with how <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a> actually work, but neither author proposed a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for measuring <a href=https://en.wikipedia.org/wiki/Logography>logography</a>. In this article we propose a novel measure of the degree of <a href=https://en.wikipedia.org/wiki/Logography>logography</a> that uses an attention-based sequence-to-sequence model trained to predict the spelling of a token from its pronunciation in context. In an ideal <a href=https://en.wikipedia.org/wiki/Phonogram_(linguistics)>phonographic system</a>, the model should need to attend to only the current token in order to compute how to spell it, and this would show in the attention matrix activations. In contrast, with a <a href=https://en.wikipedia.org/wiki/Logogram>logographic system</a>, where a given <a href=https://en.wikipedia.org/wiki/Pronunciation>pronunciation</a> might correspond to several different spellings, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> would need to attend to a broader context. The ratio of the activation outside the token and the total activation forms the basis of our <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a>. We compare this with a simple lexical measure, and an entropic measure, as well as several other neural models, and argue that on balance our attention-based measure accords best with intuition about how logographic various systems are.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-3.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-3--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-3.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-3.20/>Decoding Word Embeddings with Brain-Based Semantic Features</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-3--20><div class="card-body p-3 small">Word embeddings are vectorial semantic representations built with either counting or predicting techniques aimed at capturing shades of meaning from word co-occurrences. Since their introduction, these <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> have been criticized for lacking interpretable dimensions. This property of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> limits our understanding of the <a href=https://en.wikipedia.org/wiki/Semantic_feature>semantic features</a> they actually encode. Moreover, it contributes to the black box nature of the tasks in which they are used, since the reasons for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> performance often remain opaque to humans. In this contribution, we explore the semantic properties encoded in word embeddings by mapping them onto interpretable vectors, consisting of explicit and neurobiologically motivated semantic features (Binder et al. Our exploration takes into account different types of embeddings, including factorized count vectors and predict models (Skip-Gram, GloVe, etc.), as well as the most recent contextualized representations (i.e., ELMo and BERT). In our analysis, we first evaluate the quality of the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> in a retrieval task, then we shed light on the semantic features that are better encoded in each embedding type. A large number of probing tasks is finally set to assess how the original and the mapped embeddings perform in discriminating semantic categories. For each probing task, we identify the most relevant semantic features and we show that there is a correlation between the <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> performance and how they encode those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. This study sets itself as a step forward in understanding which aspects of meaning are captured by <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a>, by proposing a new and simple method to carve human-interpretable semantic representations from <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional vectors</a>.</div></div></div><hr><div id=2021cl-4><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2021.cl-4/>Computational Linguistics, Volume 47, Issue 4 - December 2021</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-4.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-4--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-4.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-4.27/>Abstractive Text Summarization : Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization</a></strong><br><a href=/people/p/panagiotis-kouris/>Panagiotis Kouris</a>
|
<a href=/people/g/georgios-alexandridis/>Georgios Alexandridis</a>
|
<a href=/people/a/andreas-stafylopatis/>Andreas Stafylopatis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-4--27><div class="card-body p-3 small">Abstract Nowadays, most research conducted in the field of abstractive text summarization focuses on neural-based models alone, without considering their combination with knowledge-based approaches that could further enhance their efficiency. In this direction, this work presents a novel framework that combines sequence-to-sequence neural-based text summarization along with structure and semantic-based methodologies. The proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is capable of dealing with the problem of out-of-vocabulary or rare words, improving the performance of the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>. The overall <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is based on a well-defined theoretical model of knowledge-based content generalization and deep learning predictions for generating abstractive summaries. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is composed of three key elements : (i) a pre-processing task, (ii) a machine learning methodology, and (iii) a post-processing task. The pre-processing task is a knowledge-based approach, based on ontological knowledge resources, <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a>, and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, along with content generalization, that transforms ordinary text into a generalized form. A deep learning model of attentive encoder-decoder architecture, which is expanded to enable a coping and coverage mechanism, as well as reinforcement learning and transformer-based architectures, is trained on a generalized version of text-summary pairs, learning to predict summaries in a generalized form. The post-processing task utilizes knowledge resources, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a>, and <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic algorithms</a> based on text similarity methods in order to transform the generalized version of a predicted summary to a final, human-readable form.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-4.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-4--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-4.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cl-4.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cl-4.28/>The (Un)Suitability of Automatic Evaluation Metrics for Text Simplification</a></strong><br><a href=/people/f/fernando-alva-manchego/>Fernando Alva-Manchego</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-4--28><div class="card-body p-3 small">Abstract In order to simplify sentences, several rewriting operations can be performed, such as replacing complex words per simpler synonyms, deleting unnecessary information, and splitting long sentences. Despite this multi-operation nature, evaluation of automatic simplification systems relies on <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> that moderately correlate with human judgments on the <a href=https://en.wikipedia.org/wiki/Simplicity>simplicity</a> achieved by executing specific <a href=https://en.wikipedia.org/wiki/Operation_(mathematics)>operations</a> (e.g., <a href=https://en.wikipedia.org/wiki/Simplicity>simplicity gain</a> based on lexical replacements). In this article, we investigate how well existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> can assess sentence-level simplifications where multiple <a href=https://en.wikipedia.org/wiki/Operation_(mathematics)>operations</a> may have been applied and which, therefore, require more general simplicity judgments. For that, we first collect a new and more reliable <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> for evaluating the correlation of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> and human judgments of overall simplicity. Second, we conduct the first meta-evaluation of automatic metrics in Text Simplification, using our new data set (and other existing data) to analyze the variation of the correlation between metrics&#8217; scores and human judgments across three dimensions : the perceived simplicity level, the system type, and the set of references used for computation. We show that these three aspects affect the correlations and, in particular, highlight the limitations of commonly used operation-specific metrics. Finally, based on our findings, we propose a set of recommendations for automatic evaluation of multi-operation simplifications, suggesting which metrics to compute and how to interpret their scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-4.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-4--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-4.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-4.30/>Are Ellipses Important for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>?</a></strong><br><a href=/people/p/payal-khullar/>Payal Khullar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-4--30><div class="card-body p-3 small">Abstract This article describes an experiment to evaluate the impact of different types of ellipses discussed in <a href=https://en.wikipedia.org/wiki/Theoretical_linguistics>theoretical linguistics</a> on Neural Machine Translation (NMT), using English to Hindi / Telugu as source and target languages. Evaluation with manual methods shows that most of the errors made by Google NMT are located in the clause containing the <a href=https://en.wikipedia.org/wiki/Ellipsis>ellipsis</a>, the frequency of such errors is slightly more in <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a> than <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, and the translation adequacy shows improvement when ellipses are reconstructed with their antecedents. These findings not only confirm the importance of <a href=https://en.wikipedia.org/wiki/Ellipsis>ellipses</a> and their resolution for MT, but also hint toward a possible correlation between the translation of discourse devices like <a href=https://en.wikipedia.org/wiki/Ellipsis>ellipses</a> with the morphological incongruity of the source and target. We also observe that not all <a href=https://en.wikipedia.org/wiki/Ellipse>ellipses</a> are translated poorly and benefit from reconstruction, advocating for a disparate treatment of different <a href=https://en.wikipedia.org/wiki/Ellipse>ellipses</a> in MT research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cl-4.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cl-4--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cl-4.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cl-4.31/>LFG Generation from Acyclic F-Structures is NP-Hard<span class=acl-fixed-case>LFG</span> Generation from Acyclic <span class=acl-fixed-case>F</span>-Structures is <span class=acl-fixed-case>NP</span>-Hard</a></strong><br><a href=/people/j/jurgen-wedekind/>Jürgen Wedekind</a>
|
<a href=/people/r/ronald-m-kaplan/>Ronald M. Kaplan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cl-4--31><div class="card-body p-3 small">Abstract The universal generation problem for LFG grammars is the problem of determining whether a given <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammar</a> derives any terminal string with a given f-structure. It is known that this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is decidable for acyclic f-structures. In this brief note, we show that for those f-structures the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is nonetheless intractable. This holds even for <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammars</a> that are off-line parsable.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>