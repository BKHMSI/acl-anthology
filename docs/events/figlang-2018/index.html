<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Figurative Language Processing (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Figurative Language Processing (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w18-09>Proceedings of the Workshop on Figurative Language Processing</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li></ul></div></div><div id=w18-09><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-09.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-09/>Proceedings of the Workshop on Figurative Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0900/>Proceedings of the Workshop on Figurative Language Processing</a></strong><br><a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a>
|
<a href=/people/p/patricia-lichtenstein/>Patricia Lichtenstein</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a>
|
<a href=/people/c/chee-wee/>Chee Wee</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0902/>Linguistic Features of <a href=https://en.wikipedia.org/wiki/Sarcasm>Sarcasm</a> and Metaphor Production Quality</a></strong><br><a href=/people/s/stephen-skalicky/>Stephen Skalicky</a>
|
<a href=/people/s/scott-crossley/>Scott Crossley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0902><div class="card-body p-3 small">Using <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> to detect <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a> has provided a deeper in-sight into <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a>. The purpose of this study is to assess whether <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> can help explain differences in quality of figurative language. In this study a large corpus of metaphors and sarcastic responses are collected from human subjects and rated for figurative language quality based on <a href=https://en.wikipedia.org/wiki/Metaphor>theoretical components of metaphor</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a>, and <a href=https://en.wikipedia.org/wiki/Creativity>creativity</a>. Using natural language processing tools, specific linguistic features related to lexical sophistication and semantic cohesion were used to predict the human ratings of figurative language quality. Results demonstrate <a href=https://en.wikipedia.org/wiki/Linguistic_feature>linguistic features</a> were able to predict small amounts of variance in metaphor and sarcasm production quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0905/>Catching Idiomatic Expressions in EFL Essays<span class=acl-fixed-case>EFL</span> Essays</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0905><div class="card-body p-3 small">This paper presents an exploratory study on large-scale detection of idiomatic expressions in <a href=https://en.wikipedia.org/wiki/Essay>essays</a> written by non-native speakers of English. We describe a computational search procedure for automatic detection of idiom-candidate phrases in essay texts. The study used a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of essays</a> written during a standardized examination of English language proficiency. Automatically-flagged candidate expressions were manually annotated for <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomaticity</a>. The study found that <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> are widely used in <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>EFL essays</a>. The study also showed that a search algorithm that accommodates the syntactic and lexical exibility of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a> can increase the <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> of idiom instances by 30 %, but it also increases the amount of false positives.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0906/>Predicting Human Metaphor Paraphrase Judgments with Deep Neural Networks</a></strong><br><a href=/people/y/yuri-bizzoni/>Yuri Bizzoni</a>
|
<a href=/people/s/shalom-lappin/>Shalom Lappin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0906><div class="card-body p-3 small">We propose a new annotated corpus for metaphor interpretation by <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrase</a>, and a novel DNN model for performing this task. Our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> consists of 200 sets of 5 sentences, with each set containing one reference metaphorical sentence, and four ranked candidate paraphrases. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained for a binary classification of paraphrase candidates, and then used to predict graded paraphrase acceptability. It reaches an encouraging 75 % accuracy on the binary classification task, and high Pearson (.75) and Spearman (.68) correlations on the gradient judgment prediction task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0907/>A Report on the 2018 VUA Metaphor Detection Shared Task<span class=acl-fixed-case>VUA</span> Metaphor Detection Shared Task</a></strong><br><a href=/people/c/chee-wee-leong/>Chee Wee (Ben) Leong</a>
|
<a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0907><div class="card-body p-3 small">As the community working on computational approaches to <a href=https://en.wikipedia.org/wiki/Figurative_language>figurative language</a> is growing and as methods and data become increasingly diverse, it is important to create widely shared empirical knowledge of the level of system performance in a range of contexts, thus facilitating progress in this area. One way of creating such shared knowledge is through benchmarking multiple systems on a common dataset. We report on the shared task on metaphor identification on the VU Amsterdam Metaphor Corpus conducted at the NAACL 2018 Workshop on Figurative Language Processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0908/>An LSTM-CRF Based Approach to Token-Level Metaphor Detection<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> Based Approach to Token-Level Metaphor Detection</a></strong><br><a href=/people/m/malay-pramanick/>Malay Pramanick</a>
|
<a href=/people/a/ashim-gupta/>Ashim Gupta</a>
|
<a href=/people/p/pabitra-mitra/>Pabitra Mitra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0908><div class="card-body p-3 small">Automatic processing of <a href=https://en.wikipedia.org/wiki/Figurative_language>figurative languages</a> is gaining popularity in NLP community for their ubiquitous nature and increasing volume. In this era of <a href=https://en.wikipedia.org/wiki/Web_2.0>web 2.0</a>, automatic analysis of sarcasm and <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> is important for their extensive usage. Metaphors are a part of <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative language</a> that compares different concepts, often on a <a href=https://en.wikipedia.org/wiki/Cognition>cognitive level</a>. Many approaches have been proposed for automatic detection of metaphors, even using <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential models</a> or <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. In this paper, we propose a method for detection of metaphors at the token level using a hybrid model of Bidirectional-LSTM and CRF. We used fewer <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, as compared to the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Sequential_model>sequential model</a>. On experimentation with VUAMC, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> obtained an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.674.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-0911" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-0911/>Bigrams and BiLSTMs Two <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> for Sequential Metaphor Detection<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>s Two Neural Networks for Sequential Metaphor Detection</a></strong><br><a href=/people/y/yuri-bizzoni/>Yuri Bizzoni</a>
|
<a href=/people/m/mehdi-ghanimifard/>Mehdi Ghanimifard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0911><div class="card-body p-3 small">We present and compare two alternative deep neural architectures to perform word-level metaphor detection on text : a bi-LSTM model and a new structure based on recursive feed-forward concatenation of the input. We discuss different versions of such models and the effect that input manipulation-specifically, reducing the length of sentences and introducing concreteness scores for words-have on their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0913/>Neural Metaphor Detecting with CNN-LSTM Model<span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> Model</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/s/sixing-wu/>Sixing Wu</a>
|
<a href=/people/z/zhigang-yuan/>Zhigang Yuan</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0913><div class="card-body p-3 small">Metaphors are <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>figurative languages</a> widely used in daily life and literatures. It&#8217;s an important task to detect the <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> evoked by texts. Thus, the metaphor shared task is aimed to extract <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> from plain texts at <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>word level</a>. We propose to use a CNN-LSTM model for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our model combines CNN and LSTM layers to utilize both local and long-range contextual information for identifying metaphorical information. In addition, we compare the performance of the softmax classifier and conditional random field (CRF) for sequential labeling in this task. We also incorporated some additional features such as part of speech (POS) tags and word cluster to improve the performance of model. Our best <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> achieved 65.06 % <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> in the all POS testing subtask and 67.15 % in the verbs testing subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0914/>Di-LSTM Contrast : A Deep Neural Network for Metaphor Detection<span class=acl-fixed-case>LSTM</span> Contrast : A Deep Neural Network for Metaphor Detection</a></strong><br><a href=/people/k/krishnkant-swarnkar/>Krishnkant Swarnkar</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0914><div class="card-body p-3 small">The contrast between the contextual and general meaning of a word serves as an important clue for detecting its <a href=https://en.wikipedia.org/wiki/Metaphor>metaphoricity</a>. In this paper, we present a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural architecture</a> for metaphor detection which exploits this contrast. Additionally, we also use cost-sensitive learning by re-weighting examples, and baseline features like concreteness ratings, POS and WordNet-based features. The best performing <a href=https://en.wikipedia.org/wiki/System>system</a> of ours achieves an overall F1 score of 0.570 on All POS category and 0.605 on the Verbs category at the Metaphor Shared Task 2018.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0915/>Conditional Random Fields for Metaphor Detection</a></strong><br><a href=/people/a/anna-mosolova/>Anna Mosolova</a>
|
<a href=/people/i/ivan-bondarenko/>Ivan Bondarenko</a>
|
<a href=/people/v/vadim-fomin/>Vadim Fomin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0915><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for detecting <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a> in sentences which was used in Shared Task on Metaphor Detection by First Workshop on Figurative Language Processing. The <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is based on different <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Fields</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0916.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0916 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0916 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0916/>Detecting Figurative Word Occurrences Using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/a/agnieszka-mykowiecka/>Agnieszka Mykowiecka</a>
|
<a href=/people/a/aleksander-wawer/>Aleksander Wawer</a>
|
<a href=/people/m/malgorzata-marciniak/>Malgorzata Marciniak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0916><div class="card-body p-3 small">The paper addresses detection of figurative usage of words in <a href=https://en.wikipedia.org/wiki/English_language>English text</a>. The chosen method was to use <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural nets</a> fed by pretrained word embeddings. The obtained results show that simple <a href=https://en.wikipedia.org/wiki/Solution_concept>solutions</a>, based on <a href=https://en.wikipedia.org/wiki/Word_embedding>words embeddings</a> only, are comparable to complex solutions, using many sources of information which are not available for languages less-studied than <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0917.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0917 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0917 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0917/>Multi-Module Recurrent Neural Networks with Transfer Learning</a></strong><br><a href=/people/f/filip-skurniak/>Filip Skurniak</a>
|
<a href=/people/m/maria-janicka/>Maria Janicka</a>
|
<a href=/people/a/aleksander-wawer/>Aleksander Wawer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0917><div class="card-body p-3 small">This paper describes multiple solutions designed and tested for the problem of word-level metaphor detection. The proposed systems are all based on variants of recurrent neural network architectures. Specifically, we explore multiple sources of information : pre-trained word embeddings (Glove), a dictionary of language concreteness and a transfer learning scenario based on the states of an encoder network from neural network machine translation system. One of the architectures is based on combining all three systems : (1) Neural CRF (Conditional Random Fields), trained directly on the metaphor data set ; (2) Neural Machine Translation encoder of a transfer learning scenario ; (3) a neural network used to predict final labels, trained directly on the metaphor data set. Our results vary between test sets : Neural CRF standalone is the best one on submission data, while combined system scores the highest on a test subset randomly selected from training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-0918.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-0918 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-0918 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-0918/>Using Language Learner Data for Metaphor Detection</a></strong><br><a href=/people/e/egon-stemle/>Egon Stemle</a>
|
<a href=/people/a/alexander-onysko/>Alexander Onysko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-0918><div class="card-body p-3 small">This article describes the <a href=https://en.wikipedia.org/wiki/System>system</a> that participated in the shared task on metaphor detection on the Vrije University Amsterdam Metaphor Corpus (VUA). The ST was part of the workshop on processing figurative language at the 16th annual conference of the North American Chapter of the Association for Computational Linguistics (NAACL2018). The system combines a small assertion of trending techniques, which implement matured methods from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>ML</a> ; in particular, the system uses word embeddings from standard corpora and from <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> representing different proficiency levels of language learners in a LSTM BiRNN architecture. The <a href=https://en.wikipedia.org/wiki/System>system</a> is available under the APLv2 open-source license.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>