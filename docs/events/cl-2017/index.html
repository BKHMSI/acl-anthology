<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Computational Linguistics Journal (2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Computational Linguistics Journal (2017)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#j17-1>Computational Linguistics, Volume 43, Issue 1 - April 2017</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li><li><a class=align-middle href=#j17-2>Computational Linguistics, Volume 43, Issue 2 - June 2017</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#j17-3>Computational Linguistics, Volume 43, Issue 3 - September 2017</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#j17-4>Computational Linguistics, Volume 43, Issue 4 - December 2017</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li></ul></div></div><div id=j17-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/J17-1/>Computational Linguistics, Volume 43, Issue 1 - April 2017</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-1000/>Computational Linguistics, Volume 43, Issue 1 - <span class=acl-fixed-case>A</span>pril 2017</a></strong><br></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-1001/>A Statistical, Grammar-Based Approach to Microplanning</a></strong><br><a href=/people/c/claire-gardent/>Claire Gardent</a>
|
<a href=/people/l/laura-perez-beltrachini/>Laura Perez-Beltrachini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-1001><div class="card-body p-3 small">Although there has been much work in recent years on data-driven natural language generation, little attention has been paid to the fine-grained interactions that arise during microplanning between aggregation, surface realization, and <a href=https://en.wikipedia.org/wiki/Sentence_segmentation>sentence segmentation</a>. In this article, we propose a hybrid symbolic / statistical approach to jointly model the constraints regulating these <a href=https://en.wikipedia.org/wiki/Interaction_(statistics)>interactions</a>. Our approach integrates a small handwritten grammar, a statistical hypertagger, and a surface realization algorithm. It is applied to the verbalization of knowledge base queries and tested on 13 <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> to demonstrate domain independence. We evaluate our <a href=https://en.wikipedia.org/wiki/Tactic_(method)>approach</a> in several ways. A quantitative analysis shows that the hybrid approach outperforms a purely symbolic approach in terms of both <a href=https://en.wikipedia.org/wiki/Speed>speed</a> and <a href=https://en.wikipedia.org/wiki/Coverage_(telecommunication)>coverage</a>. Results from a human study indicate that users find the output of this hybrid statistic / symbolic system more fluent than both a template-based and a purely symbolic grammar-based approach. Finally, we illustrate by means of examples that our approach can account for various factors impacting <a href=https://en.wikipedia.org/wiki/Aggregate_data>aggregation</a>, <a href=https://en.wikipedia.org/wiki/Sentence_segmentation>sentence segmentation</a>, and surface realization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-1002/>A Game-Theoretic Approach to Word Sense Disambiguation</a></strong><br><a href=/people/r/rocco-tripodi/>Rocco Tripodi</a>
|
<a href=/people/m/marcello-pelillo/>Marcello Pelillo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-1002><div class="card-body p-3 small">This article presents a new model for <a href=https://en.wikipedia.org/wiki/Word_sense_disambiguation>word sense disambiguation</a> formulated in terms of <a href=https://en.wikipedia.org/wiki/Evolutionary_game_theory>evolutionary game theory</a>, where each word to be disambiguated is represented as a node on a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> whose edges represent word relations and senses are represented as classes. The words simultaneously update their class membership preferences according to the senses that neighboring words are likely to choose. We use distributional information to weigh the influence that each word has on the decisions of the others and semantic similarity information to measure the strength of compatibility among the choices. With this information we can formulate the word sense disambiguation problem as a <a href=https://en.wikipedia.org/wiki/Constraint_satisfaction_problem>constraint satisfaction problem</a> and solve it using tools derived from <a href=https://en.wikipedia.org/wiki/Game_theory>game theory</a>, maintaining the textual coherence. The model is based on two ideas : Similar words should be assigned to similar classes and the meaning of a word does not depend on all the words in a text but just on some of them. The article provides an in-depth motivation of the idea of modeling the <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation problem</a> in terms of <a href=https://en.wikipedia.org/wiki/Game_theory>game theory</a>, which is illustrated by an example. The conclusion presents an extensive analysis on the combination of <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measures</a> to use in the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> and a comparison with state-of-the-art systems. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms state-of-the-art algorithms and can be applied to different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> and in different scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-1003/>Multilingual Metaphor Processing : Experiments with Semi-Supervised and Unsupervised Learning</a></strong><br><a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a>
|
<a href=/people/l/lin-sun/>Lin Sun</a>
|
<a href=/people/e/e-dario-gutierrez/>Elkin Darío Gutiérrez</a>
|
<a href=/people/p/patricia-lichtenstein/>Patricia Lichtenstein</a>
|
<a href=/people/s/srini-narayanan/>Srini Narayanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-1003><div class="card-body p-3 small">Highly frequent in language and communication, <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a> represents a significant challenge for Natural Language Processing (NLP) applications. Computational work on <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a> has traditionally evolved around the use of hand-coded knowledge, making the <a href=https://en.wikipedia.org/wiki/System>systems</a> hard to scale. Recent years have witnessed a rise in <a href=https://en.wikipedia.org/wiki/Statistics>statistical approaches</a> to metaphor processing. However, these approaches often require extensive human annotation effort and are predominantly evaluated within a <a href=https://en.wikipedia.org/wiki/Domain_(biology)>limited domain</a>. In contrast, we experiment with weakly supervised and unsupervised techniqueswith little or no annotationto generalize higher-level mechanisms of metaphor from distributional properties of concepts. We investigate different levels and types of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> (learning from linguistic examples vs. learning from a given set of metaphorical mappings vs. learning without annotation) in flat and hierarchical, unconstrained and constrained clustering settings. Our aim is to identify the optimal type of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> for a <a href=https://en.wikipedia.org/wiki/Machine_learning>learning algorithm</a> that discovers patterns of metaphorical association from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>. In order to investigate the scalability and adaptability of our models, we applied them to data in three languages from different language groupsEnglish, Spanish, and Russianachieving state-of-the-art results with little supervision. Finally, we demonstrate that <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a> can facilitate and scale up cross-linguistic research on <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-1004/>Argumentation Mining in User-Generated Web Discourse</a></strong><br><a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-1004><div class="card-body p-3 small">The goal of argumentation mining, an evolving research field in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a>, is to design methods capable of analyzing people&#8217;s argumentation. In this article, we go beyond the state of the art in several ways. (i) We deal with actual Web data and take up the challenges given by the variety of registers, multiple domains, and unrestricted noisy user-generated Web discourse. (ii) We bridge the gap between normative argumentation theories and argumentation phenomena encountered in actual data by adapting an argumentation model tested in an extensive annotation study. (iii) We create a new gold standard corpus (90k tokens in 340 documents) and experiment with several machine learning methods to identify argument components. We offer the <a href=https://en.wikipedia.org/wiki/Data>data</a>, source codes, and annotation guidelines to the community under free licenses. Our findings show that argumentation mining in user-generated Web discourse is a feasible but challenging task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-1005/>Hashtag Sense Clustering Based on Temporal Similarity</a></strong><br><a href=/people/g/giovanni-stilo/>Giovanni Stilo</a>
|
<a href=/people/p/paola-velardi/>Paola Velardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-1005><div class="card-body p-3 small">Hashtags are creative labels used in <a href=https://en.wikipedia.org/wiki/Microblogging>micro-blogs</a> to characterize the topic of a message / discussion. Regardless of the use for which they were originally intended, <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> can not be used as a means to cluster messages with similar content. First, because <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> are created in a spontaneous and highly dynamic way by users in multiple languages, the same topic can be associated with different <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a>, and conversely, the same <a href=https://en.wikipedia.org/wiki/Hashtag>hashtag</a> may refer to different topics in different time periods. Second, contrary to common words, hashtag disambiguation is complicated by the fact that no <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>sense catalogs</a> (e.g., <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> or WordNet) are available ; and, furthermore, hashtag labels are difficult to analyze, as they often consist of <a href=https://en.wikipedia.org/wiki/Acronym>acronyms</a>, concatenated words, and so forth. A common way to determine the meaning of <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> has been to analyze their context, but, as we have just pointed out, <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> can have multiple and variable meanings. In this article, we propose a temporal sense clustering algorithm based on the idea that semantically related hashtags have similar and synchronous usage patterns.</div></div></div><hr><div id=j17-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/J17-2/>Computational Linguistics, Volume 43, Issue 2 - June 2017</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2000/>Computational Linguistics, Volume 43, Issue 2 - June 2017</a></strong><br></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2001/>A Comprehensive Analysis of Bilingual Lexicon Induction</a></strong><br><a href=/people/a/ann-irvine/>Ann Irvine</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-2001><div class="card-body p-3 small">Bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages. In this article we present the most comprehensive analysis of bilingual lexicon induction to date. We present experiments on a wide range of languages and data sizes. We examine translation into English from 25 foreign languages : <a href=https://en.wikipedia.org/wiki/Albanian_language>Albanian</a>, <a href=https://en.wikipedia.org/wiki/Azerbaijani_language>Azeri</a>, <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>, <a href=https://en.wikipedia.org/wiki/Bosnian_language>Bosnian</a>, <a href=https://en.wikipedia.org/wiki/Bulgarian_language>Bulgarian</a>, <a href=https://en.wikipedia.org/wiki/Cebuano_language>Cebuano</a>, <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>, <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian</a>, <a href=https://en.wikipedia.org/wiki/Latvian_language>Latvian</a>, <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali</a>, <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a>, <a href=https://en.wikipedia.org/wiki/Serbian_language>Serbian</a>, <a href=https://en.wikipedia.org/wiki/Slovak_language>Slovak</a>, <a href=https://en.wikipedia.org/wiki/Somali_language>Somali</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>, Ukrainian, <a href=https://en.wikipedia.org/wiki/Uzbek_language>Uzbek</a>, <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>, and <a href=https://en.wikipedia.org/wiki/Welsh_language>Welsh</a>. We analyze the behavior of bilingual lexicon induction on low-frequency words, rather than testing solely on high-frequency words, as previous research has done. Low-frequency words are more relevant to <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>, where systems typically lack translations of rare words that fall outside of their training data. We systematically explore a wide range of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> and phenomena that affect the quality of the translations discovered by bilingual lexicon induction. We provide illustrative examples of the highest ranking translations for orthogonal signals of translation equivalence like contextual similarity and temporal similarity. We analyze the effects of frequency and burstiness, and the sizes of the seed bilingual dictionaries and the monolingual training corpora. Additionally, we introduce a novel discriminative approach to bilingual lexicon induction. Our <a href=https://en.wikipedia.org/wiki/Discriminative_model>discriminative model</a> is capable of combining a wide variety of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that individually provide only weak indications of translation equivalence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2002/>Greedy Transition-Based Dependency Parsing with Stack LSTMs<span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-2002><div class="card-body p-3 small">We introduce a greedy transition-based parser that learns to represent <a href=https://en.wikipedia.org/wiki/State_(computer_science)>parser states</a> using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. Our primary innovation that enables us to do this efficiently is a new control structure for sequential neural networksthe stack long short-term memory unit (LSTM). Like the conventional <a href=https://en.wikipedia.org/wiki/Stack_(abstract_data_type)>stack data structures</a> used in transition-based parsers, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. Our model captures three facets of the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>&#8217;s state : (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of transition actions taken by the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. In addition, we compare two different word representations : (i) standard word vectors based on look-up tables and (ii) character-based models of words. Although standard word embedding models work well in all languages, the character-based models improve the handling of out-of-vocabulary words, particularly in <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphologically rich languages</a>. Finally, we discuss the use of <a href=https://en.wikipedia.org/wiki/Oracle_machine>dynamic oracles</a> in training the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>. During <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, dynamic oracles alternate between sampling parser states from the training data and from the model as it is being learned, making the model more robust to the kinds of errors that will be made at test time. Training our model with <a href=https://en.wikipedia.org/wiki/Oracle_machine>dynamic oracles</a> yields a linear-time greedy parser with very competitive performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2003/>Statistical Models for Unsupervised, Semi-Supervised Supervised Transliteration Mining</a></strong><br><a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/h/helmut-schmid/>Helmut Schmid</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-2003><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> that efficiently mines <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration pairs</a> in a consistent fashion in three different settings : unsupervised, semi-supervised, and supervised transliteration mining. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> interpolates two sub-models, one for the generation of transliteration pairs and one for the generation of non-transliteration pairs (i.e., noise). The <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is trained on <a href=https://en.wikipedia.org/wiki/Noisy_data>noisy unlabeled data</a> using the <a href=https://en.wikipedia.org/wiki/EM_algorithm>EM algorithm</a>. During training the transliteration sub-model learns to generate transliteration pairs and the fixed non-transliteration model generates the noise pairs. After training, the unlabeled data is disambiguated based on the <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior probabilities</a> of the two <a href=https://en.wikipedia.org/wiki/Statistical_model>sub-models</a>. We evaluate our transliteration mining system on data from a transliteration mining shared task and on parallel corpora. For three out of four language pairs, our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms all semi-supervised and supervised systems that participated in the NEWS 2010 shared task. On word pairs extracted from <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> with fewer than 2 % <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration pairs</a>, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves up to 86.7 % F-measure with 77.9 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and 97.8 % <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2004/>Identifying and Avoiding Confusion in Dialogue with People with Alzheimer’s Disease<span class=acl-fixed-case>A</span>lzheimer’s Disease</a></strong><br><a href=/people/h/hamidreza-chinaei/>Hamidreza Chinaei</a>
|
<a href=/people/l/leila-chan-currie/>Leila Chan Currie</a>
|
<a href=/people/a/andrew-danks/>Andrew Danks</a>
|
<a href=/people/h/hubert-lin/>Hubert Lin</a>
|
<a href=/people/t/tejas-mehta/>Tejas Mehta</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-2004><div class="card-body p-3 small">Alzheimer&#8217;s disease (AD) is an increasingly prevalent <a href=https://en.wikipedia.org/wiki/Cognitive_disorder>cognitive disorder</a> in which <a href=https://en.wikipedia.org/wiki/Memory>memory</a>, <a href=https://en.wikipedia.org/wiki/Language>language</a>, and <a href=https://en.wikipedia.org/wiki/Executive_functions>executive function</a> deteriorate, usually in that order. There is a growing need to support individuals with AD and other forms of <a href=https://en.wikipedia.org/wiki/Dementia>dementia</a> in their daily lives, and our goal is to do so through speech-based interaction. Given that 33 % of conversations with people with middle-stage AD involve a breakdown in communication, it is vital that automated dialogue systems be able to identify those breakdowns and, if possible, avoid them. In this article, we discuss several linguistic features that are verbal indicators of <a href=https://en.wikipedia.org/wiki/Confusion>confusion</a> in AD (including vocabulary richness, parse tree structures, and acoustic cues) and apply several <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> to identify dialogue-relevant confusion from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> with up to 82 % accuracy. We also learn dialogue strategies to avoid <a href=https://en.wikipedia.org/wiki/Confusion>confusion</a> in the first place, which is accomplished using a <a href=https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process>partially observable Markov decision process</a> and which obtains accuracies (up to 96.1 %) that are significantly higher than several baselines. This work represents a major step towards automated dialogue systems for individuals with dementia.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2005/>Framing QA as Building and Ranking Intersentence Answer Justifications<span class=acl-fixed-case>QA</span> as Building and Ranking Intersentence Answer Justifications</a></strong><br><a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-2005><div class="card-body p-3 small">We propose a question answering (QA) approach for standardized science exams that both identifies correct answers and produces compelling human-readable justifications for why those answers are correct. Our method first identifies the actual information needed in a question using psycholinguistic concreteness norms, then uses this information need to construct answer justifications by aggregating multiple sentences from different knowledge bases using syntactic and lexical information. We then jointly rank answers and their justifications using a reranking perceptron that treats <a href=https://en.wikipedia.org/wiki/Theory_of_justification>justification quality</a> as a <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a>. We evaluate our method on 1,000 multiple-choice questions from elementary school science exams, and empirically demonstrate that it performs better than several strong baselines, including neural network approaches. Our best configuration answers 44 % of the questions correctly, where the top justifications for 57 % of these correct answers contain a compelling human-readable justification that explains the inference required to arrive at the correct answer. We include a detailed characterization of the justification quality for both our method and a strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, and show that <a href=https://en.wikipedia.org/wiki/Information_aggregation>information aggregation</a> is key to addressing the information need in complex questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-2006/>Squib : Effects of Cognitive Effort on the Resolution of Overspecified Descriptions<span class=acl-fixed-case>S</span>quib: Effects of Cognitive Effort on the Resolution of Overspecified Descriptions</a></strong><br><a href=/people/i/ivandre-paraboni/>Ivandré Paraboni</a>
|
<a href=/people/a/alex-gwo-jen-lan/>Alex Gwo Jen Lan</a>
|
<a href=/people/m/matheus-mendes-de-santana/>Matheus Mendes de Sant’Ana</a>
|
<a href=/people/f/flavio-luiz-coutinho/>Flávio Luiz Coutinho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-2006><div class="card-body p-3 small">Studies in referring expression generation (REG) have shown different effects of referential overspecification on the resolution of certain descriptions. To further investigate effects of this kind, this article reports two <a href=https://en.wikipedia.org/wiki/Eye-tracking>eye-tracking</a> experiments that measure the time required to recognize target objects based on different kinds of information. Results suggest that referential overspecification may be either helpful or detrimental to identification depending on the kind of information that is actually overspecified, an insight that may be useful for the design of more informed hearer-oriented REG algorithms.</div></div></div><hr><div id=j17-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/J17-3/>Computational Linguistics, Volume 43, Issue 3 - September 2017</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-3000/>Computational Linguistics, Volume 43, Issue 3 - September 2017</a></strong><br></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-3001/>Hybrid Grammars for Parsing of Discontinuous Phrase Structures and Non-Projective Dependency Structures</a></strong><br><a href=/people/k/kilian-gebhardt/>Kilian Gebhardt</a>
|
<a href=/people/m/mark-jan-nederhof/>Mark-Jan Nederhof</a>
|
<a href=/people/h/heiko-vogler/>Heiko Vogler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-3001><div class="card-body p-3 small">We explore the concept of hybrid grammars, which formalize and generalize a range of existing frameworks for dealing with discontinuous syntactic structures. Covered are both <a href=https://en.wikipedia.org/wiki/Discontinuity_(linguistics)>discontinuous phrase structures</a> and non-projective dependency structures. Technically, hybrid grammars are related to synchronous grammars, where one grammar component generates linear structures and another generates hierarchical structures. By coupling <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical elements</a> of both components together, <a href=https://en.wikipedia.org/wiki/Discontinuity_(linguistics)>discontinuous structures</a> result. Several types of hybrid grammars are characterized. We also discuss <a href=https://en.wikipedia.org/wiki/Grammar_induction>grammar induction</a> from <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>. The main advantage over existing frameworks is the ability of hybrid grammars to separate discontinuity of the desired structures from <a href=https://en.wikipedia.org/wiki/Time_complexity>time complexity</a> of <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. This permits exploration of a large variety of <a href=https://en.wikipedia.org/wiki/Parsing>parsing algorithms</a> for discontinuous structures, with different properties. This is confirmed by the reported experimental results, which show a wide variety of <a href=https://en.wikipedia.org/wiki/Time_complexity>running time</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, and frequency of parse failures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-3002/>Translation Divergences in ChineseEnglish Machine Translation : An Empirical Investigation<span class=acl-fixed-case>C</span>hinese–<span class=acl-fixed-case>E</span>nglish Machine Translation: An Empirical Investigation</a></strong><br><a href=/people/d/dun-deng/>Dun Deng</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-3002><div class="card-body p-3 small">In this article, we conduct an empirical investigation of translation divergences between <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> relying on a parallel treebank. To do this, we first devise a hierarchical alignment scheme where Chinese and English parse trees are aligned in a way that eliminates conflicts and redundancies between word alignments and syntactic parses to prevent the generation of spurious translation divergences. Using this Hierarchically Aligned ChineseEnglish Parallel Treebank (HACEPT), we are able to semi-automatically identify and categorize the translation divergences between the two languages and quantify each type of translation divergence. Our results show that the translation divergences are much broader than described in previous studies that are largely based on anecdotal evidence and <a href=https://en.wikipedia.org/wiki/Linguistics>linguistic knowledge</a>. The distribution of the translation divergences also shows that some high-profile translation divergences that motivate previous research are actually very rare in our data, whereas other translation divergences that have previously received little attention actually exist in large quantities. We also show that HACEPT allows the extraction of syntax-based translation rules, most of which are expressive enough to capture the translation divergences, and point out that the syntactic annotation in existing treebanks is not optimal for extracting such translation rules. We also discuss the implications of our study for attempts to bridge translation divergences by devising shared semantic representations across languages. Our quantitative results lend further support to the observation that although it is possible to bridge some translation divergences with semantic representations, other translation divergences are open-ended, thus building a semantic representation that captures all possible translation divergences may be impractical.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=J17-3003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/J17-3003/>A Kernel Independence Test for Geographical Language Variation</a></strong><br><a href=/people/d/dong-nguyen/>Dong Nguyen</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-3003><div class="card-body p-3 small">Quantifying the degree of <a href=https://en.wikipedia.org/wiki/Spatial_dependence>spatial dependence</a> for <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>linguistic variables</a> is a key task for analyzing dialectal variation. However, existing <a href=https://en.wikipedia.org/wiki/Methods_of_detecting_exoplanets>approaches</a> have important drawbacks. First, they are based on parametric models of dependence, which limits their power in cases where the underlying parametric assumptions are violated. Second, they are not applicable to all types of linguistic data : Some approaches apply only to <a href=https://en.wikipedia.org/wiki/Frequency>frequencies</a>, others to boolean indicators of whether a linguistic variable is present. We present a new <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for measuring geographical language variation, which solves both of these problems. Our approach builds on Reproducing Kernel Hilbert Space (RKHS) representations for <a href=https://en.wikipedia.org/wiki/Nonparametric_statistics>nonparametric statistics</a>, and takes the form of a <a href=https://en.wikipedia.org/wiki/Test_statistic>test statistic</a> that is computed from pairs of individual geotagged observations without aggregation into predefined geographical bins. We compare this test with prior work using synthetic data as well as a diverse set of real data sets : a corpus of Dutch tweets, a Dutch syntactic atlas, and a data set of letters to the editor in North American newspapers. Our proposed <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>test</a> is shown to support robust inferences across a broad range of scenarios and types of data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-3004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-3004/>AutoExtend : Combining Word Embeddings with Semantic Resources<span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>E</span>xtend: Combining Word Embeddings with Semantic Resources</a></strong><br><a href=/people/s/sascha-rothe/>Sascha Rothe</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-3004><div class="card-body p-3 small">We present AutoExtend, a system that combines word embeddings with semantic resources by learning embeddings for non-word objects like synsets and entities and learning word embeddings that incorporate the semantic information from the resource. The method is based on encoding and decoding the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and is flexible in that it can take any <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> as input and does not need an additional <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training corpus</a>. The obtained <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> live in the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> as the input word embeddings. A sparse tensor formalization guarantees efficiency and <a href=https://en.wikipedia.org/wiki/Parallelizability>parallelizability</a>. We use <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, <a href=https://en.wikipedia.org/wiki/GermaNet>GermaNet</a>, and <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> as semantic resources. AutoExtend achieves state-of-the-art performance on Word-in-Context Similarity and Word Sense Disambiguation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-3005/>Parsing Argumentation Structures in Persuasive Essays</a></strong><br><a href=/people/c/christian-stab/>Christian Stab</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-3005><div class="card-body p-3 small">In this article, we present a novel approach for parsing argumentation structures. We identify argument components using <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a> at the token level and apply a new joint model for detecting argumentation structures. The proposed model globally optimizes argument component types and argumentative relations using <a href=https://en.wikipedia.org/wiki/Integer_linear_programming>Integer Linear Programming</a>. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms challenging heuristic baselines on two different types of <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>. Moreover, we introduce a novel corpus of persuasive essays annotated with <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation structures</a>. We show that our annotation scheme and annotation guidelines successfully guide human annotators to substantial agreement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-3006/>The Agreement Measure cat a Complement to Focused on Categorization of a Continuum</a></strong><br><a href=/people/y/yann-mathet/>Yann Mathet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-3006><div class="card-body p-3 small">Agreement on unitizing, where several annotators freely put units of various sizes and categories on a continuum, is difficult to assess because of the simultaneaous discrepancies in positioning and categorizing. The recent agreement measure offers an overall solution that simultaneously takes into account positions and categories. In this article, I propose the additional coefficient cat, which complements by assessing the agreement on categorization of a continuum, putting aside positional discrepancies. When applied to pure categorization (with predefined units), <a href=https://en.wikipedia.org/wiki/Cat>cat</a> behaves the same way as the famous dedicated Krippendorff&#8217;s, even with missing values, which proves its consistency. A variation of <a href=https://en.wikipedia.org/wiki/Cat>cat</a> is also proposed that provides an in-depth assessment of <a href=https://en.wikipedia.org/wiki/Categorization>categorizing</a> for each individual category. The entire family of coefficients is implemented in <a href=https://en.wikipedia.org/wiki/Free_software>free software</a>.</div></div></div><hr><div id=j17-4><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/J17-4/>Computational Linguistics, Volume 43, Issue 4 - December 2017</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-4000/>Computational Linguistics, Volume 43, Issue 4 - <span class=acl-fixed-case>D</span>ecember 2017</a></strong><br></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-4001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-4001 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-4001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-4001/>Discourse Structure in Machine Translation Evaluation</a></strong><br><a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a>
|
<a href=/people/l/lluis-marquez/>Lluís Màrquez</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-4001><div class="card-body p-3 small">In this article, we explore the potential of using sentence-level discourse structure for machine translation evaluation. We first design discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory (RST). Then, we show that a simple <a href=https://en.wikipedia.org/wiki/Linear_combination>linear combination</a> with these measures can help improve various existing machine translation evaluation metrics regarding correlation with human judgments both at the segment level and at the system level. This suggests that discourse information is complementary to the information used by many of the existing evaluation metrics, and thus it could be taken into account when developing richer evaluation metrics, such as the WMT-14 winning combined metric DiscoTKparty. We also provide a detailed analysis of the relevance of various discourse elements and relations from the RST parse trees for machine translation evaluation. In particular, we show that (i) all aspects of the RST tree are relevant, (ii) <a href=https://en.wikipedia.org/wiki/Nuclearity>nuclearity</a> is more useful than relation type, and (iii) the similarity of the translation RST tree to the reference RST tree is positively correlated with translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-4002/>Adapting to Learner Errors with Minimal Supervision</a></strong><br><a href=/people/a/alla-rozovskaya/>Alla Rozovskaya</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/m/mark-sammons/>Mark Sammons</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-4002><div class="card-body p-3 small">This article considers the problem of correcting errors made by English as a Second Language writers from a machine learning perspective, and addresses an important issue of developing an appropriate <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training paradigm</a> for the task, one that accounts for error patterns of non-native writers using minimal supervision. Existing training approaches present a trade-off between large amounts of cheap data offered by the native-trained models and additional knowledge of learner error patterns provided by the more expensive method of training on annotated learner data. We propose a novel training approach that draws on the strengths offered by the two standard training paradigmsof training either on native or on annotated learner dataand that outperforms both of these standard methods. Using the key observation that parameters relating to <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>error regularities</a> exhibited by non-native writers are relatively simple, we develop models that can incorporate knowledge about <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>error regularities</a> based on a small annotated sample but that are otherwise trained on native English data. The key contribution of this article is the introduction and analysis of two methods for adapting the learned models to error patterns of non-native writers ; one method that applies to generative classifiers and a second that applies to discriminative classifiers. Both <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> demonstrated state-of-the-art performance in several text correction competitions. In particular, the Illinois system that implements these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> ranked at the top in two recent CoNLL shared tasks on error correction.1 We conduct further evaluation of the proposed approaches studying the effect of using <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error data</a> from speakers of the same native language, languages that are closely related linguistically, and unrelated languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/J17-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-J17-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-J17-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/J17-4004/>HyperLex : A Large-Scale Evaluation of Graded Lexical Entailment<span class=acl-fixed-case>H</span>yper<span class=acl-fixed-case>L</span>ex: A Large-Scale Evaluation of Graded Lexical Entailment</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/d/daniela-gerz/>Daniela Gerz</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/f/felix-hill/>Felix Hill</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-J17-4004><div class="card-body p-3 small">We introduce HyperLexa data set and evaluation resource that quantifies the extent of the semantic category membership, that is, type-of relation, also known as <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponymyhypernymy</a> or lexical entailment (LE) relation between 2,616 concept pairs. Cognitive psychology research has established that typicality and category / class membership are computed in <a href=https://en.wikipedia.org/wiki/Semantic_memory>human semantic memory</a> as a gradual rather than <a href=https://en.wikipedia.org/wiki/Binary_relation>binary relation</a>. Nevertheless, most <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP research</a> and existing large-scale inventories of concept category membership (WordNet, <a href=https://en.wikipedia.org/wiki/DBPedia>DBPedia</a>, etc.) treat category membership and LE as binary. To address this, we asked hundreds of <a href=https://en.wikipedia.org/wiki/First_language>native English speakers</a> to indicate typicality and strength of <a href=https://en.wikipedia.org/wiki/Categorization>category membership</a> between a diverse range of concept pairs on a <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing platform</a>. Our results confirm that category membership and LE are indeed more gradual than <a href=https://en.wikipedia.org/wiki/Binary_relation>binary</a>. We then compare these human judgments with the predictions of automatic systems, which reveals a huge gap between human performance and state-of-the-art LE, distributional and representation learning models, and substantial differences between the models themselves. We discuss a pathway for improving semantic models to overcome this discrepancy, and indicate future application areas for improved graded LE systems.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>