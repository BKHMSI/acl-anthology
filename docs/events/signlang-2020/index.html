<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on the Representation and Processing of Sign Languages (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on the Representation and Processing of Sign Languages (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020signlang-1>Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li></ul></div></div><div id=2020signlang-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.signlang-1/>Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.0/>Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives</a></strong><br><a href=/people/e/eleni-efthimiou/>Eleni Efthimiou</a>
|
<a href=/people/s/stavroula-evita-fotinea/>Stavroula-Evita Fotinea</a>
|
<a href=/people/t/thomas-hanke/>Thomas Hanke</a>
|
<a href=/people/j/julie-a-hochgesang/>Julie A. Hochgesang</a>
|
<a href=/people/j/jette-kristoffersen/>Jette Kristoffersen</a>
|
<a href=/people/j/johanna-mesch/>Johanna Mesch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.1/>Back and Forth between Theory and Application : Shared Phonological Coding Between ASL Signbank and ASL-LEX<span class=acl-fixed-case>ASL</span> <span class=acl-fixed-case>S</span>ignbank and <span class=acl-fixed-case>ASL</span>-<span class=acl-fixed-case>LEX</span></a></strong><br><a href=/people/a/amelia-becker/>Amelia Becker</a>
|
<a href=/people/d/donovan-catt/>Donovan Catt</a>
|
<a href=/people/j/julie-a-hochgesang/>Julie A. Hochgesang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--1><div class="card-body p-3 small">The development of signed language lexical databases, digital organizations that describe different phonological features of and attempt to establish relationships between signs has resulted in a renewed interest in the phonological descriptions used to uniquely identify and organize the lexicons of respective sign languages (van der Kooij, 2002 ; Fenlon et al., 2016 ; Brentari et al., 2018). Throughout the mutually shared coding process involved in organizing two lexical databases, ASL Signbank (Hochgesang, Crasborn and Lillo-Martin, 2020) and ASL-LEX (Caselli et al., 2016), issues have arisen that require revisiting how phonological features and categories are to be applied and even decided upon, and which would adequately distinguish lexical contrast for respective sign languages. The paper concludes by exploring the inverse of the theory-to-database relationship. Examples are given of theoretical implications and research questions that arise from consequences of language resource building. These are presented as evidence that not only does <a href=https://en.wikipedia.org/wiki/Theory>theory</a> impact organization of databases but that the process of database creation can also inform our theories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.4/>Measuring <a href=https://en.wikipedia.org/wiki/Lexical_similarity>Lexical Similarity</a> across <a href=https://en.wikipedia.org/wiki/Sign_language>Sign Languages</a> in Global Signbank<span class=acl-fixed-case>G</span>lobal <span class=acl-fixed-case>S</span>ignbank</a></strong><br><a href=/people/c/carl-borstell/>Carl Börstell</a>
|
<a href=/people/o/onno-crasborn/>Onno Crasborn</a>
|
<a href=/people/l/lori-whynot/>Lori Whynot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--4><div class="card-body p-3 small">Lexicostatistics is the main method used in previous work measuring linguistic distances between sign languages. As a method, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> disregards any possible structural / grammatical similarity, instead focusing exclusively on lexical items, but <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is time consuming as <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> requires some comparable phonological coding (i.e. form description) as well as concept matching (i.e. meaning description) of signs across the <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a> to be compared. In this paper, we present a novel approach for measuring <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical similarity</a> across any two sign languages using the Global Signbank platform, a <a href=https://en.wikipedia.org/wiki/Lexical_database>lexical database</a> of uniformly coded signs. The method involves a feature-by-feature comparison of all matched <a href=https://en.wikipedia.org/wiki/Phonological_feature>phonological features</a>. This method can be used in two distinct ways : 1) automatically comparing the amount of lexical overlap between two <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a> (with a more detailed feature-description than previous lexicostatistical methods) ; 2) finding exact form-matches across languages that are either matched or mismatched in meaning (i.e. true or false friends). We show the feasability of this method by comparing three languages (datasets) in Global Signbank, and are currently expanding both the size of these three as well as the total number of datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.6/>PE2LGP Animator : A Tool To Animate A Portuguese Sign Language Avatar<span class=acl-fixed-case>PE</span>2<span class=acl-fixed-case>LGP</span> Animator: A Tool To Animate A <span class=acl-fixed-case>P</span>ortuguese <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage Avatar</a></strong><br><a href=/people/p/pedro-cabral/>Pedro Cabral</a>
|
<a href=/people/m/matilde-goncalves/>Matilde Gonçalves</a>
|
<a href=/people/h/hugo-nicolau/>Hugo Nicolau</a>
|
<a href=/people/l/luisa-coheur/>Luísa Coheur</a>
|
<a href=/people/r/ruben-santos/>Ruben Santos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--6><div class="card-body p-3 small">Software for the production of sign languages is much less common than for <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken languages</a>. Such <a href=https://en.wikipedia.org/wiki/Software>software</a> usually relies on 3D humanoid avatars to produce signs which, inevitably, necessitates the use of <a href=https://en.wikipedia.org/wiki/Animation>animation</a>. One barrier to the use of popular animation tools is their complexity and steep learning curve, which can be hard to master for inexperienced users. Here, we present PE2LGP, an <a href=https://en.wikipedia.org/wiki/Authoring_system>authoring system</a> that features a <a href=https://en.wikipedia.org/wiki/Avatar_(computing)>3D avatar</a> that signs <a href=https://en.wikipedia.org/wiki/Portuguese_Sign_Language>Portuguese Sign Language</a>. Our <a href=https://en.wikipedia.org/wiki/Animator>Animator</a> is designed specifically to craft sign language animations using a key frame method, and is meant to be easy to use and learn to users without animation skills. We conducted a preliminary evaluation of the <a href=https://en.wikipedia.org/wiki/Animator>Animator</a>, where we animated seven Portuguese Sign Language sentences and asked four sign language users to evaluate their quality. This evaluation revealed that the <a href=https://en.wikipedia.org/wiki/System>system</a>, in spite of its simplicity, is indeed capable of producing comprehensible messages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.7/>Translating an <a href=https://en.wikipedia.org/wiki/Aesop&#8217;s_Fables>Aesop’s Fable</a> to <a href=https://en.wikipedia.org/wiki/Filipino_Sign_Language>Filipino Sign Language</a> through <a href=https://en.wikipedia.org/wiki/Animation>3D Animation</a><span class=acl-fixed-case>A</span>esop’s Fable to <span class=acl-fixed-case>F</span>ilipino <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage through 3<span class=acl-fixed-case>D</span> Animation</a></strong><br><a href=/people/m/mark-cueto/>Mark Cueto</a>
|
<a href=/people/w/winnie-he/>Winnie He</a>
|
<a href=/people/r/rei-untiveros/>Rei Untiveros</a>
|
<a href=/people/j/josh-zuniga/>Josh Zuñiga</a>
|
<a href=/people/j/joanna-pauline-rivera/>Joanna Pauline Rivera</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--7><div class="card-body p-3 small">According to the National Statistics Office (2003) in the 2000 Population Census, the deaf community in the Philippines numbered to about 121,000 deaf and hard of hearing Filipinos. Deaf and hard of hearing Filipinos in these communities use the Filipino Sign Language (FSL) as the main method of manual communication. Deaf and hard of hearing children experience difficulty in developing reading and writing skills through traditional methods of teaching used primarily for hearing children. This study aims to translate an <a href=https://en.wikipedia.org/wiki/Aesop&#8217;s_Fables>Aesop&#8217;s fable</a> to <a href=https://en.wikipedia.org/wiki/Filipino_Sign_Language>Filipino Sign Language</a> with the use of <a href=https://en.wikipedia.org/wiki/Animation>3D animation</a> resulting to a video output. The video created contains a 3D animated avatar performing the sign translations to FSL (mainly focusing on hand gestures which includes hand shape, palm orientation, location, and movement) on screen beside their English text equivalent and related images. The final output was then evaluated by FSL deaf signers. Evaluation results showed that the final output can potentially be used as a learning material. In order to make it more effective as a learning material, it is very important to consider the animation&#8217;s appearance, <a href=https://en.wikipedia.org/wiki/Speed>speed</a>, naturalness, and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. In this paper, the common action units were also listed for easier construction of animations of the signs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.9/>Elicitation and Corpus of Spontaneous Sign Language Discourse Representation Diagrams</a></strong><br><a href=/people/m/michael-filhol/>Michael Filhol</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--9><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Sign_language>Sign Languages</a> have no standard written form, many signers do capture their language in some form of spontaneous graphical form. We list a few <a href=https://en.wikipedia.org/wiki/Use_case>use cases</a> (discourse preparation, deverbalising for <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, etc.) and give examples of <a href=https://en.wikipedia.org/wiki/Diagram>diagrams</a>. After hypothesising that they contain regular patterns of significant value, we propose to build a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of such <a href=https://en.wikipedia.org/wiki/Production_(economics)>productions</a>. The main contribution of this paper is the specification of the elicitation protocol, explaining the variables that are likely to affect the diagrams collected. We conclude with a report on the current state of a collection following this <a href=https://en.wikipedia.org/wiki/Protocol_(science)>protocol</a>, and a few observations on the collected contents. A first prospect is the standardisation of a scheme to represent SL discourse in a way that would make them sharable. A subsequent longer-term prospect is for this scheme to be owned by users and with time be shaped into a script for their language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.11/>Signing as Input for a Dictionary Query : Matching Signs Based on Joint Positions of the Dominant Hand</a></strong><br><a href=/people/m/manolis-fragkiadakis/>Manolis Fragkiadakis</a>
|
<a href=/people/v/victoria-nyst/>Victoria Nyst</a>
|
<a href=/people/p/peter-van-der-putten/>Peter van der Putten</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--11><div class="card-body p-3 small">This study presents a new <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to search sign language lexica, using a full sign as input for a query. Thus, a dictionary user can look up information about a sign by signing the sign to a webcam. The recorded sign is then compared to potential matching signs in the lexicon. As such, it provides a new way of searching sign language dictionaries to complement existing methods based on (spoken language) glosses or phonological features, like <a href=https://en.wikipedia.org/wiki/Handshape>handshape</a> or <a href=https://en.wikipedia.org/wiki/Location>location</a>. The method utilizes OpenPose to extract the body and finger joint positions. Dynamic Time Warping (DTW) is used to quantify the variation of the trajectory of the dominant hand and the average trajectories of the fingers. Ten people with various degrees of sign language proficiency have participated in this study. Each subject viewed a set of 20 signs from the newly compiled Ghanaian sign language lexicon and was asked to replicate the signs. The results show that DTW can predict the matching sign with 87 % and 74 % accuracy at the Top-10 and Top-5 ranking level respectively by using only the trajectory of the dominant hand. Additionally, more proficient signers obtain 90 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> at the Top-10 ranking. The <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> has the potential to be used also as a variation measurement tool to quantify the difference in signing between different signers or sign languages in general.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.14/>An Isolated-Signing RGBD Dataset of 100 American Sign Language Signs Produced by Fluent ASL Signers<span class=acl-fixed-case>RGBD</span> Dataset of 100 <span class=acl-fixed-case>A</span>merican <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage Signs Produced by Fluent <span class=acl-fixed-case>ASL</span> Signers</a></strong><br><a href=/people/s/saad-hassan/>Saad Hassan</a>
|
<a href=/people/l/larwan-berke/>Larwan Berke</a>
|
<a href=/people/e/elahe-vahdani/>Elahe Vahdani</a>
|
<a href=/people/l/longlong-jing/>Longlong Jing</a>
|
<a href=/people/y/yingli-tian/>Yingli Tian</a>
|
<a href=/people/m/matt-huenerfauth/>Matt Huenerfauth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--14><div class="card-body p-3 small">We have collected a new dataset consisting of color and depth videos of fluent American Sign Language (ASL) signers performing sequences of 100 ASL signs from a Kinect v2 sensor. This directed dataset had originally been collected as part of an ongoing collaborative project, to aid in the development of a sign-recognition system for identifying occurrences of these 100 signs in video. The set of words consist of vocabulary items that would commonly be learned in a first-year ASL course offered at a university, although the specific set of signs selected for inclusion in the dataset had been motivated by project-related factors. Given increasing interest among sign-recognition and other computer-vision researchers in red-green-blue-depth (RBGD) video, we release this dataset for use by the research community. In addition to the RGB video files, we share depth and HD face data as well as additional features of <a href=https://en.wikipedia.org/wiki/Face>face</a>, <a href=https://en.wikipedia.org/wiki/Hand>hands</a>, and <a href=https://en.wikipedia.org/wiki/Human_body>body</a> produced through post-processing of this data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.16/>Sign Language Motion Capture Dataset for Data-driven Synthesis</a></strong><br><a href=/people/p/pavel-jedlicka/>Pavel Jedlička</a>
|
<a href=/people/z/zdenek-krnoul/>Zdeněk Krňoul</a>
|
<a href=/people/j/jakub-kanis/>Jakub Kanis</a>
|
<a href=/people/m/milos-zelezny/>Miloš Železný</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--16><div class="card-body p-3 small">This paper presents a new 3D motion capture dataset of <a href=https://en.wikipedia.org/wiki/Czech_Sign_Language>Czech Sign Language (CSE)</a>. Its main purpose is to provide the data for further analysis and data-based automatic synthesis of CSE utterances. The content of the <a href=https://en.wikipedia.org/wiki/Data>data</a> in the given limited domain of weather forecasts was carefully selected by the CSE linguists to provide the necessary utterances needed to produce any new <a href=https://en.wikipedia.org/wiki/Weather_forecasting>weather forecast</a>. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> was recorded using the state-of-the-art motion capture (MoCap) technology to provide the most precise trajectories of the motion. In general, MoCap is a device capable of accurate recording of motion directly in <a href=https://en.wikipedia.org/wiki/Three-dimensional_space>3D space</a>. The <a href=https://en.wikipedia.org/wiki/Data>data</a> contains trajectories of body, arms, hands and face markers recorded at once to provide consistent data without the need for the time alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.20/>Recognition of Static Features in <a href=https://en.wikipedia.org/wiki/Sign_language>Sign Language</a> Using Key-Points</a></strong><br><a href=/people/i/ioannis-koulierakis/>Ioannis Koulierakis</a>
|
<a href=/people/g/georgios-siolas/>Georgios Siolas</a>
|
<a href=/people/e/eleni-efthimiou/>Eleni Efthimiou</a>
|
<a href=/people/e/evita-fotinea/>Evita Fotinea</a>
|
<a href=/people/a/andreas-georgios-stafylopatis/>Andreas-Georgios Stafylopatis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--20><div class="card-body p-3 small">In this paper we report on a research effort focusing on recognition of static features of sign formation in single sign videos. Three sequential models have been developed for <a href=https://en.wikipedia.org/wiki/Handshape>handshape</a>, palm orientation and location of sign formation respectively, which make use of key-points extracted via OpenPose software. The <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> have been applied to a Danish and a Greek Sign Language dataset, providing results around 96 %. Moreover, during the reported research, a method has been developed for identifying the time-frame of real signing in the video, which allows to ignore transition frames during sign recognition processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.22/>Machine Learning for Enhancing Dementia Screening in Ageing Deaf Signers of British Sign Language<span class=acl-fixed-case>B</span>ritish <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage</a></strong><br><a href=/people/x/xing-liang/>Xing Liang</a>
|
<a href=/people/b/bencie-woll/>Bencie Woll</a>
|
<a href=/people/k/kapetanios-epaminondas/>Kapetanios Epaminondas</a>
|
<a href=/people/a/anastasia-angelopoulou/>Anastasia Angelopoulou</a>
|
<a href=/people/r/reda-al-batat/>Reda Al-Batat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--22><div class="card-body p-3 small">Ageing trend in populations is correlated with increased prevalence of <a href=https://en.wikipedia.org/wiki/Cognitive_deficit>acquired cognitive impairments</a> such as <a href=https://en.wikipedia.org/wiki/Dementia>dementia</a>. Although there is no cure for <a href=https://en.wikipedia.org/wiki/Dementia>dementia</a>, a timely diagnosis helps in obtaining necessary support and appropriate medication. With this in mind, researchers are working urgently to develop effective technological tools that can help doctors undertake early identification of cognitive disorder. In this paper, we introduce an automatic dementia screening system for ageing Deaf signers of British Sign Language (BSL), using Convolutional Neural Networks (CNN), by analysing the sign space envelope and facial expression of BSL signers using normal 2D videos from BSL corpus. Our approach firstly establishes an accurate real-time hand trajectory tracking model together with a real-time landmark facial motion analysis model to identify differences in sign space envelope and facial movement as the keys to identifying language changes associated with <a href=https://en.wikipedia.org/wiki/Dementia>dementia</a>. Based on the differences in patterns obtained from facial and trajectory motion data, CNN models (ResNet50 / VGG16) are fine-tuned using Keras deep learning models to incrementally identify and improve dementia recognition rates. We report the results for two methods using different modalities (sign trajectory and facial motion), together with the performance comparisons between different deep learning CNN models in ResNet50 and VGG16. The experiments show the effectiveness of our deep learning based approach in terms of sign space tracking, facial motion tracking and early stage dementia performance assessment tasks. The results are validated against cognitive assessment scores as of our ground truth data with a test set performance of 87.88 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.23/>Machine Translation from <a href=https://en.wikipedia.org/wiki/Spoken_language>Spoken Language</a> to <a href=https://en.wikipedia.org/wiki/Sign_language>Sign Language</a> using Pre-trained Language Model as Encoder</a></strong><br><a href=/people/t/taro-miyazaki/>Taro Miyazaki</a>
|
<a href=/people/y/yusuke-morita/>Yusuke Morita</a>
|
<a href=/people/m/masanori-sano/>Masanori Sano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--23><div class="card-body p-3 small">Sign language is the first language for those who were born deaf or lost their hearing in early childhood, so such individuals require services provided with <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a>. To achieve flexible open-domain services with <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a>, <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translations</a> into <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a> are needed. Machine translations generally require large-scale training corpora, but there are only small corpora for <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a>. To overcome this data-shortage scenario, we developed a method that involves using a pre-trained language model of spoken language as the initial model of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> of the <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation model</a>. We evaluated our method by comparing it to baseline methods, including phrase-based machine translation, using only 130,000 phrase pairs of training data. Our method outperformed the baseline method, and we found that one of the reasons of translation error is from <a href=https://en.wikipedia.org/wiki/Pointing>pointing</a>, which is a special feature used in <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a>. We also conducted trials to improve the translation quality for <a href=https://en.wikipedia.org/wiki/Pointing>pointing</a>. The results are somewhat disappointing, so we believe that there is still room for improving translation quality, especially for <a href=https://en.wikipedia.org/wiki/Pointing>pointing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.27/>Automatic Classification of Handshapes in <a href=https://en.wikipedia.org/wiki/Russian_Sign_Language>Russian Sign Language</a><span class=acl-fixed-case>R</span>ussian <span class=acl-fixed-case>S</span>ign <span class=acl-fixed-case>L</span>anguage</a></strong><br><a href=/people/m/medet-mukushev/>Medet Mukushev</a>
|
<a href=/people/a/alfarabi-imashev/>Alfarabi Imashev</a>
|
<a href=/people/v/vadim-kimmelman/>Vadim Kimmelman</a>
|
<a href=/people/a/anara-sandygulova/>Anara Sandygulova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--27><div class="card-body p-3 small">Handshapes are one of the basic parameters of signs, and any <a href=https://en.wikipedia.org/wiki/Phonology>phonological or phonetic analysis</a> of a <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a> must account for <a href=https://en.wikipedia.org/wiki/Handshapes>handshapes</a>. Many <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a> have been carefully analysed by sign language linguists to create handshape inventories. This has theoretical implications, but also applied use, as it is important due to the need of generating corpora for sign languages that can be searched, filtered, sorted by different sign components (such as handshapes, orientation, location, movement, etc.). However, it is a very time-consuming process, thus only a handful of <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a> have such inventories. This work proposes a process of automatically generating such inventories for sign languages by applying automatic hand detection, cropping, and clustering techniques. We applied our proposed method to a commonly used resource : the Spreadthesign online dictionary (www.spreadthesign.com), in particular to Russian Sign Language (RSL). We then manually verified the data to be able to perform <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Thus, the proposed <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> can serve as an alternative approach to manual annotation, and can help linguists in answering numerous research questions in relation to handshape frequencies in sign languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.30/>BosphorusSign22k Sign Language Recognition Dataset<span class=acl-fixed-case>B</span>osphorus<span class=acl-fixed-case>S</span>ign22k Sign Language Recognition Dataset</a></strong><br><a href=/people/o/ogulcan-ozdemir/>Oğulcan Özdemir</a>
|
<a href=/people/a/ahmet-alp-kindiroglu/>Ahmet Alp Kındıroğlu</a>
|
<a href=/people/n/necati-cihan-camgoz/>Necati Cihan Camgöz</a>
|
<a href=/people/l/lale-akarun/>Lale Akarun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--30><div class="card-body p-3 small">Sign Language Recognition is a challenging research domain. It has recently seen several advancements with the increased availability of data. In this paper, we introduce the BosphorusSign22k, a publicly available large scale sign language dataset aimed at <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>, <a href=https://en.wikipedia.org/wiki/Computer_vision>video recognition</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning research communities</a>. The primary objective of this dataset is to serve as a new benchmark in Turkish Sign Language Recognition for its vast lexicon, the high number of repetitions by native signers, high recording quality, and the unique syntactic properties of the signs it encompasses. We also provide state-of-the-art human pose estimates to encourage other tasks such as Sign Language Production. We survey other publicly available datasets and expand on how BosphorusSign22k can contribute to future research that is being made possible through the widespread availability of similar Sign Language resources. We have conducted extensive experiments and present baseline results to underpin future research on our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.34/>Video-to-HamNoSys Automated Annotation System<span class=acl-fixed-case>H</span>am<span class=acl-fixed-case>N</span>o<span class=acl-fixed-case>S</span>ys Automated Annotation System</a></strong><br><a href=/people/v/victor-skobov/>Victor Skobov</a>
|
<a href=/people/y/yves-lepage/>Yves Lepage</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--34><div class="card-body p-3 small">The Hamburg Notation System (HamNoSys) was developed for movement annotation of any sign language (SL) and can be used to produce signing animations for a virtual avatar with the JASigning platform. This provides the potential to use <a href=https://en.wikipedia.org/wiki/HamNoSys>HamNoSys</a>, i.e., <a href=https://en.wikipedia.org/wiki/String_(computer_science)>strings of characters</a>, as a representation of an SL corpus instead of <a href=https://en.wikipedia.org/wiki/Video>video material</a>. Processing <a href=https://en.wikipedia.org/wiki/String_(computer_science)>strings of characters</a> instead of images can significantly contribute to <a href=https://en.wikipedia.org/wiki/Sign_language>sign language research</a>. However, the complexity of <a href=https://en.wikipedia.org/wiki/HamNoSys>HamNoSys</a> makes it difficult to annotate without a lot of time and effort. Therefore <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> has to be automatized. This work proposes a conceptually new <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>approach</a> to this problem. It includes a new tree representation of the HamNoSys grammar that serves as a basis for the generation of grammatical training data and classification of complex movements using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. Our automatic annotation system relies on HamNoSys grammar structure and can potentially be used on already existing SL corpora. It is retrainable for specific settings such as <a href=https://en.wikipedia.org/wiki/Camera_angle>camera angles</a>, <a href=https://en.wikipedia.org/wiki/Film_speed>speed</a>, and <a href=https://en.wikipedia.org/wiki/Gesture_recognition>gestures</a>. Our approach is conceptually different from other SL recognition solutions and offers a developed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.signlang-1.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--signlang-1--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.signlang-1.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.signlang-1.35/>Cross-Lingual Keyword Search for Sign Language</a></strong><br><a href=/people/n/nazif-can-tamer/>Nazif Can Tamer</a>
|
<a href=/people/m/murat-saraclar/>Murat Saraçlar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--signlang-1--35><div class="card-body p-3 small">Sign language research most often relies on exhaustively annotated and segmented data, which is scarce even for the most studied <a href=https://en.wikipedia.org/wiki/Sign_language>sign languages</a>. However, parallel corpora consisting of sign language interpreting are rarely explored. By utilizing such data for the task of <a href=https://en.wikipedia.org/wiki/Keyword_search>keyword search</a>, this work aims to enable <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> from <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a> with the queries from the translated written language. With the written language translations as labels, we train a weakly supervised keyword search model for <a href=https://en.wikipedia.org/wiki/Sign_language>sign language</a> and further improve the retrieval performance with two context modeling strategies. In our experiments, we compare the gloss retrieval and cross language retrieval performance on RWTH-PHOENIX-Weather 2014 T dataset.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>