<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Joint Conference on Lexical and Computational Semantics (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Joint Conference on Lexical and Computational Semantics (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#s18-2>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a>
<span class="badge badge-info align-middle ml-1">19&nbsp;papers</span></li></ul></div></div><div id=s18-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/S18-2/>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2000/>Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</a></strong><br><a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2001/>Resolving Event Coreference with <a href=https://en.wikipedia.org/wiki/Supervised_learning>Supervised Representation Learning</a> and Clustering-Oriented Regularization</a></strong><br><a href=/people/k/kian-kenyon-dean/>Kian Kenyon-Dean</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/d/doina-precup/>Doina Precup</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2001><div class="card-body p-3 small">We present an approach to event coreference resolution by developing a general <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> that uses <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised representation learning</a>. We propose a neural network architecture with novel Clustering-Oriented Regularization (CORE) terms in the <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a>. These terms encourage the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to create embeddings of event mentions that are amenable to <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>. We then use <a href=https://en.wikipedia.org/wiki/Agglomerative_clustering>agglomerative clustering</a> on these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> to build event coreference chains. For both within- and cross-document coreference on the ECB+ corpus, our model obtains better results than models that require significantly more pre-annotated information. This work provides insight and motivating results for a new general approach to solving coreference and clustering problems with <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2002 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2002/>Learning distributed event representations with a multi-task approach</a></strong><br><a href=/people/x/xudong-hong/>Xudong Hong</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2002><div class="card-body p-3 small">Human world knowledge contains information about prototypical events and their participants and locations. In this paper, we train the first models using <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> that can both predict missing event participants and also perform semantic role classification based on semantic plausibility. Our best-performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is an improvement over the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on thematic fit modelling tasks. The event embeddings learned by the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can additionally be used effectively in an event similarity task, also outperforming the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2003/>Assessing Meaning Components in German Complex Verbs : A Collection of Source-Target Domains and Directionality<span class=acl-fixed-case>G</span>erman Complex Verbs: A Collection of Source-Target Domains and Directionality</a></strong><br><a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/m/maximilian-koper/>Maximilian KÃ¶per</a>
|
<a href=/people/s/sylvia-springorum/>Sylvia Springorum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2003><div class="card-body p-3 small">This paper presents a collection to assess <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning components</a> in German complex verbs, which frequently undergo <a href=https://en.wikipedia.org/wiki/Semantic_change>meaning shifts</a>. We use a novel strategy to obtain source and target domain characterisations via sentence generation rather than sentence annotation. A selection of <a href=https://en.wikipedia.org/wiki/Arrow_(symbol)>arrows</a> adds spatial directional information to the generated contexts. We provide a broad qualitative description of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, and a series of standard <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> experiments verifies the quantitative reliability of the presented resource. The setup for collecting the meaning components is applicable also to other languages, regarding complex verbs as well as other language-specific targets that involve meaning shifts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2007 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2007/>Mixing Context Granularities for Improved Entity Linking on Question Answering Data across Entity Categories</a></strong><br><a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2007><div class="card-body p-3 small">The first stage of every knowledge base question answering approach is to link entities in the input question. We investigate <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> in the context of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering task</a> and present a jointly optimized neural architecture for entity mention detection and entity disambiguation that models the surrounding context on different levels of granularity. We use the Wikidata knowledge base and available question answering datasets to create benchmarks for <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> on question answering data. Our approach outperforms the previous state-of-the-art <a href=https://en.wikipedia.org/wiki/System>system</a> on this <a href=https://en.wikipedia.org/wiki/Data>data</a>, resulting in an average 8 % improvement of the final score. We further demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> delivers a strong performance across different entity categories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2008/>Quantitative Semantic Variation in the Contexts of Concrete and Abstract Words</a></strong><br><a href=/people/d/daniela-naumann/>Daniela Naumann</a>
|
<a href=/people/d/diego-frassinelli/>Diego Frassinelli</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2008><div class="card-body p-3 small">Across disciplines, researchers are eager to gain insight into empirical features of <a href=https://en.wikipedia.org/wiki/Abstract_and_concrete>abstract vs. concrete concepts</a>. In this work, we provide a detailed characterisation of the distributional nature of <a href=https://en.wikipedia.org/wiki/Abstract_and_concrete>abstract and concrete words</a> across 16,620 <a href=https://en.wikipedia.org/wiki/English_nouns>English nouns</a>, verbs and adjectives. Specifically, we investigate the following questions : (1) What is the distribution of concreteness in the contexts of concrete and abstract target words? (2) What are the differences between concrete and abstract words in terms of contextual semantic diversity? (3) How does the <a href=https://en.wikipedia.org/wiki/Entropy_(information_theory)>entropy</a> of concrete and abstract word contexts differ? Overall, our studies show consistent differences in the distributional representation of concrete and abstract words, thus challenging existing theories of cognition and providing a more fine-grained description of their nature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2010/>The Limitations of Cross-language Word Embeddings Evaluation</a></strong><br><a href=/people/a/amir-bakarov/>Amir Bakarov</a>
|
<a href=/people/r/roman-suvorov/>Roman Suvorov</a>
|
<a href=/people/i/ilya-sochenkov/>Ilya Sochenkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2010><div class="card-body p-3 small">The aim of this work is to explore the possible limitations of existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> of cross-language word embeddings evaluation, addressing the lack of correlation between intrinsic and extrinsic cross-language evaluation methods. To prove this hypothesis, we construct English-Russian datasets for extrinsic and intrinsic evaluation tasks and compare performances of 5 different cross-language models on them. The results say that the scores even on different <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>intrinsic benchmarks</a> do not correlate to each other. We can conclude that the use of human references as ground truth for cross-language word embeddings is not proper unless one does not understand how do <a href=https://en.wikipedia.org/wiki/First_language>native speakers</a> process <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> in their cognition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2011/>How Gender and Skin Tone Modifiers Affect Emoji Semantics in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a><span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2011><div class="card-body p-3 small">In this paper we analyze the use of <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> with respect to <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and <a href=https://en.wikipedia.org/wiki/Human_skin_color>skin tone</a>. By gathering a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of over twenty two million tweets from United States some findings are clearly highlighted after performing a simple frequency-based analysis. Moreover, we carry out a <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic analysis</a> on the usage of <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and their modifiers (e.g. gender and skin tone) by embedding all <a href=https://en.wikipedia.org/wiki/Word>words</a>, <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and modifiers into the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>. Our analyses reveal that some <a href=https://en.wikipedia.org/wiki/Stereotype>stereotypes</a> related to the <a href=https://en.wikipedia.org/wiki/Human_skin_color>skin color</a> and gender seem to be reflected on the use of these modifiers. For example, <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> representing <a href=https://en.wikipedia.org/wiki/List_of_gestures>hand gestures</a> are more widely utilized with lighter skin tones, and the usage across <a href=https://en.wikipedia.org/wiki/Human_skin_color>skin tones</a> differs significantly. At the same time, the vector corresponding to the male modifier tends to be semantically close to <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> related to business or technology, whereas their female counterparts appear closer to <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> about love or makeup.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2014" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2014/>Learning Patient Representations from Text</a></strong><br><a href=/people/d/dmitriy-dligach/>Dmitriy Dligach</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2014><div class="card-body p-3 small">Mining <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic health records</a> for patients who satisfy a set of predefined criteria is known in <a href=https://en.wikipedia.org/wiki/Health_informatics>medical informatics</a> as <a href=https://en.wikipedia.org/wiki/Phenotype>phenotyping</a>. Phenotyping has numerous applications such as <a href=https://en.wikipedia.org/wiki/Prediction>outcome prediction</a>, clinical trial recruitment, and <a href=https://en.wikipedia.org/wiki/Retrospective>retrospective studies</a>. Supervised machine learning for <a href=https://en.wikipedia.org/wiki/Phenotype>phenotyping</a> typically relies on sparse patient representations such as <a href=https://en.wikipedia.org/wiki/Bag-of-words>bag-of-words</a>. We consider an alternative that involves learning patient representations. We develop a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network model</a> for learning patient representations and show that the learned <a href=https://en.wikipedia.org/wiki/Mental_representation>representations</a> are general enough to obtain state-of-the-art performance on a standard comorbidity detection task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2015" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2015/>Polarity Computations in Flexible Categorial Grammar</a></strong><br><a href=/people/h/hai-hu/>Hai Hu</a>
|
<a href=/people/l/larry-moss/>Larry Moss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2015><div class="card-body p-3 small">This paper shows how to take parse trees in CCG and algorithmically find the polarities of all the constituents. Our work uses the well-known polarization principle corresponding to <a href=https://en.wikipedia.org/wiki/Function_application>function application</a>, and we have extended this with principles for type raising and composition. We provide an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, extending the polarity marking algorithm of van Benthem. We discuss how our <a href=https://en.wikipedia.org/wiki/System>system</a> works in practice, taking input from the <a href=https://en.wikipedia.org/wiki/Compiler-compiler>C&C parser</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2017 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2017/>Halo : Learning Semantics-Aware Representations for Cross-Lingual Information Extraction<span class=acl-fixed-case>H</span>alo: Learning Semantics-Aware Representations for Cross-Lingual Information Extraction</a></strong><br><a href=/people/h/hongyuan-mei/>Hongyuan Mei</a>
|
<a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2017><div class="card-body p-3 small">Cross-lingual information extraction (CLIE) is an important and challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, especially in low resource scenarios. To tackle this challenge, we propose a training method, called Halo, which enforces the local region of each hidden state of a neural model to only generate target tokens with the same semantic structure tag. This simple but powerful technique enables a neural model to learn semantics-aware representations that are robust to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>, without introducing any extra parameter, thus yielding better <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> in both high and low resource settings.<i>Halo</i>, which enforces the local region of each hidden state of a neural model\n to only generate target tokens with the same semantic structure tag. This\n simple but powerful technique enables a neural model to learn\n semantics-aware representations that are robust to noise, without\n introducing any extra parameter, thus yielding better generalization in\n both high and low resource settings.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2019/>Predicting Word Embeddings Variability</a></strong><br><a href=/people/b/benedicte-pierrejean/>BÃ©nÃ©dicte Pierrejean</a>
|
<a href=/people/l/ludovic-tanguy/>Ludovic Tanguy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2019><div class="card-body p-3 small">Neural word embeddings models (such as those built with word2vec) are known to have stability problems : when retraining a model with the exact same hyperparameters, words neighborhoods may change. We propose a method to estimate such variation, based on the overlap of neighbors of a given word in two <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained with identical <a href=https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)>hyperparameters</a>. We show that this inherent variation is not negligible, and that it does not affect every word in the same way. We examine the influence of several features that are intrinsic to a word, corpus or embedding model and provide a methodology that can predict the variability (and as such, reliability) of a word representation in a semantic vector space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2020/>Integrating Multiplicative Features into Supervised Distributional Methods for <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Entailment</a></a></strong><br><a href=/people/t/tu-vu/>Tu Vu</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2020><div class="card-body p-3 small">Supervised distributional methods are applied successfully in lexical entailment, but recent work questioned whether these methods actually learn a relation between two words. Specifically, Levy et al. (2015) claimed that <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifiers</a> learn only separate properties of each word. We suggest a cheap and easy way to boost the performance of these <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> by integrating multiplicative features into commonly used <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. We provide an extensive evaluation with different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> and evaluation setups, and suggest a suitable evaluation setup for the task, eliminating biases existing in previous ones.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2021/>Deep Affix Features Improve Neural Named Entity Recognizers</a></strong><br><a href=/people/v/vikas-yadav/>Vikas Yadav</a>
|
<a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2021><div class="card-body p-3 small">We propose a practical model for named entity recognition (NER) that combines word and character-level information with a specific learned representation of the prefixes and suffixes of the word. We apply this approach to multilingual and multi-domain NER and show that it achieves state of the art results on the CoNLL 2002 Spanish and Dutch and CoNLL 2003 German NER datasets, consistently achieving 1.5-2.3 percent over the state of the art without relying on any dictionary features. Additionally, we show improvement on SemEval 2013 task 9.1 DrugNER, achieving state of the art results on the MedLine dataset and the second best results overall (-1.3 % from state of the art). We also establish a new benchmark on the I2B2 2010 Clinical NER dataset with 84.70 <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2023" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2023/>Hypothesis Only Baselines in Natural Language Inference</a></strong><br><a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/j/jason-naradowsky/>Jason Naradowsky</a>
|
<a href=/people/a/aparajita-haldar/>Aparajita Haldar</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2023><div class="card-body p-3 small">We propose a hypothesis only baseline for diagnosing Natural Language Inference (NLI). Especially when an NLI dataset assumes inference is occurring based purely on the relationship between a context and a hypothesis, it follows that assessing entailment relations while ignoring the provided context is a degenerate solution. Yet, through experiments on 10 distinct NLI datasets, we find that this approach, which we refer to as a hypothesis-only model, is able to significantly outperform a majority-class baseline across a number of NLI datasets. Our analysis suggests that statistical irregularities may allow a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to perform NLI in some datasets beyond what should be achievable without access to the context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2025/>Term Definitions Help Hypernymy Detection</a></strong><br><a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2025><div class="card-body p-3 small">Existing methods of hypernymy detection mainly rely on statistics over a big corpus, either mining some co-occurring patterns like animals such as cats or embedding words of interest into context-aware vectors. These approaches are therefore limited by the availability of a large enough corpus that can cover all terms of interest and provide sufficient contextual information to represent their meaning. In this work, we propose a new paradigm, HyperDef, for hypernymy detection expressing word meaning by encoding word definitions, along with context driven representation. This has two main benefits : (i) Definitional sentences express (sense-specific) corpus-independent meanings of words, hence definition-driven approaches enable strong generalization once trained, the model is expected to work well in open-domain testbeds ; (ii) Global context from a large corpus and definitions provide complementary information for words. Consequently, our model, HyperDef, once trained on task-agnostic data, gets state-of-the-art results in multiple benchmarks</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=S18-2026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/S18-2026/>Agree or Disagree : Predicting Judgments on Nuanced Assertions</a></strong><br><a href=/people/m/michael-wojatzki/>Michael Wojatzki</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a>
|
<a href=/people/s/saif-mohammad/>Saif Mohammad</a>
|
<a href=/people/s/svetlana-kiritchenko/>Svetlana Kiritchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2026><div class="card-body p-3 small">Being able to predict whether people agree or disagree with an assertion (i.e. an explicit, self-contained statement) has several applications ranging from predicting how many people will like or dislike a social media post to classifying posts based on whether they are in accordance with a particular point of view. We formalize this as two NLP tasks : predicting judgments of (i) individuals and (ii) groups based on the text of the assertion and previous judgments. We evaluate a wide range of approaches on a crowdsourced data set containing over 100,000 judgments on over 2,000 assertions. We find that predicting individual judgments is a hard task with our best results only slightly exceeding a majority baseline, but that judgments of groups can be more reliably predicted using a Siamese neural network, which outperforms all other approaches by a wide margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2029/>Measuring Frame Instance Relatedness</a></strong><br><a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/r/roque-lopez-condori/>Roque Lopez Condori</a>
|
<a href=/people/e/elena-cabrio/>Elena Cabrio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2029><div class="card-body p-3 small">Frame semantics is a well-established <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> to represent the <a href=https://en.wikipedia.org/wiki/Semantics>meaning of natural language</a> in <a href=https://en.wikipedia.org/wiki/Computational_semantics>computational terms</a>. In this work, we aim to propose a quantitative measure of relatedness between pairs of frame instances. We test our method on a dataset of sentence pairs, highlighting the correlation between our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> and human judgments of semantic similarity. Furthermore, we propose an application of our <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a> for clustering frame instances to extract prototypical knowledge from <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/S18-2030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-S18-2030 data-toggle=collapse aria-expanded=false aria-controls=abstract-S18-2030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/S18-2030/>Solving Feature Sparseness in Text Classification using Core-Periphery Decomposition</a></strong><br><a href=/people/x/xia-cui/>Xia Cui</a>
|
<a href=/people/s/sadamori-kojaku/>Sadamori Kojaku</a>
|
<a href=/people/n/naoki-masuda/>Naoki Masuda</a>
|
<a href=/people/d/danushka-bollegala/>Danushka Bollegala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-S18-2030><div class="card-body p-3 small">Feature sparseness is a problem common to cross-domain and short-text classification tasks. To overcome this feature sparseness problem, we propose a novel method based on graph decomposition to find candidate <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for expanding feature vectors. Specifically, we first create a feature-relatedness graph, which is subsequently decomposed into core-periphery (CP) pairs and use the peripheries as the expansion candidates of the cores. We expand both training and test instances using the computed related features and use them to train a <a href=https://en.wikipedia.org/wiki/Text_classification>text classifier</a>. We observe that prioritising <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that are common to both training and test instances as cores during the <a href=https://en.wikipedia.org/wiki/CP_decomposition>CP decomposition</a> to further improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of text classification. We evaluate the proposed CP-decomposition-based feature expansion method on benchmark datasets for cross-domain sentiment classification and short-text classification. Our experimental results show that the proposed method consistently outperforms all baselines on short-text classification tasks, and perform competitively with pivot-based cross-domain sentiment classification methods.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright Â©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>