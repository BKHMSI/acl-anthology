<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>SIGDial Conference (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>SIGDial Conference (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021sigdial-1>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</a>
<span class="badge badge-info align-middle ml-1">24&nbsp;papers</span></li></ul></div></div><div id=2021sigdial-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.sigdial-1/>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.0/>Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue</a></strong><br><a href=/people/h/haizhou-li/>Haizhou Li</a>
|
<a href=/people/g/gina-anne-levow/>Gina-Anne Levow</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a>
|
<a href=/people/c/chitralekha-gupta/>Chitralekha Gupta</a>
|
<a href=/people/b/berrak-sisman/>Berrak Sisman</a>
|
<a href=/people/s/siqi-cai/>Siqi Cai</a>
|
<a href=/people/d/david-vandyke/>David Vandyke</a>
|
<a href=/people/n/nina-dethlefs/>Nina Dethlefs</a>
|
<a href=/people/y/yan-wu/>Yan Wu</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=cSNGdDL-MVY" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.4/>Individual Interaction Styles : Evidence from a Spoken Chat Corpus</a></strong><br><a href=/people/n/nigel-ward/>Nigel Ward</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--4><div class="card-body p-3 small">here is increasing interest in modeling style choices in <a href=https://en.wikipedia.org/wiki/Dialogue>dialog</a>, for example for enabling <a href=https://en.wikipedia.org/wiki/Dialogue>dialog systems</a> to adapt to their users. It is commonly assumed that each user has his or her own stable characteristics, but for interaction style the truth of this assumption has not been well examined. I investigated using a vector-space model of interaction styles, derived from the Switchboard corpus of telephone conversations and a broad set of prosodic-behavior features. While most individuals exhibited interaction style tendencies, these were generally far from stable, with a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a> based on individual tendencies outperforming a speaker-independent model by only 3.6 %. The tendencies were somewhat stronger for some speakers, including generally males, and for some dimensions of variation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=JIGvcylPvPI" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.6/>Improving Named Entity Recognition in Spoken Dialog Systems by Context and Speech Pattern Modeling</a></strong><br><a href=/people/m/minh-nguyen/>Minh Nguyen</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--6><div class="card-body p-3 small">While named entity recognition (NER) from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> has been around as long as NER from written text has, the accuracy of NER from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> has generally been much lower than that of NER from <a href=https://en.wikipedia.org/wiki/Written_language>text</a>. The rise in popularity of spoken dialog systems such as <a href=https://en.wikipedia.org/wiki/Siri>Siri</a> or <a href=https://en.wikipedia.org/wiki/Amazon_Alexa>Alexa</a> highlights the need for more accurate <a href=https://en.wikipedia.org/wiki/Natural_language_understanding>NER</a> from speech because <a href=https://en.wikipedia.org/wiki/Natural_language_understanding>NER</a> is a core component for understanding what users said in dialogs. Deployed spoken dialog systems receive user input in the form of automatic speech recognition (ASR) transcripts, and simply applying NER model trained on written text to ASR transcripts often leads to low accuracy because compared to written text, ASR transcripts lack important cues such as <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a> and <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a>. Besides, errors in ASR transcripts also make <a href=https://en.wikipedia.org/wiki/Near-infrared_spectroscopy>NER</a> from <a href=https://en.wikipedia.org/wiki/Speech>speech</a> challenging. We propose two models that exploit dialog context and speech pattern clues to extract named entities more accurately from open-domain dialogs in spoken dialog systems. Our results show the benefit of modeling dialog context and speech patterns in two settings : a standard setting with random partition of data and a more realistic but also more difficult setting where many named entities encountered during deployment are unseen during training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=0hDaafkctwI" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.7/>SoDA : On-device Conversational Slot Extraction<span class=acl-fixed-case>S</span>o<span class=acl-fixed-case>DA</span>: On-device Conversational Slot Extraction</a></strong><br><a href=/people/s/sujith-ravi/>Sujith Ravi</a>
|
<a href=/people/z/zornitsa-kozareva/>Zornitsa Kozareva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--7><div class="card-body p-3 small">We propose a novel on-device neural sequence labeling model which uses embedding-free projections and character information to construct compact word representations to learn a sequence model using a combination of bidirectional LSTM with self-attention and CRF. Unlike typical dialog models that rely on huge, complex neural network architectures and large-scale pre-trained Transformers to achieve state-of-the-art results, our method achieves comparable results to BERT and even outperforms its smaller variant DistilBERT on conversational slot extraction tasks. Our method is faster than BERT models while achieving significant model size reductionour model requires 135x and 81x fewer model parameters than BERT and DistilBERT, respectively. We conduct experiments on multiple conversational datasets and show significant improvements over existing methods including recent on-device models. Experimental results and ablation studies also show that our neural models preserve tiny memory footprint necessary to operate on smart devices, while still maintaining high performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=Y4OAaQzoIhA" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.9/>ARTA : Collection and Classification of Ambiguous Requests and Thoughtful Actions<span class=acl-fixed-case>ARTA</span>: Collection and Classification of Ambiguous Requests and Thoughtful Actions</a></strong><br><a href=/people/s/shohei-tanaka/>Shohei Tanaka</a>
|
<a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--9><div class="card-body p-3 small">Human-assisting systems such as dialogue systems must take thoughtful, appropriate actions not only for clear and unambiguous user requests, but also for ambiguous user requests, even if the users themselves are not aware of their potential requirements. To construct such a dialogue agent, we collected a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and developed a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> that classifies ambiguous user requests into corresponding system actions. In order to collect a high-quality corpus, we asked workers to input antecedent user requests whose pre-defined actions could be regarded as thoughtful. Although multiple actions could be identified as thoughtful for a single <a href=https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol>user request</a>, annotating all combinations of <a href=https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol>user requests</a> and <a href=https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol>system actions</a> is impractical. For this reason, we fully annotated only the test data and left the annotation of the training data incomplete. In order to train the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a> on such training data, we applied the positive / unlabeled (PU) learning method, which assumes that only a part of the data is labeled with positive examples. The experimental results show that the PU learning method achieved better performance than the general positive / negative (PN) learning method to classify thoughtful actions given an ambiguous user request.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=2BkbrFFGTFA" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.14/>Velocidapter : Task-oriented Dialogue Comprehension Modeling Pairing Synthetic Text Generation with Domain Adaptation</a></strong><br><a href=/people/i/ibrahim-taha-aksu/>Ibrahim Taha Aksu</a>
|
<a href=/people/z/zhengyuan-liu/>Zhengyuan Liu</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--14><div class="card-body p-3 small">We introduce a synthetic dialogue generation framework, Velocidapter, which addresses the corpus availability problem for dialogue comprehension. Velocidapter augments datasets by simulating synthetic conversations for a task-oriented dialogue domain, requiring a small amount of bootstrapping work for each new domain. We evaluate the efficacy of our framework on a task-oriented dialogue comprehension dataset, MRCWOZ, which we curate by annotating questions for slots in the restaurant, taxi, and hotel domains of the MultiWOZ 2.2 dataset (Zang et al., 2020). We run experiments within a low-resource setting, where we pretrain a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on SQuAD, fine-tuning it on either a small original data or on the synthetic data generated by our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. Velocidapter shows significant improvements using both the transformer-based BERTBase and BiDAF as base models. We further show that the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> is easy to use by novice users and conclude that Velocidapter can greatly help training over task-oriented dialogues, especially for low-resourced emerging domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=HcurPPqHcrY" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.16/>A Simple yet Effective Method for Sentence Ordering</a></strong><br><a href=/people/a/aili-shen/>Aili Shen</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--16><div class="card-body p-3 small">Sentence ordering is the task of arranging a given bag of sentences so as to maximise the coherence of the overall text. In this work, we propose a simple yet effective <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training method</a> that improves the capacity of models to capture overall text coherence based on training over pairs of sentences / segments. Experimental results show the superiority of our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in in- and cross-domain settings. The utility of our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> is also verified over a multi-document summarisation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=04Urc5LRBlk" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.18/>Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair Coherence Scoring</a></strong><br><a href=/people/l/linzi-xing/>Linzi Xing</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--18><div class="card-body p-3 small">Dialogue topic segmentation is critical in several dialogue modeling problems. However, popular <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approaches</a> only exploit surface features in assessing topical coherence among utterances. In this work, we address this limitation by leveraging supervisory signals from the utterance-pair coherence scoring task. First, we present a simple yet effective strategy to generate a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training corpus</a> for utterance-pair coherence scoring. Then, we train a BERT-based neural utterance-pair coherence model with the obtained <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training corpus</a>. Finally, such <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is used to measure the topical relevance between utterances, acting as the basis of the segmentation inference. Experiments on three public datasets in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> demonstrate that our proposal outperforms the state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=Wtma3lm9AMc" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.21/>Contrastive Response Pairs for Automatic Evaluation of Non-task-oriented Neural Conversational Models</a></strong><br><a href=/people/k/koshiro-okano/>Koshiro Okano</a>
|
<a href=/people/y/yu-suzuki/>Yu Suzuki</a>
|
<a href=/people/m/masaya-kawamura/>Masaya Kawamura</a>
|
<a href=/people/t/tsuneo-kato/>Tsuneo Kato</a>
|
<a href=/people/a/akihiro-tamura/>Akihiro Tamura</a>
|
<a href=/people/j/jianming-wu/>Jianming Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--21><div class="card-body p-3 small">Responses generated by neural conversational models (NCMs) for non-task-oriented systems are difficult to evaluate. We propose contrastive response pairs (CRPs) for automatically evaluating responses from non-task-oriented NCMs. We conducted an error analysis on responses generated by an encoder-decoder recurrent neural network (RNN) type NCM and created three types of CRPs corresponding to the three most frequent errors found in the analysis. Three NCMs of different response quality were objectively evaluated with the CRPs and compared to a subjective assessment. The correctness obtained by the three types of CRPs were consistent with the results of the subjective assessment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=zQuaI9czmJk" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.25/>Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems : A Survey</a></strong><br><a href=/people/v/vevake-balaraman/>Vevake Balaraman</a>
|
<a href=/people/s/seyedmostafa-sheikhalishahi/>Seyedmostafa Sheikhalishahi</a>
|
<a href=/people/b/bernardo-magnini/>Bernardo Magnini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--25><div class="card-body p-3 small">This paper aims at providing a comprehensive overview of recent developments in dialogue state tracking (DST) for task-oriented conversational systems. We introduce the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, the main <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that have been exploited as well as their evaluation metrics, and we analyze several proposed approaches. We distinguish between static ontology DST models, which predict a fixed set of dialogue states, and dynamic ontology models, which can predict dialogue states even when the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> changes. We also discuss the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s ability to track either single or multiple domains and to scale to new domains, both in terms of <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> and zero-shot learning. We cover a period from 2013 to 2020, showing a significant increase of multiple domain methods, most of them utilizing pre-trained language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=yNtYLKCo3xI" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.26/>Scikit-talk : A toolkit for processing real-world conversational speech data<span class=acl-fixed-case>S</span>cikit-talk: A toolkit for processing real-world conversational speech data</a></strong><br><a href=/people/a/andreas-liesenfeld/>Andreas Liesenfeld</a>
|
<a href=/people/g/gabor-parti/>Gabor Parti</a>
|
<a href=/people/c/chu-ren-huang/>Chu-Ren Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--26><div class="card-body p-3 small">We present Scikit-talk, an open-source toolkit for processing collections of real-world conversational speech in <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a>. First of its kind, the <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> equips those interested in studying or modeling conversations with an easy-to-use interface to build and explore large collections of transcriptions and annotations of talk-in-interaction. Designed for applications in <a href=https://en.wikipedia.org/wiki/Speech_processing>speech processing</a> and Conversational AI, Scikit-talk provides tools to custom-build datasets for tasks such as intent prototyping, dialog flow testing, and conversation design. Its preprocessor module comes with several pre-built interfaces for common transcription formats, which aim to make working across multiple data sources more accessible. The explorer module provides a collection of tools to explore and analyse this data type via <a href=https://en.wikipedia.org/wiki/String_matching>string matching</a> and unsupervised machine learning techniques. Scikit-talk serves as a platform to collect and connect different transcription formats and representations of talk, enabling the user to quickly build multilingual datasets of varying detail and granularity. Thus, the toolkit aims to make working with authentic conversational speech data in <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> more accessible and to provide the user with comprehensive options to work with representations of talk in appropriate detail for any downstream task. For the latest updates and information on currently supported languages and language resources, please refer to : https://pypi.org/project/scikit-talk/<i>preprocessor</i> module comes with several pre-built interfaces for common transcription formats, which aim to make working across multiple data sources more accessible. The <i>explorer</i> module provides a collection of tools to explore and analyse this data type via string matching and unsupervised machine learning techniques. Scikit-talk serves as a platform to collect and connect different transcription formats and representations of talk, enabling the user to quickly build multilingual datasets of varying detail and granularity. Thus, the toolkit aims to make working with authentic conversational speech data in Python more accessible and to provide the user with comprehensive options to work with representations of talk in appropriate detail for any downstream task. For the latest updates and information on currently supported languages and language resources, please refer to: https://pypi.org/project/scikit-talk/</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=0FxAJvs93WA" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.31/>Summarizing Behavioral Change Goals from SMS Exchanges to Support Health Coaches<span class=acl-fixed-case>SMS</span> Exchanges to Support Health Coaches</a></strong><br><a href=/people/i/itika-gupta/>Itika Gupta</a>
|
<a href=/people/b/barbara-di-eugenio/>Barbara Di Eugenio</a>
|
<a href=/people/b/brian-d-ziebart/>Brian D. Ziebart</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/b/ben-s-gerber/>Ben S. Gerber</a>
|
<a href=/people/l/lisa-k-sharp/>Lisa K. Sharp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--31><div class="card-body p-3 small">Regular physical activity is associated with a reduced risk of chronic diseases such as type 2 diabetes and improved <a href=https://en.wikipedia.org/wiki/Mental_health>mental well-being</a>. Yet, more than half of the US population is insufficiently active. Health coaching has been successful in promoting healthy behaviors. In this paper, we present our work towards assisting <a href=https://en.wikipedia.org/wiki/Coaching>health coaches</a> by extracting the physical activity goal the user and coach negotiate via <a href=https://en.wikipedia.org/wiki/Text_messaging>text messages</a>. We show that information captured by dialogue acts can help to improve the goal extraction results. We employ both traditional and transformer-based machine learning models for dialogue acts prediction and find them statistically indistinguishable in performance on our health coaching dataset. Moreover, we discuss the feedback provided by the health coaches when evaluating the correctness of the extracted goal summaries. This work is a step towards building a virtual assistant health coach to promote a healthy lifestyle.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=vSNq0OOGRc0" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.33" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.33/>CIDER : Commonsense Inference for Dialogue Explanation and Reasoning<span class=acl-fixed-case>CIDER</span>: Commonsense Inference for Dialogue Explanation and Reasoning</a></strong><br><a href=/people/d/deepanway-ghosal/>Deepanway Ghosal</a>
|
<a href=/people/p/pengfei-hong/>Pengfei Hong</a>
|
<a href=/people/s/siqi-shen/>Siqi Shen</a>
|
<a href=/people/n/navonil-majumder/>Navonil Majumder</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--33><div class="card-body p-3 small">Commonsense inference to understand and explain <a href=https://en.wikipedia.org/wiki/Human_language>human language</a> is a fundamental research problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Explaining human conversations poses a great challenge as it requires contextual understanding, planning, <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, and several aspects of <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a> including causal, temporal, and commonsense reasoning. In this work, we introduce CIDER a manually curated dataset that contains dyadic dialogue explanations in the form of implicit and explicit knowledge triplets inferred using contextual commonsense inference. Extracting such rich explanations from <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> can be conducive to improving several downstream <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>. The annotated triplets are categorized by the type of <a href=https://en.wikipedia.org/wiki/Common_knowledge_(logic)>commonsense knowledge</a> present (e.g., causal, conditional, temporal). We set up three different tasks conditioned on the annotated dataset : Dialogue-level Natural Language Inference, Span Extraction, and Multi-choice Span Selection. Baseline results obtained with transformer-based models reveal that the tasks are difficult, paving the way for promising future research. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and the baseline implementations are publicly available at https://github.com/declare-lab/CIDER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=9IAwjDa0Wp0" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.37/>How Should Agents Ask Questions For <a href=https://en.wikipedia.org/wiki/Situated_learning>Situated Learning</a>? An Annotated Dialogue Corpus</a></strong><br><a href=/people/f/felix-gervits/>Felix Gervits</a>
|
<a href=/people/a/antonio-roque/>Antonio Roque</a>
|
<a href=/people/g/gordon-briggs/>Gordon Briggs</a>
|
<a href=/people/m/matthias-scheutz/>Matthias Scheutz</a>
|
<a href=/people/m/matthew-marge/>Matthew Marge</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--37><div class="card-body p-3 small">Intelligent agents that are confronted with novel concepts in situated environments will need to ask their human teammates questions to learn about the <a href=https://en.wikipedia.org/wiki/Universe>physical world</a>. To better understand this problem, we need data about asking questions in situated task-based interactions. To this end, we present the Human-Robot Dialogue Learning (HuRDL) Corpus-a novel dialogue corpus collected in an online interactive virtual environment in which human participants play the role of a robot performing a collaborative tool-organization task. We describe the <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus data</a> and a corresponding <a href=https://en.wikipedia.org/wiki/Annotation>annotation scheme</a> to offer insight into the form and content of questions that humans ask to facilitate learning in a situated environment. We provide the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> as an empirically-grounded resource for improving question generation in situated intelligent agents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=oBT795ipFFM" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.40/>What to Fact-Check : Guiding Check-Worthy Information Detection in News Articles through Argumentative Discourse Structure</a></strong><br><a href=/people/t/tariq-alhindi/>Tariq Alhindi</a>
|
<a href=/people/b/brennan-mcmanus/>Brennan McManus</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--40><div class="card-body p-3 small">Most existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for automatic fact-checking start with a precompiled list of claims to verify. We investigate the understudied problem of determining what statements in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> are worthy to fact-check. We annotate the <a href=https://en.wikipedia.org/wiki/Argument_structure>argument structure</a> of 95 <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> in the <a href=https://en.wikipedia.org/wiki/Global_warming>climate change domain</a> that are fact-checked by climate scientists at climatefeedback.org. We release the first multi-layer annotated corpus for both argumentative discourse structure (argument types and relations) and for fact-checked statements in news articles. We discuss the connection between <a href=https://en.wikipedia.org/wiki/Argument_structure>argument structure</a> and check-worthy statements and develop several baseline models for detecting check-worthy statements in the <a href=https://en.wikipedia.org/wiki/Climate_change_modeling>climate change domain</a>. Our preliminary results show that using information about argumentative discourse structure shows slight but statistically significant improvement over a baseline of local discourse structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=bYXcZg_VWiE" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.41/>How open are the conversations with open-domain chatbots? A proposal for Speech Event based evaluation</a></strong><br><a href=/people/a/a-seza-dogruoz/>A. Seza Doğruöz</a>
|
<a href=/people/g/gabriel-skantze/>Gabriel Skantze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--41><div class="card-body p-3 small">Open-domain chatbots are supposed to converse freely with humans without being restricted to a topic, task or domain. However, the boundaries and/or contents of open-domain conversations are not clear. To clarify the boundaries of openness, we conduct two studies : First, we classify the types of <a href=https://en.wikipedia.org/wiki/Speech>speech events</a> encountered in a chatbot evaluation data set (i.e., Meena by Google) and find that these conversations mainly cover the small talk category and exclude the other speech event categories encountered in real life human-human communication. Second, we conduct a small-scale pilot study to generate <a href=https://en.wikipedia.org/wiki/Online_chat>online conversations</a> covering a wider range of <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech event categories</a> between two humans vs. a human and a state-of-the-art <a href=https://en.wikipedia.org/wiki/Chatbot>chatbot</a> (i.e., Blender by Facebook). A human evaluation of these generated <a href=https://en.wikipedia.org/wiki/Conversation>conversations</a> indicates a preference for human-human conversations, since the human-chatbot conversations lack coherence in most speech event categories. Based on these results, we suggest (a) using the term <a href=https://en.wikipedia.org/wiki/Small_talk>small talk</a> instead of <a href=https://en.wikipedia.org/wiki/Open_domain>open-domain</a> for the current <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> which are not that open in terms of conversational abilities yet, and (b) revising the evaluation methods to test the chatbot conversations against other speech events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=_tJhKdtu8EM" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.44/>DTAFA : Decoupled Training Architecture for Efficient FAQ Retrieval<span class=acl-fixed-case>DTAFA</span>: Decoupled Training Architecture for Efficient <span class=acl-fixed-case>FAQ</span> Retrieval</a></strong><br><a href=/people/h/haytham-assem/>Haytham Assem</a>
|
<a href=/people/s/sourav-dutta/>Sourav Dutta</a>
|
<a href=/people/e/edward-burgin/>Edward Burgin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--44><div class="card-body p-3 small">Automated Frequently Asked Question (FAQ) retrieval provides an effective procedure to provide prompt responses to natural language based queries, providing an efficient platform for large-scale service-providing companies for presenting readily available information pertaining to customers&#8217; questions. We propose DTAFA, a novel multi-lingual FAQ retrieval system that aims at improving the top-1 retrieval accuracy with the least number of parameters. We propose two decoupled deep learning architectures trained for (i) candidate generation via text classification for a user question, and (ii) learning fine-grained semantic similarity between user questions and the FAQ repository for candidate refinement. We validate our <a href=https://en.wikipedia.org/wiki/System>system</a> using real-life enterprise data as well as <a href=https://en.wikipedia.org/wiki/Open-source_data>open source dataset</a>. Empirically we show that DTAFA achieves better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> compared to existing <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> while requiring nearly 30 lesser number of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training parameters</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=jfB1gE1wP6Y" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.45/>Projection of Turn Completion in Incremental Spoken Dialogue Systems</a></strong><br><a href=/people/e/erik-ekstedt/>Erik Ekstedt</a>
|
<a href=/people/g/gabriel-skantze/>Gabriel Skantze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--45><div class="card-body p-3 small">The ability to take turns in a fluent way (i.e., without long response delays or frequent interruptions) is a fundamental aspect of any spoken dialog system. However, practical speech recognition services typically induce a long response delay, as it takes time before the processing of the user&#8217;s utterance is complete. There is a considerable amount of research indicating that humans achieve fast response times by projecting what the interlocutor will say and estimating upcoming turn completions. In this work, we implement this mechanism in an incremental spoken dialog system, by using a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> that generates possible futures to project upcoming completion points. In theory, this could make the <a href=https://en.wikipedia.org/wiki/System>system</a> more responsive, while still having access to <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> not yet processed by the <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognizer</a>. We conduct a small study which indicates that this is a viable approach for practical dialog systems, and that this is a promising direction for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=AwHuUPEpJFA" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.50" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.50/>Do Encoder Representations of Generative Dialogue Models have sufficient summary of the Information about the task?</a></strong><br><a href=/people/p/prasanna-parthasarathi/>Prasanna Parthasarathi</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a>
|
<a href=/people/s/sarath-chandar/>Sarath Chandar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--50><div class="card-body p-3 small">Predicting the next utterance in dialogue is contingent on encoding of users&#8217; input text to generate appropriate and relevant response in data-driven approaches. Although the semantic and syntactic quality of the language generated is evaluated, more often than not, the encoded representation of input is not evaluated. As the representation of the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> is essential for predicting the appropriate response, evaluation of <a href=https://en.wikipedia.org/wiki/Encoder>encoder representation</a> is a challenging yet important problem. In this work, we showcase evaluating the text generated through human or automatic metrics is not sufficient to appropriately evaluate soundness of the language understanding of dialogue models and, to that end, propose a set of probe tasks to evaluate encoder representation of different <a href=https://en.wikipedia.org/wiki/Encoder>language encoders</a> commonly used in dialogue models. From experiments, we observe that some of the probe tasks are easier and some are harder for even sophisticated model architectures to learn. And, through experiments we observe that RNN based architectures have lower performance on automatic metrics on text generation than transformer model but perform better than the transformer model on the probe tasks indicating that RNNs might preserve task information better than the Transformers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=usZQulwdOZs" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.52" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.52/>Schema-Guided Paradigm for Zero-Shot Dialog</a></strong><br><a href=/people/s/shikib-mehri/>Shikib Mehri</a>
|
<a href=/people/m/maxine-eskenazi/>Maxine Eskenazi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--52><div class="card-body p-3 small">Developing mechanisms that flexibly adapt dialog systems to unseen tasks and domains is a major challenge in dialog research. Neural models implicitly memorize task-specific dialog policies from the training data. We posit that this implicit memorization has precluded zero-shot transfer learning. To this end, we leverage the schema-guided paradigm, wherein the task-specific dialog policy is explicitly provided to the model. We introduce the Schema Attention Model (SAM) and improved schema representations for the STAR corpus. SAM obtains significant improvement in zero-shot settings, with a +22 F1 score improvement over prior work. These results validate the feasibility of zero-shot generalizability in <a href=https://en.wikipedia.org/wiki/Dialogue>dialog</a>. Ablation experiments are also presented to demonstrate the efficacy of <a href=https://en.wikipedia.org/wiki/Samarium_selenide>SAM</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=XNiUdhaW6LI" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.53" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.53/>Coreference-Aware Dialogue Summarization</a></strong><br><a href=/people/z/zhengyuan-liu/>Zhengyuan Liu</a>
|
<a href=/people/k/ke-shi/>Ke Shi</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--53><div class="card-body p-3 small">Summarizing conversations via neural approaches has been gaining research traction lately, yet it is still challenging to obtain practical solutions. Examples of such challenges include unstructured information exchange in dialogues, informal interactions between speakers, and dynamic role changes of speakers as the dialogue evolves. Many of such challenges result in complex coreference links. Therefore, in this work, we investigate different approaches to explicitly incorporate coreference information in neural abstractive dialogue summarization models to tackle the aforementioned challenges. Experimental results show that the proposed approaches achieve state-of-the-art performance, implying it is useful to utilize coreference information in dialogue summarization. Evaluation results on factual correctness suggest such coreference-aware models are better at tracing the information flow among interlocutors and associating accurate status / actions with the corresponding <a href=https://en.wikipedia.org/wiki/Interlocutor_(linguistics)>interlocutors</a> and person mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=0xiQe0OPwBA" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.54/>Weakly Supervised Extractive Summarization with Attention</a></strong><br><a href=/people/y/yingying-zhuang/>Yingying Zhuang</a>
|
<a href=/people/y/yichao-lu/>Yichao Lu</a>
|
<a href=/people/s/simi-wang/>Simi Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--54><div class="card-body p-3 small">Automatic summarization aims to extract important information from large amounts of <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual data</a> in order to create a shorter version of the original texts while preserving its information. Training traditional extractive summarization models relies heavily on human-engineered labels such as sentence-level annotations of summary-worthiness. However, in many use cases, such human-engineered labels do not exist and manually annotating thousands of documents for the purpose of training models may not be feasible. On the other hand, indirect signals for summarization are often available, such as agent actions for customer service dialogues, headlines for news articles, <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnosis</a> for <a href=https://en.wikipedia.org/wiki/Electronic_health_record>Electronic Health Records</a>, etc. In this paper, we develop a general framework that generates extractive summarization as a byproduct of <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning tasks</a> for indirect signals via the help of attention mechanism. We test our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on customer service dialogues and experimental results demonstrated that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can reliably select informative sentences and words for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>automatic summarization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=CnHqotO89jQ" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.55/>Incremental temporal summarization in multi-party meetings</a></strong><br><a href=/people/r/ramesh-manuvinakurike/>Ramesh Manuvinakurike</a>
|
<a href=/people/s/saurav-sahay/>Saurav Sahay</a>
|
<a href=/people/w/wenda-chen/>Wenda Chen</a>
|
<a href=/people/l/lama-nachman/>Lama Nachman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--55><div class="card-body p-3 small">In this work, we develop a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for incremental temporal summarization in a multiparty dialogue. We use crowd-sourcing paradigm with a model-in-loop approach for collecting the summaries and compare the data with the expert summaries. We leverage the question generation paradigm to automatically generate questions from the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>, which can be used to validate the <a href=https://en.wikipedia.org/wiki/Participation_(decision_making)>user participation</a> and potentially also draw attention of the user towards the contents then need to summarize. We then develop several <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> for abstractive summary generation in the Incremental temporal scenario. We perform a detailed analysis of the results and show that including the past context into the summary generation yields better summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigdial-1.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigdial-1--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigdial-1.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href="https://www.youtube.com/watch?v=FLsqwyGx4zM" data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigdial-1.58" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.sigdial-1.58/>Large-Scale Quantitative Evaluation of Dialogue Agents’ Response Strategies against Offensive Users</a></strong><br><a href=/people/h/haojun-li/>Haojun Li</a>
|
<a href=/people/d/dilara-soylu/>Dilara Soylu</a>
|
<a href=/people/c/christopher-d-manning/>Christopher Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigdial-1--58><div class="card-body p-3 small">As voice assistants and dialogue agents grow in popularity, so does the abuse they receive. We conducted a large-scale quantitative evaluation of the effectiveness of 4 response types (avoidance, why, empathetic, and counter), and 2 additional factors (using a redirect or a voluntarily provided name) that have not been tested by prior work. We measured their direct effectiveness on real users in-the-wild by the re-offense ratio, length of conversation after the initial response, and number of turns until the next re-offense. Our experiments confirm prior lab studies in showing that <a href=https://en.wikipedia.org/wiki/Empathy>empathetic responses</a> perform better than generic avoidance responses as well as counter responses. We show that dialogue agents should almost always guide offensive users to a new topic through the use of redirects and use the user&#8217;s name if provided. As compared to a baseline avoidance strategy employed by commercial agents, our best <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> is able to reduce the re-offense ratio from 92 % to 43 %.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>