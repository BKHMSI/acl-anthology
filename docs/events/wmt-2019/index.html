<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Statistical Machine Translation (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Statistical Machine Translation (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w19-52>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w19-53>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a>
<span class="badge badge-info align-middle ml-1">37&nbsp;papers</span></li><li><a class=align-middle href=#w19-54>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a>
<span class="badge badge-info align-middle ml-1">25&nbsp;papers</span></li></ul></div></div><div id=w19-52><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-52.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-52/>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5200/>Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5203/>Incorporating <a href=https://en.wikipedia.org/wiki/Source_code>Source Syntax</a> into Transformer-Based Neural Machine Translation</a></strong><br><a href=/people/a/anna-currey/>Anna Currey</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5203><div class="card-body p-3 small">Transformer-based neural machine translation (NMT) has recently achieved state-of-the-art performance on many machine translation tasks. However, recent work (Raganato and Tiedemann, 2018 ; Tang et al., 2018 ; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neural network-based counterparts, particularly in low-resource cases. In this paper, we incorporate constituency parse information into a Transformer NMT model. We leverage linearized parses of the source training sentences in order to inject <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> into the Transformer architecture without modifying it. We introduce two methods : a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences. We evaluate our methods on low-resource translation from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into twenty target languages, showing consistent improvements of 1.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on average across diverse target languages for the multi-task technique. We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low- and medium-resource NMT but degenerates high-resource English-German translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5205/>Generalizing Back-Translation in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/miguel-graca/>Miguel Graça</a>
|
<a href=/people/y/yunsu-kim/>Yunsu Kim</a>
|
<a href=/people/j/julian-schamper/>Julian Schamper</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5205><div class="card-body p-3 small">Back-translation data augmentation by translating target monolingual data is a crucial component in modern neural machine translation (NMT). In this work, we reformulate back-translation in the scope of cross-entropy optimization of an NMT model, clarifying its underlying mathematical assumptions and approximations beyond its heuristic usage. Our formulation covers broader synthetic data generation schemes, including sampling from a target-to-source NMT model. With this formulation, we point out fundamental problems of the <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling-based approaches</a> and propose to remedy them by (i) disabling label smoothing for the target-to-source model and (ii) sampling from a restricted search space. Our statements are investigated on the WMT 2018 German-English news translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5208.Supplementary.zip data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5208.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5208" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5208/>The Effect of Translationese in Machine Translation Test Sets</a></strong><br><a href=/people/m/mike-zhang/>Mike Zhang</a>
|
<a href=/people/a/antonio-toral/>Antonio Toral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5208><div class="card-body p-3 small">The effect of translationese has been studied in the field of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>, mostly with respect to training data. We study in depth the effect of translationese on <a href=https://en.wikipedia.org/wiki/Test_data>test data</a>, using the test sets from the last three editions of WMT&#8217;s news shared task, containing 17 translation directions. We show evidence that (i) the use of translationese in test sets results in inflated human evaluation scores for MT systems ; (ii) in some cases system rankings do change and (iii) the impact translationese has on a translation direction is inversely correlated to the translation quality attainable by state-of-the-art MT systems for that direction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5209/>Customizing Neural Machine Translation for <a href=https://en.wikipedia.org/wiki/Subtitling>Subtitling</a></a></strong><br><a href=/people/e/evgeny-matusov/>Evgeny Matusov</a>
|
<a href=/people/p/patrick-wilken/>Patrick Wilken</a>
|
<a href=/people/y/yota-georgakopoulou/>Yota Georgakopoulou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5209><div class="card-body p-3 small">In this work, we customized a neural machine translation system for translation of subtitles in the domain of <a href=https://en.wikipedia.org/wiki/Entertainment>entertainment</a>. The neural translation model was adapted to the subtitling content and style and extended by a simple, yet effective technique for utilizing inter-sentence context for short sentences such as dialog turns. The main contribution of the paper is a novel subtitle segmentation algorithm that predicts the end of a subtitle line given the previous word-level context using a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> learned from human segmentation decisions. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is combined with subtitle length and duration constraints established in the subtitling industry. We conducted a thorough human evaluation with two post-editors (English-to-Spanish translation of a <a href=https://en.wikipedia.org/wiki/Documentary_film>documentary</a> and a sitcom). It showed a notable productivity increase of up to 37 % as compared to translating from scratch and significant reductions in human translation edit rate in comparison with the post-editing of the baseline non-adapted system without a learned segmentation model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5210/>Integration of Dubbing Constraints into <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/a/ashutosh-saboo/>Ashutosh Saboo</a>
|
<a href=/people/t/timo-baumann/>Timo Baumann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5210><div class="card-body p-3 small">Translation systems aim to perform a meaning-preserving conversion of linguistic material (typically text but also speech) from a source to a target language (and, to a lesser degree, the corresponding socio-cultural contexts). Dubbing, i.e., the lip-synchronous translation and revoicing of speech adds to this constraints about the close matching of phonetic and resulting visemic synchrony characteristics of source and target material. There is an inherent conflict between a <a href=https://en.wikipedia.org/wiki/Translation>translation</a>&#8217;s meaning preservation and &#8216;dubbability&#8217; and the resulting trade-off can be controlled by weighing the synchrony constraints. We introduce our work, which to the best of our knowledge is the first of its kind, on integrating synchrony constraints into the machine translation paradigm. We present first results for the integration of synchrony constraints into encoder decoder-based neural machine translation and show that considerably more &#8216;dubbable&#8217; translations can be achieved with only a small impact on <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a>, and dubbability improves more steeply than <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> degrades.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5211" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5211/>Widening the Representation Bottleneck in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Lexical Shortcuts</a></strong><br><a href=/people/d/denis-emelin/>Denis Emelin</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5211><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a> is a state-of-the-art neural translation model that uses <a href=https://en.wikipedia.org/wiki/Attention>attention</a> to iteratively refine lexical representations with information drawn from the surrounding context. Lexical features are fed into the first layer and propagated through a deep network of hidden layers. We argue that the need to represent and propagate lexical features in each layer limits the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s capacity for learning and representing other information relevant to the task. To alleviate this bottleneck, we introduce gated shortcut connections between the embedding layer and each subsequent layer within the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>. This enables the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to access relevant lexical content dynamically, without expending limited resources on storing it within intermediate states. We show that the proposed modification yields consistent improvements over a baseline transformer on standard WMT translation tasks in 5 translation directions (0.9 BLEU on average) and reduces the amount of <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical information</a> passed along the hidden layers. We furthermore evaluate different ways to integrate lexical connections into the transformer architecture and present ablation experiments exploring the effect of proposed shortcuts on model behavior.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5212.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5212/>A High-Quality Multilingual Dataset for Structured Documentation Translation</a></strong><br><a href=/people/k/kazuma-hashimoto/>Kazuma Hashimoto</a>
|
<a href=/people/r/raffaella-buschiazzo/>Raffaella Buschiazzo</a>
|
<a href=/people/j/james-bradbury/>James Bradbury</a>
|
<a href=/people/t/teresa-marshall/>Teresa Marshall</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5212><div class="card-body p-3 small">This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text. Unlike widely-used datasets for translation of plain text, we collect XML-structured parallel text segments from the online documentation for an enterprise software platform. These <a href=https://en.wikipedia.org/wiki/Web_page>Web pages</a> have been professionally translated from English into 16 languages and maintained by domain experts, and around 100,000 text segments are available for each language pair. We build and evaluate translation models for seven target languages from <a href=https://en.wikipedia.org/wiki/English_language>English</a>, with several different copy mechanisms and an XML-constrained beam search. We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable 17 16 translation settings. Our experiments show that learning to translate with the <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>XML tags</a> improves translation accuracy, and the <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> accurately generates <a href=https://en.wikipedia.org/wiki/XML>XML structures</a>. We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>. We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications, including suitability for <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>.</div></div></div><hr><div id=w19-53><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-53.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-53/>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5300/>Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5302.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5302/>Results of the WMT19 Metrics Shared Task : Segment-Level and Strong MT Systems Pose Big Challenges<span class=acl-fixed-case>WMT</span>19 Metrics Shared Task: Segment-Level and Strong <span class=acl-fixed-case>MT</span> Systems Pose Big Challenges</a></strong><br><a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5302><div class="card-body p-3 small">This paper presents the results of the WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>. 13 research groups submitted 24 metrics, 10 of which are reference-less metrics and constitute submissions to the joint task with WMT19 Quality Estimation Task, QE as a Metric. In addition, we computed 11 baseline metrics, with 8 commonly applied baselines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reimplementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> correlates with the WMT19 official manual ranking, and segment level, how well the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5303/>Findings of the First Shared Task on Machine Translation Robustness</a></strong><br><a href=/people/x/xian-li/>Xian Li</a>
|
<a href=/people/p/paul-michel/>Paul Michel</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5303><div class="card-body p-3 small">We share the findings of the first shared task on improving <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of Machine Translation (MT). The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve <a href=https://en.wikipedia.org/wiki/Mathematical_model>models&#8217; robustness</a> to noisy input and domain mismatch. We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations. As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc. All submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved large improvements over <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, with the best improvement having +22.33 <a href=https://en.wikipedia.org/wiki/British_thermal_unit>BLEU</a>. We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson&#8217;s r = 0.94 and 0.95). Furthermore, we conducted a qualitative analysis of the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a> using compare-mt, which revealed their salient differences in handling challenges in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Such analysis provides additional insights when there is occasional disagreement between <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a> and <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, e.g. systems better at producing <a href=https://en.wikipedia.org/wiki/Colloquialism>colloquial expressions</a> received higher score from <a href=https://en.wikipedia.org/wiki/Judgement>human judgment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5304/>The University of Edinburgh’s Submissions to the WMT19 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s Submissions to the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/r/rachel-bawden/>Rachel Bawden</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/u/ulrich-germann/>Ulrich Germann</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/f/faheem-kirefu/>Faheem Kirefu</a>
|
<a href=/people/a/antonio-valerio-miceli-barone/>Antonio Valerio Miceli Barone</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5304><div class="card-body p-3 small">The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions : <a href=https://en.wikipedia.org/wiki/English_language>EnglishGujarati</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>EnglishChinese</a>, <a href=https://en.wikipedia.org/wiki/German_language>GermanEnglish</a>, and <a href=https://en.wikipedia.org/wiki/Czech_language>EnglishCzech</a>. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For EnglishGujarati, we also explored <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised MT</a> with cross-lingual language model pre-training, and translation pivoting through <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. For <a href=https://en.wikipedia.org/wiki/Translation>translation</a> to and from <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, we investigated character-based tokenisation vs. sub-word segmentation of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese text</a>. For GermanEnglish, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. For EnglishCzech, we compared different preprocessing and tokenisation regimes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5305/>GTCOM Neural Machine Translation Systems for WMT19<span class=acl-fixed-case>GTCOM</span> Neural Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/c/chao-bei/>Chao Bei</a>
|
<a href=/people/h/hao-zong/>Hao Zong</a>
|
<a href=/people/c/conghu-yuan/>Conghu Yuan</a>
|
<a href=/people/q/qingming-liu/>Qingming Liu</a>
|
<a href=/people/b/baoyong-fan/>Baoyong Fan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5305><div class="card-body p-3 small">This paper describes the Global Tone Communication Co., Ltd.&#8217;s submission of the WMT19 shared news translation task. We participate in six directions : English to (Gujarati, Lithuanian and Finnish) and (Gujarati, Lithuanian and Finnish) to English. Further, we get the best BLEU scores in the directions of English to <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a> and Lithuanian to English (28.2 and 36.3 respectively) among all the participants. The submitted systems mainly focus on <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, knowledge distillation and <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a> to build a competitive model for this task. Also, we apply <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> to filter <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a>, back-translated data and parallel data. The techniques we apply for data filtering include filtering by <a href=https://en.wikipedia.org/wiki/Rule-based_system>rules</a>, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>. Besides, We conduct several experiments to validate different knowledge distillation techniques and right-to-left (R2L) reranking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5309/>DBMS-KU Interpolation for WMT19 News Translation Task<span class=acl-fixed-case>DBMS</span>-<span class=acl-fixed-case>KU</span> Interpolation for <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/s/sari-dewi-budiwati/>Sari Dewi Budiwati</a>
|
<a href=/people/a/al-hafiz-akbar-maulana-siagian/>Al Hafiz Akbar Maulana Siagian</a>
|
<a href=/people/t/tirana-noor-fatyanosa/>Tirana Noor Fatyanosa</a>
|
<a href=/people/m/masayoshi-aritsugi/>Masayoshi Aritsugi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5309><div class="card-body p-3 small">This paper presents the participation of DBMS-KU Interpolation system in WMT19 shared task, namely, Kazakh-English language pair. We examine the use of <a href=https://en.wikipedia.org/wiki/Interpolation>interpolation method</a> using a different language model order. Our Interpolation system combines a direct translation with <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> as a pivot language. We use 3-gram and 5-gram language model orders to perform the language translation in this work. To reduce noise in the pivot translation process, we prune the phrase table of source-pivot and pivot-target. Our experimental results show that our Interpolation system outperforms the <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>Baseline</a> in terms of <a href=https://en.wikipedia.org/wiki/Baseline_(surveying)>BLEU-cased score</a> by +0.5 and +0.1 points in Kazakh-English and English-Kazakh, respectively. In particular, using the 5-gram language model order in our system could obtain better BLEU-cased score than utilizing the 3-gram one. Interestingly, we found that by employing the Interpolation system could reduce the perplexity score of English-Kazakh when using 3-gram language model order.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5310/>Lingua Custodia at WMT’19 : Attempts to Control Terminology<span class=acl-fixed-case>WMT</span>’19: Attempts to Control Terminology</a></strong><br><a href=/people/f/franck-burlot/>Franck Burlot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5310><div class="card-body p-3 small">This paper describes Lingua Custodia&#8217;s submission to the WMT&#8217;19 news shared task for German-to-French on the topic of the EU elections. We report experiments on the adaptation of the terminology of a <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation system</a> to a specific topic, aimed at providing more accurate translations of specific entities like <a href=https://en.wikipedia.org/wiki/Political_party>political parties</a> and <a href=https://en.wikipedia.org/wiki/Personal_name>person names</a>, given that the shared task provided no in-domain training parallel data dealing with the restricted topic. Our primary submission to the shared task uses backtranslation generated with a type of decoding allowing the insertion of constraints in the output in order to guarantee the correct translation of specific terms that are not necessarily observed in the data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5311.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5311/>The TALP-UPC Machine Translation Systems for WMT19 News Translation Task : Pivoting Techniques for Low Resource MT<span class=acl-fixed-case>TALP</span>-<span class=acl-fixed-case>UPC</span> Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19 News Translation Task: Pivoting Techniques for Low Resource <span class=acl-fixed-case>MT</span></a></strong><br><a href=/people/n/noe-casas/>Noe Casas</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a>
|
<a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/c/christine-basta/>Christine Basta</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5311><div class="card-body p-3 small">In this article, we describe the TALP-UPC research group participation in the WMT19 news translation shared task for <a href=https://en.wikipedia.org/wiki/Kazakh_language>Kazakh-English</a>. Given the low amount of parallel training data, we resort to using Russian as pivot language, training subword-based statistical translation systems for Russian-Kazakh and Russian-English that were then used to create two synthetic pseudo-parallel corpora for Kazakh-English and English-Kazakh respectively. Finally, a self-attention model based on the decoder part of the Transformer architecture was trained on the two pseudo-parallel corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5315/>UdS-DFKI Participation at WMT 2019 : Low-Resource (en-gu) and Coreference-Aware (en-de) Systems<span class=acl-fixed-case>U</span>d<span class=acl-fixed-case>S</span>-<span class=acl-fixed-case>DFKI</span> Participation at <span class=acl-fixed-case>WMT</span> 2019: Low-Resource (en-gu) and Coreference-Aware (en-de) Systems</a></strong><br><a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a>
|
<a href=/people/d/dana-ruiter/>Dana Ruiter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5315><div class="card-body p-3 small">This paper describes the UdS-DFKI submission to the WMT2019 news translation task for GujaratiEnglish (low-resourced pair) and GermanEnglish (document-level evaluation). Our systems rely on the on-line extraction of parallel sentences from comparable corpora for the first scenario and on the inclusion of coreference-related information in the training data in the second one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5316 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5316/>The IIIT-H Gujarati-English Machine Translation System for WMT19<span class=acl-fixed-case>IIIT</span>-<span class=acl-fixed-case>H</span> <span class=acl-fixed-case>G</span>ujarati-<span class=acl-fixed-case>E</span>nglish Machine Translation System for <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/v/vikrant-goyal/>Vikrant Goyal</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Misra Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5316><div class="card-body p-3 small">This paper describes the Neural Machine Translation system of IIIT-Hyderabad for the GujaratiEnglish news translation shared task of WMT19. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is basedon encoder-decoder framework with <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. We experimented with Multilingual Neural MT models. Our experiments show that Multilingual Neural Machine Translation leveraging parallel data from related language pairs helps in significant BLEU improvements upto 11.5, for low resource language pairs like Gujarati-English</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5317 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5317/>Kingsoft’s Neural Machine Translation System for WMT19<span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/x/xinze-guo/>Xinze Guo</a>
|
<a href=/people/c/chang-liu/>Chang Liu</a>
|
<a href=/people/x/xiaolong-li/>Xiaolong Li</a>
|
<a href=/people/y/yiran-wang/>Yiran Wang</a>
|
<a href=/people/g/guoliang-li/>Guoliang Li</a>
|
<a href=/people/f/feng-wang/>Feng Wang</a>
|
<a href=/people/z/zhitao-xu/>Zhitao Xu</a>
|
<a href=/people/l/liuyi-yang/>Liuyi Yang</a>
|
<a href=/people/l/li-ma/>Li Ma</a>
|
<a href=/people/c/changliang-li/>Changliang Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5317><div class="card-body p-3 small">This paper describes the Kingsoft AI Lab&#8217;s submission to the WMT2019 news translation shared task. We participated in two language directions : <a href=https://en.wikipedia.org/wiki/English_language>English-Chinese</a> and <a href=https://en.wikipedia.org/wiki/Standard_Chinese>Chinese-English</a>. For both language directions, we trained several variants of Transformer models using the provided parallel data enlarged with a large quantity of back-translated monolingual data. The best <a href=https://en.wikipedia.org/wiki/Translation>translation</a> result was obtained with <a href=https://en.wikipedia.org/wiki/Musical_ensemble>ensemble</a> and reranking techniques. According to automatic metrics (BLEU) our Chinese-English system reached the second highest score, and our English-Chinese system reached the second highest score for this subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5320.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5320/>The MLLP-UPV Supervised Machine Translation Systems for WMT19 News Translation Task<span class=acl-fixed-case>MLLP</span>-<span class=acl-fixed-case>UPV</span> Supervised Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/j/javier-iranzo-sanchez/>Javier Iranzo-Sánchez</a>
|
<a href=/people/g/goncal-garces-diaz-munio/>Gonçal Garcés Díaz-Munío</a>
|
<a href=/people/j/jorge-civera/>Jorge Civera</a>
|
<a href=/people/a/alfons-juan/>Alfons Juan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5320><div class="card-body p-3 small">This paper describes the participation of the MLLP research group of the Universitat Politcnica de Valncia in the WMT 2019 News Translation Shared Task. In this edition, we have submitted <a href=https://en.wikipedia.org/wiki/Linguistic_system>systems</a> for the German English and German French language pairs, participating in both directions of each pair. Our submitted systems, based on the Transformer architecture, make ample use of data filtering, synthetic data and <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> through <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5321.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5321/>Microsoft Translator at WMT 2019 : Towards Large-Scale Document-Level Neural Machine Translation<span class=acl-fixed-case>M</span>icrosoft Translator at <span class=acl-fixed-case>WMT</span> 2019: Towards Large-Scale Document-Level Neural Machine Translation</a></strong><br><a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5321><div class="card-body p-3 small">This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task for English-German. Our main focus is document-level neural machine translation with deep transformer models. We start with strong sentence-level baselines, trained on large-scale data created via data-filtering and noisy back-translation and find that <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> seems to mainly help with translationese input. We explore <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning techniques</a>, deeper models and different ensembling strategies to counter these effects. Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train transformer translation models. We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries. We further explore multi-task training for the incorporation of document-level source language monolingual data via the BERT-objective on the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and two-pass decoding for combinations of sentence-level and document-level systems. Based on preliminary human evaluation results, evaluators strongly prefer the document-level systems over our comparable sentence-level system. The document-level systems also seem to score higher than the human references in source-based direct assessment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5322.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5322/>CUNI Submission for Low-Resource Languages in WMT News 2019<span class=acl-fixed-case>CUNI</span> Submission for Low-Resource Languages in <span class=acl-fixed-case>WMT</span> News 2019</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5322><div class="card-body p-3 small">This paper describes the CUNI submission to the WMT 2019 News Translation Shared Task for the low-resource languages : Gujarati-English and Kazakh-English. We participated in both language pairs in both translation directions. Our system combines <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from a different high-resource language pair followed by training on backtranslated monolingual data. Thanks to the simultaneous training in both directions, we can iterate the backtranslation process. We are using the Transformer model in a constrained submission.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5324/>A Comparison on Fine-grained Pre-trained Embeddings for the WMT19Chinese-English News Translation Task<span class=acl-fixed-case>WMT</span>19<span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish News Translation Task</a></strong><br><a href=/people/z/zhenhao-li/>Zhenhao Li</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5324><div class="card-body p-3 small">This paper describes our submission to the WMT 2019 Chinese-English (zh-en) news translation shared task. Our systems are based on RNN architectures with pre-trained embeddings which utilize character and sub-character information. We compare <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with these different granularity levels using different evaluating metics. We find that a finer granularity embeddings can help the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> according to character level evaluation and that the pre-trained embeddings can also be beneficial for <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> performance marginally when the training data is limited.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5327 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5327/>Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring</a></strong><br><a href=/people/z/zihan-liu/>Zihan Liu</a>
|
<a href=/people/y/yan-xu/>Yan Xu</a>
|
<a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5327><div class="card-body p-3 small">This paper describes CAiRE&#8217;s submission to the unsupervised machine translation track of the WMT&#8217;19 news shared task from <a href=https://en.wikipedia.org/wiki/German_language>German</a> to <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data. We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> separately, and they are aligned using <a href=https://en.wikipedia.org/wiki/MUSE>MUSE</a> (Conneau et al., 2018). To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5330 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5330/>NICT’s Unsupervised Neural and Statistical Machine Translation Systems for the WMT19 News Translation Task<span class=acl-fixed-case>NICT</span>’s Unsupervised Neural and Statistical Machine Translation Systems for the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/b/benjamin-marie/>Benjamin Marie</a>
|
<a href=/people/h/haipeng-sun/>Haipeng Sun</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5330><div class="card-body p-3 small">This paper presents the NICT&#8217;s participation in the WMT19 unsupervised news translation task. We participated in the unsupervised translation direction : <a href=https://en.wikipedia.org/wiki/German_language>German-Czech</a>. Our primary submission to the task is the result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers (constraint&#8217;), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and English-Kazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5333 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5333" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5333/>Facebook FAIR’s WMT19 News Translation Task Submission<span class=acl-fixed-case>F</span>acebook <span class=acl-fixed-case>FAIR</span>’s <span class=acl-fixed-case>WMT</span>19 News Translation Task Submission</a></strong><br><a href=/people/n/nathan-ng/>Nathan Ng</a>
|
<a href=/people/k/kyra-yee/>Kyra Yee</a>
|
<a href=/people/a/alexei-baevski/>Alexei Baevski</a>
|
<a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a>
|
<a href=/people/s/sergey-edunov/>Sergey Edunov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5333><div class="card-body p-3 small">This paper describes Facebook FAIR&#8217;s submission to the WMT19 shared news translation task. We participate in four language directions, <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> and <a href=https://en.wikipedia.org/wiki/English_language>English-Russian</a> in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on domain-specific data, then decode using noisy channel model reranking. Our <a href=https://en.wikipedia.org/wiki/System>system</a> improves on our previous <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction EnglishRussian.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5335 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5335.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5335/>Tilde’s Machine Translation Systems for WMT 2019<span class=acl-fixed-case>WMT</span> 2019</a></strong><br><a href=/people/m/marcis-pinnis/>Marcis Pinnis</a>
|
<a href=/people/r/rihards-krislauks/>Rihards Krišlauks</a>
|
<a href=/people/m/matiss-rikters/>Matīss Rikters</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5335><div class="card-body p-3 small">The paper describes the development process of Tilde&#8217;s NMT systems for the WMT 2019 shared task on news translation. We trained systems for the English-Lithuanian and Lithuanian-English translation directions in constrained and unconstrained tracks. We build upon the best <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> of the previous year&#8217;s competition and combine them with recent advancements in the field. We also present a new method to ensure source domain adherence in back-translated data. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved a shared first place in human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5336 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5336.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5336/>Apertium-fin-engRule-based Shallow Machine Translation for WMT 2019 Shared Task<span class=acl-fixed-case>WMT</span> 2019 Shared Task</a></strong><br><a href=/people/t/tommi-a-pirinen/>Tommi Pirinen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5336><div class="card-body p-3 small">In this paper we describe a rule-based, bi-directional machine translation system for the FinnishEnglish language pair. The baseline system was based on the existing data of FinnWordNet, omorfi and apertium-eng. We have built the <a href=https://en.wikipedia.org/wiki/Disambiguation>disambiguation</a>, lexical selection and translation rules by hand. The <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a> and <a href=https://en.wikipedia.org/wiki/Rule-based_system>rules</a> have been developed based on the shared task data. We describe in this article the use of the shared task data as a kind of a test-driven development workflow in RBMT development and show that it suits perfectly to a modern software engineering continuous integration workflow of RBMT and yields big increases to BLEU scores with minimal effort.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5337.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5337 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5337 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5337/>English-Czech Systems in WMT19 : Document-Level Transformer<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>C</span>zech Systems in <span class=acl-fixed-case>WMT</span>19: Document-Level Transformer</a></strong><br><a href=/people/m/martin-popel/>Martin Popel</a>
|
<a href=/people/d/dominik-machacek/>Dominik Macháček</a>
|
<a href=/people/m/michal-auersperger/>Michal Auersperger</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/p/pavel-pecina/>Pavel Pecina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5337><div class="card-body p-3 small">We describe our NMT systems submitted to the WMT19 shared task in EnglishCzech news translation. Our systems are based on the Transformer model implemented in either Tensor2Tensor (T2 T) or Marian framework. We aimed at improving the adequacy and coherence of translated documents by enlarging the context of the source and target. Instead of translating each sentence independently, we split the document into possibly overlapping multi-sentence segments. In case of the T2 T implementation, this document-level-trained system achieves a +0.6 BLEU improvement (p 0.05) relative to the same system applied on isolated sentences. To assess the potential effect document-level models might have on lexical coherence, we performed a semi-automatic analysis, which revealed only a few sentences improved in this aspect. Thus, we can not draw any conclusions from this week evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5340 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5340/>CUED@WMT19 : EWC&LMs<span class=acl-fixed-case>CUED</span>@<span class=acl-fixed-case>WMT</span>19:<span class=acl-fixed-case>EWC</span>&<span class=acl-fixed-case>LM</span>s</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/a/adria-de-gispert/>Adrià de Gispert</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5340><div class="card-body p-3 small">Two techniques provide the fabric of the Cambridge University Engineering Department&#8217;s (CUED) entry to the WMT19 evaluation campaign : elastic weight consolidation (EWC) and different forms of language modelling (LMs). We report substantial gains by fine-tuning very strong baselines on former WMT test sets using a combination of checkpoint averaging and EWC. A sentence-level Transformer LM and a document-level LM based on a modified Transformer architecture yield further gains. As in previous years, we also extract n-gram probabilities from SMT lattices which can be seen as a source-conditioned n-gram LM.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5342 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5342/>University of Tartu’s Multilingual Multi-domain WMT19 News Translation Shared Task Submission<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>T</span>artu’s Multilingual Multi-domain <span class=acl-fixed-case>WMT</span>19 News Translation Shared Task Submission</a></strong><br><a href=/people/a/andre-tattar/>Andre Tättar</a>
|
<a href=/people/e/elizaveta-korotkova/>Elizaveta Korotkova</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5342><div class="card-body p-3 small">This paper describes the University of Tartu&#8217;s submission to the news translation shared task of WMT19, where the core idea was to train a single multilingual system to cover several language pairs of the shared task and submit its results. We only used the constrained data from the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a>. We describe our approach and its results and discuss the technical issues we faced.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5344.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5344 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5344 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5344/>The LMU Munich Unsupervised Machine Translation System for WMT19<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Unsupervised Machine Translation System for <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5344><div class="card-body p-3 small">We describe LMU Munich&#8217;s machine translation system for GermanCzech translation which was used to participate in the WMT19 shared task on unsupervised news translation. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> only from both languages. The final model is an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised neural model</a> using established techniques for unsupervised translation such as denoising autoencoding and online back-translation. We bootstrap the model with masked language model pretraining and enhance it with back-translations from an unsupervised phrase-based system which is itself bootstrapped using unsupervised bilingual word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5345.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5345 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5345 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5345/>Combining Local and Document-Level Context : The LMU Munich Neural Machine Translation System at WMT19<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Neural Machine Translation System at <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5345><div class="card-body p-3 small">We describe LMU Munich&#8217;s machine translation system for EnglishGerman translation which was used to participate in the WMT19 shared task on supervised news translation. We specifically participated in the document-level MT track. The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contextual information and integration of large-scale document-level context with a less rich representation. We train this <a href=https://en.wikipedia.org/wiki/Physical_model>model</a> by fine-tuning a big Transformer baseline. Our experimental results show that document-level context provides for large improvements in translation quality, and adding a rich representation of the previous sentence provides a small additional gain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5347 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5347/>The University of Helsinki Submissions to the WMT19 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submissions to the <span class=acl-fixed-case>WMT</span>19 News Translation Task</a></strong><br><a href=/people/a/aarne-talman/>Aarne Talman</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a>
|
<a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/a/arvi-hurskainen/>Arvi Hurskainen</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5347><div class="card-body p-3 small">In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs : <a href=https://en.wikipedia.org/wiki/German_language>English-German</a>, <a href=https://en.wikipedia.org/wiki/Finnish_language>English-Finnish</a> and <a href=https://en.wikipedia.org/wiki/Finnish_language>Finnish-English</a>. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5352 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5352.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5352/>A Test Suite and Manual Evaluation of Document-Level NMT at WMT19<span class=acl-fixed-case>NMT</span> at <span class=acl-fixed-case>WMT</span>19</a></strong><br><a href=/people/k/katerina-rysova/>Kateřina Rysová</a>
|
<a href=/people/m/magdalena-rysova/>Magdaléna Rysová</a>
|
<a href=/people/t/tomas-musil/>Tomáš Musil</a>
|
<a href=/people/l/lucie-polakova/>Lucie Poláková</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5352><div class="card-body p-3 small">As the quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> rises and neural machine translation (NMT) is moving from sentence to document level translations, it is becoming increasingly difficult to evaluate the output of translation systems. We provide a test suite for WMT19 aimed at assessing discourse phenomena of MT systems participating in the News Translation Task. We have manually checked the outputs and identified types of translation errors that are relevant to document-level translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5355 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5355.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-5355" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-5355/>SAO WMT19 Test Suite : Machine Translation of Audit Reports<span class=acl-fixed-case>SAO</span> <span class=acl-fixed-case>WMT</span>19 Test Suite: Machine Translation of Audit Reports</a></strong><br><a href=/people/t/tereza-vojtechova/>Tereza Vojtěchová</a>
|
<a href=/people/m/michal-novak/>Michal Novák</a>
|
<a href=/people/m/milos-kloucek/>Miloš Klouček</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5355><div class="card-body p-3 small">This paper describes a machine translation test set of documents from the auditing domain and its use as one of the test suites in the WMT19 News Translation Task for translation directions involving <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports. The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary. For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details. Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5356 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5356/>WMDO : Fluency-based Word Mover’s Distance for Machine Translation Evaluation<span class=acl-fixed-case>WMDO</span>: Fluency-based Word Mover’s Distance for Machine Translation Evaluation</a></strong><br><a href=/people/j/julian-chow/>Julian Chow</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/p/pranava-swaroop-madhyastha/>Pranava Madhyastha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5356><div class="card-body p-3 small">We propose <a href=https://en.wikipedia.org/wiki/WMDO>WMDO</a>, a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> based on distance between distributions in the semantic vector space. Matching in the <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> has been investigated for translation evaluation, but the constraints of a translation&#8217;s word order have not been fully explored. Building on the Word Mover&#8217;s Distance metric and various word embeddings, we introduce a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5357.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5357 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5357 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5357/>Meteor++ 2.0 : Adopt Syntactic Level Paraphrase Knowledge into Machine Translation Evaluation</a></strong><br><a href=/people/y/yinuo-guo/>Yinuo Guo</a>
|
<a href=/people/j/junfeng-hu/>Junfeng Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5357><div class="card-body p-3 small">This paper describes Meteor++ 2.0, our submission to the WMT19 Metric Shared Task. The well known Meteor metric improves machine translation evaluation by introducing paraphrase knowledge. However, it only focuses on the <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical level</a> and utilizes consecutive n-grams paraphrases. In this work, we take into consideration syntactic level paraphrase knowledge, which sometimes may be skip-grams. We describe how such knowledge can be extracted from Paraphrase Database (PPDB) and integrated into Meteor-based metrics. Experiments on WMT15 and WMT17 evaluation datasets show that the newly proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> outperforms all previous versions of Meteor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5358.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5358 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5358 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5358/>YiSi-a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources<span class=acl-fixed-case>Y</span>i<span class=acl-fixed-case>S</span>i - a Unified Semantic <span class=acl-fixed-case>MT</span> Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources</a></strong><br><a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5358><div class="card-body p-3 small">We present YiSi, a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources. Underneath the interface with different language resources settings, YiSi uses the same representation for the two sentences in assessment. Besides, we show significant improvement in the correlation of YiSi-1&#8217;s scores with human judgment is made by using contextual embeddings in multilingual BERTBidirectional Encoder Representations from Transformers to evaluate lexical semantic similarity. YiSi is open source and publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5359.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5359 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5359 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5359/>EED : Extended Edit Distance Measure for Machine Translation<span class=acl-fixed-case>EED</span>: Extended Edit Distance Measure for Machine Translation</a></strong><br><a href=/people/p/peter-stanchev/>Peter Stanchev</a>
|
<a href=/people/w/weiyue-wang/>Weiyue Wang</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5359><div class="card-body p-3 small">Over the years a number of machine translation metrics have been developed in order to evaluate the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and quality of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine-generated translations</a>. Metrics such as <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and TER have been used for decades. However, with the rapid progress of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>, the need for better <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> is growing. This paper proposes an extension of the <a href=https://en.wikipedia.org/wiki/Edit_distance>edit distance</a>, which achieves better human correlation, whilst remaining fast, flexible and easy to understand.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5360.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5360 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5360 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5360/>Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/r/ryoma-yoshimura/>Ryoma Yoshimura</a>
|
<a href=/people/h/hiroki-shimanaka/>Hiroki Shimanaka</a>
|
<a href=/people/y/yukio-matsumura/>Yukio Matsumura</a>
|
<a href=/people/h/hayahide-yamagishi/>Hayahide Yamagishi</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5360><div class="card-body p-3 small">In this paper, we introduce our participation in the WMT 2019 Metric Shared Task. We propose an improved version of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence BLEU</a> using filtered pseudo-references. We propose a method to filter pseudo-references by <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> for automatic evaluation of machine translation (MT). We use the outputs of off-the-shelf MT systems as pseudo-references filtered by <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> in addition to a single human reference (gold reference). We use BERT fine-tuned with paraphrase corpus to filter pseudo-references by checking the paraphrasability with the gold reference. Our experimental results of the WMT 2016 and 2017 datasets show that our method achieved higher correlation with human evaluation than the sentence BLEU (SentBLEU) baselines with a single reference and with unfiltered pseudo-references.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5361.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5361 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5361 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5361/>Naver Labs Europe’s Systems for the WMT19 Machine Translation Robustness Task<span class=acl-fixed-case>E</span>urope’s Systems for the <span class=acl-fixed-case>WMT</span>19 Machine Translation Robustness Task</a></strong><br><a href=/people/a/alexandre-berard/>Alexandre Berard</a>
|
<a href=/people/i/ioan-calapodescu/>Ioan Calapodescu</a>
|
<a href=/people/c/claude-roux/>Claude Roux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5361><div class="card-body p-3 small">This paper describes the <a href=https://en.wikipedia.org/wiki/System>systems</a> that we submitted to the WMT19 Machine Translation robustness task. This task aims to improve MT&#8217;s robustness to noise found on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, like <a href=https://en.wikipedia.org/wiki/Informal_language>informal language</a>, spelling mistakes and other orthographic variations. The organizers provide parallel data extracted from a <a href=https://en.wikipedia.org/wiki/Social_media>social media website</a> in two language pairs : French-English and Japanese-English (one for each language direction). The goal is to obtain the best scores on unseen test sets from the same source, according to automatic metrics (BLEU) and human evaluation. We propose one single and one ensemble system for each translation direction. Our <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble models</a> ranked first in all language pairs, according to BLEU evaluation. We discuss the pre-processing choices that we made, and present our solutions for <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to noise and domain adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5363.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5363 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5363 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5363/>System Description : The Submission of FOKUS to the WMT 19 Robustness Task<span class=acl-fixed-case>FOKUS</span> to the <span class=acl-fixed-case>WMT</span> 19 Robustness Task</a></strong><br><a href=/people/c/cristian-grozea/>Cristian Grozea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5363><div class="card-body p-3 small">This paper describes the systems of Fraunhofer FOKUS for the WMT 2019 machine translation robustness task. We have made submissions to the EN-FR, FR-EN, and JA-EN language pairs. The first two were made with a baseline translator, trained on clean data for the WMT 2019 biomedical translation task. These baselines improved over the baselines from the MTNT paper by 2 to 4 BLEU points, but where not trained on the same data. The last one used the same model class and <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training procedure</a>, with induced typos in the training data to increase the <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5364.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5364 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5364 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5364/>CUNI System for the WMT19 Robustness Task<span class=acl-fixed-case>CUNI</span> System for the <span class=acl-fixed-case>WMT</span>19 Robustness Task</a></strong><br><a href=/people/j/jindrich-helcl/>Jindřich Helcl</a>
|
<a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/m/martin-popel/>Martin Popel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5364><div class="card-body p-3 small">We present our submission to the WMT19 Robustness Task. Our baseline system is the Charles University (CUNI) Transformer system trained for the WMT18 shared task on News Translation. Quantitative results show that the CUNI Transformer system is already far more robust to noisy input than the LSTM-based baseline provided by the task organizers. We further improved the performance of our model by fine-tuning on the in-domain noisy data without influencing the translation quality on the news domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5365.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5365 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5365 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5365/>NTT’s Machine Translation Systems for WMT19 Robustness Task<span class=acl-fixed-case>NTT</span>’s Machine Translation Systems for <span class=acl-fixed-case>WMT</span>19 Robustness Task</a></strong><br><a href=/people/s/soichiro-murakami/>Soichiro Murakami</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5365><div class="card-body p-3 small">This paper describes NTT&#8217;s submission to the WMT19 robustness task. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> mainly focuses on translating <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noisy text</a> (e.g., posts on Twitter), which presents different difficulties from typical translation tasks such as <a href=https://en.wikipedia.org/wiki/News>news</a>. Our submission combined techniques including utilization of a synthetic corpus, <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>, and a <a href=https://en.wikipedia.org/wiki/Placeholder_name>placeholder mechanism</a>, which significantly improved over the previous <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. Experimental results revealed the placeholder mechanism, which temporarily replaces the non-standard tokens including <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a> with special placeholder tokens during <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, improves <a href=https://en.wikipedia.org/wiki/Translation>translation accuracy</a> even with noisy texts.</div></div></div><hr><div id=w19-54><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-54.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-54/>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5400/>Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a>
|
<a href=/people/k/karin-verspoor/>Karin Verspoor</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5404/>Findings of the WMT 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions<span class=acl-fixed-case>WMT</span> 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions</a></strong><br><a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/j/juan-pino/>Juan Pino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5404><div class="card-body p-3 small">Following the WMT 2018 Shared Task on Parallel Corpus Filtering, we posed the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting 2 % and 10 % of the highest-quality data to be used to train machine translation systems. This year, the task tackled the low resource condition of <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-English</a> and Sinhala-English. Eleven participants from companies, national research labs, and universities participated in this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5405/>RTM Stacking Results for Machine Translation Performance Prediction<span class=acl-fixed-case>RTM</span> Stacking Results for Machine Translation Performance Prediction</a></strong><br><a href=/people/e/ergun-bicici/>Ergun Biçici</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5405><div class="card-body p-3 small">We obtain new results using referential translation machines with increased number of learning models in the set of results that are stacked to obtain a better mixture of experts prediction. We combine <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted from the word-level predictions with the sentence- or document-level features, which significantly improve the results on the training sets but decrease the test set results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5407/>QE BERT : Bilingual BERT Using Multi-task Learning for Neural Quality Estimation<span class=acl-fixed-case>QE</span> <span class=acl-fixed-case>BERT</span>: Bilingual <span class=acl-fixed-case>BERT</span> Using Multi-task Learning for Neural Quality Estimation</a></strong><br><a href=/people/h/hyun-kim/>Hyun Kim</a>
|
<a href=/people/j/joon-ho-lim/>Joon-Ho Lim</a>
|
<a href=/people/h/hyun-ki-kim/>Hyun-Ki Kim</a>
|
<a href=/people/s/seung-hoon-na/>Seung-Hoon Na</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5407><div class="card-body p-3 small">For translation quality estimation at word and sentence levels, this paper presents a novel approach based on BERT that recently has achieved impressive results on various natural language processing tasks. Our proposed model is re-purposed BERT for the translation quality estimation and uses <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> for the sentence-level task and word-level subtasks (i.e., source word, target word, and target gap). Experimental results on Quality Estimation shared task of WMT19 show that our systems show competitive results and provide significant improvements over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5408/>MIPT System for World-Level Quality Estimation<span class=acl-fixed-case>MIPT</span> System for World-Level Quality Estimation</a></strong><br><a href=/people/m/mikhail-mosyagin/>Mikhail Mosyagin</a>
|
<a href=/people/v/varvara-logacheva/>Varvara Logacheva</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5408><div class="card-body p-3 small">We explore different model architectures for the WMT 19 shared task on word-level quality estimation of automatic translation. We start with a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> similar to Shef-bRNN, which we modify by using conditional random fields for sequence labelling. Additionally, we use a different approach for labelling gaps and source words. We further develop this model by including features from different sources such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, baseline features for the task and transformer encoders. We evaluate the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the English-German dataset for the corresponding <a href=https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5409/>NJU Submissions for the WMT19 Quality Estimation Shared Task<span class=acl-fixed-case>NJU</span> Submissions for the <span class=acl-fixed-case>WMT</span>19 Quality Estimation Shared Task</a></strong><br><a href=/people/h/hou-qi/>Hou Qi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5409><div class="card-body p-3 small">In this paper, we describe the submissions of the team from Nanjing University for the WMT19 sentence-level Quality Estimation (QE) shared task on English-German language pair. We develop two approaches based on a two-stage neural QE model consisting of a <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extractor</a> and a quality estimator. More specifically, one of the proposed approaches employs the translation knowledge between the two languages from two different translation directions ; while the other one employs extra monolingual knowledge from both source and target sides, obtained by pre-training deep self-attention networks. To efficiently train these two-stage models, a joint learning training method is applied. Experiments show that the ensemble model of the above two models achieves the best results on the benchmark dataset of the WMT17 sentence-level QE shared task and obtains competitive results in WMT19, ranking 3rd out of 10 submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5410 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5410/>Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings</a></strong><br><a href=/people/e/elizaveta-yankovskaya/>Elizaveta Yankovskaya</a>
|
<a href=/people/a/andre-tattar/>Andre Tättar</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5410><div class="card-body p-3 small">We propose the use of pre-trained embeddings as features of a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> for sentence-level quality estimation of machine translation. In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model. In the second proposed method we use as an input features not only pre-trained embeddings, but also <a href=https://en.wikipedia.org/wiki/Log_probability>log probability</a> of any <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT) system</a>. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5411/>SOURCE : SOURce-Conditional Elmo-style Model for Machine Translation Quality Estimation<span class=acl-fixed-case>SOURCE</span>: <span class=acl-fixed-case>SOUR</span>ce-Conditional Elmo-style Model for Machine Translation Quality Estimation</a></strong><br><a href=/people/j/junpei-zhou/>Junpei Zhou</a>
|
<a href=/people/z/zhisong-zhang/>Zhisong Zhang</a>
|
<a href=/people/z/zecong-hu/>Zecong Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5411><div class="card-body p-3 small">Quality estimation (QE) of machine translation (MT) systems is a task of growing importance. It reduces the cost of <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>, allowing machine-translated text to be used in formal occasions. In this work, we describe our submission system in WMT 2019 sentence-level QE task. We mainly explore the utilization of pre-trained translation models in <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a> and adopt a bi-directional translation-like strategy. The <a href=https://en.wikipedia.org/wiki/Strategy_(game_theory)>strategy</a> is similar to ELMo, but additionally conditions on source sentences. Experiments on WMT QE dataset show that our <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a>, which makes the pre-training slightly harder, can bring improvements for <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a>. In WMT-2019 QE task, our system ranked in the second place on En-De NMT dataset and the third place on En-Ru NMT dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5416/>Effort-Aware Neural Automatic Post-Editing</a></strong><br><a href=/people/a/amirhossein-tebbifakhr/>Amirhossein Tebbifakhr</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5416><div class="card-body p-3 small">For this round of the WMT 2019 APE shared task, our submission focuses on addressing the over-correction problem in <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE</a>. Over-correction occurs when the APE system tends to rephrase an already correct MT output, and the resulting sentence is penalized by a reference-based evaluation against human post-edits. Our intuition is that this problem can be prevented by informing the <a href=https://en.wikipedia.org/wiki/System>system</a> about the predicted quality of the MT output or, in other terms, the expected amount of needed corrections. For this purpose, following the common approach in multilingual NMT, we prepend a special token to the beginning of both the source text and the MT output indicating the required amount of <a href=https://en.wikipedia.org/wiki/Post-editing>post-editing</a>. Following the best submissions to the WMT 2018 APE shared task, our backbone architecture is based on multi-source Transformer to encode both the MT output and the corresponding source text. We participated both in the English-German and English-Russian subtasks. In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and -0.47 TER. In the second subtask, where the higher quality of the MT output increases the risk of over-correction, none of our submitted runs was able to improve the MT output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5417 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5417.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5417/>UdS Submission for the WMT 19 Automatic Post-Editing Task<span class=acl-fixed-case>U</span>d<span class=acl-fixed-case>S</span> Submission for the <span class=acl-fixed-case>WMT</span> 19 Automatic Post-Editing Task</a></strong><br><a href=/people/h/hongfei-xu/>Hongfei Xu</a>
|
<a href=/people/q/qiuhui-liu/>Qiuhui Liu</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5417><div class="card-body p-3 small">In this paper, we describe our submission to the English-German APE shared task at WMT 2019. We utilize and adapt an NMT architecture originally developed for exploiting context information to <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE</a>, implement this in our own transformer model and explore joint training of the <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APE task</a> with a de-noising encoder.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5418/>Terminology-Aware Segmentation and Domain Feature for the WMT19 Biomedical Translation Task<span class=acl-fixed-case>WMT</span>19 Biomedical Translation Task</a></strong><br><a href=/people/c/casimiro-pio-carrino/>Casimiro Pio Carrino</a>
|
<a href=/people/b/bardia-rafieian/>Bardia Rafieian</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5418><div class="card-body p-3 small">In this work, we give a description of the TALP-UPC systems submitted for the WMT19 Biomedical Translation Task. Our proposed strategy is NMT model-independent and relies only on one ingredient, a biomedical terminology list. We first extracted such a terminology list by labelling biomedical words in our training dataset using the BabelNet API. Then, we designed a data preparation strategy to insert the <a href=https://en.wikipedia.org/wiki/Term_(logic)>terms information</a> at a token level. Finally, we trained the Transformer model with this terms-informed data. Our best-submitted system ranked 2nd and 3rd for Spanish-English and English-Spanish translation directions, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5420/>Huawei’s NMT Systems for the WMT 2019 Biomedical Translation Task<span class=acl-fixed-case>NMT</span> Systems for the <span class=acl-fixed-case>WMT</span> 2019 Biomedical Translation Task</a></strong><br><a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/j/jianfeng-liu/>Jianfeng Liu</a>
|
<a href=/people/l/liangyou-li/>Liangyou Li</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5420><div class="card-body p-3 small">This paper describes Huawei&#8217;s neural machine translation systems for the WMT 2019 biomedical translation shared task. We trained and fine-tuned our systems on a combination of out-of-domain and in-domain parallel corpora for six translation directions covering EnglishChinese, EnglishFrench and EnglishGerman language pairs. Our submitted systems achieve the best BLEU scores on EnglishFrench and EnglishGerman language pairs according to the official evaluation results. In the EnglishChinese translation task, our <a href=https://en.wikipedia.org/wiki/System>systems</a> are in the second place. The enhanced performance is attributed to more in-domain training and more sophisticated <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> developed. Development of translation models and transfer learning (or domain adaptation) methods has significantly contributed to the progress of the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5421/>UCAM Biomedical Translation at WMT19 : Transfer Learning Multi-domain Ensembles<span class=acl-fixed-case>UCAM</span> Biomedical Translation at <span class=acl-fixed-case>WMT</span>19: Transfer Learning Multi-domain Ensembles</a></strong><br><a href=/people/d/danielle-saunders/>Danielle Saunders</a>
|
<a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5421><div class="card-body p-3 small">The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experimented with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of <a href=https://en.wikipedia.org/wiki/English_language>English-Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5425.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5425 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5425 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5425/>Machine Translation from an Intercomprehension Perspective</a></strong><br><a href=/people/y/yu-chen/>Yu Chen</a>
|
<a href=/people/t/tania-avgustinova/>Tania Avgustinova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5425><div class="card-body p-3 small">Within the first shared task on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> between similar languages, we present our first attempts on Czech to Polish machine translation from an intercomprehension perspective. We propose methods based on the <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>mutual intelligibility</a> of the two languages, taking advantage of their orthographic and phonological similarity, in the hope to improve over our baselines. The <a href=https://en.wikipedia.org/wiki/Translation>translation</a> results are evaluated using <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. On this <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, none of our <a href=https://en.wikipedia.org/wiki/Proposal_(business)>proposals</a> could outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on the final test set. The current setups are rather preliminary, and there are several potential improvements we can try in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5426.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5426 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5426 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5426/>Utilizing Monolingual Data in NMT for Similar Languages : Submission to Similar Language Translation Task<span class=acl-fixed-case>NMT</span> for Similar Languages: Submission to Similar Language Translation Task</a></strong><br><a href=/people/j/jyotsana-khatri/>Jyotsana Khatri</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5426><div class="card-body p-3 small">This paper describes our submission to Shared Task on Similar Language Translation in Fourth Conference on Machine Translation (WMT 2019). We submitted three systems for Hindi-Nepali direction in which we have examined the performance of a RNN based NMT system, a semi-supervised NMT system where monolingual data of both languages is utilized using the architecture by and a system trained with extra synthetic sentences generated using copy of source and target sentences without using any additional monolingual data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5427.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5427 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5427 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5427/>Neural Machine Translation : Hindi-Nepali<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>N</span>epali</a></strong><br><a href=/people/s/sahinur-rahman-laskar/>Sahinur Rahman Laskar</a>
|
<a href=/people/p/partha-pakray/>Partha Pakray</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5427><div class="card-body p-3 small">With the extensive use of Machine Translation (MT) technology, there is progressively interest in directly translating between pairs of similar languages. Because the main challenge is to overcome the limitation of available <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> to produce a precise MT output. Current work relies on the Neural Machine Translation (NMT) with attention mechanism for the similar language translation of WMT19 shared task in the context of Hindi-Nepali pair. The NMT systems trained the Hindi-Nepali parallel corpus and tested, analyzed in Hindi Nepali translation. The official result declared at WMT19 shared task, which shows that our NMT system obtained Bilingual Evaluation Understudy (BLEU) score 24.6 for primary configuration in Nepali to Hindi translation. Also, we have achieved <a href=https://en.wikipedia.org/wiki/BLEU>BLEU score</a> 53.7 (Hindi to Nepali) and 49.1 (Nepali to Hindi) in contrastive system type.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5429 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5429/>Panlingua-KMI MT System for Similar Language Translation Task at WMT 2019<span class=acl-fixed-case>KMI</span> <span class=acl-fixed-case>MT</span> System for Similar Language Translation Task at <span class=acl-fixed-case>WMT</span> 2019</a></strong><br><a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/akanksha-bansal/>Akanksha Bansal</a>
|
<a href=/people/p/priya-rani/>Priya Rani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5429><div class="card-body p-3 small">The present paper enumerates the development of Panlingua-KMI Machine Translation (MT) systems for Hindi Nepali language pair, designed as part of the Similar Language Translation Task at the WMT 2019 Shared Task. The Panlingua-KMI team conducted a series of experiments to explore both the phrase-based statistical (PBSMT) and neural methods (NMT). Among the 11 MT systems prepared under this task, 6 PBSMT systems were prepared for <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-Hindi</a>, 1 PBSMT for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi-Nepali</a> and 2 NMT systems were developed for <a href=https://en.wikipedia.org/wiki/Nepali_language>NepaliHindi</a>. The results show that PBSMT could be an effective method for developing MT systems for <a href=https://en.wikipedia.org/wiki/Lingua_franca>closely-related languages</a>. Our Hindi-Nepali PBSMT system was ranked 2nd among the 13 systems submitted for the pair and our Nepali-Hindi PBSMTsystem was ranked 4th among the 12 systems submitted for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5430.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5430 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5430 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5430/>UDSDFKI Submission to the WMT2019 CzechPolish Similar Language Translation Shared Task<span class=acl-fixed-case>UDS</span>–<span class=acl-fixed-case>DFKI</span> Submission to the <span class=acl-fixed-case>WMT</span>2019 <span class=acl-fixed-case>C</span>zech–<span class=acl-fixed-case>P</span>olish Similar Language Translation Shared Task</a></strong><br><a href=/people/s/santanu-pal/>Santanu Pal</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5430><div class="card-body p-3 small">In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages : <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> and <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and Nepali, and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. Participants could choose to participate in any of these three tracks and submit system outputs in any translation direction. We report the results obtained by our <a href=https://en.wikipedia.org/wiki/System>system</a> in translating from <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and comment on the impact of out-of-domain test data in the performance of our <a href=https://en.wikipedia.org/wiki/System>system</a>. UDS-DFKI achieved competitive performance ranking second among ten teams in <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> to Polish translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5431 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5431/>Neural Machine Translation of Low-Resource and Similar Languages with Backtranslation</a></strong><br><a href=/people/m/michael-przystupa/>Michael Przystupa</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5431><div class="card-body p-3 small">We present our contribution to the WMT19 Similar Language Translation shared task. We investigate the utility of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> on three low-resource, similar language pairs : <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech Polish</a>, and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi Nepali</a>. Since state-of-the-art neural machine translation systems still require large amounts of bitext, which we do not have for the pairs we consider, we focus primarily on incorporating monolingual data into our models with backtranslation. In our analysis, we found Transformer models to work best on Spanish Portuguese and Czech Polish translation, whereas LSTMs with global attention worked best on Hindi Nepali translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5433 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W19-5433.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W19-5433/>Dual Monolingual Cross-Entropy Delta Filtering of Noisy Parallel Data</a></strong><br><a href=/people/a/amittai-axelrod/>Amittai Axelrod</a>
|
<a href=/people/a/anish-kumar/>Anish Kumar</a>
|
<a href=/people/s/steve-sloto/>Steve Sloto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5433><div class="card-body p-3 small">We introduce a purely monolingual approach to <a href=https://en.wikipedia.org/wiki/Filter_(signal_processing)>filtering</a> for parallel data from a noisy corpus in a low-resource scenario. Our work is inspired by Junczysdowmunt:2018, but we relax the requirements to allow for cases where no parallel data is available. Our primary contribution is a dual monolingual cross-entropy delta criterion modified from Cynical data selection Axelrod:2017, and is competitive (within 1.8 BLEU) with the best bilingual filtering method when used to train SMT systems. Our approach is featherweight, and runs end-to-end on a standard laptop in three hours.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5434 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5434/>NRC Parallel Corpus Filtering System for WMT 2019<span class=acl-fixed-case>NRC</span> Parallel Corpus Filtering System for <span class=acl-fixed-case>WMT</span> 2019</a></strong><br><a href=/people/g/gabriel-bernier-colborne/>Gabriel Bernier-Colborne</a>
|
<a href=/people/c/chi-kiu-lo/>Chi-kiu Lo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5434><div class="card-body p-3 small">We describe the National Research Council Canada team&#8217;s submissions to the parallel corpus filtering task at the Fourth Conference on <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5436.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5436 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5436 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5436/>Quality and Coverage : The AFRL Submission to the WMT19 Parallel Corpus Filtering for Low-Resource Conditions Task<span class=acl-fixed-case>AFRL</span> Submission to the <span class=acl-fixed-case>WMT</span>19 Parallel Corpus Filtering for Low-Resource Conditions Task</a></strong><br><a href=/people/g/grant-erdmann/>Grant Erdmann</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5436><div class="card-body p-3 small">The WMT19 Parallel Corpus Filtering For Low-Resource Conditions Task aims to test various methods of filtering a noisy parallel corpora, to make them useful for training machine translation systems. This year the noisy corpora are the relatively low-resource language pairs of <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-English</a> and Sinhala-English. This papers describes the Air Force Research Laboratory (AFRL) submissions, including preprocessing methods and scoring metrics. Numerical results indicate a benefit over <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> and the relative benefits of different options.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5437.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5437 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5437 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5437/>Webinterpret Submission to the WMT2019 Shared Task on Parallel Corpus Filtering<span class=acl-fixed-case>WMT</span>2019 Shared Task on Parallel Corpus Filtering</a></strong><br><a href=/people/j/jesus-gonzalez-rubio/>Jesús González-Rubio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5437><div class="card-body p-3 small">This document describes the participation of Webinterpret in the shared task on parallel corpus filtering at the Fourth Conference on Machine Translation (WMT 2019). Here, we describe the main characteristics of our approach and discuss the results obtained on the data sets published for the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5439.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5439 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5439 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5439/>Filtering of Noisy Parallel Corpora Based on Hypothesis Generation</a></strong><br><a href=/people/z/zuzanna-parcheta/>Zuzanna Parcheta</a>
|
<a href=/people/g/german-sanchis-trilles/>Germán Sanchis-Trilles</a>
|
<a href=/people/f/francisco-casacuberta/>Francisco Casacuberta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5439><div class="card-body p-3 small">The filtering task of noisy parallel corpora in WMT2019 aims to challenge participants to create filtering methods to be useful for training machine translation systems. In this work, we introduce a noisy parallel corpora filtering system based on generating hypotheses by means of a translation model. We train translation models in both language pairs : NepaliEnglish and SinhalaEnglish using provided parallel corpora. We select the training subset for three language pairs (Nepali, <a href=https://en.wikipedia.org/wiki/Sinhala_language>Sinhala</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> to English) jointly using bilingual cross-entropy selection to create the best possible translation model for both language pairs. Once the translation models are trained, we translate the noisy corpora and generate a hypothesis for each sentence pair. We compute the smoothed BLEU score between the target sentence and generated hypothesis. In addition, we apply several rules to discard very noisy or inadequate sentences which can lower the translation score. These <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> are based on <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>, source and target similarity and source language detection. We compare our results with the baseline published on the shared task website, which uses the Zipporah model, over which we achieve significant improvements in one of the conditions in the shared task. The designed <a href=https://en.wikipedia.org/wiki/Filter_(software)>filtering system</a> is domain independent and all experiments are conducted using <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-5441.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-5441 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-5441 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-5441/>The University of Helsinki Submission to the WMT19 Parallel Corpus Filtering Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submission to the <span class=acl-fixed-case>WMT</span>19 Parallel Corpus Filtering Task</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-5441><div class="card-body p-3 small">This paper describes the University of Helsinki Language Technology group&#8217;s participation in the WMT 2019 parallel corpus filtering task. Our scores were produced using a two-step strategy. First, we individually applied a series of <a href=https://en.wikipedia.org/wiki/Filter_(software)>filters</a> to remove the &#8216;bad&#8217; quality sentences. Then, we produced scores for each sentence by weighting these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with a <a href=https://en.wikipedia.org/wiki/Statistical_model>classification model</a>. This methodology allowed us to build a simple and reliable <a href=https://en.wikipedia.org/wiki/System>system</a> that is easily adaptable to other language pairs.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>