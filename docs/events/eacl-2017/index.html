<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>European Chapter of the Association for Computational Linguistics (2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>European Chapter of the Association for Computational Linguistics (2017)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#e17-1>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a>
<span class="badge badge-info align-middle ml-1">103&nbsp;papers</span></li><li><a class=align-middle href=#e17-2>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a>
<span class="badge badge-info align-middle ml-1">98&nbsp;papers</span></li><li><a class=align-middle href=#e17-3>Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">21&nbsp;papers</span></li><li><a class=align-middle href=#e17-4>Proceedings of the Student Research Workshop at the 15th Conference of the European Chapter of the Association for Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#e17-5>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li></ul></div></div><div id=e17-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/E17-1/>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1000/>Proceedings of the 15th Conference of the <span class=acl-fixed-case>E</span>uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</a></strong><br><a href=/people/m/mirella-lapata/>Mirella Lapata</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1001/>Gated End-to-End Memory Networks</a></strong><br><a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/j/julien-perez/>Julien Perez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1001><div class="card-body p-3 small">Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks (MemN2N) have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>. Concretely, we develop a Gated End-to-End trainable Memory Network architecture (GMemN2N). From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>. Then, we show improvements on the Dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> sets the new state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1002" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1002/>Neural Tree Indexers for Text Understanding</a></strong><br><a href=/people/t/tsendsuren-munkhdalai/>Tsendsuren Munkhdalai</a>
|
<a href=/people/h/hong-yu/>Hong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1002><div class="card-body p-3 small">Recurrent neural networks (RNNs) process input text sequentially and model the conditional transition between word tokens. In contrast, the advantages of recursive networks include that they explicitly model the <a href=https://en.wikipedia.org/wiki/Composition_(language)>compositionality</a> and the recursive structure of natural language. However, the current <a href=https://en.wikipedia.org/wiki/Recursion_(computer_science)>recursive architecture</a> is limited by its dependence on <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>syntactic tree</a>. In this paper, we introduce a robust syntactic parsing-independent tree structured model, Neural Tree Indexers (NTI) that provides a middle ground between the sequential RNNs and the syntactic treebased recursive models. NTI constructs a full n-ary tree by processing the input text with its node function in a bottom-up fashion. Attention mechanism can then be applied to both <a href=https://en.wikipedia.org/wiki/Mathematical_structure>structure</a> and node function. We implemented and evaluated a binary tree model of NTI, showing the model achieved the state-of-the-art performance on three different NLP tasks : natural language inference, answer sentence selection, and sentence classification, outperforming state-of-the-art recurrent and recursive neural networks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1003/>Exploring Different Dimensions of Attention for Uncertainty Detection</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1003><div class="card-body p-3 small">Neural networks with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> have proven effective for many natural language processing tasks. In this paper, we develop <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> for uncertainty detection. In particular, we generalize standardly used <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> by introducing external attention and sequence-preserving attention. These novel architectures differ from standard approaches in that they use external resources to compute attention weights and preserve <a href=https://en.wikipedia.org/wiki/Sequence>sequence information</a>. We compare them to other <a href=https://en.wikipedia.org/wiki/Configuration_(geometry)>configurations</a> along different dimensions of attention. Our novel architectures set the new state of the art on a Wikipedia benchmark dataset and perform similar to the state-of-the-art model on a biomedical benchmark which uses a large set of linguistic features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1004/>Classifying Illegal Activities on <a href=https://en.wikipedia.org/wiki/Tor_(anonymity_network)>Tor Network</a> Based on Web Textual Contents</a></strong><br><a href=/people/m/mhd-wesam-al-nabki/>Mhd Wesam Al Nabki</a>
|
<a href=/people/e/eduardo-fidalgo/>Eduardo Fidalgo</a>
|
<a href=/people/e/enrique-alegre/>Enrique Alegre</a>
|
<a href=/people/i/ivan-de-paz/>Ivan de Paz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1004><div class="card-body p-3 small">The freedom of the Deep Web offers a safe place where people can express themselves anonymously but they also can conduct <a href=https://en.wikipedia.org/wiki/Crime>illegal activities</a>. In this paper, we present and make publicly available a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for Darknet active domains, which we call Darknet Usage Text Addresses (DUTA). We built DUTA by sampling the <a href=https://en.wikipedia.org/wiki/Tor_(anonymity_network)>Tor network</a> during two months and manually labeled each address into 26 classes. Using DUTA, we conducted a comparison between two well-known text representation techniques crossed by three different <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifiers</a> to categorize the <a href=https://en.wikipedia.org/wiki/Tor_(anonymity_network)>Tor hidden services</a>. We also fixed the pipeline elements and identified the aspects that have a critical influence on the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results. We found that the combination of TFIDF words representation with <a href=https://en.wikipedia.org/wiki/Logistic_regression>Logistic Regression classifier</a> achieves 96.6 % of 10 folds cross-validation accuracy and a macro F1 score of 93.7 % when classifying a subset of illegal activities from DUTA. The good performance of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> might support potential tools to help the authorities in the detection of these activities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1005/>When is <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> effective? Semantic sequence prediction under varying data conditions</a></strong><br><a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1005><div class="card-body p-3 small">Multitask learning has been applied successfully to a range of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, mostly <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphosyntactic</a>. However, little is known on when <a href=https://en.wikipedia.org/wiki/Machine_to_machine>MTL</a> works and whether there are data characteristics that help to determine the success of <a href=https://en.wikipedia.org/wiki/Machine_to_machine>MTL</a>. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary task configurations, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. When successful, <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>auxiliary tasks</a> with compact and more uniform label distributions are preferable.<i>when</i> MTL works and whether there are data characteristics that help to determine the success of MTL. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary task configurations, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 tasks. When successful, auxiliary tasks with compact and more uniform label distributions are preferable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1006/>Learning Compositionality Functions on Word Embeddings for Modelling Attribute Meaning in Adjective-Noun Phrases</a></strong><br><a href=/people/m/matthias-hartung/>Matthias Hartung</a>
|
<a href=/people/f/fabian-kaupmann/>Fabian Kaupmann</a>
|
<a href=/people/s/soufian-jebbara/>Soufian Jebbara</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1006><div class="card-body p-3 small">Word embeddings have been shown to be highly effective in a variety of lexical semantic tasks. They tend to capture meaningful relational similarities between individual words, at the expense of lacking the capabilty of making the underlying <a href=https://en.wikipedia.org/wiki/Semantic_relation>semantic relation</a> explicit. In this paper, we investigate the <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>attribute relation</a> that often holds between the constituents of <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>adjective-noun phrases</a>. We use CBOW word embeddings to represent word meaning and learn a compositionality function that combines the individual constituents into a phrase representation, thus capturing the compositional attribute meaning. The resulting embedding model, while being fully interpretable, outperforms count-based distributional vector space models that are tailored to attribute meaning in the two tasks of attribute selection and phrase similarity prediction. Moreover, as the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> captures a generalized layer of attribute meaning, it bears the potential to be used for predictions over various attribute inventories without <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>re-training</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1007" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1007/>Hypernyms under Siege : Linguistically-motivated Artillery for Hypernymy Detection</a></strong><br><a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1007><div class="card-body p-3 small">The fundamental role of <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has motivated the development of many methods for the automatic identification of this relation, most of which rely on word distribution. We investigate an extensive number of such <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised measures</a>, using several distributional semantic models that differ by <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context type</a> and feature weighting. We analyze the performance of the different <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> based on their linguistic motivation. Comparison to the state-of-the-art supervised methods shows that while supervised methods generally outperform the unsupervised ones, the former are sensitive to the distribution of training instances, hurting their reliability. Being based on general linguistic hypotheses and independent from training data, unsupervised measures are more robust, and therefore are still useful artillery for hypernymy detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1008 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1008/>Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network</a></strong><br><a href=/people/k/kim-anh-nguyen/>Kim Anh Nguyen</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1008><div class="card-body p-3 small">Distinguishing between <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> and <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In addition to the lexical and syntactic information, we successfully integrate the distance between the related words along the syntactic path as a new pattern feature. The results from <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> experiments show that AntSynNET improves the performance over prior <a href=https://en.wikipedia.org/wiki/Pattern_recognition>pattern-based methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1009/>Unsupervised Does Not Mean Uninterpretable : The Case for Word Sense Induction and Disambiguation</a></strong><br><a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/e/eugen-ruppert/>Eugen Ruppert</a>
|
<a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1009><div class="card-body p-3 small">The current trend in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> is the use of highly opaque models, e.g. neural networks and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. While these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> yield state-of-the-art results on a range of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, their drawback is poor interpretability. On the example of word sense induction and disambiguation (WSID), we show that it is possible to develop an interpretable <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that matches the state-of-the-art models in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Namely, we present an unsupervised, knowledge-free WSID approach, which is interpretable at three levels : word sense inventory, sense feature representations, and disambiguation procedure. Experiments show that our model performs on par with state-of-the-art word sense embeddings and other <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised systems</a> while offering the possibility to justify its decisions in <a href=https://en.wikipedia.org/wiki/Human-readable_medium>human-readable form</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1010/>Word Sense Disambiguation : A Unified Evaluation Framework and Empirical Comparison</a></strong><br><a href=/people/a/alessandro-raganato/>Alessandro Raganato</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1010><div class="card-body p-3 small">Word Sense Disambiguation is a long-standing task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>, lying at the core of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>human language understanding</a>. However, the evaluation of <a href=https://en.wikipedia.org/wiki/Automation>automatic systems</a> has been problematic, mainly due to the lack of a reliable evaluation framework. In this paper we develop a unified evaluation framework and analyze the performance of various Word Sense Disambiguation systems in a fair setup. The results show that <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a> clearly outperform knowledge-based models. Among the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised systems</a>, a <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a> trained on conventional local features still proves to be a hard baseline to beat. Nonetheless, recent approaches exploiting <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> on unlabeled corpora achieve promising results, surpassing this hard baseline in most test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1011" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1011/>Which is the Effective Way for Gaokao : <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a> or <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a>?<span class=acl-fixed-case>G</span>aokao: Information Retrieval or Neural Networks?</a></strong><br><a href=/people/s/shangmin-guo/>Shangmin Guo</a>
|
<a href=/people/x/xiangrong-zeng/>Xiangrong Zeng</a>
|
<a href=/people/s/shizhu-he/>Shizhu He</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/j/jun-zhao/>Jun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1011><div class="card-body p-3 small">As one of the most important test of China, <a href=https://en.wikipedia.org/wiki/Gaokao>Gaokao</a> is designed to be difficult enough to distinguish the excellent high school students. In this work, we detailed the Gaokao History Multiple Choice Questions(GKHMC) and proposed two different approaches to address them using various resources. One approach is based on entity search technique (IR approach), the other is based on text entailment approach where we specifically employ deep neural networks(NN approach). The result of experiment on our collected real Gaokao questions showed that they are good at different categories of questions, that is IR approach performs much better at entity questions(EQs) while NN approach shows its advantage on sentence questions(SQs). We achieve state-of-the-art performance and show that it&#8217;s indispensable to apply hybrid method when participating in the real-world tests.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1012/>If You Ca n’t Beat Them Join Them : Handcrafted Features Complement Neural Nets for Non-Factoid Answer Reranking</a></strong><br><a href=/people/d/dasha-bogdanova/>Dasha Bogdanova</a>
|
<a href=/people/j/jennifer-foster/>Jennifer Foster</a>
|
<a href=/people/d/daria-dzendzik/>Daria Dzendzik</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1012><div class="card-body p-3 small">We show that a neural approach to the task of non-factoid answer reranking can benefit from the inclusion of tried-and-tested handcrafted features. We present a neural network architecture based on a combination of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> that are used to encode questions and answers, and a <a href=https://en.wikipedia.org/wiki/Multilayer_perceptron>multilayer perceptron</a>. We show how this approach can be combined with additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, in particular, the discourse features used by previous research. Our neural approach achieves state-of-the-art performance on a public dataset from Yahoo ! Answers and its performance is further improved by incorporating the discourse features. Additionally, we present a new dataset of Ask Ubuntu questions where the hybrid approach also achieves good results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1013" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1013/>Chains of Reasoning over Entities, Relations, and Text using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/arvind-neelakantan/>Arvind Neelakantan</a>
|
<a href=/people/d/david-belanger/>David Belanger</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1013><div class="card-body p-3 small">Our goal is to combine the rich multi-step inference of symbolic logical reasoning with the generalization capabilities of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. We are particularly interested in complex reasoning about entities and relations in <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs to compose the distributed semantics of multi-hop paths in KBs ; however for multiple reasons, the approach lacks accuracy and practicality. This paper proposes three significant modeling advances : (1) we learn to jointly reason about <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>, entities, and entity-types ; (2) we use neural attention modeling to incorporate multiple paths ; (3) we learn to share strength in a single RNN that represents logical composition across all relations. On a large-scale Freebase+ClueWeb prediction task, we achieve 25 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a>, and a 53 % <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error reduction</a> on sparse relations due to shared strength. On chains of reasoning in <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> we reduce error in mean quantile by 84 % versus previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.<i>entities, and entity-types</i>; (2) we use neural attention modeling to incorporate <i>multiple paths</i>; (3) we learn to <i>share strength in a single RNN</i> that represents logical composition across all relations. On a large-scale Freebase+ClueWeb prediction task, we achieve 25% error reduction, and a 53% error reduction on sparse relations due to shared strength. On chains of reasoning in WordNet we reduce error in mean quantile by 84% versus previous state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1015/>Multitask Learning for Mental Health Conditions with Limited Social Media Data</a></strong><br><a href=/people/a/adrian-benton/>Adrian Benton</a>
|
<a href=/people/m/margaret-mitchell/>Margaret Mitchell</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1015><div class="card-body p-3 small">Language contains information about the author&#8217;s demographic attributes as well as their <a href=https://en.wikipedia.org/wiki/Mental_state>mental state</a>, and has been successfully leveraged in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> to predict either one alone. However, demographic attributes and mental states also interact with each other, and we are the first to demonstrate how to use them jointly to improve the prediction of mental health conditions across the board. We model the different conditions as tasks in a multitask learning (MTL) framework, and establish for the first time the potential of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> in the prediction of mental health from online user-generated text. The framework we propose significantly improves over all baselines and single-task models for predicting mental health conditions, with particularly significant gains for conditions with limited data. In addition, our best MTL model can predict the presence of conditions (neuroatypicality) more generally, further reducing the error of the strong feed-forward baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1017/>Computational Argumentation Quality Assessment in <a href=https://en.wikipedia.org/wiki/Natural_language>Natural Language</a></a></strong><br><a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/n/nona-naderi/>Nona Naderi</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/y/yonatan-bilu/>Yonatan Bilu</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/t/tim-alberdingk-thijm/>Tim Alberdingk Thijm</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1017><div class="card-body p-3 small">Research on computational argumentation faces the problem of how to automatically assess the quality of an argument or argumentation. While different quality dimensions have been approached in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, a common understanding of argumentation quality is still missing. This paper presents the first holistic work on computational argumentation quality in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. We comprehensively survey the diverse existing theories and approaches to assess logical, rhetorical, and dialectical quality dimensions, and we derive a systematic taxonomy from these. In addition, we provide a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with 320 arguments, annotated for all 15 dimensions in the <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomy</a>. Our results establish a common ground for research on computational argumentation quality assessment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1018/>A method for in-depth comparative evaluation : How (dis)similar are outputs of pos taggers, dependency parsers and coreference resolvers really?</a></strong><br><a href=/people/d/don-tuggener/>Don Tuggener</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1018><div class="card-body p-3 small">This paper proposes a generic <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for the comparative evaluation of system outputs. The approach is able to quantify the pairwise differences between two outputs and to unravel in detail what the differences consist of. We apply our approach to three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>Computational Linguistics</a>, i.e. POS tagging, <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parsing</a>, and <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. We find that system outputs are more distinct than the (often) small differences in evaluation scores seem to suggest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1020/>Integrating <a href=https://en.wikipedia.org/wiki/Meaning_(philosophy_of_language)>Meaning</a> into Quality Evaluation of <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/o/osman-baskaya/>Osman Başkaya</a>
|
<a href=/people/e/eray-yildiz/>Eray Yildiz</a>
|
<a href=/people/d/doruk-tunaoglu/>Doruk Tunaoğlu</a>
|
<a href=/people/m/mustafa-tolga-eren/>Mustafa Tolga Eren</a>
|
<a href=/people/a/a-seza-dogruoz/>A. Seza Doğruöz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1020><div class="card-body p-3 small">Machine translation (MT) quality is evaluated through comparisons between MT outputs and the human translations (HT). Traditionally, this <a href=https://en.wikipedia.org/wiki/Validity_(logic)>evaluation</a> relies on form related features (e.g. lexicon and syntax) and ignores the transfer of meaning reflected in HT outputs. Instead, we evaluate the <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> of MT outputs through meaning related features (e.g. polarity, subjectivity) with two experiments. In the first experiment, the meaning related features are compared to human rankings individually. In the second experiment, combinations of meaning related features and other quality metrics are utilized to predict the same human rankings. The results of our experiments confirm the benefit of these features in predicting human evaluation of translation quality in addition to traditional <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> which focus mainly on form.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1021/>Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages</a></strong><br><a href=/people/m/michael-schlichtkrull/>Michael Schlichtkrull</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1021><div class="card-body p-3 small">In cross-lingual dependency annotation projection, information is often lost during <a href=https://en.wikipedia.org/wiki/Language_transfer>transfer</a> because of early decoding. We present an end-to-end graph-based neural network dependency parser that can be trained to reproduce matrices of edge scores, which can be directly projected across word alignments. We show that our approach to cross-lingual dependency parsing is not only simpler, but also achieves an absolute improvement of 2.25 % averaged across 10 languages compared to the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1022" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1022/>Parsing Universal Dependencies without training<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies without training</a></strong><br><a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/z/zeljko-agic/>Željko Agić</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1022><div class="card-body p-3 small">We present UDP, the first training-free parser for Universal Dependencies (UD). Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is based on <a href=https://en.wikipedia.org/wiki/PageRank>PageRank</a> and a small set of specific dependency head rules. UDP features two-step decoding to guarantee that <a href=https://en.wikipedia.org/wiki/Function_word>function words</a> are attached as <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>leaf nodes</a>. The <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> requires no training, and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD. The <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> has very few parameters and distinctly robust to domain change across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1024/>Stance Classification of Context-Dependent Claims</a></strong><br><a href=/people/r/roy-bar-haim/>Roy Bar-Haim</a>
|
<a href=/people/i/indrajit-bhattacharya/>Indrajit Bhattacharya</a>
|
<a href=/people/f/francesco-dinuzzo/>Francesco Dinuzzo</a>
|
<a href=/people/a/amrita-saha/>Amrita Saha</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1024><div class="card-body p-3 small">Recent work has addressed the problem of detecting relevant claims for a given controversial topic. We introduce the complementary task of Claim Stance Classification, along with the first <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark dataset</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We decompose this problem into : (a) open-domain target identification for topic and claim (b) sentiment classification for each target, and (c) open-domain contrast detection between the topic and the claim targets. Manual annotation of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> confirms the applicability and validity of our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. We describe an implementation of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, focusing on a novel <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for contrast detection. Our approach achieves promising results, and is shown to outperform several baselines, which represent the common practice of applying a single, monolithic classifier for stance classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1025/>Exploring the Impact of Pragmatic Phenomena on Irony Detection in Tweets : A Multilingual Corpus Study</a></strong><br><a href=/people/j/jihen-karoui/>Jihen Karoui</a>
|
<a href=/people/f/farah-benamara/>Farah Benamara</a>
|
<a href=/people/v/veronique-moriceau/>Véronique Moriceau</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a>
|
<a href=/people/c/cristina-bosco/>Cristina Bosco</a>
|
<a href=/people/n/nathalie-aussenac-gilles/>Nathalie Aussenac-Gilles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1025><div class="card-body p-3 small">This paper provides a linguistic and pragmatic analysis of the phenomenon of <a href=https://en.wikipedia.org/wiki/Irony>irony</a> in order to represent how Twitter&#8217;s users exploit <a href=https://en.wikipedia.org/wiki/Irony>irony devices</a> within their communication strategies for generating textual contents. We aim to measure the impact of a wide-range of pragmatic phenomena in the interpretation of irony, and to investigate how these <a href=https://en.wikipedia.org/wiki/Phenomenon>phenomena</a> interact with contexts local to the tweet. Informed by linguistic theories, we propose for the first time a multi-layered annotation schema for <a href=https://en.wikipedia.org/wiki/Irony>irony</a> and its application to a corpus of French, English and Italian tweets. We detail each layer, explore their interactions, and discuss our results according to a qualitative and quantitative perspective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1026/>A Multi-View Sentiment Corpus</a></strong><br><a href=/people/d/debora-nozza/>Debora Nozza</a>
|
<a href=/people/e/elisabetta-fersini/>Elisabetta Fersini</a>
|
<a href=/people/e/enza-messina/>Enza Messina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1026><div class="card-body p-3 small">Sentiment Analysis is a broad task that involves the analysis of various aspect of the <a href=https://en.wikipedia.org/wiki/Natural_language>natural language text</a>. However, most of the <a href=https://en.wikipedia.org/wiki/List_of_art_media>approaches</a> in the state of the art usually investigate independently each aspect, i.e. Subjectivity Classification, Sentiment Polarity Classification, <a href=https://en.wikipedia.org/wiki/Emotion_recognition>Emotion Recognition</a>, Irony Detection. In this paper we present a Multi-View Sentiment Corpus (MVSC), which comprises 3000 English microblog posts related the movie domain. Three independent annotators manually labelled MVSC, following a broad annotation schema about different aspects that can be grasped from natural language text coming from <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. The contribution is therefore a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> that comprises five different views for each message, i.e. subjective / objective, sentiment polarity, implicit / explicit, <a href=https://en.wikipedia.org/wiki/Irony>irony</a>, <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>. In order to allow a more detailed investigation on the human labelling behaviour, we provide the annotations of each human annotator involved.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1027/>A Systematic Study of Neural Discourse Models for Implicit Discourse Relation</a></strong><br><a href=/people/a/attapol-rutherford/>Attapol Rutherford</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1027><div class="card-body p-3 small">Inferring implicit discourse relations in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language text</a> is the most difficult subtask in discourse parsing. Many neural network models have been proposed to tackle this problem. However, the comparison for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is not unified, so we could hardly draw clear conclusions about the effectiveness of various <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>. Here, we propose neural network models that are based on feedforward and long-short term memory architecture and systematically study the effects of varying structures. To our surprise, the best-configured feedforward architecture outperforms LSTM-based model in most cases despite thorough tuning. Further, we compare our best feedforward system with competitive convolutional and recurrent networks and find that feedforward can actually be more effective. For the first time for this task, we compile and publish outputs from previous neural and non-neural systems to establish the standard for further comparison.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1029/>Dialog state tracking, a machine reading approach using Memory Network</a></strong><br><a href=/people/j/julien-perez/>Julien Perez</a>
|
<a href=/people/f/fei-liu-unimelb/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1029><div class="card-body p-3 small">In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> and the natural language understanding modules. This paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an End-to-End Memory Network, MemN2N, a memory-enhanced neural network architecture. We evaluate the proposed approach on the second Dialog State Tracking Challenge (DSTC-2) dataset. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> has been converted for the occasion in order to frame the hidden state variable inference as a question-answering task based on a sequence of utterances extracted from a dialog. We show that the proposed <a href=https://en.wikipedia.org/wiki/Music_tracker>tracker</a> gives encouraging results. Then, we propose to extend the DSTC-2 dataset with specific reasoning capabilities requirement like <a href=https://en.wikipedia.org/wiki/Counting>counting</a>, list maintenance, yes-no question answering and indefinite knowledge management. Finally, we present encouraging results using our proposed MemN2N based tracking model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1030/>Sentence Segmentation in Narrative Transcripts from Neuropsychological Tests using Recurrent Convolutional Neural Networks</a></strong><br><a href=/people/m/marcos-treviso/>Marcos Treviso</a>
|
<a href=/people/c/christopher-shulby/>Christopher Shulby</a>
|
<a href=/people/s/sandra-aluisio/>Sandra Aluísio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1030><div class="card-body p-3 small">Automated discourse analysis tools based on Natural Language Processing (NLP) aiming at the diagnosis of language-impairing dementias generally extract several textual metrics of narrative transcripts. However, the absence of sentence boundary segmentation in the transcripts prevents the direct application of NLP methods which rely on these marks in order to function properly, such as taggers and <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. We present the first steps taken towards automatic neuropsychological evaluation based on narrative discourse analysis, presenting a new automatic sentence segmentation method for impaired speech. Our model uses recurrent convolutional neural networks with prosodic, Part of Speech (PoS) features, and word embeddings. It was evaluated intrinsically on impaired, spontaneous speech as well as normal, prepared speech and presents better results for healthy elderly (CTL) (F1 = 0.74) and Mild Cognitive Impairment (MCI) patients (F1 = 0.70) than the Conditional Random Fields method (F1 = 0.55 and 0.53, respectively) used in the same context of our study. The results suggest that our model is robust for impaired speech and can be used in automated discourse analysis tools to differentiate narratives produced by MCI and CTL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1031/>Joint, Incremental Disfluency Detection and Utterance Segmentation from <a href=https://en.wikipedia.org/wiki/Speech>Speech</a></a></strong><br><a href=/people/j/julian-hough/>Julian Hough</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1031><div class="card-body p-3 small">We present the joint task of incremental disfluency detection and utterance segmentation and a simple deep learning system which performs it on transcripts and ASR results. We show how the constraints of the two <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> interact. Our joint-task system outperforms the equivalent individual task systems, provides competitive results and is suitable for future use in conversation agents in the psychiatric domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1032/>From Segmentation to Analyses : a Probabilistic Model for Unsupervised Morphology Induction</a></strong><br><a href=/people/t/toms-bergmanis/>Toms Bergmanis</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1032><div class="card-body p-3 small">A major motivation for unsupervised morphological analysis is to reduce the sparse data problem in under-resourced languages. Most previous work focus on segmenting surface forms into their constituent morphs (taking : tak + ing), but surface form segmentation does not solve the sparse data problem as the analyses of take and taking are not connected to each other. We present a system that adapts the MorphoChains system (Narasimhan et al., 2015) to provide morphological analyses that aim to abstract over spelling differences in functionally similar morphs. This results in analyses that are not compelled to use all the orthographic material of a word (stopping : stop + ing) or limited to only that material (acidified : acid + ify + ed). On average across six typologically varied languages our <a href=https://en.wikipedia.org/wiki/System>system</a> has a similar or better F-score on EMMA (a measure of underlying morpheme accuracy) than three strong baselines ; moreover, the total number of distinct morphemes identified by our <a href=https://en.wikipedia.org/wiki/System>system</a> is on average 12.8 % lower than for Morfessor (Virpioja et al., 2013), a state-of-the-art surface segmentation system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1034/>Universal Dependencies and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a> for Hungarian-and on the Price of Universality<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies and Morphology for <span class=acl-fixed-case>H</span>ungarian - and on the Price of Universality</a></strong><br><a href=/people/v/veronika-vincze/>Veronika Vincze</a>
|
<a href=/people/k/katalin-ilona-simko/>Katalin Simkó</a>
|
<a href=/people/z/zsolt-szanto/>Zsolt Szántó</a>
|
<a href=/people/r/richard-farkas/>Richárd Farkas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1034><div class="card-body p-3 small">In this paper, we present how the principles of universal dependencies and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> have been adapted to <a href=https://en.wikipedia.org/wiki/Hungarian_language>Hungarian</a>. We report the most challenging <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical phenomena</a> and our solutions to those. On the basis of the adapted guidelines, we have converted and manually corrected 1,800 sentences from the Szeged Treebank to universal dependency format. We also introduce experiments on this manually annotated corpus for evaluating automatic conversion and the added value of language-specific, i.e. non-universal, annotations. Our results reveal that converting to universal dependencies is not necessarily trivial, moreover, using language-specific morphological features may have an impact on overall performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1035/>Addressing the Data Sparsity Issue in Neural AMR Parsing<span class=acl-fixed-case>AMR</span> Parsing</a></strong><br><a href=/people/x/xiaochang-peng/>Xiaochang Peng</a>
|
<a href=/people/c/chuan-wang/>Chuan Wang</a>
|
<a href=/people/d/daniel-gildea/>Daniel Gildea</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1035><div class="card-body p-3 small">Neural attention models have achieved great success in different <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>. However, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1037/>Enumeration of Extractive Oracle Summaries</a></strong><br><a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/m/masaaki-nishino/>Masaaki Nishino</a>
|
<a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1037><div class="card-body p-3 small">To analyze the limitations and the future directions of the extractive summarization paradigm, this paper proposes an Integer Linear Programming (ILP) formulation to obtain extractive oracle summaries in terms of ROUGE-N. We also propose an algorithm that enumerates all of the <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle summaries</a> for a set of reference summaries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following : (1) room still exists to improve the performance of extractive summarization ; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1038/>Neural Semantic Encoders</a></strong><br><a href=/people/t/tsendsuren-munkhdalai/>Tsendsuren Munkhdalai</a>
|
<a href=/people/h/hong-yu/>Hong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1038><div class="card-body p-3 small">We present a memory augmented neural network for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> : Neural Semantic Encoders. NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves over time and maintains the understanding of input sequences through read, compose and write operations. NSE can also access 1 multiple and shared memories. In this paper, we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, sentence classification, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>document sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> where NSE achieved state-of-the-art performance when evaluated on publically available benchmarks. For example, our shared-memory model showed an encouraging result on <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, improving an attention-based baseline by approximately 1.0 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1039/>Efficient Benchmarking of NLP APIs using Multi-armed Bandits<span class=acl-fixed-case>NLP</span> <span class=acl-fixed-case>API</span>s using Multi-armed Bandits</a></strong><br><a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/t/tuan-tran/>Tuan Dung Tran</a>
|
<a href=/people/m/mark-carman/>Mark Carman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1039><div class="card-body p-3 small">Comparing NLP systems to select the best one for a task of interest, such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, is critical for practitioners and researchers. A rigorous approach involves setting up a hypothesis testing scenario using the performance of the <a href=https://en.wikipedia.org/wiki/System>systems</a> on query documents. However, often the hypothesis testing approach needs to send a lot of document queries to the systems, which can be problematic. In this paper, we present an effective alternative based on the multi-armed bandit (MAB). We propose a hierarchical generative model to represent the uncertainty in the performance measures of the competing systems, to be used by <a href=https://en.wikipedia.org/wiki/Thompson_sampling>Thompson Sampling</a> to solve the resulting MAB. Experimental results on both synthetic and real data show that our approach requires significantly fewer queries compared to the standard benchmarking technique to identify the best <a href=https://en.wikipedia.org/wiki/System>system</a> according to <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1040/>Character-Word LSTM Language Models<span class=acl-fixed-case>LSTM</span> Language Models</a></strong><br><a href=/people/l/lyan-verwimp/>Lyan Verwimp</a>
|
<a href=/people/j/joris-pelemans/>Joris Pelemans</a>
|
<a href=/people/h/hugo-van-hamme/>Hugo Van hamme</a>
|
<a href=/people/p/patrick-wambacq/>Patrick Wambacq</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1040><div class="card-body p-3 small">We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating <a href=https://en.wikipedia.org/wiki/Word_embedding>word and character embeddings</a>, we achieve up to 2.77 % relative improvement on <a href=https://en.wikipedia.org/wiki/English_language>English</a> compared to a baseline model with a similar amount of parameters and 4.57 % on <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>. Moreover, we also outperform baseline word-level models with a larger number of parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1041/>A Hierarchical Neural Model for Learning Sequences of Dialogue Acts</a></strong><br><a href=/people/q/quan-hung-tran/>Quan Hung Tran</a>
|
<a href=/people/i/ingrid-zukerman/>Ingrid Zukerman</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1041><div class="card-body p-3 small">We propose a novel hierarchical Recurrent Neural Network (RNN) for learning sequences of Dialogue Acts (DAs). The input in this task is a sequence of utterances (i.e., conversational contributions) comprising a sequence of tokens, and the output is a sequence of DA labels (one label per utterance). Our model leverages the hierarchical nature of dialogue data by using two nested RNNs that capture long-range dependencies at the dialogue level and the utterance level. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is combined with an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> that focuses on salient tokens in utterances. Our experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms strong baselines on two popular datasets, Switchboard and MapTask ; and our detailed empirical analysis highlights the impact of each aspect of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1042/>A Network-based End-to-End Trainable Task-oriented Dialogue System</a></strong><br><a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/d/david-vandyke/>David Vandyke</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/s/steve-young/>Steve Young</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1042><div class="card-body p-3 small">Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of <a href=https://en.wikipedia.org/wiki/Handicraft>handcrafting</a>, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> easily and without making too many assumptions about the task at hand. The results show that the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1043 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1043/>May I take your order? A Neural Model for Extracting Structured Information from Conversations<span class=acl-fixed-case>I</span> take your order? A Neural Model for Extracting Structured Information from Conversations</a></strong><br><a href=/people/b/baolin-peng/>Baolin Peng</a>
|
<a href=/people/m/michael-seltzer/>Michael Seltzer</a>
|
<a href=/people/y/y-c-ju/>Y.C. Ju</a>
|
<a href=/people/g/geoffrey-zweig/>Geoffrey Zweig</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1043><div class="card-body p-3 small">In this paper we tackle a unique and important problem of extracting a structured order from the conversation a customer has with an order taker at a restaurant. This is motivated by an actual <a href=https://en.wikipedia.org/wiki/System>system</a> under development to assist in the order taking process. We develop a sequence-to-sequence model that is able to map from unstructured conversational input to the structured form that is conveyed to the kitchen and appears on the customer receipt. This problem is critically different from other tasks like <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> where sequence-to-sequence models have been used : the input includes two sides of a conversation ; the output is highly structured ; and logical manipulations must be performed, for example when the customer changes his mind while ordering. We present a novel sequence-to-sequence model that incorporates a special attention-memory gating mechanism and conversational role markers. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves performance over both a phrase-based machine translation approach and a standard sequence-to-sequence model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1044/>A Two-stage Sieve Approach for Quote Attribution</a></strong><br><a href=/people/g/grace-muzny/>Grace Muzny</a>
|
<a href=/people/m/michael-fang/>Michael Fang</a>
|
<a href=/people/a/angel-chang/>Angel Chang</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1044><div class="card-body p-3 small">We present a deterministic sieve-based system for attributing quotations in literary text and a new dataset : QuoteLi3. Quote attribution, determining who said what in a given text, is important for tasks like creating dialogue systems, and in newer areas like computational literary studies, where it creates opportunities to analyze novels at scale rather than only a few at a time. We release QuoteLi3, which contains more than 6,000 annotations linking quotes to speaker mentions and quotes to speaker entities, and introduce a new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for quote attribution. Our two-stage algorithm first links quotes to mentions, then mentions to entities. Using two stages encapsulates difficult sub-problems and improves system performance. The modular design allows us to tune for overall performance or higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, which is useful for many real-world use cases. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves an average <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 87.5 across three novels, outperforming previous systems, and can be tuned for <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> of 90.4 at a <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>recall</a> of 65.1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1045/>Out-of-domain FrameNet Semantic Role Labeling<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et Semantic Role Labeling</a></strong><br><a href=/people/s/silvana-hartmann/>Silvana Hartmann</a>
|
<a href=/people/i/ilia-kuznetsov/>Ilia Kuznetsov</a>
|
<a href=/people/m/m-teresa-martin-valdivia/>Teresa Martin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1045><div class="card-body p-3 small">Domain dependence of NLP systems is one of the major obstacles to their application in large-scale text analysis, also restricting the applicability of FrameNet semantic role labeling (SRL) systems. Yet, current FrameNet SRL systems are still only evaluated on a single in-domain test set. For the first time, we study the domain dependence of FrameNet SRL on a wide range of <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark sets</a>. We create a novel test set for FrameNet SRL based on user-generated web text and find that the major bottleneck for out-of-domain FrameNet SRL is the frame identification step. To address this problem, we develop a simple, yet efficient <a href=https://en.wikipedia.org/wiki/System>system</a> based on distributed word representations. Our <a href=https://en.wikipedia.org/wiki/System>system</a> closely approaches the state-of-the-art in-domain while outperforming the best available frame identification system out-of-domain. We publish our <a href=https://en.wikipedia.org/wiki/System>system</a> and test data for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1046/>TDParse : Multi-target-specific sentiment recognition on Twitter<span class=acl-fixed-case>TDP</span>arse: Multi-target-specific sentiment recognition on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/b/bo-wang/>Bo Wang</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a>
|
<a href=/people/a/arkaitz-zubiaga/>Arkaitz Zubiaga</a>
|
<a href=/people/r/rob-procter/>Rob Procter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1046><div class="card-body p-3 small">Existing target-specific sentiment recognition methods consider only a single target per tweet, and have been shown to miss nearly half of the actual targets mentioned. We present a <a href=https://en.wikipedia.org/wiki/Twitter>corpus of UK election tweets</a>, with an average of 3.09 entities per tweet and more than one type of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a> in half of the tweets. This requires a method for multi-target specific sentiment recognition, which we develop by using the context around a target as well as syntactic dependencies involving the target. We present results of our method on both a benchmark corpus of single targets and the multi-target election corpus, showing state-of-the art performance in both corpora and outperforming previous approaches to multi-target sentiment task as well as deep learning models for single-target sentiment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1048/>An Extensive Empirical Evaluation of Character-Based Morphological Tagging for 14 Languages</a></strong><br><a href=/people/g/georg-heigold/>Georg Heigold</a>
|
<a href=/people/g/gunter-neumann/>Guenter Neumann</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1048><div class="card-body p-3 small">This paper investigates neural character-based morphological tagging for languages with complex morphology and large tag sets. Character-based approaches are attractive as they can handle rarely- and unseen words gracefully. We evaluate on 14 languages and observe consistent gains over a state-of-the-art morphological tagger across all languages except for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>, where we match the state-of-the-art. We compare two architectures for computing character-based word vectors using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent (RNN)</a> and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional (CNN) nets</a>. We show that the CNN based approach performs slightly worse and less consistently than the RNN based approach. Small but systematic gains are observed when combining the two <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> by <a href=https://en.wikipedia.org/wiki/Assembly_language>ensembling</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1049/>Neural Multi-Source Morphological Reinflection</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1049><div class="card-body p-3 small">We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one <a href=https://en.wikipedia.org/wiki/Form_(document)>source form</a> since different <a href=https://en.wikipedia.org/wiki/Form_(document)>source forms</a> can provide complementary information, e.g., different <a href=https://en.wikipedia.org/wiki/Word_stem>stems</a>. We further present a novel extension to the encoder-decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1050/>Online Automatic Post-editing for MT in a Multi-Domain Translation Environment<span class=acl-fixed-case>MT</span> in a Multi-Domain Translation Environment</a></strong><br><a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/g/gebremedhen-gebremelak/>Gebremedhen Gebremelak</a>
|
<a href=/people/m/matteo-negri/>Matteo Negri</a>
|
<a href=/people/m/marco-turchi/>Marco Turchi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1050><div class="card-body p-3 small">Automatic post-editing (APE) for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a> aims to fix recurrent errors made by the <a href=https://en.wikipedia.org/wiki/Machine_translation>MT decoder</a> by learning from correction examples. In controlled evaluation scenarios, the representativeness of the training set with respect to the test data is a key factor to achieve good performance. Real-life scenarios, however, do not guarantee such favorable learning conditions. Ideally, to be integrated in a real professional translation workflow (e.g. to play a role in computer-assisted translation framework), APE tools should be flexible enough to cope with continuous streams of diverse data coming from different domains / genres. To cope with this problem, we propose an online APE framework that is : i) robust to data diversity (i.e. capable to learn and apply correction rules in the right contexts) and ii) able to evolve over time (by continuously extending and refining its knowledge). In a comparative evaluation, with English-German test data coming in random order from two different domains, we show the effectiveness of our approach, which outperforms a strong <a href=https://en.wikipedia.org/wiki/Batch_processing>batch system</a> and the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> in online APE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1051" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1051/>An Incremental Parser for Abstract Meaning Representation<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation</a></strong><br><a href=/people/m/marco-damonte/>Marco Damonte</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/g/giorgio-satta/>Giorgio Satta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1051><div class="card-body p-3 small">Abstract Meaning Representation (AMR) is a semantic representation for natural language that embeds annotations related to traditional tasks such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>, <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> and co-reference resolution. We describe a transition-based parser for AMR that parses sentences left-to-right, in <a href=https://en.wikipedia.org/wiki/Time_complexity>linear time</a>. We further propose a test-suite that assesses specific subtasks that are helpful in comparing AMR parsers, and show that our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is competitive with the state of the art on the LDC2015E86 dataset and that it outperforms state-of-the-art parsers for recovering named entities and handling polarity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1052/>Integrated Learning of Dialog Strategies and Semantic Parsing</a></strong><br><a href=/people/a/aishwarya-padmakumar/>Aishwarya Padmakumar</a>
|
<a href=/people/j/jesse-thomason/>Jesse Thomason</a>
|
<a href=/people/r/raymond-mooney/>Raymond J. Mooney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1052><div class="card-body p-3 small">Natural language understanding and <a href=https://en.wikipedia.org/wiki/Dialog_management>dialog management</a> are two integral components of interactive dialog systems. Previous research has used machine learning techniques to individually optimize these <a href=https://en.wikipedia.org/wiki/Component_(graph_theory)>components</a>, with different forms of direct and indirect supervision. We present an approach to integrate the learning of both a dialog strategy using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>, and a semantic parser for robust <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>, using only natural dialog interaction for supervision. Experimental results on a simulated task of robot instruction demonstrate that joint learning of both <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> improves dialog performance over learning either of these <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1053/>Unsupervised AMR-Dependency Parse Alignment<span class=acl-fixed-case>AMR</span>-Dependency Parse Alignment</a></strong><br><a href=/people/w/wei-te-chen/>Wei-Te Chen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1053><div class="card-body p-3 small">In this paper, we introduce an Abstract Meaning Representation (AMR) to Dependency Parse aligner. Alignment is a preliminary step for AMR parsing, and our aligner improves current AMR parser performance. Our aligner involves several different features, including named entity tags and semantic role labels, and uses Expectation-Maximization training. Results show that our aligner reaches an 87.1 % F-Score score with the experimental data, and enhances AMR parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1054/>Improving Chinese Semantic Role Labeling using High-quality Surface and Deep Case Frames<span class=acl-fixed-case>C</span>hinese Semantic Role Labeling using High-quality Surface and Deep Case Frames</a></strong><br><a href=/people/g/gongye-jin/>Gongye Jin</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1054><div class="card-body p-3 small">This paper presents a method for applying automatically acquired knowledge to semantic role labeling (SRL). We use a large amount of automatically extracted knowledge to improve the performance of SRL. We present two varieties of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a>, which we call surface case frames and deep case frames. Although the surface case frames are compiled from syntactic parses and can be used as rich syntactic knowledge, they have limited capability for resolving semantic ambiguity. To compensate the deficiency of the <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>surface case frames</a>, we compile <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>deep case frames</a> from automatic semantic roles. We also consider <a href=https://en.wikipedia.org/wiki/Quality_management>quality management</a> for both types of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> in order to get rid of the noise brought from the automatic analyses. The experimental results show that Chinese SRL can be improved using automatically acquired knowledge and the <a href=https://en.wikipedia.org/wiki/Quality_management>quality management</a> shows a positive effect on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1055/>Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities</a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1055><div class="card-body p-3 small">Entities are essential elements of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. In this paper, we present <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for learning multi-level representations of entities on three complementary levels : character (character patterns in entity names extracted, e.g., by neural networks), word (embeddings of words in entity names) and entity (entity embeddings). We investigate state-of-the-art learning methods on each level and find large differences, e.g., for <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, traditional ngram features and the subword model of fasttext (Bojanowski et al., 2016) on the character level ; for <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> (Mikolov et al., 2013) on the word level ; and for the order-aware model wang2vec (Ling et al., 2015a) on the entity level. We confirm experimentally that each level of <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> contributes complementary information and a joint representation of all three levels improves the existing embedding based baseline for fine-grained entity typing by a large margin. Additionally, we show that adding information from entity descriptions further improves multi-level representations of entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1056/>The ContrastMedium Algorithm : Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links<span class=acl-fixed-case>C</span>ontrast<span class=acl-fixed-case>M</span>edium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links</a></strong><br><a href=/people/s/stefano-faralli/>Stefano Faralli</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1056><div class="card-body p-3 small">In this paper, we present ContrastMedium, an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manually selected input root and leaf concepts. This is achieved by leveraging structural information from a companion reference taxonomy, to which the input <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> is linked (either automatically or manually). When used in conjunction with methods for hypernym acquisition and knowledge base linking, our methodology provides a complete solution for end-to-end taxonomy induction. We conduct experiments using automatically acquired knowledge graphs, as well as a SemEval benchmark, and show that our method is able to achieve high performance on the task of taxonomy induction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1057/>Probabilistic Inference for Cold Start Knowledge Base Population with Prior World Knowledge</a></strong><br><a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/m/marjorie-freedman/>Marjorie Freedman</a>
|
<a href=/people/t/talya-meltzer/>Talya Meltzer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1057><div class="card-body p-3 small">Building knowledge bases (KB) automatically from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> is crucial for many applications such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search</a>. The problem is very challenging and has been divided into sub-problems such as mention and named entity recognition, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> and <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. However, combining these components has shown to be under-constrained and often produces KBs with supersize entities and common-sense errors in relations (a person has multiple birthdates). The errors are difficult to resolve solely with IE tools but become obvious with <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> at the corpus level. By analyzing <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> and a large text collection, we found that per-relation cardinality and the popularity of entities follow the <a href=https://en.wikipedia.org/wiki/Power_law>power-law distribution</a> favoring flat long tails with low-frequency instances. We present a probabilistic joint inference algorithm to incorporate this <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> during KB construction. Our approach yields state-of-the-art performance on the TAC Cold Start task, and 42 % and 19.4 % relative improvements in F1 over our baseline on Cold Start hop-1 and all-hop queries respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1058" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1058/>Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema</a></strong><br><a href=/people/p/patrick-verga/>Patrick Verga</a>
|
<a href=/people/a/arvind-neelakantan/>Arvind Neelakantan</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1058><div class="card-body p-3 small">Universal schema predicts the types of entities and relations in a knowledge base (KB) by jointly embedding the union of all available schema typesnot only types from multiple structured databases (such as <a href=https://en.wikipedia.org/wiki/Freebase>Freebase</a> or Wikipedia infoboxes), but also types expressed as textual patterns from raw text. This prediction is typically modeled as a matrix completion problem, with one type per column, and either one or two entities per row (in the case of <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity types</a> or <a href=https://en.wikipedia.org/wiki/Binary_relation>binary relation types</a>, respectively). Factorizing this sparsely observed matrix yields a learned vector embedding for each row and each column. In this paper we explore the problem of making predictions for entities or entity-pairs unseen at training time (and hence without a pre-learned row embedding). We propose an approach having no per-row parameters at all ; rather we produce a row vector on the fly using a learned aggregation function of the vectors of the observed columns for that row. We experiment with various aggregation functions, including neural network attention models. Our approach can be understood as a natural language database, in that questions about KB entities are answered by attending to textual or database evidence. In experiments predicting both relations and entity types, we demonstrate that despite having an order of magnitude fewer parameters than traditional universal schema, we can match the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the traditional model, and more importantly, we can now make predictions about unseen rows with nearly the same <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> as rows available at training time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1059/>Learning to Generate Product Reviews from Attributes</a></strong><br><a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1059><div class="card-body p-3 small">Automatically generating product reviews is a meaningful, yet not well-studied task in <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Traditional natural language generation methods rely extensively on hand-crafted rules and predefined templates. This paper presents an attention-enhanced attribute-to-sequence model to generate product reviews for given attribute information, such as <a href=https://en.wikipedia.org/wiki/User_(computing)>user</a>, product, and rating. The attribute encoder learns to represent input attributes as vectors. Then, the sequence decoder generates reviews by conditioning its output on these vectors. We also introduce an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> to jointly generate <a href=https://en.wikipedia.org/wiki/Review_article>reviews</a> and align words with input attributes. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained end-to-end to maximize the likelihood of target product reviews given the <a href=https://en.wikipedia.org/wiki/Variable_and_attribute_(research)>attributes</a>. We build a publicly available dataset for the review generation task by leveraging the Amazon book reviews and their metadata. Experiments on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> show that our approach outperforms baseline methods and the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> significantly improves the performance of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1060" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1060/>Learning to generate one-sentence biographies from <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a><span class=acl-fixed-case>W</span>ikidata</a></strong><br><a href=/people/a/andrew-chisholm/>Andrew Chisholm</a>
|
<a href=/people/w/will-radford/>Will Radford</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1060><div class="card-body p-3 small">We investigate the generation of one-sentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> incorporates a novel secondary objective that helps ensure <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> generates sentences that contain the input facts. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a BLEU score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1061" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1061/>Transition-Based Deep Input Linearization</a></strong><br><a href=/people/r/ratish-puduppully/>Ratish Puduppully</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1061><div class="card-body p-3 small">Traditional methods for deep NLG adopt pipeline approaches comprising stages such as constructing syntactic input, predicting function words, linearizing the syntactic input and generating the surface forms. Though easier to visualize, pipeline approaches suffer from error propagation. In addition, information available across modules can not be leveraged by all modules. We construct a transition-based model to jointly perform <a href=https://en.wikipedia.org/wiki/Linearization>linearization</a>, function word prediction and morphological generation, which considerably improves upon the accuracy compared to a pipelined baseline system. On a standard deep input linearization shared task, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves the best results reported so far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1064" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1064/>Tackling Error Propagation through <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> : A Case of Greedy Dependency Parsing</a></strong><br><a href=/people/m/minh-le/>Minh Lê</a>
|
<a href=/people/a/antske-fokkens/>Antske Fokkens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1064><div class="card-body p-3 small">Error propagation is a common problem in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes are made early in a process. In this paper, we apply <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to greedy dependency parsing which is known to suffer from <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a> and confirm that <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> reduces the occurrence of <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1065 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1065/>Noisy-context surprisal as a human sentence processing cost model</a></strong><br><a href=/people/r/richard-futrell/>Richard Futrell</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1065><div class="card-body p-3 small">We use the noisy-channel theory of human sentence comprehension to develop an incremental processing cost model that unifies and extends key features of expectation-based and memory-based models. In this model, which we call noisy-context surprisal, the processing cost of a word is the surprisal of the word given a noisy representation of the preceding context. We show that this model accounts for an outstanding puzzle in sentence comprehension, language-dependent structural forgetting effects (Gibson and Thomas, 1999 ; Vasishth et al., 2010 ; Frank et al., 2016), which are previously not well modeled by either expectation-based or memory-based approaches. Additionally, we show that this model derives and generalizes locality effects (Gibson, 1998 ; Demberg and Keller, 2008), a signature prediction of memory-based models. We give <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus-based evidence</a> for a key assumption in this derivation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1066 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1066/>Task-Specific Attentive Pooling of Phrase Alignments Contributes to Sentence Matching</a></strong><br><a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1066><div class="card-body p-3 small">This work studies comparatively two typical sentence matching tasks : textual entailment (TE) and answer selection (AS), observing that weaker phrase alignments are more critical in TE, while stronger phrase alignments deserve more attention in AS. The key to reach this observation lies in phrase detection, phrase representation, phrase alignment, and more importantly how to connect those aligned phrases of different matching degrees with the final <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a>. Prior work (i) has limitations in phrase generation and representation, or (ii) conducts <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> at word and phrase levels by handcrafted features or (iii) utilizes a single framework of <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> without considering the characteristics of specific tasks, which limits the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>&#8217;s effectiveness across tasks. We propose an architecture based on Gated Recurrent Unit that supports (i) representation learning of phrases of arbitrary granularity and (ii) task-specific attentive pooling of phrase alignments between two sentences. Experimental results on TE and AS match our observation and show the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1067" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1067/>On-demand Injection of Lexical Knowledge for Recognising Textual Entailment</a></strong><br><a href=/people/p/pascual-martinez-gomez/>Pascual Martínez-Gómez</a>
|
<a href=/people/k/koji-mineshima/>Koji Mineshima</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a>
|
<a href=/people/d/daisuke-bekki/>Daisuke Bekki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1067><div class="card-body p-3 small">We approach the recognition of textual entailment using logical semantic representations and a <a href=https://en.wikipedia.org/wiki/Automated_theorem_proving>theorem prover</a>. In this setup, lexical divergences that preserve semantic entailment between the source and target texts need to be explicitly stated. However, recognising subsentential semantic relations is not trivial. We address this problem by monitoring the proof of the theorem and detecting unprovable sub-goals that share <a href=https://en.wikipedia.org/wiki/Predicate_(mathematical_logic)>predicate arguments</a> with <a href=https://en.wikipedia.org/wiki/Premise>logical premises</a>. If a <a href=https://en.wikipedia.org/wiki/Binary_relation>linguistic relation</a> exists, then an appropriate <a href=https://en.wikipedia.org/wiki/Axiom>axiom</a> is constructed on-demand and the theorem proving continues. Experiments show that this approach is effective and precise, producing a <a href=https://en.wikipedia.org/wiki/System>system</a> that outperforms other <a href=https://en.wikipedia.org/wiki/Logic_programming>logic-based systems</a> and is competitive with state-of-the-art <a href=https://en.wikipedia.org/wiki/Statistics>statistical methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1068/>Learning to Predict Denotational Probabilities For Modeling Entailment</a></strong><br><a href=/people/a/alice-lai/>Alice Lai</a>
|
<a href=/people/j/julia-hockenmaier/>Julia Hockenmaier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1068><div class="card-body p-3 small">We propose a framework that captures the denotational probabilities of words and phrases by embedding them in a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, and present a method to induce such an <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> from a dataset of denotational probabilities. We show that our model successfully predicts denotational probabilities for unseen phrases, and that its predictions are useful for textual entailment datasets such as SICK and SNLI.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1070 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1070/>Argument Strength is in the Eye of the Beholder : Audience Effects in <a href=https://en.wikipedia.org/wiki/Persuasion>Persuasion</a></a></strong><br><a href=/people/s/stephanie-lukin/>Stephanie Lukin</a>
|
<a href=/people/p/pranav-anand/>Pranav Anand</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a>
|
<a href=/people/s/steve-whittaker/>Steve Whittaker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1070><div class="card-body p-3 small">Americans spend about a third of their time online, with many participating in online conversations on social and political issues. We hypothesize that social media arguments on such issues may be more engaging and persuasive than traditional media summaries, and that particular types of people may be more or less convinced by particular styles of argument, e.g. emotional arguments may resonate with some personalities while factual arguments resonate with others. We report a set of experiments testing at large scale how audience variables interact with argument style to affect the persuasiveness of an argument, an under-researched topic within <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. We show that belief change is affected by <a href=https://en.wikipedia.org/wiki/Personality_psychology>personality factors</a>, with conscientious, open and agreeable people being more convinced by emotional arguments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1071/>A Language-independent and Compositional Model for Personality Trait Recognition from Short Texts</a></strong><br><a href=/people/f/fei-liu-unimelb/>Fei Liu</a>
|
<a href=/people/j/julien-perez/>Julien Perez</a>
|
<a href=/people/s/scott-nowson/>Scott Nowson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1071><div class="card-body p-3 small">There have been many attempts at automatically recognising author personality traits from text, typically incorporating <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> with conventional <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a>, e.g. linear regression or <a href=https://en.wikipedia.org/wiki/Support_vector_machine>Support Vector Machines</a>. In this work, we propose to use deep-learning-based models with atomic features of text the characters to build hierarchical, vectorial word and sentence representations for the task of trait inference. On a corpus of tweets, this method shows state-of-the-art performance across five traits and three languages (English, Spanish and Italian) compared with prior work in <a href=https://en.wikipedia.org/wiki/Author_profiling>author profiling</a>. The results, supported by preliminary visualisation work, are encouraging for the ability to detect <a href=https://en.wikipedia.org/wiki/Complex_traits>complex human traits</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1072/>A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments</a></strong><br><a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1072><div class="card-body p-3 small">While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> remain vague. We observe that whether or not an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> uses a particular feature set (sentence IDs) accounts for a significant performance gap among these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a>. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1073/>Online Learning of Task-specific Word Representations with a Joint Biconvex Passive-Aggressive Algorithm</a></strong><br><a href=/people/p/pascal-denis/>Pascal Denis</a>
|
<a href=/people/l/liva-ralaivola/>Liva Ralaivola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1073><div class="card-body p-3 small">This paper presents a new, efficient method for learning task-specific word vectors using a variant of the Passive-Aggressive algorithm. Specifically, this <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> learns a word embedding matrix in tandem with the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier parameters</a> in an online fashion, solving a <a href=https://en.wikipedia.org/wiki/Convex_optimization>bi-convex constrained optimization</a> at each iteration. We provide a theoretical analysis of this new <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> in terms of regret bounds, and evaluate it on both synthetic data and NLP classification problems, including text classification and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. In the latter case, we compare various pre-trained word vectors to initialize our word embedding matrix, and show that the <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a> learned by our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> vastly outperforms the initial <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a>, with performance results comparable or above the state-of-the-art on these tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1074/>Nonsymbolic Text Representation</a></strong><br><a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1074><div class="card-body p-3 small">We introduce the first generic text representation model that is completely nonsymbolic, i.e., it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text. This applies to training the parameters of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on a training corpus as well as to applying it when computing the representation of a new text. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better than prior work on an <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and a text denoising task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1076.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1076 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1076 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1076/>Event extraction from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> using Non-Parametric Bayesian Mixture Model with Word Embeddings<span class=acl-fixed-case>T</span>witter using Non-Parametric <span class=acl-fixed-case>B</span>ayesian Mixture Model with Word Embeddings</a></strong><br><a href=/people/d/deyu-zhou/>Deyu Zhou</a>
|
<a href=/people/x/xuan-zhang/>Xuan Zhang</a>
|
<a href=/people/y/yulan-he/>Yulan He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1076><div class="card-body p-3 small">To extract structured representations of newsworthy events from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised models</a> typically assume that tweets involving the same named entities and expressed using similar words are likely to belong to the same event. Hence, they group tweets into clusters based on the <a href=https://en.wikipedia.org/wiki/Co-occurrence>co-occurrence patterns</a> of <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> and <a href=https://en.wikipedia.org/wiki/Index_term>topical keywords</a>. However, there are two main limitations. First, they require the number of events to be known beforehand, which is not realistic in practical applications. Second, they do n&#8217;t recognise that the same named entity might be referred to by multiple mentions and tweets using different mentions would be wrongly assigned to different events. To overcome these limitations, we propose a non-parametric Bayesian mixture model with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> for <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event extraction</a>, in which the number of events can be inferred automatically and the issue of lexical variations for the same named entity can be dealt with properly. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has been evaluated on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with sizes ranging between 2,499 and over 60 million tweets. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the baseline approach on all <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> by 5-8 % in <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1077/>End-to-end Relation Extraction using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a> and <a href=https://en.wikipedia.org/wiki/Markov_logic_network>Markov Logic Networks</a><span class=acl-fixed-case>M</span>arkov <span class=acl-fixed-case>L</span>ogic <span class=acl-fixed-case>N</span>etworks</a></strong><br><a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/g/girish-palshikar/>Girish Palshikar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1077><div class="card-body p-3 small">End-to-end relation extraction refers to identifying boundaries of entity mentions, entity types of these mentions and appropriate <a href=https://en.wikipedia.org/wiki/Semantic_relation>semantic relation</a> for each pair of mentions. Traditionally, separate <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a> were trained for each of these tasks and were used in a pipeline fashion where output of one <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is fed as input to another. But it was observed that addressing some of these <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> jointly results in better performance. We propose a single, joint neural network based model to carry out all the three tasks of boundary identification, <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity type classification</a> and relation type classification. This <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is referred to as All Word Pairs model (AWP-NN) as it assigns an appropriate label to each word pair in a given sentence for performing end-to-end relation extraction. We also propose to refine output of the AWP-NN model by using inference in Markov Logic Networks (MLN) so that additional <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> can be effectively incorporated. We demonstrate effectiveness of our approach by achieving better end-to-end relation extraction performance than all 4 previous joint modelling approaches, on the standard dataset of ACE 2004.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1078 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1078/>Trust, but Verify ! Better Entity Linking through Automatic Verification</a></strong><br><a href=/people/b/benjamin-heinzerling/>Benjamin Heinzerling</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a>
|
<a href=/people/c/chin-yew-lin/>Chin-Yew Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1078><div class="card-body p-3 small">We introduce automatic verification as a post-processing step for entity linking (EL). The proposed method trusts EL system results collectively, by assuming entity mentions are mostly linked correctly, in order to create a semantic profile of the given text using geospatial and temporal information, as well as fine-grained entity types. This profile is then used to automatically verify each linked mention individually, i.e., to predict whether it has been linked correctly or not. Verification allows leveraging a rich set of global and pairwise features that would be prohibitively expensive for EL systems employing global inference. Evaluation shows consistent improvements across datasets and systems. In particular, when applied to state-of-the-art systems, our method yields an absolute improvement in <a href=https://en.wikipedia.org/wiki/Linker_(computing)>linking</a> performance of up to 1.7 F1 on AIDA / CoNLL&#8217;03 and up to 2.4 F1 on the English TAC KBP 2015 TEDL dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1079 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1079/>Named Entity Recognition in the Medical Domain with Constrained CRF Models<span class=acl-fixed-case>CRF</span> Models</a></strong><br><a href=/people/c/charles-jochim/>Charles Jochim</a>
|
<a href=/people/l/lea-deleris/>Léa Deleris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1079><div class="card-body p-3 small">This paper investigates how to improve performance on information extraction tasks by constraining and sequencing CRF-based approaches. We consider two different relation extraction tasks, both from the <a href=https://en.wikipedia.org/wiki/Medical_literature>medical literature</a> : <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>dependence relations</a> and <a href=https://en.wikipedia.org/wiki/Probability>probability statements</a>. We explore whether adding <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> can lead to an improvement over standard CRF decoding. Results on our relation extraction tasks are promising, showing significant increases in performance from both (i) adding <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> to post-process the output of a baseline CRF, which captures <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a>, and (ii) further allowing flexibility in the application of those <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> by leveraging a binary classifier as a pre-processing step.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1080.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1080 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1080 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1080/>Learning and Knowledge Transfer with Memory Networks for Machine Comprehension</a></strong><br><a href=/people/m/mohit-yadav/>Mohit Yadav</a>
|
<a href=/people/l/lovekesh-vig/>Lovekesh Vig</a>
|
<a href=/people/g/gautam-shroff/>Gautam Shroff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1080><div class="card-body p-3 small">Enabling machines to read and comprehend <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a> remains an unfulfilled goal for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP research</a>. Recent research efforts on the machine comprehension task have managed to achieve close to ideal performance on <a href=https://en.wikipedia.org/wiki/Simulation>simulated data</a>. However, achieving similar levels of performance on small real world datasets has proved difficult ; major challenges stem from the large vocabulary size, complex grammar, and, the frequent ambiguities in linguistic structure. On the other hand, the requirement of human generated annotations for training, in order to ensure a sufficiently diverse set of questions is prohibitively expensive. Motivated by these practical issues, we propose a novel curriculum inspired training procedure for Memory Networks to improve the performance for machine comprehension with relatively small volumes of training data. Additionally, we explore various <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training regimes</a> for Memory Networks to allow <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a> from a closely related domain having larger volumes of <a href=https://en.wikipedia.org/wiki/Data>labelled data</a>. We also suggest the use of a <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> to incorporate the asymmetric nature of <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge transfer</a>. Our experiments demonstrate improvements on <a href=https://en.wikipedia.org/wiki/Dailymail>Dailymail</a>, <a href=https://en.wikipedia.org/wiki/CNN>CNN</a>, and MCTest datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1081/>If No Media Were Allowed inside the Venue, Was Anybody Allowed?</a></strong><br><a href=/people/z/zahra-sarabi/>Zahra Sarabi</a>
|
<a href=/people/e/eduardo-blanco/>Eduardo Blanco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1081><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> to understand <a href=https://en.wikipedia.org/wiki/Negation>negation</a> in positive terms. Specifically, we extract <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>positive meaning</a> from <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> when the <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation cue</a> syntactically modifies a noun or adjective. Our approach is grounded on generating potential positive interpretations automatically, and then scoring them. Experimental results show that interpretations scored high can be reliably identified.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1082 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1082/>Metaheuristic Approaches to Lexical Substitution and Simplification</a></strong><br><a href=/people/s/sallam-abualhaija/>Sallam Abualhaija</a>
|
<a href=/people/t/tristan-miller/>Tristan Miller</a>
|
<a href=/people/j/judith-eckle-kohler/>Judith Eckle-Kohler</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/k/karl-heinz-zimmermann/>Karl-Heinz Zimmermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1082><div class="card-body p-3 small">In this paper, we propose using metaheuristicsin particular, simulated annealing and the new D-Bees algorithmto solve <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> as an optimization problem within a knowledge-based lexical substitution system. We are the first to perform such an extrinsic evaluation of metaheuristics, for which we use two standard lexical substitution datasets, one <a href=https://en.wikipedia.org/wiki/English_language>English</a> and one <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We find that D-Bees has robust performance for both languages, and performs better than <a href=https://en.wikipedia.org/wiki/Simulated_annealing>simulated annealing</a>, though both achieve good results. Moreover, the D-Beesbased lexical substitution system outperforms state-of-the-art systems on several evaluation metrics. We also show that D-Bees achieves competitive performance in <a href=https://en.wikipedia.org/wiki/Lexical_simplification>lexical simplification</a>, a variant of <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1083/>Paraphrasing Revisited with <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/j/jonathan-mallinson/>Jonathan Mallinson</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1083><div class="card-body p-3 small">Recognizing and generating paraphrases is an important component in many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>. A well-established technique for automatically extracting paraphrases leverages bilingual corpora to find meaning-equivalent phrases in a single language by pivoting over a shared translation in another language. In this paper we revisit bilingual pivoting in the context of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and present a paraphrasing model based purely on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Our model represents <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> in a continuous space, estimates the degree of semantic relatedness between text segments of arbitrary length, and generates candidate <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrases</a> for any source input. Experimental results across tasks and datasets show that neural paraphrases outperform those obtained with conventional phrase-based pivoting approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1084/>Multilingual Training of Crosslingual Word Embeddings</a></strong><br><a href=/people/l/long-duong/>Long Duong</a>
|
<a href=/people/h/hiroshi-kanayama/>Hiroshi Kanayama</a>
|
<a href=/people/t/tengfei-ma/>Tengfei Ma</a>
|
<a href=/people/s/steven-bird/>Steven Bird</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1084><div class="card-body p-3 small">Crosslingual word embeddings represent lexical items from different languages using the same <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, enabling crosslingual transfer. Most prior work constructs <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for a pair of languages, with English on one side. We investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space. In this way, we can exploit and combine strength of many languages. We obtained high performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1085/>Building Lexical Vector Representations from Concept Definitions</a></strong><br><a href=/people/d/danilo-silva-de-carvalho/>Danilo Silva de Carvalho</a>
|
<a href=/people/m/minh-le-nguyen/>Minh Le Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1085><div class="card-body p-3 small">The use of distributional language representations have opened new paths in solving a variety of NLP problems. However, alternative approaches can take advantage of information unavailable through pure <a href=https://en.wikipedia.org/wiki/Statistics>statistical means</a>. This paper presents a method for building vector representations from meaning unit blocks called concept definitions, which are obtained by extracting information from a curated linguistic resource (Wiktionary). The <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> obtained in this way can be compared through conventional cosine similarity and are also interpretable by humans. Evaluation was conducted in semantic similarity and relatedness test sets, with results indicating a performance comparable to other methods based on single linguistic resource extraction. The results also indicate noticeable performance gains when combining distributional similarity scores with the ones obtained using this approach. Additionally, a discussion on the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>&#8217;s shortcomings is provided in the analysis of error cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1086/>ShotgunWSD : An unsupervised algorithm for global word sense disambiguation inspired by <a href=https://en.wikipedia.org/wiki/DNA_sequencing>DNA sequencing</a><span class=acl-fixed-case>S</span>hotgun<span class=acl-fixed-case>WSD</span>: An unsupervised algorithm for global word sense disambiguation inspired by <span class=acl-fixed-case>DNA</span> sequencing</a></strong><br><a href=/people/a/andrei-butnaru/>Andrei Butnaru</a>
|
<a href=/people/r/radu-tudor-ionescu/>Radu Tudor Ionescu</a>
|
<a href=/people/f/florentina-hristea/>Florentina Hristea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1086><div class="card-body p-3 small">In this paper, we present a novel <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised algorithm</a> for word sense disambiguation (WSD) at the document level. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is inspired by a widely-used approach in the field of <a href=https://en.wikipedia.org/wiki/Genetics>genetics</a> for <a href=https://en.wikipedia.org/wiki/Whole_genome_sequencing>whole genome sequencing</a>, known as the Shotgun sequencing technique. The proposed WSD algorithm is based on three main steps. First, a brute-force WSD algorithm is applied to short context windows (up to 10 words) selected from the document in order to generate a short list of likely sense configurations for each window. In the second step, these local sense configurations are assembled into longer composite configurations based on suffix and prefix matching. The resulted configurations are ranked by their length, and the sense of each word is chosen based on a <a href=https://en.wikipedia.org/wiki/Electoral_system>voting scheme</a> that considers only the top k configurations in which the word appears. We compare our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> with other state-of-the-art unsupervised WSD algorithms and demonstrate better performance, sometimes by a very large margin. We also show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can yield better performance than the Most Common Sense (MCS) baseline on one data set. Moreover, our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> has a very small number of parameters, is robust to parameter tuning, and, unlike other bio-inspired methods, it gives a deterministic solution (it does not involve random choices).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1087" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1087/>LanideNN : Multilingual Language Identification on Character Window<span class=acl-fixed-case>L</span>anide<span class=acl-fixed-case>NN</span>: Multilingual Language Identification on Character Window</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1087><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a>, a common first step in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, we want to automatically determine the language of some input text. Monolingual language identification assumes that the given document is written in one language. In multilingual language identification, the document is usually in two or three languages and we just want their names. We aim one step further and propose a method for textual language identification where languages can change arbitrarily and the goal is to identify the spans of each of the languages. Our method is based on Bidirectional Recurrent Neural Networks and it performs well in monolingual and multilingual language identification tasks on six datasets covering 131 languages. The method keeps the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> also for short documents and across domains, so it is ideal for off-the-shelf use without preparation of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1088/>Cross-Lingual Word Embeddings for Low-Resource Language Modeling</a></strong><br><a href=/people/o/oliver-adams/>Oliver Adams</a>
|
<a href=/people/a/adam-makarucha/>Adam Makarucha</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/s/steven-bird/>Steven Bird</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1088><div class="card-body p-3 small">Most languages have no established <a href=https://en.wikipedia.org/wiki/Writing_system>writing system</a> and minimal written records. However, <a href=https://en.wikipedia.org/wiki/Textual_data>textual data</a> is essential for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, and particularly important for training <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to support <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>. Even in cases where text data is missing, there are some languages for which bilingual lexicons are available, since creating lexicons is a fundamental task of documentary linguistics. We investigate the use of such <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> to improve <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> when textual training data is limited to as few as a thousand sentences. The method involves learning cross-lingual word embeddings as a preliminary step in training monolingual language models. Results across a number of languages show that <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are improved by this pre-training. Application to <a href=https://en.wikipedia.org/wiki/Yongning_Na>Yongning Na</a>, a threatened language, highlights challenges in deploying the approach in real low-resource environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1090 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1090/>Psycholinguistic Models of Sentence Processing Improve Sentence Readability Ranking</a></strong><br><a href=/people/d/david-m-howcroft/>David M. Howcroft</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1090><div class="card-body p-3 small">While previous research on <a href=https://en.wikipedia.org/wiki/Readability>readability</a> has typically focused on document-level measures, recent work in areas such as <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> has pointed out the need of sentence-level readability measures. Much of <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> has focused for many years on processing measures that provide difficulty estimates on a word-by-word basis. However, these psycholinguistic measures have not yet been tested on sentence readability ranking tasks. In this paper, we use four psycholinguistic measures : idea density, surprisal, integration cost, and embedding depth to test whether these features are predictive of readability levels. We find that psycholinguistic features significantly improve performance by up to 3 percentage points over a standard document-level readability metric baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1091 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1091/>Web-Scale Language-Independent Cataloging of Noisy Product Listings for E-Commerce<span class=acl-fixed-case>E</span>-Commerce</a></strong><br><a href=/people/p/pradipto-das/>Pradipto Das</a>
|
<a href=/people/y/yandi-xia/>Yandi Xia</a>
|
<a href=/people/a/aaron-levine/>Aaron Levine</a>
|
<a href=/people/g/giuseppe-di-fabbrizio/>Giuseppe Di Fabbrizio</a>
|
<a href=/people/a/ankur-datta/>Ankur Datta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1091><div class="card-body p-3 small">The cataloging of product listings through <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy categorization</a> is a fundamental problem for any <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce marketplace</a>, with applications ranging from personalized search recommendations to <a href=https://en.wikipedia.org/wiki/Query_understanding>query understanding</a>. However, manual and rule based approaches to <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> are not scalable. In this paper, we compare several <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> for categorizing listings in both English and Japanese product catalogs. We show empirically that a combination of words from product titles, navigational breadcrumbs, and <a href=https://en.wikipedia.org/wiki/List_price>list prices</a>, when available, improves results significantly. We outline a novel method using correspondence topic models and a lightweight manual process to reduce <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> from mis-labeled data in the training set. We contrast linear models, gradient boosted trees (GBTs) and convolutional neural networks (CNNs), and show that GBTs and CNNs yield the highest gains in error reduction. Finally, we show GBTs applied in a language-agnostic way on a large-scale Japanese e-commerce dataset have improved <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy categorization</a> performance over current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> based on deep belief network models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1092 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1092/>Recognizing Insufficiently Supported Arguments in Argumentative Essays</a></strong><br><a href=/people/c/christian-stab/>Christian Stab</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1092><div class="card-body p-3 small">In this paper, we propose a new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> for assessing the quality of <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>natural language arguments</a>. The premises of a well-reasoned argument should provide enough evidence for accepting or rejecting its claim. Although this criterion, known as <a href=https://en.wikipedia.org/wiki/Necessity_and_sufficiency>sufficiency</a>, is widely adopted in <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation theory</a>, there are no empirical studies on its applicability to real arguments. In this work, we show that human annotators substantially agree on the sufficiency criterion and introduce a novel annotated corpus. Furthermore, we experiment with feature-rich SVMs and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> and achieve 84 % accuracy for automatically identifying insufficiently supported arguments. The final <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> as well as the annotation guideline are freely available for encouraging future research on argument quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1093 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1093/>Distributed Document and Phrase Co-embeddings for Descriptive Clustering</a></strong><br><a href=/people/m/motoki-sato/>Motoki Sato</a>
|
<a href=/people/a/austin-j-brockmeier/>Austin J. Brockmeier</a>
|
<a href=/people/g/georgios-kontonatsios/>Georgios Kontonatsios</a>
|
<a href=/people/t/tingting-mu/>Tingting Mu</a>
|
<a href=/people/j/john-y-goulermas/>John Y. Goulermas</a>
|
<a href=/people/j/junichi-tsujii/>Jun’ichi Tsujii</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1093><div class="card-body p-3 small">Descriptive document clustering aims to automatically discover groups of semantically related documents and to assign a meaningful label to characterise the content of each cluster. In this paper, we present a descriptive clustering approach that employs a distributed representation model, namely the paragraph vector model, to capture semantic similarities between documents and phrases. The proposed method uses a joint representation of phrases and documents (i.e., a co-embedding) to automatically select a <a href=https://en.wikipedia.org/wiki/Linguistic_description>descriptive phrase</a> that best represents each document cluster. We evaluate our method by comparing its performance to an existing state-of-the-art descriptive clustering method that also uses co-embedding but relies on a bag-of-words representation. Results obtained on benchmark datasets demonstrate that the paragraph vector-based method obtains superior performance over the existing approach in both identifying clusters and assigning appropriate descriptive labels to them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1094 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1094/>SMARTies : Sentiment Models for Arabic Target entities<span class=acl-fixed-case>SMART</span>ies: Sentiment Models for <span class=acl-fixed-case>A</span>rabic Target entities</a></strong><br><a href=/people/n/noura-farra/>Noura Farra</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1094><div class="card-body p-3 small">We consider entity-level sentiment analysis in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, a morphologically rich language with increasing resources. We present a <a href=https://en.wikipedia.org/wiki/System>system</a> that is applied to complex posts written in response to Arabic newspaper articles. Our goal is to identify important entity targets within the post along with the polarity expressed about each target. We achieve significant improvements over multiple baselines, demonstrating that the use of specific morphological representations improves the performance of identifying both important targets and their sentiment, and that the use of distributional semantic clusters further boosts performances for these representations, especially when richer linguistic resources are not available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1095 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1095/>Exploring <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> of Spanish tweets<span class=acl-fixed-case>S</span>panish tweets</a></strong><br><a href=/people/i/isabel-segura-bedmar/>Isabel Segura-Bedmar</a>
|
<a href=/people/a/antonio-quiros/>Antonio Quirós</a>
|
<a href=/people/p/paloma-martinez/>Paloma Martínez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1095><div class="card-body p-3 small">Spanish is the third-most used language on the internet, after <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, with a total of 7.7 % (more than 277 million of users) and a huge internet growth of more than 1,400 %. However, most work on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> has been focused on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. This paper describes a deep learning system for Spanish sentiment analysis. To the best of our knowledge, this is the first work that explores the use of a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> to polarity classification of Spanish tweets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1096/>Contextual Bidirectional Long Short-Term Memory Recurrent Neural Network Language Models : A Generative Approach to Sentiment Analysis</a></strong><br><a href=/people/a/amr-mousa/>Amr Mousa</a>
|
<a href=/people/b/bjorn-schuller/>Björn Schuller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1096><div class="card-body p-3 small">Traditional learning-based approaches to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> of written text use the concept of bag-of-words or bag-of-n-grams, where a document is viewed as a set of terms or short combinations of terms disregarding grammar rules or <a href=https://en.wikipedia.org/wiki/Word_order>word order</a>. Novel approaches de-emphasize this concept and view the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as a sequence classification problem. In this context, <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNNs)</a> have achieved significant success. The idea is to use RNNs as discriminative binary classifiers to predict a positive or negative sentiment label at every word position then perform a type of pooling to get a sentence-level polarity. Here, we investigate a novel generative approach in which a separate <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> is estimated for every sentiment using language models (LMs) based on long short-term memory (LSTM) RNNs. We introduce a novel type of LM using a modified version of bidirectional LSTM (BLSTM) called contextual BLSTM (cBLSTM), where the probability of a word is estimated based on its full left and right contexts. Our approach is compared with a BLSTM binary classifier. Significant improvements are observed in classifying the IMDB movie review dataset. Further improvements are achieved via model combination.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1097/>Large-scale Opinion Relation Extraction with Distantly Supervised Neural Network</a></strong><br><a href=/people/c/changzhi-sun/>Changzhi Sun</a>
|
<a href=/people/y/yuanbin-wu/>Yuanbin Wu</a>
|
<a href=/people/m/man-lan/>Man Lan</a>
|
<a href=/people/s/shiliang-sun/>Shiliang Sun</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1097><div class="card-body p-3 small">We investigate the task of open domain opinion relation extraction. Different from works on manually labeled corpus, we propose an efficient distantly supervised framework based on <a href=https://en.wikipedia.org/wiki/Pattern_matching>pattern matching</a> and neural network classifiers. The <a href=https://en.wikipedia.org/wiki/Pattern_recognition>patterns</a> are designed to automatically generate training data, and the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a> is design to capture various lexical and syntactic features. The result <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is fast and scalable on large-scale corpus. We test the <a href=https://en.wikipedia.org/wiki/System>system</a> on the <a href=https://en.wikipedia.org/wiki/Amazon_(company)>Amazon online review dataset</a>. The result shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to achieve promising performances without any human annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1098/>Decoding with <a href=https://en.wikipedia.org/wiki/Finite-state_transducer>Finite-State Transducers</a> on GPUs<span class=acl-fixed-case>GPU</span>s</a></strong><br><a href=/people/a/arturo-argueta/>Arturo Argueta</a>
|
<a href=/people/d/david-chiang/>David Chiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1098><div class="card-body p-3 small">Weighted finite automata and transducers (including hidden Markov models and conditional random fields) are widely used in natural language processing (NLP) to perform tasks such as morphological analysis, <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, chunking, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, and others. Parallelizing finite state algorithms on graphics processing units (GPUs) would benefit many areas of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Although researchers have implemented GPU versions of basic graph algorithms, no work, to our knowledge, has been done on GPU algorithms for weighted finite automata. We introduce a GPU implementation of the Viterbi and forward-backward algorithm, achieving speedups of up to 4x over our serial implementations running on different computer architectures and 3335x over widely used tools such as OpenFST.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1100 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1100" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1100/>A Multifaceted Evaluation of Neural versus Phrase-Based Machine Translation for 9 Language Directions</a></strong><br><a href=/people/a/antonio-toral/>Antonio Toral</a>
|
<a href=/people/v/victor-m-sanchez-cartagena/>Víctor M. Sánchez-Cartagena</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1100><div class="card-body p-3 small">We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a> and performance across different error categories. We find out that translations produced by neural machine translation systems are considerably different, more fluent and more accurate in terms of <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> compared to those produced by phrase-based systems. Neural machine translation systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/E17-1101.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/E17-1101/>Personalized Machine Translation : Preserving Original Author Traits</a></strong><br><a href=/people/e/ella-rabinovich/>Ella Rabinovich</a>
|
<a href=/people/r/raj-nath-patel/>Raj Nath Patel</a>
|
<a href=/people/s/shachar-mirkin/>Shachar Mirkin</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/s/shuly-wintner/>Shuly Wintner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1101><div class="card-body p-3 small">The language that we produce reflects our personality, and various personal and demographic characteristics can be detected in natural language texts. We focus on one particular <a href=https://en.wikipedia.org/wiki/Trait_theory>personal trait</a> of the author, <a href=https://en.wikipedia.org/wiki/Gender>gender</a>, and study how it is manifested in original texts and in <a href=https://en.wikipedia.org/wiki/Translation>translations</a>. We show that author&#8217;s gender has a powerful, clear signal in originals texts, but this <a href=https://en.wikipedia.org/wiki/Signal_(IPC)>signal</a> is obfuscated in <a href=https://en.wikipedia.org/wiki/Translation>human and machine translation</a>. We then propose simple domain-adaptation techniques that help retain the original gender traits in the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a>, without harming the quality of the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a>, thereby creating more personalized machine translation systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1103/>Grouping business news stories based on salience of named entities</a></strong><br><a href=/people/l/llorenc-escoter/>Llorenç Escoter</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/m/mian-du/>Mian Du</a>
|
<a href=/people/a/anisia-katinskaia/>Anisia Katinskaia</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1103><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/News_aggregator>news aggregation systems</a> focused on broad news domains, certain stories may appear in multiple articles. Depending on the relative importance of the story, the number of versions can reach dozens or hundreds within a day. The text in these versions may be nearly identical or quite different. Linking multiple versions of a story into a single group brings several important benefits to the end-userreducing the cognitive load on the reader, as well as signaling the relative importance of the story. We present a grouping algorithm, and explore several vector-based representations of input documents : from a baseline using keywords, to a method using saliencea measure of importance of named entities in the text. We demonstrate that <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> beyond <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> yield substantial improvements, verified on a manually-annotated corpus of business news stories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1104" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1104/>Very Deep Convolutional Networks for Text Classification</a></strong><br><a href=/people/a/alexis-conneau/>Alexis Conneau</a>
|
<a href=/people/h/holger-schwenk/>Holger Schwenk</a>
|
<a href=/people/l/loic-barrault/>Loïc Barrault</a>
|
<a href=/people/y/yann-lecun/>Yann Lecun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1104><div class="card-body p-3 small">The dominant approach for many NLP tasks are <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>, in particular LSTMs, and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a>. However, these <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a>. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth : using up to 29 convolutional layers, we report improvements over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to <a href=https://en.wikipedia.org/wiki/Text_processing>text processing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1105 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1105/>PageRank for Argument Relevance<span class=acl-fixed-case>P</span>age<span class=acl-fixed-case>R</span>ank” for Argument Relevance</a></strong><br><a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a>
|
<a href=/people/y/yamen-ajjour/>Yamen Ajjour</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1105><div class="card-body p-3 small">Future <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> are expected to deliver pro and con arguments in response to queries on controversial topics. While <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> is now in the focus of research, the question of how to retrieve the relevant arguments remains open. This paper proposes a radical model to assess <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> objectively at web scale : the <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> of an argument&#8217;s conclusion is decided by what other arguments reuse it as a premise. We build an argument graph for this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that we analyze with a recursive weighting scheme, adapting key ideas of <a href=https://en.wikipedia.org/wiki/PageRank>PageRank</a>. In experiments on a large ground-truth argument graph, the resulting relevance scores correlate with human average judgments. We outline what natural language challenges must be faced at <a href=https://en.wikipedia.org/wiki/Scalability>web scale</a> in order to stepwise bring argument relevance to <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search engines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1106/>Predicting Counselor Behaviors in Motivational Interviewing Encounters</a></strong><br><a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a>
|
<a href=/people/k/kenneth-resnicow/>Kenneth Resnicow</a>
|
<a href=/people/s/satinder-singh/>Satinder Singh</a>
|
<a href=/people/l/lawrence-an/>Lawrence An</a>
|
<a href=/people/k/kathy-j-goggin/>Kathy J. Goggin</a>
|
<a href=/people/d/delwyn-catley/>Delwyn Catley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1106><div class="card-body p-3 small">As the number of people receiving psycho-therapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors&#8217; language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing (MI) coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters ; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors ; and third, we develop an automatic system to predict these behaviors. We introduce a new set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> based on <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntactic patterns</a>, and show that they lead to accuracy figures of up to 90 %, which represent a significant improvement with respect to <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> used in the past.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1107 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1107/>Authorship Attribution Using Text Distortion</a></strong><br><a href=/people/e/efstathios-stamatatos/>Efstathios Stamatatos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1107><div class="card-body p-3 small">Authorship attribution is associated with important applications in <a href=https://en.wikipedia.org/wiki/Forensic_science>forensics</a> and humanities research. A crucial point in this field is to quantify the personal style of writing, ideally in a way that is not affected by changes in topic or genre. In this paper, we present a novel method that enhances <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship attribution</a> effectiveness by introducing a text distortion step before extracting <a href=https://en.wikipedia.org/wiki/Stylometry>stylometric measures</a>. The proposed method attempts to mask topic-specific information that is not related to the personal style of authors. Based on experiments on two main tasks in <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship attribution</a>, <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>closed-set attribution</a> and <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship verification</a>, we demonstrate that the proposed approach can enhance existing methods especially under cross-topic conditions, where the training and test corpora do not match in topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1108 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1108/>Structured Learning for Temporal Relation Extraction from Clinical Records</a></strong><br><a href=/people/a/artuur-leeuwenberg/>Artuur Leeuwenberg</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1108><div class="card-body p-3 small">We propose a scalable structured learning model that jointly predicts temporal relations between events and temporal expressions (TLINKS), and the relation between these <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a> and the document creation time (DCTR). We employ a structured perceptron, together with integer linear programming constraints for document-level inference during training and prediction to exploit relational properties of temporality, together with global learning of the relations at the document level. Moreover, this study gives insights in the results of integrating constraints for temporal relation extraction when using structured learning and prediction. Our best system outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the art</a> on both the CONTAINS TLINK task, and the DCTR task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1109 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1109/>Entity Extraction in Biomedical Corpora : An Approach to Evaluate Word Embedding Features with PSO based Feature Selection<span class=acl-fixed-case>PSO</span> based Feature Selection</a></strong><br><a href=/people/s/shweta-yadav/>Shweta Yadav</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/s/sriparna-saha/>Sriparna Saha</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1109><div class="card-body p-3 small">Text mining has drawn significant attention in recent past due to the rapid growth in biomedical and clinical records. Entity extraction is one of the fundamental components for <a href=https://en.wikipedia.org/wiki/Biomedical_text_mining>biomedical text mining</a>. In this paper, we propose a novel approach of <a href=https://en.wikipedia.org/wiki/Feature_selection>feature selection</a> for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity extraction</a> that exploits the concept of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and Particle Swarm Optimization (PSO). The system utilizes word embedding features along with several other <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> extracted by studying the properties of the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. We obtain an interesting observation that compact word embedding features as determined by PSO are more effective compared to the entire word embedding feature set for entity extraction. The proposed system is evaluated on three benchmark biomedical datasets such as GENIA, GENETAG, and AiMed. The effectiveness of the proposed approach is evident with significant performance gains over the baseline models as well as the other existing systems. We observe improvements of 7.86 %, 5.27 % and 7.25 % <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> points over the baseline models for GENIA, GENETAG, and AiMed dataset respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1110 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1110/>Distant Supervision for Relation Extraction beyond the Sentence Boundary</a></strong><br><a href=/people/c/chris-quirk/>Chris Quirk</a>
|
<a href=/people/h/hoifung-poon/>Hoifung Poon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1110><div class="card-body p-3 small">The growing demand for structured knowledge has led to great interest in <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>, especially in cases with limited supervision. However, existing distance supervision approaches only extract <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to cross-sentence relation extraction. At the core of our approach is a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph representation</a> that can incorporate both standard dependencies and <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>, thus providing a unifying way to model <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a> within and across sentences. We extract features from multiple paths in this <a href=https://en.wikipedia.org/wiki/Graph_of_a_function>graph</a>, increasing <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and robustness when confronted with <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variation</a> and analysis error. Experiments on an important extraction task for <a href=https://en.wikipedia.org/wiki/Precision_medicine>precision medicine</a> show that our approach can learn an accurate cross-sentence extractor, using only a small existing <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1111 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1111/>Noise Mitigation for Neural Entity Typing and <a href=https://en.wikipedia.org/wiki/Relation_extraction>Relation Extraction</a></a></strong><br><a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a>
|
<a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1111><div class="card-body p-3 small">In this paper, we address two different types of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in information extraction models : noise from distant supervision and noise from pipeline input features. Our target tasks are <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity typing</a> and <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation extraction</a>. For the first noise type, we introduce multi-instance multi-label learning algorithms using neural network models, and apply them to fine-grained entity typing for the first time. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art supervised approach which uses global embeddings of entities. For the second noise type, we propose ways to improve the integration of noisy entity type predictions into <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. Our experiments show that probabilistic predictions are more robust than discrete predictions and that joint training of the two tasks performs best.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1113 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1113/>Using support vector machines and state-of-the-art <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic alignment</a> to identify cognates in multi-lingual wordlists</a></strong><br><a href=/people/g/gerhard-jager/>Gerhard Jäger</a>
|
<a href=/people/j/johann-mattis-list/>Johann-Mattis List</a>
|
<a href=/people/p/pavel-sofroniev/>Pavel Sofroniev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1113><div class="card-body p-3 small">Most current approaches in phylogenetic linguistics require as input multilingual word lists partitioned into sets of etymologically related words (cognates). Cognate identification is so far done manually by experts, which is time consuming and as of yet only available for a small number of well-studied language families. Automatizing this step will greatly expand the empirical scope of phylogenetic methods in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>, as raw wordlists (in phonetic transcription) are much easier to obtain than wordlists in which cognate words have been fully identified and annotated, even for under-studied languages. A couple of different <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> have been proposed in the past, but they are either disappointing regarding their performance or not applicable to larger datasets. Here we present a new approach that uses support vector machines to unify different state-of-the-art methods for <a href=https://en.wikipedia.org/wiki/Phonetic_transcription>phonetic alignment</a> and cognate detection within a single framework. Training and evaluating these method on a typologically broad collection of gold-standard data shows it to be superior to the existing state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1114 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1114/>A Multi-task Approach to Predict Likability of Books</a></strong><br><a href=/people/s/suraj-maharjan/>Suraj Maharjan</a>
|
<a href=/people/j/john-arevalo/>John Arevalo</a>
|
<a href=/people/m/manuel-montes/>Manuel Montes</a>
|
<a href=/people/f/fabio-a-gonzalez/>Fabio A. González</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1114><div class="card-body p-3 small">We investigate the value of <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> and <a href=https://en.wikipedia.org/wiki/Neural_network>neural network models</a> for predicting successful writing. Similar to previous work, we treat this as a binary classification task and explore new strategies to automatically learn representations from book contents. We evaluate our <a href=https://en.wikipedia.org/wiki/Software_feature>feature set</a> on two different <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> created from Project Gutenberg books. The first presents a novel approach for generating the gold standard labels for the task and the <a href=https://en.wikipedia.org/wiki/Other_(philosophy)>other</a> is based on prior research. Using a combination of hand-crafted and recurrent neural network learned representations in a dual learning setting, we obtain the best performance of 73.50 % weighted F1-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1115 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1115/>A Data-Oriented Model of Literary Language</a></strong><br><a href=/people/a/andreas-van-cranenburgh/>Andreas van Cranenburgh</a>
|
<a href=/people/r/rens-bod/>Rens Bod</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1115><div class="card-body p-3 small">We consider the task of predicting how literary a text is, with a gold standard from human ratings. Aside from a standard bigram baseline, we apply rich syntactic tree fragments, mined from the training set, and a series of hand-picked features. Our model is the first to distinguish degrees of highly and less literary novels using a variety of <a href=https://en.wikipedia.org/wiki/Lexicon>lexical and syntactic features</a>, and explains 76.0 % of the variation in literary ratings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1116 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-1116/>Aye or naw, whit dae ye hink? <a href=https://en.wikipedia.org/wiki/Scottish_independence>Scottish independence</a> and linguistic identity on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a><span class=acl-fixed-case>S</span>cottish independence and linguistic identity on social media</a></strong><br><a href=/people/p/philippa-shoemark/>Philippa Shoemark</a>
|
<a href=/people/d/debnil-sur/>Debnil Sur</a>
|
<a href=/people/l/luke-shrimpton/>Luke Shrimpton</a>
|
<a href=/people/i/iain-murray/>Iain Murray</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1116><div class="card-body p-3 small">Political surveys have indicated a relationship between a sense of Scottish identity and voting decisions in the 2014 <a href=https://en.wikipedia.org/wiki/2014_Scottish_independence_referendum>Scottish Independence Referendum</a>. Identity is often reflected in language use, suggesting the intuitive hypothesis that individuals who support <a href=https://en.wikipedia.org/wiki/Scottish_independence>Scottish independence</a> are more likely to use distinctively Scottish words than those who oppose it. In the first large-scale study of sociolinguistic variation on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in the UK, we identify distinctively Scottish terms in a data-driven way, and find that these <a href=https://en.wikipedia.org/wiki/Terminology>terms</a> are indeed used at a higher rate by users of pro-independence hashtags than by users of anti-independence hashtags. However, we also find that in general people are less likely to use distinctively Scottish words in tweets with referendum-related hashtags than in their general Twitter activity. We attribute this difference to style shifting relative to audience, aligning with previous work showing that Twitter users tend to use fewer local variants when addressing a broader audience.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1118 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1118" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1118/>Incremental Discontinuous Phrase Structure Parsing with the GAP Transition<span class=acl-fixed-case>GAP</span> Transition</a></strong><br><a href=/people/m/maximin-coavoux/>Maximin Coavoux</a>
|
<a href=/people/b/benoit-crabbe/>Benoît Crabbé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1118><div class="card-body p-3 small">This article introduces a novel transition system for discontinuous lexicalized constituent parsing called SR-GAP. It is an extension of the shift-reduce algorithm with an additional gap transition. Evaluation on two German treebanks shows that SR-GAP outperforms the previous best transition-based discontinuous parser (Maier, 2015) by a large margin (it is notably twice as accurate on the prediction of discontinuous constituents), and is competitive with the state of the art (Fernndez-Gonzlez and Martins, 2015). As a side contribution, we adapt span features (Hall et al., 2014) to discontinuous parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-1119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-1119 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-1119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-1119" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-1119/>Neural Architectures for Fine-grained Entity Type Classification</a></strong><br><a href=/people/s/sonse-shimaoka/>Sonse Shimaoka</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-1119><div class="card-body p-3 small">In this work, we investigate several neural network architectures for fine-grained entity type classification and make three key contributions. Despite being a natural comparison and addition, previous work on attentive neural architectures have not considered hand-crafted features and we combine these with learnt features and establish that they complement each other. Additionally, through quantitative analysis we establish that the <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> learns to attend over syntactic heads and the phrase containing the mention, both of which are known to be strong hand-crafted features for our task. We introduce parameter sharing between labels through a hierarchical encoding method, that in low-dimensional projections show clear clusters for each type hierarchy. Lastly, despite using the same evaluation dataset, the literature frequently compare <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained using different data. We demonstrate that the choice of training data has a drastic impact on performance, which decreases by as much as 9.85 % loose micro F1 score for a previously proposed method. Despite this discrepancy, our best model achieves state-of-the-art results with 75.36 % loose micro F1 score on the well-established Figer (GOLD) dataset and we report the best results for models trained using publicly available data for the OntoNotes dataset with 64.93 % loose micro F1 score.</div></div></div><hr><div id=e17-2><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/E17-2/>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2000/>Proceedings of the 15th Conference of the <span class=acl-fixed-case>E</span>uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</a></strong><br><a href=/people/m/mirella-lapata/>Mirella Lapata</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2001/>Multilingual Back-and-Forth Conversion between Content and Function Head for Easy Dependency Parsing</a></strong><br><a href=/people/r/ryosuke-kohita/>Ryosuke Kohita</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2001><div class="card-body p-3 small">Universal Dependencies (UD) is becoming a standard annotation scheme cross-linguistically, but it is argued that this scheme centering on content words is harder to parse than the conventional one centering on function words. To improve the <a href=https://en.wikipedia.org/wiki/Parsing>parsability</a> of UD, we propose a back-and-forth conversion algorithm, in which we preprocess the training treebank to increase <a href=https://en.wikipedia.org/wiki/Parsing>parsability</a>, and reconvert the parser outputs to follow the UD scheme as a postprocess. We show that this technique consistently improves <a href=https://en.wikipedia.org/wiki/Lisp_(programming_language)>LAS</a> across languages even with a state-of-the-art <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>, in particular on core dependency arcs such as nominal modifier. We also provide an in-depth analysis to understand why our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> increases <a href=https://en.wikipedia.org/wiki/Parsing>parsability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2005/>Using Twitter Language to Predict the Real Estate Market<span class=acl-fixed-case>T</span>witter Language to Predict the Real Estate Market</a></strong><br><a href=/people/m/mohammadzaman-zamani/>Mohammadzaman Zamani</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2005><div class="card-body p-3 small">We explore whether <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> can provide a window into community real estate -foreclosure rates and price changes- beyond that of traditional economic and demographic variables. We find language use in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> not only predicts real estate outcomes as well as traditional variables across counties, but that including <a href=https://en.wikipedia.org/wiki/Twitter>Twitter language</a> in traditional models leads to a significant improvement (e.g. from Pearson r = : 50 to r = : 59 for price changes). We overcome the challenge of the relative sparsity and noise in Twitter language variables by showing that training on the <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>residual error</a> of the traditional <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> leads to more accurate overall assessments. Finally, we discover that it is <a href=https://en.wikipedia.org/wiki/List_of_Latin-script_digraphs>Twitter language</a> related to business (e.g. &#8216;company&#8217;, &#8216;marketing&#8217;) and <a href=https://en.wikipedia.org/wiki/Technology>technology</a> (e.g. &#8216;technology&#8217;, &#8216;internet&#8217;), among others, that yield predictive power over <a href=https://en.wikipedia.org/wiki/Economics>economics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2006 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2006/>Lexical Simplification with Neural Ranking</a></strong><br><a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2006><div class="card-body p-3 small">We present a new Lexical Simplification approach that exploits Neural Networks to learn substitutions from the Newsela corpus-a large set of professionally produced simplifications. We extract candidate substitutions by combining the Newsela corpus with a retrofitted context-aware word embeddings model and rank them using a new neural regression model that learns rankings from annotated data. This strategy leads to the highest <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>Accuracy</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>Precision</a> and F1 scores to date in standard datasets for the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2008/>Crowd-Sourced Iterative Annotation for Narrative Summarization Corpora</a></strong><br><a href=/people/j/jessica-ouyang/>Jessica Ouyang</a>
|
<a href=/people/s/serina-chang/>Serina Chang</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2008><div class="card-body p-3 small">We present an iterative annotation process for producing aligned, parallel corpora of abstractive and extractive summaries for <a href=https://en.wikipedia.org/wiki/Narrative>narrative</a>. Our approach uses a combination of trained annotators and <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a>, allowing us to elicit human-generated summaries and alignments quickly and at low cost. We use <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> to annotate aligned phrases with the text-to-text generation techniques needed to transform each phrase into the other. We apply this process to a corpus of 476 personal narratives, which we make available on the Web.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2009/>Broad Context Language Modeling as Reading Comprehension</a></strong><br><a href=/people/z/zewei-chu/>Zewei Chu</a>
|
<a href=/people/h/hai-wang/>Hai Wang</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a>
|
<a href=/people/d/david-mcallester/>David McAllester</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2009><div class="card-body p-3 small">Progress in <a href=https://en.wikipedia.org/wiki/Reading_comprehension>text understanding</a> has been driven by large datasets that test particular capabilities, like recent <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> (Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence. We view LAMBADA as a <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension problem</a> and apply comprehension models based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Though these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3 % to 49 %. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> or external knowledge is needed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2010/>Detecting negation scope is easy, except when it is n’t</a></strong><br><a href=/people/f/federico-fancellu/>Federico Fancellu</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/h/hangfeng-he/>Hangfeng He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2010><div class="card-body p-3 small">Several <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> have been annotated with negation scopethe set of words whose meaning is negated by a cue like the word notleading to the development of <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> that detect negation scope with high accuracy. We show that for nearly all of these corpora, this high accuracy can be attributed to a single fact : they frequently annotate negation scope as a single span of text delimited by <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>. For negation scopes not of this form, detection accuracy is low and under-sampling the easy training examples does not substantially improve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. We demonstrate that this is partly an artifact of annotation guidelines, and we argue that future negation scope annotation efforts should focus on these more difficult cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2011 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2011/>MT / IE : Cross-lingual Open Information Extraction with Neural Sequence-to-Sequence Models<span class=acl-fixed-case>MT</span>/<span class=acl-fixed-case>IE</span>: Cross-lingual Open Information Extraction with Neural Sequence-to-Sequence Models</a></strong><br><a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2011><div class="card-body p-3 small">Cross-lingual information extraction is the task of distilling facts from foreign language (e.g. Chinese text) into representations in another language that is preferred by the user (e.g. English tuples). Conventional pipeline solutions decompose the task as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> followed by <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> (or vice versa). We propose a joint solution with a neural sequence model, and show that it outperforms the pipeline in a cross-lingual open information extraction setting by 1-4 BLEU and 0.5-0.8 <a href=https://en.wikipedia.org/wiki/F-number>F1</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2012/>Learning to Negate Adjectives with Bilinear Models</a></strong><br><a href=/people/l/laura-rimell/>Laura Rimell</a>
|
<a href=/people/a/amandla-mabona/>Amandla Mabona</a>
|
<a href=/people/l/luana-bulat/>Luana Bulat</a>
|
<a href=/people/d/douwe-kiela/>Douwe Kiela</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2012><div class="card-body p-3 small">We learn a <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> that negates <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> by predicting an adjective&#8217;s antonym in an arbitrary word embedding model. We show that both <a href=https://en.wikipedia.org/wiki/Linear_model>linear models</a> and <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> improve on this task when they have access to a vector representing the <a href=https://en.wikipedia.org/wiki/Semantic_domain>semantic domain</a> of the input word, e.g. a centroid of temperature words when predicting the antonym of &#8216;cold&#8217;. We introduce a continuous class-conditional bilinear neural network which is able to negate <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2013 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2013/>Instances and concepts in distributional space</a></strong><br><a href=/people/g/gemma-boleda/>Gemma Boleda</a>
|
<a href=/people/a/abhijeet-gupta/>Abhijeet Gupta</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2013><div class="card-body p-3 small">Instances (Mozart) are ontologically distinct from concepts or classes (composer). Natural language encompasses both, but instances have received comparatively little attention in <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a>. Our results show that instances and concepts differ in their <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional properties</a>. We also establish that <a href=https://en.wikipedia.org/wiki/Instance_(computer_science)>instantiation detection (Mozart composer)</a> is generally easier than hypernymy detection (chemist scientist), and that results on the influence of input representation do not transfer from <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponymy</a> to <a href=https://en.wikipedia.org/wiki/Instance_(computer_science)>instantiation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2014/>Is this a Child, a Girl or a Car? Exploring the Contribution of Distributional Similarity to Learning Referential Word Meanings</a></strong><br><a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2014><div class="card-body p-3 small">There has recently been a lot of work trying to use images of referents of words for improving vector space meaning representations derived from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>. We investigate the opposite direction, as it were, trying to improve visual word predictors that identify objects in images, by exploiting distributional similarity information during <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>. We show that for certain <a href=https://en.wikipedia.org/wiki/Word>words</a> (such as entry-level nouns or hypernyms), we can indeed learn better referential word meanings by taking into account their semantic similarity to other words. For other words, there is no or even a detrimental effect, compared to a learning setup that presents even semantically related objects as negative instances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2015 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2015/>The Semantic Proto-Role Linking Model</a></strong><br><a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/k/kyle-rawlins/>Kyle Rawlins</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2015><div class="card-body p-3 small">We propose the semantic proto-role linking model, which jointly induces both predicate-specific semantic roles and predicate-general semantic proto-roles based on semantic proto-role property likelihood judgments. We use this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to empirically evaluate Dowty&#8217;s thematic proto-role linking theory.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2016/>The Language of Place : Semantic Value from <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>Geospatial Context</a></a></strong><br><a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2016><div class="card-body p-3 small">There is a relationship between what we say and where we say it. Word embeddings are usually trained assuming that semantically-similar words occur within the same textual contexts. We investigate the extent to which <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantically-similar words</a> occur within the same <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>geospatial contexts</a>. We enrich a corpus of geolocated Twitter posts with physical data derived from <a href=https://en.wikipedia.org/wiki/Google_Places>Google Places</a> and <a href=https://en.wikipedia.org/wiki/OpenStreetMap>OpenStreetMap</a>, and train word embeddings using the resulting geospatial contexts. Intrinsic evaluation of the resulting vectors shows that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>geographic context</a> alone does provide useful information about <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2018/>A Rich Morphological Tagger for <a href=https://en.wikipedia.org/wiki/English_language>English</a> : Exploring the Cross-Linguistic Tradeoff Between <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a> and <a href=https://en.wikipedia.org/wiki/Syntax>Syntax</a><span class=acl-fixed-case>E</span>nglish: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax</a></strong><br><a href=/people/c/christo-kirov/>Christo Kirov</a>
|
<a href=/people/j/john-sylak-glassman/>John Sylak-Glassman</a>
|
<a href=/people/r/rebecca-knowles/>Rebecca Knowles</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/m/matt-post/>Matt Post</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2018><div class="card-body p-3 small">A traditional claim in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> is that all human languages are equally expressiveable to convey the same wide range of meanings. Morphologically rich languages, such as <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, rely on overt inflectional and derivational morphology to convey many semantic distinctions. Languages with comparatively limited morphology, such as <a href=https://en.wikipedia.org/wiki/English_language>English</a>, should be able to accomplish the same using a combination of syntactic and contextual cues. We capitalize on this idea by training a tagger for <a href=https://en.wikipedia.org/wiki/English_language>English</a> that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. The high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2019" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2019/>Context-Aware Prediction of Derivational Word-forms</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2019><div class="card-body p-3 small">Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose a new task of predicting the <a href=https://en.wikipedia.org/wiki/Derivation_(differential_algebra)>derivational form</a> of a given base-form lemma that is appropriate for a given context. We present an encoder-decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under lexicon agnostic setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2020/>Comparing Character-level Neural Language Models Using a Lexical Decision Task</a></strong><br><a href=/people/g/gael-le-godais/>Gaël Le Godais</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2020><div class="card-body p-3 small">What is the information captured by neural network models of language? We address this question in the case of character-level recurrent neural language models. These <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not have explicit word representations ; do they acquire implicit ones? We assess the lexical capacity of a network using the <a href=https://en.wikipedia.org/wiki/Lexical_decision_task>lexical decision task</a> common in <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a> : the <a href=https://en.wikipedia.org/wiki/System>system</a> is required to decide whether or not a string of characters forms a word. We explore how accuracy on this task is affected by the architecture of the <a href=https://en.wikipedia.org/wiki/Telecommunications_network>network</a>, focusing on cell type (LSTM vs. SRN), depth and width. We also compare these architectural properties to a simple count of the parameters of the <a href=https://en.wikipedia.org/wiki/Network_analysis_(electrical_circuits)>network</a>. The overall number of parameters in the <a href=https://en.wikipedia.org/wiki/Flow_network>network</a> turns out to be the most important predictor of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> ; in particular, there is little evidence that deeper networks are beneficial for this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2021/>Optimal encoding !-Information Theory constrains article omission in newspaper headlines</a></strong><br><a href=/people/r/robin-lemke/>Robin Lemke</a>
|
<a href=/people/e/eva-horch/>Eva Horch</a>
|
<a href=/people/i/ingo-reich/>Ingo Reich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2021><div class="card-body p-3 small">In this paper we pursue the hypothesis that the distribution of article omission specifically is constrained by principles of <a href=https://en.wikipedia.org/wiki/Information_theory>Information Theory</a> (Shannon 1948). In particular, <a href=https://en.wikipedia.org/wiki/Information_theory>Information Theory</a> predicts a stronger preference for article omission before nouns which are relatively unpredictable in context of the preceding words. We investigated article omission in <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_Germany>German newspaper headlines</a> with a corpus and acceptability rating study. Both support our hypothesis : Articles are inserted more often before unpredictable nouns and subjects perceive article omission before predictable nouns as more well-formed than before unpredictable ones. This suggests that <a href=https://en.wikipedia.org/wiki/Information_theory>information theoretic principles</a> constrain the distribution of article omission in <a href=https://en.wikipedia.org/wiki/Headline>headlines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2022 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2022/>A Computational Analysis of the Language of Drug Addiction</a></strong><br><a href=/people/c/carlo-strapparava/>Carlo Strapparava</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2022><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational analysis</a> of the language of drug users when talking about their drug experiences. We introduce a new dataset of over 4,000 descriptions of experiences reported by users of four main drug types, and show that we can predict with an F1-score of up to 88 % the drug behind a certain experience. We also perform an analysis of the dominant psycholinguistic processes and dominant emotions associated with each drug type, which sheds light on the characteristics of drug users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2023/>A Practical Perspective on Latent Structured Prediction for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a></a></strong><br><a href=/people/i/iryna-haponchyk/>Iryna Haponchyk</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2023><div class="card-body p-3 small">Latent structured prediction theory proposes powerful methods such as Latent Structural SVM (LSSVM), which can potentially be very appealing for coreference resolution (CR). In contrast, only small work is available, mainly targeting the latent structured perceptron (LSP). In this paper, we carried out a practical study comparing for the first time <a href=https://en.wikipedia.org/wiki/Educational_technology>online learning</a> with LSSVM. We analyze the intricacies that may have made initial attempts to use LSSVM fail, i.e., a huge <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a> and much lower accuracy produced by Kruskal&#8217;s spanning tree algorithm. In this respect, we also propose a new effective feature selection approach for improving system efficiency. The results show that LSP, if correctly parameterized, produces the same performance as LSSVM, being much more efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2024/>On the Need of <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross Validation</a> for Discourse Relation Classification</a></strong><br><a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2024><div class="card-body p-3 small">The task of implicit discourse relation classification has received increased attention in recent years, including two CoNNL shared tasks on the topic. Existing machine learning models for the task train on sections 2-21 of the PDTB and test on section 23, which includes a total of 761 implicit discourse relations. In this paper, we&#8217;d like to make a methodological point, arguing that the standard test set is too small to draw conclusions about whether the inclusion of certain features constitute a genuine improvement, or whether one got lucky with some properties of the <a href=https://en.wikipedia.org/wiki/Test_set>test set</a>, and argue for the adoption of <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>cross validation</a> for the discourse relation classification task by the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2025" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2025/>Using the Output Embedding to Improve <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a></a></strong><br><a href=/people/o/ofir-press/>Ofir Press</a>
|
<a href=/people/l/lior-wolf/>Lior Wolf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2025><div class="card-body p-3 small">We study the topmost weight matrix of neural network language models. We show that this <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a> constitutes a valid <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. When training <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a>, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2026/>Identifying beneficial task relations for <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> in deep neural networks</a></strong><br><a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2026><div class="card-body p-3 small">Multi-task learning (MTL) in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a>. While it has brought significant improvements in a number of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2027 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2027/>Effective search space reduction for spell correction using character neural embeddings</a></strong><br><a href=/people/h/harshit-pande/>Harshit Pande</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2027><div class="card-body p-3 small">We present a novel, unsupervised, and distance measure agnostic method for search space reduction in spell correction using neural character embeddings. The embeddings are learned by skip-gram word2vec training on sequences generated from dictionary words in a phonetic information-retentive manner. We report a very high performance in terms of both success rates and reduction of search space on the Birkbeck spelling error corpus. To the best of our knowledge, this is the first application of <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> to spell correction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2028/>Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis</a></strong><br><a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2028><div class="card-body p-3 small">The popular skip-gram model induces <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> by exploiting the signal from word-context coocurrence. We offer a new interpretation of <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a> based on exponential family PCA-a form of matrix factorization to generalize the <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram model</a> to tensor factorization. In turn, this lets us train <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> through richer higher-order coocurrences, e.g., triples that include positional information (to incorporate syntax) or <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological information</a> (to share parameters across related words). We experiment on 40 languages and show our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> improves upon <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2029/>Latent Variable Dialogue Models and their Diversity</a></strong><br><a href=/people/k/kris-cao/>Kris Cao</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2029><div class="card-body p-3 small">We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the &#8216;boring output&#8217; issue of deterministic dialogue models. Experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2030 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2030/>Age Group Classification with Speech and Metadata Multimodality Fusion</a></strong><br><a href=/people/d/denys-katerenchuk/>Denys Katerenchuk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2030><div class="card-body p-3 small">Children comprise a significant proportion of TV viewers and it is worthwhile to customize the experience for them. However, identifying who is a child in the audience can be a challenging task. We present initial studies of a novel <a href=https://en.wikipedia.org/wiki/Methodology>method</a> which combines utterances with user metadata. In particular, we develop an ensemble of different machine learning techniques on different subsets of data to improve child detection. Our initial results show an 9.2 % absolute improvement over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, leading to a state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2031 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2031/>Automatically augmenting an emotion dataset improves <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> using audio</a></strong><br><a href=/people/e/egor-lakomkin/>Egor Lakomkin</a>
|
<a href=/people/c/cornelius-weber/>Cornelius Weber</a>
|
<a href=/people/s/stefan-wermter/>Stefan Wermter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2031><div class="card-body p-3 small">In this work, we tackle a problem of <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech emotion classification</a>. One of the issues in the area of affective computation is that the amount of annotated data is very limited. On the other hand, the number of ways that the same emotion can be expressed verbally is enormous due to variability between speakers. This is one of the factors that limits performance and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>. We propose a simple method that extracts <a href=https://en.wikipedia.org/wiki/Sampling_(music)>audio samples</a> from <a href=https://en.wikipedia.org/wiki/Film>movies</a> using textual sentiment analysis. As a result, it is possible to automatically construct a larger dataset of <a href=https://en.wikipedia.org/wiki/Sampling_(signal_processing)>audio samples</a> with positive, negative emotional and neutral speech. We show that pretraining <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a> on such a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> yields better results on the challenging EmotiW corpus. This experiment shows a potential benefit of combining textual sentiment analysis with <a href=https://en.wikipedia.org/wiki/Voice_(phonetics)>vocal information</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2033 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2033/>Hybrid Dialog State Tracker with ASR Features<span class=acl-fixed-case>ASR</span> Features</a></strong><br><a href=/people/m/miroslav-vodolan/>Miroslav Vodolán</a>
|
<a href=/people/r/rudolf-kadlec/>Rudolf Kadlec</a>
|
<a href=/people/j/jan-kleindienst/>Jan Kleindienst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2033><div class="card-body p-3 small">This paper presents a hybrid dialog state tracker enhanced by trainable Spoken Language Understanding (SLU) for slot-filling dialog systems. Our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> is inspired by previously proposed neural-network-based belief-tracking systems. In addition, we extended some parts of our <a href=https://en.wikipedia.org/wiki/Modular_programming>modular architecture</a> with <a href=https://en.wikipedia.org/wiki/Differentiable_function>differentiable rules</a> to allow <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end training</a>. We hypothesize that these rules allow our <a href=https://en.wikipedia.org/wiki/Music_tracker>tracker</a> to generalize better than pure machine-learning based systems. For evaluation, we used the Dialog State Tracking Challenge (DSTC) 2 dataset-a popular belief tracking testbed with dialogs from restaurant information system. To our knowledge, our hybrid tracker sets a new state-of-the-art result in three out of four categories within the DSTC2.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2034 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2034/>Morphological Analysis without Expert Annotation</a></strong><br><a href=/people/g/garrett-nicolai/>Garrett Nicolai</a>
|
<a href=/people/g/grzegorz-kondrak/>Grzegorz Kondrak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2034><div class="card-body p-3 small">The task of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analysis</a> is to produce a complete list of lemma+tag analyses for a given <a href=https://en.wikipedia.org/wiki/Logical_form>word-form</a>. We propose a discriminative string transduction approach which exploits plain inflection tables and raw text corpora, thus obviating the need for expert annotation. Experiments on four languages demonstrate that our system has much higher coverage than a hand-engineered FST analyzer, and is more accurate than a state-of-the-art morphological tagger.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2035 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2035/>Morphological Analysis of the Dravidian Language Family<span class=acl-fixed-case>D</span>ravidian Language Family</a></strong><br><a href=/people/a/arun-kumar/>Arun Kumar</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/l/lluis-padro/>Lluís Padró</a>
|
<a href=/people/a/antoni-oliver/>Antoni Oliver</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2035><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Dravidian_languages>Dravidian languages</a> are one of the most widely spoken language families in the world, yet there are very few annotated resources available to NLP researchers. To remedy this, we create DravMorph, a <a href=https://en.wikipedia.org/wiki/Speech_corpus>corpus</a> annotated for morphological segmentation and <a href=https://en.wikipedia.org/wiki/Part_of_speech>part-of-speech</a>. Additionally, we exploit novel features and higher-order models to set state-of-the-art results on these corpora on both tasks, beating techniques proposed in the literature by as much as 4 points in segmentation F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2036 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2036/>BabelDomains : Large-Scale Domain Labeling of Lexical Resources<span class=acl-fixed-case>B</span>abel<span class=acl-fixed-case>D</span>omains: Large-Scale Domain Labeling of Lexical Resources</a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2036><div class="card-body p-3 small">In this paper we present BabelDomains, a unified resource which provides lexical items with information about domains of knowledge. We propose an automatic method that uses knowledge from various lexical resources, exploiting both distributional and graph-based clues, to accurately propagate domain information. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> intrinsically on two lexical resources (WordNet and BabelNet), achieving a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> over 80 % in both cases. Finally, we show the potential of BabelDomains in a supervised learning setting, clustering training data by domain for hypernym discovery.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2037 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2037" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2037/>JFLEG : A Fluency Corpus and Benchmark for Grammatical Error Correction<span class=acl-fixed-case>JFLEG</span>: A Fluency Corpus and Benchmark for Grammatical Error Correction</a></strong><br><a href=/people/c/courtney-napoles/>Courtney Napoles</a>
|
<a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2037><div class="card-body p-3 small">We present a new parallel corpus, JHU FLuency-Extended GUG corpus (JFLEG) for developing and evaluating grammatical error correction (GEC). Unlike other <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct <a href=https://en.wikipedia.org/wiki/Error_(linguistics)>grammatical errors</a> but also make the original text more native sounding. We describe the types of corrections made and benchmark four leading GEC systems on this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, identifying specific areas in which they do well and how they can improve. JFLEG fulfills the need for a new gold standard to properly assess the current state of <a href=https://en.wikipedia.org/wiki/General_Electric_Company>GEC</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2039 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2039/>The Parallel Meaning Bank : Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations<span class=acl-fixed-case>P</span>arallel <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>B</span>ank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations</a></strong><br><a href=/people/l/lasha-abzianidze/>Lasha Abzianidze</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/k/kilian-evang/>Kilian Evang</a>
|
<a href=/people/h/hessel-haagsma/>Hessel Haagsma</a>
|
<a href=/people/r/rik-van-noord/>Rik van Noord</a>
|
<a href=/people/p/pierre-ludmann/>Pierre Ludmann</a>
|
<a href=/people/d/duc-duy-nguyen/>Duc-Duy Nguyen</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2039><div class="card-body p-3 small">The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four languages (English, German, Italian, and Dutch). Our approach is based on cross-lingual projection : automatically produced (and manually corrected) semantic annotations for English sentences are mapped onto their word-aligned translations, assuming that the translations are meaning-preserving. The <a href=https://en.wikipedia.org/wiki/Semantic_annotation>semantic annotation</a> consists of five main steps : (i) segmentation of the text in sentences and lexical items ; (ii) syntactic parsing with Combinatory Categorial Grammar ; (iii) universal semantic tagging ; (iv) symbolization ; and (v) compositional semantic analysis based on <a href=https://en.wikipedia.org/wiki/Discourse_representation_theory>Discourse Representation Theory</a>. These steps are performed using <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a> trained in a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised manner</a>. The employed annotation models are all language-neutral. Our first results are promising.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2040 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2040/>Cross-lingual tagger evaluation without test data</a></strong><br><a href=/people/z/zeljko-agic/>Željko Agić</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2040><div class="card-body p-3 small">We address the challenge of cross-lingual POS tagger evaluation in absence of manually annotated test data. We put forth and evaluate two dictionary-based metrics. On the tasks of accuracy prediction and system ranking, we reveal that these metrics are reliable enough to approximate test set-based evaluation, and at the same time lean enough to support assessment for truly low-resource languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2042 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2042/>The Content Types Dataset : a New Resource to Explore Semantic and Functional Characteristics of Texts</a></strong><br><a href=/people/r/rachele-sprugnoli/>Rachele Sprugnoli</a>
|
<a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/g/giovanni-moretti/>Giovanni Moretti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2042><div class="card-body p-3 small">This paper presents a new resource, called Content Types Dataset, to promote the analysis of texts as a composition of units with specific semantic and functional roles. By developing this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we also introduce a new NLP task for the automatic classification of Content Types. The annotation scheme and the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> are described together with two sets of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2043 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2043/>Continuous N-gram Representations for Authorship Attribution</a></strong><br><a href=/people/y/yunita-sari/>Yunita Sari</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/m/mark-stevenson/>Mark Stevenson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2043><div class="card-body p-3 small">This paper presents work on using continuous representations for <a href=https://en.wikipedia.org/wiki/Attribution_(copyright)>authorship attribution</a>. In contrast to previous work, which uses discrete feature representations, our model learns continuous representations for <a href=https://en.wikipedia.org/wiki/N-gram>n-gram features</a> via a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> jointly with the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification layer</a>. Experimental results demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on two datasets, while producing comparable results on the remaining two.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2044 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2044" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2044/>Reconstructing the house from the ad : <a href=https://en.wikipedia.org/wiki/Structured_prediction>Structured prediction</a> on real estate classifieds</a></strong><br><a href=/people/g/giannis-bekoulis/>Giannis Bekoulis</a>
|
<a href=/people/j/johannes-deleu/>Johannes Deleu</a>
|
<a href=/people/t/thomas-demeester/>Thomas Demeester</a>
|
<a href=/people/c/chris-develder/>Chris Develder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2044><div class="card-body p-3 small">In this paper, we address the (to the best of our knowledge) new problem of extracting a structured description of real estate properties from their natural language descriptions in <a href=https://en.wikipedia.org/wiki/Classified_advertising>classifieds</a>. We survey and present several models to (a) identify important <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> of a property (e.g.,rooms) from classifieds and (b) structure them into a tree format, with the <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entities</a> as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> and <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> representing a <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>part-of relation</a>. Experiments show that a graph-based system deriving the <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a> from an initially fully connected entity graph, outperforms a transition-based system starting from only the entity nodes, since it better reconstructs the <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2046 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2046/>Improving ROUGE for Timeline Summarization<span class=acl-fixed-case>ROUGE</span> for Timeline Summarization</a></strong><br><a href=/people/s/sebastian-martschat/>Sebastian Martschat</a>
|
<a href=/people/k/katja-markert/>Katja Markert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2046><div class="card-body p-3 small">Current evaluation metrics for timeline summarization either ignore the temporal aspect of the task or require strict date matching. We introduce variants of ROUGE that allow alignment of daily summaries via temporal distance or <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>. We argue for the suitability of these variants in a theoretical analysis and demonstrate it in a battery of task-specific tests.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2047 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2047/>Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization</a></strong><br><a href=/people/j/jun-suzuki/>Jun Suzuki</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2047><div class="card-body p-3 small">This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and control the output words based on the estimation in the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a>. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2048 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2048/>To Sing like a Mockingbird</a></strong><br><a href=/people/l/lorenzo-gatti/>Lorenzo Gatti</a>
|
<a href=/people/g/gozde-ozbal/>Gözde Özbal</a>
|
<a href=/people/o/oliviero-stock/>Oliviero Stock</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2048><div class="card-body p-3 small">Musical parody, i.e. the act of changing the lyrics of an existing and very well-known song, is a commonly used technique for creating catchy advertising tunes and for mocking people or events. Here we describe a <a href=https://en.wikipedia.org/wiki/System>system</a> for automatically producing a <a href=https://en.wikipedia.org/wiki/Parody_music>musical parody</a>, starting from a <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus of songs</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> can automatically identify characterizing words and concepts related to a novel text, which are taken from the <a href=https://en.wikipedia.org/wiki/News>daily news</a>. These concepts are then used as seeds to appropriately replace part of the original <a href=https://en.wikipedia.org/wiki/Lyrics>lyrics</a> of a song, using <a href=https://en.wikipedia.org/wiki/Metre_(poetry)>metrical</a>, <a href=https://en.wikipedia.org/wiki/Rhyme>rhyming</a> and lexical constraints. Finally, the <a href=https://en.wikipedia.org/wiki/Parody>parody</a> can be sung with a singing speech synthesizer, with no intervention from the user.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2049 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2049/>K-best Iterative Viterbi Parsing<span class=acl-fixed-case>V</span>iterbi Parsing</a></strong><br><a href=/people/k/katsuhiko-hayashi/>Katsuhiko Hayashi</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2049><div class="card-body p-3 small">This paper presents an efficient and optimal parsing algorithm for probabilistic context-free grammars (PCFGs). To achieve faster <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>, our proposal employs a pruning technique to reduce unnecessary edges in the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a>. The key is to conduct repetitively Viterbi inside and outside parsing, while gradually expanding the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> to efficiently compute <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic bounds</a> used for <a href=https://en.wikipedia.org/wiki/Parsing>pruning</a>. Our experimental results using the English Penn Treebank corpus show that the proposed <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is faster than the standard CKY parsing algorithm. In addition, we also show how to extend this <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to extract k-best Viterbi parse trees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2050 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2050/>PP Attachment : Where do We Stand?<span class=acl-fixed-case>PP</span> Attachment: Where do We Stand?</a></strong><br><a href=/people/d/daniel-de-kok/>Daniël de Kok</a>
|
<a href=/people/j/jianqiang-ma/>Jianqiang Ma</a>
|
<a href=/people/c/corina-dima/>Corina Dima</a>
|
<a href=/people/e/erhard-hinrichs/>Erhard Hinrichs</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2050><div class="card-body p-3 small">Prepostitional phrase (PP) attachment is a well known challenge to <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. In this paper, we combine the insights of different works, namely : (1) treating PP attachment as a classification task with an arbitrary number of attachment candidates ; (2) using auxiliary distributions to augment the data beyond the hand-annotated training set ; (3) using topological fields to get information about the distribution of PP attachment throughout clauses and (4) using state-of-the-art techniques such as word embeddings and neural networks. We show that jointly using these <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>techniques</a> leads to substantial improvements. We also conduct a qualitative analysis to gauge where the ceiling of the task is in a realistic setup.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2051 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2051/>Do n’t Stop Me Now ! Using Global Dynamic Oracles to Correct Training Biases of Transition-Based Dependency Parsers</a></strong><br><a href=/people/l/lauriane-aufrant/>Lauriane Aufrant</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2051><div class="card-body p-3 small">This paper formalizes a sound extension of dynamic oracles to global training, in the frame of transition-based dependency parsers. By dispensing with the pre-computation of references, this extension widens the training strategies that can be entertained for such parsers ; we show this by revisiting two standard training procedures, early-update and max-violation, to correct some of their search space sampling biases. Experimentally, on the SPMRL treebanks, this improvement increases the similarity between the train and test distributions and yields performance improvements up to 0.7 UAS, without any <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>computation overhead</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2052 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2052/>Joining Hands : Exploiting Monolingual Treebanks for Parsing of Code-mixing Data</a></strong><br><a href=/people/i/irshad-bhat/>Irshad Bhat</a>
|
<a href=/people/r/riyaz-ahmad-bhat/>Riyaz A. Bhat</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2052><div class="card-body p-3 small">In this paper, we propose efficient and less resource-intensive strategies for parsing of code-mixed data. These strategies are not constrained by in-domain annotations, rather they leverage pre-existing monolingual annotated resources for training. We show that these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can produce significantly better results as compared to an <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>informed baseline</a>. Due to lack of an evaluation set for code-mixed structures, we also present a data set of 450 Hindi and English code-mixed tweets of Hindi multilingual speakers for evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2053 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2053" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2053/>Multilingual Lexicalized Constituency Parsing with Word-Level Auxiliary Tasks</a></strong><br><a href=/people/m/maximin-coavoux/>Maximin Coavoux</a>
|
<a href=/people/b/benoit-crabbe/>Benoît Crabbé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2053><div class="card-body p-3 small">We introduce a constituency parser based on a bi-LSTM encoder adapted from recent work (Cross and Huang, 2016b ; Kiperwasser and Goldberg, 2016), which can incorporate a lower level character biLSTM (Ballesteros et al., 2015 ; Plank et al., 2016). We model two important interfaces of constituency parsing with auxiliary tasks supervised at the word level : (i) part-of-speech (POS) and morphological tagging, (ii) functional label prediction. On the SPMRL dataset, our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> obtains above state-of-the-art results on constituency parsing without requiring either predicted POS or morphological tags, and outputs labelled dependency trees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2054 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2054/>Be Precise or Fuzzy : Learning the Meaning of Cardinals and Quantifiers from Vision</a></strong><br><a href=/people/s/sandro-pezzelle/>Sandro Pezzelle</a>
|
<a href=/people/m/marco-marelli/>Marco Marelli</a>
|
<a href=/people/r/raffaella-bernardi/>Raffaella Bernardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2054><div class="card-body p-3 small">People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or <a href=https://en.wikipedia.org/wiki/Quantifier_(linguistics)>natural language quantifiers</a> (e.g. few, most, all). In humans, these two <a href=https://en.wikipedia.org/wiki/Process_(anatomy)>processes</a> underlie fairly different <a href=https://en.wikipedia.org/wiki/Cognition>cognitive and neural mechanisms</a>. Inspired by this evidence, the present study proposes two <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for learning the objective meaning of cardinals and <a href=https://en.wikipedia.org/wiki/Quantifier_(logic)>quantifiers</a> from visual scenes containing multiple objects. We show that a model capitalizing on a &#8216;fuzzy&#8217; measure of similarity is effective for learning <a href=https://en.wikipedia.org/wiki/Quantifier_(logic)>quantifiers</a>, whereas the learning of exact cardinals is better accomplished when information about number is provided.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2055 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2055/>Improving a Strong Neural Parser with Conjunction-Specific Features</a></strong><br><a href=/people/j/jessica-ficler/>Jessica Ficler</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2055><div class="card-body p-3 small">While dependency parsers reach very high overall accuracy, some dependency relations are much harder than others. In particular, dependency parsers perform poorly in coordination construction (i.e., correctly attaching the conj relation). We extend a state-of-the-art dependency parser with conjunction-specific features, focusing on the similarity between the conjuncts head words. Training the extended <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> yields an improvement in conj attachment as well as in overall dependency parsing accuracy on the Stanford dependency conversion of the Penn TreeBank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2057 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2057/>Improving Evaluation of Document-level Machine Translation Quality Estimation</a></strong><br><a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/q/qingsong-ma/>Qingsong Ma</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/c/carla-parra-escartin/>Carla Parra</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2057><div class="card-body p-3 small">Meaningful conclusions about the relative performance of NLP systems are only possible if the <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold standard</a> employed in a given <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> is both valid and reliable. In this paper, we explore the validity of human annotations currently employed in the evaluation of document-level quality estimation for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation (MT)</a>. We demonstrate the degree to which MT system rankings are dependent on weights employed in the construction of the <a href=https://en.wikipedia.org/wiki/Gold_standard>gold standard</a>, before proposing direct human assessment as a valid alternative. Experiments show direct assessment (DA) scores for documents to be highly reliable, achieving a correlation of above 0.9 in a self-replication experiment, in addition to a substantial estimated cost reduction through quality controlled crowd-sourcing. The original <a href=https://en.wikipedia.org/wiki/Gold_standard>gold standard</a> based on <a href=https://en.wikipedia.org/wiki/Post-editing>post-edits</a> incurs a 1020 times greater cost than DA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2058 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2058/>Neural Machine Translation by Minimising the Bayes-risk with Respect to Syntactic Translation Lattices<span class=acl-fixed-case>B</span>ayes-risk with Respect to Syntactic Translation Lattices</a></strong><br><a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/a/adria-de-gispert/>Adrià de Gispert</a>
|
<a href=/people/e/eva-hasler/>Eva Hasler</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2058><div class="card-body p-3 small">We present a novel scheme to combine <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> with traditional <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation (SMT)</a>. Our approach borrows ideas from linearised lattice minimum Bayes-risk decoding for <a href=https://en.wikipedia.org/wiki/Signal-to-noise_ratio>SMT</a>. The NMT score is combined with the <a href=https://en.wikipedia.org/wiki/Bayes_risk>Bayes-risk</a> of the <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translation</a> according the SMT lattice. This makes our approach much more flexible than n-best list or lattice rescoring as the neural decoder is not restricted to the SMT search space. We show an efficient and simple way to integrate risk estimation into the NMT decoder which is suitable for word-level as well as subword-unit-level NMT. We test our method on <a href=https://en.wikipedia.org/wiki/German_language>English-German</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese-English</a> and report significant gains over lattice rescoring on several data sets for both single and ensembled NMT. The MBR decoder produces entirely new hypotheses far beyond simply rescoring the SMT search space or fixing UNKs in the NMT output.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2059 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2059/>Producing Unseen <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>Morphological Variants</a> in Statistical Machine Translation</a></strong><br><a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/ales-tamchyna/>Aleš Tamchyna</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2059><div class="card-body p-3 small">Translating into <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphologically rich languages</a> is difficult. Although the coverage of lemmas may be reasonable, many morphological variants can not be learned from the training data. We present a statistical translation system that is able to produce these <a href=https://en.wikipedia.org/wiki/Inflection>inflected word forms</a>. Different from most previous work, we do not separate morphological prediction from <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a> into two consecutive steps. Our approach is novel in that it is integrated in decoding and takes advantage of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> from both the source language and the target language sides.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2060 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2060" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2060/>How Grammatical is Character-level Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs<span class=acl-fixed-case>MT</span> Quality with Contrastive Translation Pairs</a></strong><br><a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2060><div class="card-body p-3 small">Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as <a href=https://en.wikipedia.org/wiki/Agreement_(linguistics)>agreement</a> over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of <a href=https://en.wikipedia.org/wiki/Error>error</a>. We present LingEval97, a large-scale data set of 97000 contrastive translation pairs based on the WMT English-German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2061 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2061/>Neural Machine Translation with Recurrent Attention Modeling</a></strong><br><a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/z/zhiting-hu/>Zhiting Hu</a>
|
<a href=/people/y/yuntian-deng/>Yuntian Deng</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/a/alex-smola/>Alex Smola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2061><div class="card-body p-3 small">Knowing which words have been attended to in previous time steps while generating a <a href=https://en.wikipedia.org/wiki/Translation>translation</a> is a rich source of information for predicting what words will be attended to in the future. We improve upon the attention model of Bahdanau et al. (2014) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> per input word. This <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> easily captures informative features, such as <a href=https://en.wikipedia.org/wiki/Fertility>fertility</a> and regularities in relative distortion. In experiments, we show our parameterization of attention improves translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2062 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2062/>Inducing Embeddings for Rare and Unseen Words by Leveraging Lexical Resources</a></strong><br><a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2062><div class="card-body p-3 small">We put forward an approach that exploits the knowledge encoded in lexical resources in order to induce representations for words that were not encountered frequently during training. Our approach provides an advantage over the past work in that it enables vocabulary expansion not only for <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological variations</a>, but also for infrequent domain specific terms. We performed evaluations in different settings, showing that the technique can provide consistent improvements on multiple benchmarks across domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2063 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2063/>Large-scale evaluation of dependency-based DSMs : Are they worth the effort?<span class=acl-fixed-case>DSM</span>s: Are they worth the effort?</a></strong><br><a href=/people/g/gabriella-lapesa/>Gabriella Lapesa</a>
|
<a href=/people/s/stefan-evert/>Stefan Evert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2063><div class="card-body p-3 small">This paper presents a large-scale evaluation study of dependency-based distributional semantic models. We evaluate dependency-filtered and dependency-structured DSMs in a number of standard semantic similarity tasks, systematically exploring their parameter space in order to give them a fair shot against window-based models. Our results show that properly tuned window-based DSMs still outperform the dependency-based models in most tasks. There appears to be little need for the language-dependent resources and <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> associated with <a href=https://en.wikipedia.org/wiki/Syntactic_analysis>syntactic analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2064 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2064/>How Well Can We Predict Hypernyms from Word Embeddings? A Dataset-Centric Analysis</a></strong><br><a href=/people/i/ivan-sanchez/>Ivan Sanchez</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2064><div class="card-body p-3 small">One key property of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> currently under study is their capacity to encode <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a>. Previous works have used <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a> to recover hypernymy structures from <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. However, the overall results do not clearly show how well we can recover such <a href=https://en.wikipedia.org/wiki/Mathematical_structure>structures</a>. We conduct the first dataset-centric analysis that shows how only the Baroni dataset provides consistent results. We empirically show that a possible reason for its good performance is its alignment to dimensions specific of <a href=https://en.wikipedia.org/wiki/Hypernymy>hypernymy</a> : generality and similarity</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2065.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2065 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2065 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2065/>Cross-Lingual Syntactically Informed Distributed Word Representations</a></strong><br><a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2065><div class="card-body p-3 small">We develop a novel cross-lingual word representation model which injects syntactic information through dependency-based contexts into a shared cross-lingual word vector space. The model, termed CL-DepEmb, is based on the following assumptions : (1) dependency relations are largely language-independent, at least for related languages and prominent dependency links such as direct objects, as evidenced by the Universal Dependencies project ; (2) word translation equivalents take similar grammatical roles in a sentence and are therefore substitutable within their syntactic contexts. Experiments with several language pairs on word similarity and bilingual lexicon induction, two fundamental semantic tasks emphasising <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>, suggest the usefulness of the proposed syntactically informed cross-lingual word vector spaces. Improvements are observed in both tasks over standard cross-lingual offline mapping baselines trained using the same setup and an equal level of bilingual supervision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2066 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/E17-2066.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/E17-2066/>Using Word Embedding for Cross-Language Plagiarism Detection</a></strong><br><a href=/people/j/jeremy-ferrero/>Jérémy Ferrero</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/f/frederic-agnes/>Frédéric Agnès</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2066><div class="card-body p-3 small">This paper proposes to use distributed representation of words (word embeddings) in cross-language textual similarity detection. The main contributions of this paper are the following : (a) we introduce new cross-language similarity detection methods based on distributed representation of words ; (b) we combine the different methods proposed to verify their complementarity and finally obtain an overall F1 score of 89.15 % for English-French similarity detection at chunk level (88.5 % at sentence level) on a very challenging corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2067 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2067" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2067/>The Interplay of <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a> in Word Embeddings</a></strong><br><a href=/people/o/oded-avraham/>Oded Avraham</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2067><div class="card-body p-3 small">We explore the ability of word embeddings to capture both semantic and morphological similarity, as affected by the different types of linguistic properties (surface form, <a href=https://en.wikipedia.org/wiki/Lemma_(morphology)>lemma</a>, morphological tag) used to compose the representation of each word. We train several <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, where each uses a different subset of these properties to compose its <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a>. By evaluating the models on semantic and morphological measures, we reveal some useful insights on the relationship between <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2068 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2068" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2068/>Bag of Tricks for Efficient Text Classification</a></strong><br><a href=/people/a/armand-joulin/>Armand Joulin</a>
|
<a href=/people/e/edouard-grave/>Edouard Grave</a>
|
<a href=/people/p/piotr-bojanowski/>Piotr Bojanowski</a>
|
<a href=/people/t/tomas-mikolov/>Tomas Mikolov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2068><div class="card-body p-3 small">This paper explores a simple and efficient <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> on more than one billion words in less than ten minutes using a standard <a href=https://en.wikipedia.org/wiki/Multi-core_processor>multicore CPU</a>, and classify half a million sentences among 312 K classes in less than a minute.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2069.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2069 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2069 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2069/>Pulling Out the Stops : Rethinking Stopword Removal for Topic Models</a></strong><br><a href=/people/a/alexandra-schofield/>Alexandra Schofield</a>
|
<a href=/people/m/mans-magnusson/>Måns Magnusson</a>
|
<a href=/people/d/david-mimno/>David Mimno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2069><div class="card-body p-3 small">It is often assumed that <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> benefit from the use of a manually curated stopword list. Constructing this <a href=https://en.wikipedia.org/wiki/List_(abstract_data_type)>list</a> is time-consuming and often subject to user judgments about what kinds of words are important to the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and the application. Although stopword removal clearly affects which word types appear as most probable terms in topics, we argue that this improvement is superficial, and that topic inference benefits little from the practice of removing <a href=https://en.wikipedia.org/wiki/Stopword>stopwords</a> beyond very frequent terms. Removing corpus-specific stopwords after model inference is more transparent and produces similar results to removing those words prior to <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2070.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2070 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2070 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2070/>Measuring Topic Coherence through Optimal Word Buckets</a></strong><br><a href=/people/n/nitin-ramrakhiyani/>Nitin Ramrakhiyani</a>
|
<a href=/people/s/sachin-pawar/>Sachin Pawar</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/g/girish-palshikar/>Girish Palshikar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2070><div class="card-body p-3 small">Measuring topic quality is essential for scoring the learned topics and their subsequent use in <a href=https://en.wikipedia.org/wiki/Information_retrieval>Information Retrieval</a> and <a href=https://en.wikipedia.org/wiki/Text_classification>Text classification</a>. To measure quality of Latent Dirichlet Allocation (LDA) based topics learned from text, we propose a novel approach based on grouping of topic words into buckets (TBuckets). A single large bucket signifies a single <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherent theme</a>, in turn indicating high <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>topic coherence</a>. TBuckets uses word embeddings of topic words and employs <a href=https://en.wikipedia.org/wiki/Singular_value_decomposition>singular value decomposition (SVD)</a> and Integer Linear Programming based optimization to create coherent word buckets. TBuckets outperforms the state-of-the-art techniques when evaluated using 3 publicly available datasets and on another one proposed in this paper.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2071 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2071/>A Hybrid CNN-RNN Alignment Model for Phrase-Aware Sentence Classification<span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>RNN</span> Alignment Model for Phrase-Aware Sentence Classification</a></strong><br><a href=/people/s/shiou-tian-hsu/>Shiou Tian Hsu</a>
|
<a href=/people/c/changsung-moon/>Changsung Moon</a>
|
<a href=/people/p/paul-jones/>Paul Jones</a>
|
<a href=/people/n/nagiza-samatova/>Nagiza Samatova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2071><div class="card-body p-3 small">The success of sentence classification often depends on understanding both the syntactic and semantic properties of <a href=https://en.wikipedia.org/wiki/Phrase>word-phrases</a>. Recent progress on this task has been based on exploiting the <a href=https://en.wikipedia.org/wiki/Grammar>grammatical structure</a> of sentences but often this structure is difficult to parse and noisy. In this paper, we propose a structure-independent &#8216;Gated Representation Alignment&#8217; (GRA) model that blends a phrase-focused Convolutional Neural Network (CNN) approach with sequence-oriented Recurrent Neural Network (RNN). Our novel alignment mechanism allows the RNN to selectively include phrase information in a word-by-word sentence representation, and to do this without awareness of the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a>. An empirical evaluation of GRA shows higher prediction accuracy (up to 4.6 %) of fine-grained sentiment ratings, when compared to other structure-independent baselines. We also show comparable results to several structure-dependent methods. Finally, we analyzed the effect of our alignment mechanism and found that this is critical to the effectiveness of the CNN-RNN hybrid.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2072 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2072/>Multivariate Gaussian Document Representation from Word Embeddings for Text Categorization<span class=acl-fixed-case>G</span>aussian Document Representation from Word Embeddings for Text Categorization</a></strong><br><a href=/people/g/giannis-nikolentzos/>Giannis Nikolentzos</a>
|
<a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/f/francois-rousseau/>François Rousseau</a>
|
<a href=/people/y/yannis-stavrakas/>Yannis Stavrakas</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2072><div class="card-body p-3 small">Recently, there has been a lot of activity in learning distributed representations of words in <a href=https://en.wikipedia.org/wiki/Vector_space>vector spaces</a>. Although there are <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> capable of learning high-quality distributed representations of words, how to generate <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representations</a> of the same quality for phrases or documents still remains a challenge. In this paper, we propose to model each document as a multivariate Gaussian distribution based on the distributed representations of its words. We then measure the <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> between two documents based on the similarity of their distributions. Experiments on eight standard text categorization datasets demonstrate the effectiveness of the proposed approach in comparison with state-of-the-art methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2073 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2073/>Derivation of Document Vectors from Adaptation of LSTM Language Model<span class=acl-fixed-case>LSTM</span> Language Model</a></strong><br><a href=/people/w/wei-li/>Wei Li</a>
|
<a href=/people/b/brian-mak/>Brian Mak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2073><div class="card-body p-3 small">In many natural language processing (NLP) tasks, a document is commonly modeled as a <a href=https://en.wikipedia.org/wiki/Bag_of_words>bag of words</a> using the term frequency-inverse document frequency (TF-IDF) vector. One major shortcoming of the frequency-based TF-IDF feature vector is that it ignores word orders that carry syntactic and semantic relationships among the words in a document. This paper proposes a novel distributed vector representation of a document, which will be labeled as DV-LSTM, and is derived from the result of adapting a long short-term memory recurrent neural network language model by the document. DV-LSTM is expected to capture some high-level sequential information in the document, which other current document representations fail to do. It was evaluated in document genre classification in the <a href=https://en.wikipedia.org/wiki/Brown_Corpus>Brown Corpus</a> and the BNC Baby Corpus. The results show that DV-LSTM significantly outperforms TF-IDF vector and paragraph vector (PV-DM) in most cases, and their combinations may further improve the classification performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2074 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2074/>Real-Time Keyword Extraction from Conversations</a></strong><br><a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/a/antoine-tixier/>Antoine Tixier</a>
|
<a href=/people/i/ioannis-nikolentzos/>Ioannis Nikolentzos</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2074><div class="card-body p-3 small">We introduce a novel <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> to extract <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a> from meeting speech in real-time. Our approach builds on the graph-of-words representation of text and leverages the k-core decomposition algorithm and properties of submodular functions. We outperform multiple baselines in a real-time scenario emulated from the AMI and ICSI meeting corpora. Evaluation is conducted against both extractive and abstractive gold standard using two standard <a href=https://en.wikipedia.org/wiki/Performance_metric>performance metrics</a> and a newer one based on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2077 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2077/>Evaluating Persuasion Strategies and Deep Reinforcement Learning methods for Negotiation Dialogue agents</a></strong><br><a href=/people/s/simon-keizer/>Simon Keizer</a>
|
<a href=/people/m/markus-guhe/>Markus Guhe</a>
|
<a href=/people/h/heriberto-cuayahuitl/>Heriberto Cuayáhuitl</a>
|
<a href=/people/i/ioannis-efstathiou/>Ioannis Efstathiou</a>
|
<a href=/people/k/klaus-peter-engelbrecht/>Klaus-Peter Engelbrecht</a>
|
<a href=/people/m/mihai-dobre/>Mihai Dobre</a>
|
<a href=/people/a/alex-lascarides/>Alex Lascarides</a>
|
<a href=/people/o/oliver-lemon/>Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2077><div class="card-body p-3 small">In this paper we present a comparative evaluation of various <a href=https://en.wikipedia.org/wiki/Negotiation>negotiation strategies</a> within an online version of the game Settlers of Catan. The comparison is based on human subjects playing games against artificial game-playing agents (&#8216;bots&#8217;) which implement different negotiation dialogue strategies, using a chat dialogue interface to negotiate trades. Our results suggest that a negotiation strategy that uses <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion</a>, as well as a strategy that is trained from data using Deep Reinforcement Learning, both lead to an improved win rate against humans, compared to previous rule-based and supervised learning baseline dialogue negotiators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2078 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2078/>Unsupervised Dialogue Act Induction using Gaussian Mixtures<span class=acl-fixed-case>G</span>aussian Mixtures</a></strong><br><a href=/people/t/tomas-brychcin/>Tomáš Brychcín</a>
|
<a href=/people/p/pavel-kral/>Pavel Král</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2078><div class="card-body p-3 small">This paper introduces a new <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> for dialogue act induction. Given the sequence of dialogue utterances, the task is to assign them the labels representing their function in the dialogue. Utterances are represented as real-valued vectors encoding their meaning. We model the <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> as <a href=https://en.wikipedia.org/wiki/Hidden_Markov_model>Hidden Markov model</a> with emission probabilities estimated by Gaussian mixtures. We use <a href=https://en.wikipedia.org/wiki/Gibbs_sampling>Gibbs sampling</a> for <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior inference</a>. We present the results on the standard Switchboard-DAMSL corpus. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> achieves promising results compared with strong supervised baselines and outperforms other unsupervised algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2079.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2079 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2079 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2079/>Grounding Language by Continuous Observation of Instruction Following</a></strong><br><a href=/people/t/ting-han/>Ting Han</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2079><div class="card-body p-3 small">Grounded semantics is typically learnt from utterance-level meaning representations (e.g., successful database retrievals, denoted objects in <a href=https://en.wikipedia.org/wiki/Digital_image>images</a>, moves in a game). We explore learning word and utterance meanings by continuous observation of the actions of an instruction follower (IF). While an instruction giver (IG) provided a verbal description of a configuration of objects, IF recreated it using a <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>GUI</a>. Aligning these GUI actions to sub-utterance chunks allows a simple <a href=https://en.wikipedia.org/wiki/Maximum_entropy_model>maximum entropy model</a> to associate them as chunk meaning better than just providing it with the utterance-final configuration. This shows that <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> useful for incremental (word-by-word) application, as required in natural dialogue, might also be better acquired from incremental settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2081 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2081" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2081/>Efficient, Compositional, Order-sensitive n-gram Embeddings</a></strong><br><a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/p/pushpendre-rastogi/>Pushpendre Rastogi</a>
|
<a href=/people/m/m-patrick-martin/>M. Patrick Martin</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2081><div class="card-body p-3 small">We propose ECO : a new way to generate embeddings for phrases that is Efficient, Compositional, and Order-sensitive. Our method creates decompositional embeddings for words offline and combines them to create new <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for phrases in real time. Unlike other approaches, ECO can create <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for phrases not seen during training. We evaluate ECO on supervised and unsupervised tasks and demonstrate that creating phrase embeddings that are sensitive to <a href=https://en.wikipedia.org/wiki/Word_order>word order</a> can help downstream tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2082 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2082" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2082/>Integrating Semantic Knowledge into <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Embeddings</a> Based on Information Content Measurement</a></strong><br><a href=/people/h/hsin-yang-wang/>Hsin-Yang Wang</a>
|
<a href=/people/w/wei-yun-ma/>Wei-Yun Ma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2082><div class="card-body p-3 small">Distributional word representations are widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. These <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> are based on an assumption that words with a similar context tend to have a similar meaning. To improve the quality of the context-based embeddings, many researches have explored how to make full use of existing lexical resources. In this paper, we argue that while we incorporate the prior knowledge with context-based embeddings, words with different occurrences should be treated differently. Therefore, we propose to rely on the measurement of information content to control the degree of applying prior knowledge into context-based embeddings-different words would have different learning rates when adjusting their embeddings. In the result, we demonstrate that our <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> get significant improvements on two different tasks : <a href=https://en.wikipedia.org/wiki/Similarity_measure>Word Similarity</a> and <a href=https://en.wikipedia.org/wiki/Analogical_reasoning>Analogical Reasoning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2083 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2083/>Improving Neural Knowledge Base Completion with Cross-Lingual Projections</a></strong><br><a href=/people/p/patrick-klein/>Patrick Klein</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2083><div class="card-body p-3 small">In this paper we present a cross-lingual extension of a neural tensor network model for knowledge base completion. We exploit multilingual synsets from <a href=https://en.wikipedia.org/wiki/BabelNet>BabelNet</a> to translate English triples to other languages and then augment the reference knowledge base with cross-lingual triples. We project monolingual embeddings of different languages to a shared multilingual space and use them for <a href=https://en.wikipedia.org/wiki/Network_topology>network initialization</a> (i.e., as initial concept embeddings). We then train the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> with triples from the cross-lingually augmented knowledge base. Results on WordNet link prediction show that leveraging cross-lingual information yields significant gains over exploiting only monolingual triples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2084 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2084/>Modelling metaphor with attribute-based semantics</a></strong><br><a href=/people/l/luana-bulat/>Luana Bulat</a>
|
<a href=/people/s/stephen-clark/>Stephen Clark</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2084><div class="card-body p-3 small">One of the key problems in computational metaphor modelling is finding the optimal level of abstraction of semantic representations, such that these are able to capture and generalise metaphorical mechanisms. In this paper we present the first metaphor identification method that uses <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> constructed from <a href=https://en.wikipedia.org/wiki/Norm_(philosophy)>property norms</a>. Such <a href=https://en.wikipedia.org/wiki/Social_norm>norms</a> have been previously shown to provide a cognitively plausible representation of concepts in terms of <a href=https://en.wikipedia.org/wiki/Semantics>semantic properties</a>. Our results demonstrate that such property-based semantic representations provide a suitable model of cross-domain knowledge projection in metaphors, outperforming standard distributional models on a metaphor identification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2085 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2085/>When a Red Herring in Not a Red Herring : Using Compositional Methods to Detect Non-Compositional Phrases</a></strong><br><a href=/people/j/julie-weeds/>Julie Weeds</a>
|
<a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/j/jeremy-reffin/>Jeremy Reffin</a>
|
<a href=/people/d/david-weir/>David Weir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2085><div class="card-body p-3 small">Non-compositional phrases such as <a href=https://en.wikipedia.org/wiki/Red_herring>red herring</a> and weakly compositional phrases such as <a href=https://en.wikipedia.org/wiki/Spelling_bee>spelling bee</a> are an integral part of <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> (Sag, 2002). They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics. Compositionality detection therefore provides a good testbed for <a href=https://en.wikipedia.org/wiki/Compositing>compositional methods</a>. We compare an integrated compositional distributional approach, using sparse high dimensional representations, with the ad-hoc compositional approach of applying simple composition operations to state-of-the-art neural embeddings.<i>red herring</i> and weakly compositional phrases such as <i>spelling bee</i> are an integral part of natural language (Sag, 2002). They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics. Compositionality detection therefore provides a good testbed for compositional methods. We compare an integrated compositional distributional approach, using sparse high dimensional representations, with the ad-hoc compositional approach of applying simple composition operations to state-of-the-art neural embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2086 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2086/>Applying Multi-Sense Embeddings for German Verbs to Determine Semantic Relatedness and to Detect Non-Literal Language<span class=acl-fixed-case>G</span>erman Verbs to Determine Semantic Relatedness and to Detect Non-Literal Language</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2086><div class="card-body p-3 small">Up to date, the majority of <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational models</a> still determines the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a> between words (or larger linguistic units) on the type level. In this paper, we compare and extend multi-sense embeddings, in order to model and utilise <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> on the token level. We focus on the challenging class of complex verbs, and evaluate the model variants on various semantic tasks : semantic classification ; predicting compositionality ; and detecting non-literal language usage. While there is no overall best model, all models significantly outperform a word2vec single-sense skip baseline, thus demonstrating the need to distinguish between <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> in a distributional semantic model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2087 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2087" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2087/>Negative Sampling Improves Hypernymy Extraction Based on Projection Learning</a></strong><br><a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/n/nikolay-arefyev/>Nikolay Arefyev</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a>
|
<a href=/people/a/alexander-panchenko/>Alexander Panchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2087><div class="card-body p-3 small">We present a new approach to extraction of hypernyms based on projection learning and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. In contrast to <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification-based approaches</a>, <a href=https://en.wikipedia.org/wiki/Projection_(linear_algebra)>projection-based methods</a> require no candidate <a href=https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy>hyponym-hypernym pairs</a>. While it is natural to use both positive and negative training examples in supervised relation extraction, the impact of positive examples on hypernym prediction was not studied so far. In this paper, we show that explicit negative examples used for <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly improve performance compared to the state-of-the-art approach of Fu et al. (2014) on three <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2088 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2088/>A Dataset for Multi-Target Stance Detection</a></strong><br><a href=/people/p/parinaz-sobhani/>Parinaz Sobhani</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2088><div class="card-body p-3 small">Current <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for stance classification often treat each target independently, but in many <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>, there exist natural dependencies among targets, e.g., stance towards two or more politicians in an election or towards several brands of the same product. In this paper, we focus on the problem of multi-target stance detection. We present a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that we built for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Furthermore, We experiment with several neural models on the dataset and show that they are more effective in jointly modeling the overall position towards two related targets compared to <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>independent predictions</a> and other models of joint learning, such as cascading classification. We make the new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> publicly available, in order to facilitate further research in multi-target stance classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2090.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2090 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2090 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2090/>Predicting Emotional Word Ratings using Distributional Representations and Signed Clustering</a></strong><br><a href=/people/j/joao-sedoc/>João Sedoc</a>
|
<a href=/people/d/daniel-preotiuc-pietro/>Daniel Preoţiuc-Pietro</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2090><div class="card-body p-3 small">Inferring the emotional content of words is important for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>text-based sentiment analysis</a>, dialogue systems and <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a>, but word ratings are expensive to collect at scale and across languages or domains. We develop a method that automatically extends word-level ratings to unrated words using signed clustering of vector space word representations along with affect ratings. We use our method to determine a word&#8217;s valence and arousal, which determine its position on the circumplex model of affect, the most popular dimensional model of emotion. Our method achieves superior out-of-sample word rating prediction on both affective dimensions across three different languages when compared to state-of-the-art word similarity based methods. Our method can assist building word ratings for new languages and improve downstream tasks such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2091 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2091/>Attention Modeling for Targeted Sentiment</a></strong><br><a href=/people/j/jiangming-liu/>Jiangming Liu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2091><div class="card-body p-3 small">Neural network models have been used for target-dependent sentiment analysis. Previous work focus on learning a target specific representation for a given input sentence which is used for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. However, they do not explicitly model the contribution of each word in a sentence with respect to targeted sentiment polarities. We investigate an attention model to this end. In particular, a vanilla LSTM model is used to induce an attention value of the whole sentence. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is further extended to differentiate left and right contexts given a certain target following previous work. Results show that by using <a href=https://en.wikipedia.org/wiki/Attention>attention</a> to model the contribution of each word with respect to the target, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> gives significantly improved results over two standard <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarks</a>. We report the best <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2092.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2092 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2092 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2092" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2092/>EmoBank : Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis<span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>B</span>ank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis</a></strong><br><a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2092><div class="card-body p-3 small">We describe EmoBank, a corpus of 10k English sentences balancing multiple genres, which we annotated with dimensional emotion metadata in the Valence-Arousal-Dominance (VAD) representation format. EmoBank excels with a bi-perspectival and bi-representational design. On the one hand, we distinguish between writer&#8217;s and reader&#8217;s emotions, on the other hand, a subset of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> complements dimensional VAD annotations with categorical ones based on Basic Emotions. We find evidence for the supremacy of the reader&#8217;s perspective in terms of IAA and rating intensity, and achieve close-to-human performance when mapping between dimensional and categorical formats.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2093 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2093/>Structural Attention Neural Networks for improved <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a></a></strong><br><a href=/people/f/filippos-kokkinos/>Filippos Kokkinos</a>
|
<a href=/people/a/alexandros-potamianos/>Alexandros Potamianos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2093><div class="card-body p-3 small">We introduce a tree-structured attention neural network for <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentences</a> and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottom-up and top-down information propagation. Also, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> utilizes structural attention to identify the most salient representations during the construction of the <a href=https://en.wikipedia.org/wiki/Syntactic_tree>syntactic tree</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2094.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2094 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2094 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2094/>Ranking Convolutional Recurrent Neural Networks for Purchase Stage Identification on Imbalanced Twitter Data<span class=acl-fixed-case>T</span>witter Data</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/f/francine-chen/>Francine Chen</a>
|
<a href=/people/y/yan-ying-chen/>Yan-Ying Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2094><div class="card-body p-3 small">Users often use <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> to share their interest in products. We propose to identify purchase stages from Twitter data following the AIDA model (Awareness, Interest, Desire, Action). In particular, we define the task of classifying the purchase stage of each tweet in a user&#8217;s tweet sequence. We introduce RCRNN, a Ranking Convolutional Recurrent Neural Network which computes tweet representations using convolution over word embeddings and models a tweet sequence with gated recurrent units. Also, we consider various methods to cope with the imbalanced label distribution in our data and show that a ranking layer outperforms class weights.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2096 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2096/>Reranking Translation Candidates Produced by Several Bilingual Word Similarity Sources</a></strong><br><a href=/people/l/laurent-jakubina/>Laurent Jakubina</a>
|
<a href=/people/p/philippe-langlais/>Phillippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2096><div class="card-body p-3 small">We investigate the <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a> of the output of several distributional approaches on the Bilingual Lexicon Induction task. We show that reranking an n-best list produced by any of those approaches leads to very substantial improvements. We further demonstrate that combining several n-best lists by <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a> is an effective way of further boosting performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2097 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2097/>Lexicalized Reordering for Left-to-Right Hierarchical Phrase-based Translation</a></strong><br><a href=/people/m/maryam-siahbani/>Maryam Siahbani</a>
|
<a href=/people/a/anoop-sarkar/>Anoop Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2097><div class="card-body p-3 small">Phrase-based and hierarchical phrase-based (Hiero) translation models differ radically in the way reordering is modeled. Lexicalized reordering models play an important role in phrase-based MT and such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have been added to CKY-based decoders for Hiero. Watanabe et al. (2006) proposed a promising decoding algorithm for Hiero (LR-Hiero) that visits input spans in arbitrary order and produces the translation in left to right (LR) order which leads to far fewer language model calls and leads to a considerable speedup in decoding. We introduce a novel shift-reduce algorithm to LR-Hiero to decode with our lexicalized reordering model (LRM) and show that it improves translation quality for Czech-English, Chinese-English and German-English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2099 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2099/>Addressing Problems across Linguistic Levels in SMT : Combining Approaches to Model <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Morphology</a>, <a href=https://en.wikipedia.org/wiki/Syntax>Syntax</a> and Lexical Choice<span class=acl-fixed-case>SMT</span>: Combining Approaches to Model Morphology, Syntax and Lexical Choice</a></strong><br><a href=/people/m/marion-weller-di-marco/>Marion Weller-Di Marco</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2099><div class="card-body p-3 small">Many errors in phrase-based SMT can be attributed to problems on three linguistic levels : <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological complexity</a> in the target language, structural differences and <a href=https://en.wikipedia.org/wiki/Lexical_choice>lexical choice</a>. We explore combinations of linguistically motivated approaches to address these problems in English-to-German SMT and show that they are complementary to one another, but also that the popular verbal pre-ordering can cause problems on the morphological and lexical level. A discriminative classifier can overcome these problems, in particular when enriching standard lexical features with <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> geared towards verbal inflection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2100 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2100" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2100/>Machine Translation of Spanish Personal and Possessive Pronouns Using Anaphora Probabilities<span class=acl-fixed-case>S</span>panish Personal and Possessive Pronouns Using Anaphora Probabilities</a></strong><br><a href=/people/n/ngoc-quang-luong/>Ngoc Quang Luong</a>
|
<a href=/people/a/andrei-popescu-belis/>Andrei Popescu-Belis</a>
|
<a href=/people/a/annette-rios-gonzales/>Annette Rios Gonzales</a>
|
<a href=/people/d/don-tuggener/>Don Tuggener</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2100><div class="card-body p-3 small">We implement a fully probabilistic model to combine the hypotheses of a Spanish anaphora resolution system with those of a Spanish-English machine translation system. The probabilities over antecedents are converted into probabilities for the features of translated pronouns, and are integrated with phrase-based MT using an additional translation model for <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>. The system improves the translation of several Spanish personal and possessive pronouns into <a href=https://en.wikipedia.org/wiki/English_language>English</a>, by solving translation divergencies such as &#8216;ella&#8217; vs. &#8216;she&#8217;/&#8216;it&#8217; or &#8216;su&#8217; vs. &#8216;his&#8217;/&#8216;her&#8217;/&#8216;its&#8217;/&#8216;their&#8217;. On a test set with 2,286 <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>, a baseline system correctly translates 1,055 of them, while ours improves this by 41. Moreover, with oracle antecedents, <a href=https://en.wikipedia.org/wiki/Possessive>possessives</a> are translated with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 83 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2101 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2101/>Using Images to Improve Machine-Translating E-Commerce Product Listings.<span class=acl-fixed-case>E</span>-Commerce Product Listings.</a></strong><br><a href=/people/i/iacer-calixto/>Iacer Calixto</a>
|
<a href=/people/d/daniel-stein/>Daniel Stein</a>
|
<a href=/people/e/evgeny-matusov/>Evgeny Matusov</a>
|
<a href=/people/p/pintu-lohar/>Pintu Lohar</a>
|
<a href=/people/s/sheila-castilho/>Sheila Castilho</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2101><div class="card-body p-3 small">In this paper we study the impact of using <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> to machine-translate user-generated e-commerce product listings. We study how a multi-modal Neural Machine Translation (NMT) model compares to two text-only approaches : a conventional state-of-the-art attentional NMT and a Statistical Machine Translation (SMT) model. User-generated product listings often do not constitute <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>grammatical or well-formed sentences</a>. More often than not, they consist of the juxtaposition of short phrases or <a href=https://en.wikipedia.org/wiki/Index_term>keywords</a>. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> end-to-end as well as use text-only and multi-modal NMT models for re-ranking n-best lists generated by an SMT model. We qualitatively evaluate our user-generated training data also analyse how adding synthetic data impacts the results. We evaluate our models quantitatively using BLEU and TER and find that (i) additional synthetic data has a general positive impact on text-only and multi-modal NMT models, and that (ii) using a multi-modal NMT model for re-ranking n-best lists improves TER significantly across different n-best list sizes.<tex-math>n</tex-math>-best lists generated by an SMT model. We qualitatively evaluate our user-generated training data also analyse how adding synthetic data impacts the results. We evaluate our models quantitatively using BLEU and TER and find that (i) additional synthetic data has a general positive impact on text-only and multi-modal NMT models, and that (ii) using a multi-modal NMT model for re-ranking n-best lists improves TER significantly across different n-best list sizes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2105 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2105/>Large-Scale Categorization of Japanese Product Titles Using Neural Attention Models<span class=acl-fixed-case>J</span>apanese Product Titles Using Neural Attention Models</a></strong><br><a href=/people/y/yandi-xia/>Yandi Xia</a>
|
<a href=/people/a/aaron-levine/>Aaron Levine</a>
|
<a href=/people/p/pradipto-das/>Pradipto Das</a>
|
<a href=/people/g/giuseppe-di-fabbrizio/>Giuseppe Di Fabbrizio</a>
|
<a href=/people/k/keiji-shinzato/>Keiji Shinzato</a>
|
<a href=/people/a/ankur-datta/>Ankur Datta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2105><div class="card-body p-3 small">We propose a variant of Convolutional Neural Network (CNN) models, the Attention CNN (ACNN) ; for large-scale categorization of millions of Japanese items into thirty-five product categories. Compared to a state-of-the-art Gradient Boosted Tree (GBT) classifier, the proposed model reduces training time from three weeks to three days while maintaining more than 96 % accuracy. Additionally, our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> characterizes products by imputing attentive focus on word tokens in a language agnostic way. The attention words have been observed to be semantically highly correlated with the predicted categories and give us a choice of automatic feature extraction for <a href=https://en.wikipedia.org/wiki/Downstream_processing>downstream processing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2106 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2106/>Convolutional Neural Networks for Authorship Attribution of Short Texts</a></strong><br><a href=/people/p/prasha-shrestha/>Prasha Shrestha</a>
|
<a href=/people/s/sebastian-sierra/>Sebastian Sierra</a>
|
<a href=/people/f/fabio-a-gonzalez/>Fabio González</a>
|
<a href=/people/m/manuel-montes/>Manuel Montes</a>
|
<a href=/people/p/paolo-rosso/>Paolo Rosso</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2106><div class="card-body p-3 small">We present a model to perform <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship attribution</a> of tweets using <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks (CNNs)</a> over <a href=https://en.wikipedia.org/wiki/N-gram>character n-grams</a>. We also present a strategy that improves model interpretability by estimating the importance of input text fragments in the predicted classification. The experimental evaluation shows that text CNNs perform competitively and are able to outperform previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2108 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2108/>On the Relevance of Syntactic and Discourse Features for Author Profiling and Identification</a></strong><br><a href=/people/j/juan-soler-company/>Juan Soler-Company</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2108><div class="card-body p-3 small">The majority of approaches to <a href=https://en.wikipedia.org/wiki/Author_profiling>author profiling</a> and author identification focus mainly on <a href=https://en.wikipedia.org/wiki/Lexicon>lexical features</a>, i.e., on the content of a text. We argue that syntactic and discourse features play a significantly more prominent role than they were given in the past. We show that they achieve state-of-the-art performance in author and gender identification on a <a href=https://en.wikipedia.org/wiki/Text_corpus>literary corpus</a> while keeping the feature set small : the used feature set is composed of only 188 features and still outperforms the winner of the PAN 2014 shared task on author verification in the <a href=https://en.wikipedia.org/wiki/Literary_genre>literary genre</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2109 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2109/>Unsupervised Cross-Lingual Scaling of Political Texts</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/f/federico-nanni/>Federico Nanni</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2109><div class="card-body p-3 small">Political text scaling aims to linearly order parties and politicians across <a href=https://en.wikipedia.org/wiki/Political_dimension>political dimensions</a> (e.g., <a href=https://en.wikipedia.org/wiki/Left&#8211;right_political_spectrum>left-to-right ideology</a>) based on <a href=https://en.wikipedia.org/wiki/Content_(media)>textual content</a> (e.g., <a href=https://en.wikipedia.org/wiki/Public_speaking>politician speeches</a> or party manifestos). Existing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> scale texts based on relative word usage and can not be used for cross-lingual analyses. Additionally, there is little quantitative evidence that the output of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> correlates with common political dimensions like left-to-right orientation. Experimental results show that the semantically-informed scaling models better predict the party positions than the existing word-based models in two different political dimensions. Furthermore, the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> exhibit no drop in performance in the cross-lingual compared to monolingual setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2111 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2111/>Multimodal Topic Labelling</a></strong><br><a href=/people/i/ionut-sorodoc/>Ionut Sorodoc</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a>
|
<a href=/people/n/nikolaos-aletras/>Nikolaos Aletras</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2111><div class="card-body p-3 small">Topics generated by topic models are typically presented as a list of topic terms. Automatic topic labelling is the task of generating a succinct label that summarises the theme or subject of a topic, with the intention of reducing the cognitive load of end-users when interpreting these topics. Traditionally, topic label systems focus on a single label modality, e.g. textual labels. In this work we propose a multimodal approach to topic labelling using a simple <a href=https://en.wikipedia.org/wiki/Feedforward_neural_network>feedforward neural network</a>. Given a topic and a candidate image or textual label, our method automatically generates a rating for the label, relative to the topic. Experiments show that this multimodal approach outperforms single-modality topic labelling systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2112 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2112/>Detecting (Un)Important Content for Single-Document News Summarization</a></strong><br><a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/f/forrest-bao/>Forrest Bao</a>
|
<a href=/people/a/ani-nenkova/>Ani Nenkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2112><div class="card-body p-3 small">We present a robust approach for detecting intrinsic sentence importance in <a href=https://en.wikipedia.org/wiki/News>news</a>, by training on two corpora of document-summary pairs. When used for single-document summarization, our approach, combined with the beginning of document heuristic, outperforms a state-of-the-art summarizer and the beginning-of-article baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for <a href=https://en.wikipedia.org/wiki/News>news</a> have not been able to consistently outperform the strong beginning-of-article baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2113 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2113/>F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media<span class=acl-fixed-case>F</span>-Score Driven Max Margin Neural Network for Named Entity Recognition in <span class=acl-fixed-case>C</span>hinese Social Media</a></strong><br><a href=/people/h/hangfeng-he/>Hangfeng He</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2113><div class="card-body p-3 small">We focus on named entity recognition (NER) for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning model</a> based on B-LSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine <a href=https://en.wikipedia.org/wiki/Transition_probability>transition probability</a> with <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> in our model. To bridge the gap between label accuracy and <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of NER, we construct a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> which can be directly trained on <a href=https://en.wikipedia.org/wiki/F-score>F-score</a>. When considering the instability of <a href=https://en.wikipedia.org/wiki/F-score>F-score driven method</a> and meaningful information provided by label accuracy, we propose an integrated method to train on both <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> and label accuracy. Our integrated model yields 7.44 % improvement over previous state-of-the-art result.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2114 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-2114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-2114/>Discriminative Information Retrieval for Question Answering Sentence Selection</a></strong><br><a href=/people/t/tongfei-chen/>Tongfei Chen</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2114><div class="card-body p-3 small">We propose a framework for discriminative IR atop linguistic features, trained to improve the recall of answer candidate passage retrieval, the initial step in text-based question answering. We formalize this as an instance of linear feature-based IR, demonstrating a 34%-43 % improvement in <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> for <a href=https://en.wikipedia.org/wiki/Triage>candidate triage</a> for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2115 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2115/>Effective shared representations with <a href=https://en.wikipedia.org/wiki/Multitask_learning>Multitask Learning</a> for Community Question Answering</a></strong><br><a href=/people/d/daniele-bonadiman/>Daniele Bonadiman</a>
|
<a href=/people/a/antonio-uva/>Antonio Uva</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2115><div class="card-body p-3 small">An important asset of using Deep Neural Networks (DNNs) for text applications is their ability to automatically engineering features. Unfortunately, DNNs usually require a lot of training data, especially for highly semantic tasks such as community Question Answering (cQA). In this paper, we tackle the problem of data scarcity by learning the target <a href=https://en.wikipedia.org/wiki/Deep_learning>DNN</a> together with two auxiliary tasks in a multitask learning setting. We exploit the strong semantic connection between selection of comments relevant to (i) new questions and (ii) forum questions. This enables a global representation for comments, new and previous questions. The experiments of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a SemEval challenge dataset for cQA show a 20 % of relative improvement over standard DNNs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2117 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2117/>Temporal information extraction from clinical text</a></strong><br><a href=/people/j/julien-tourille/>Julien Tourille</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a>
|
<a href=/people/a/aurelie-neveol/>Aurélie Névéol</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2117><div class="card-body p-3 small">In this paper, we present a method for temporal relation extraction from clinical narratives in <a href=https://en.wikipedia.org/wiki/French_language>French</a> and in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We experiment on two comparable corpora, the MERLOT corpus and the THYME corpus, and show that a common approach can be used for both languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2118 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2118/>Neural Temporal Relation Extraction</a></strong><br><a href=/people/d/dmitriy-dligach/>Dmitriy Dligach</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/c/chen-lin/>Chen Lin</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2118><div class="card-body p-3 small">We experiment with neural architectures for temporal relation extraction and establish a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for several scenarios. We find that neural models with only tokens as input outperform state-of-the-art hand-engineered feature-based models, that convolutional neural networks outperform LSTM models, and that encoding relation arguments with XML tags outperforms a traditional position-based encoding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-2119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-2119 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-2119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-2119/>End-to-End Trainable Attentive Decoder for Hierarchical Entity Classification</a></strong><br><a href=/people/s/sanjeev-karn/>Sanjeev Karn</a>
|
<a href=/people/u/ulli-waltinger/>Ulli Waltinger</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-2119><div class="card-body p-3 small">We address fine-grained entity classification and propose a novel attention-based recurrent neural network (RNN) encoder-decoder that generates paths in the type hierarchy and can be trained end-to-end. We show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs better on fine-grained entity classification than prior work that relies on flat or local classifiers that do not directly model hierarchical structure.</div></div></div><hr><div id=e17-3><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/E17-3/>Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3000/>Proceedings of the Software Demonstrations of the 15th Conference of the <span class=acl-fixed-case>E</span>uropean Chapter of the Association for Computational Linguistics</a></strong><br><a href=/people/a/andre-f-t-martins/>André Martins</a>
|
<a href=/people/a/anselmo-penas/>Anselmo Peñas</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3001/>COVER : Covering the Semantically Tractable Questions<span class=acl-fixed-case>COVER</span>: Covering the Semantically Tractable Questions</a></strong><br><a href=/people/m/michael-minock/>Michael Minock</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3001><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, natural language questions map to expressions in a meaning representation language (MRL) over some fixed vocabulary of predicates. To do this reliably, one must guarantee that for a wide class of natural language questions (the so called semantically tractable questions), correct interpretations are always in the mapped set of possibilities. In this demonstration, we introduce the system COVER which significantly clarifies, revises and extends the basic notion of semantic tractability. COVER achieves coverage of 89 % while the earlier PRECISE system achieved coverage of 77 % on the well known GeoQuery corpus. Like PRECISE, COVER requires only a simple domain lexicon and integrates off-the-shelf syntactic parsers. Beyond PRECISE, COVER also integrates off-the-shelf <a href=https://en.wikipedia.org/wiki/Automated_theorem_proving>theorem provers</a> to provide more accurate results. COVER is written in <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python</a> and uses the <a href=https://en.wikipedia.org/wiki/NLTK>NLTK</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3002/>Common Round : Application of Language Technologies to Large-Scale Web Debates</a></strong><br><a href=/people/h/hans-uszkoreit/>Hans Uszkoreit</a>
|
<a href=/people/a/aleksandra-gabryszak/>Aleksandra Gabryszak</a>
|
<a href=/people/l/leonhard-hennig/>Leonhard Hennig</a>
|
<a href=/people/j/jorg-steffen/>Jörg Steffen</a>
|
<a href=/people/r/renlong-ai/>Renlong Ai</a>
|
<a href=/people/s/stephan-busemann/>Stephan Busemann</a>
|
<a href=/people/j/jon-dehdari/>Jon Dehdari</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a>
|
<a href=/people/g/georg-heigold/>Georg Heigold</a>
|
<a href=/people/n/nils-rethmeier/>Nils Rethmeier</a>
|
<a href=/people/r/raphael-rubino/>Raphael Rubino</a>
|
<a href=/people/s/sven-schmeier/>Sven Schmeier</a>
|
<a href=/people/p/philippe-thomas/>Philippe Thomas</a>
|
<a href=/people/h/he-wang/>He Wang</a>
|
<a href=/people/f/feiyu-xu/>Feiyu Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3002><div class="card-body p-3 small">Web debates play an important role in enabling broad participation of constituencies in social, political and economic decision-taking. However, it is challenging to organize, structure, and navigate a vast number of diverse argumentations and comments collected from many participants over a long time period. In this paper we demonstrate Common Round, a next generation platform for large-scale web debates, which provides functions for eliciting the semantic content and structures from the contributions of participants. In particular, Common Round applies <a href=https://en.wikipedia.org/wiki/Language_technology>language technologies</a> for the extraction of semantic essence from textual input, aggregation of the formulated opinions and arguments. The <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> also provides a cross-lingual access to <a href=https://en.wikipedia.org/wiki/Debate>debates</a> using <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-3003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-3003/>A Web-Based Interactive Tool for Creating, Inspecting, Editing, and Publishing Etymological Datasets</a></strong><br><a href=/people/j/johann-mattis-list/>Johann-Mattis List</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3003><div class="card-body p-3 small">The paper presents the Etymological DICtionary ediTOR (EDICTOR), a free, interactive, web-based tool designed to aid historical linguists in creating, editing, analysing, and publishing etymological datasets. The EDICTOR offers interactive solutions for important tasks in <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>, including facilitated input and segmentation of phonetic transcriptions, quantitative and qualitative analyses of phonetic and morphological data, enhanced interfaces for cognate class assignment and multiple word alignment, and automated evaluation of regular sound correspondences. As a <a href=https://en.wikipedia.org/wiki/Web_application>web-based tool</a> written in <a href=https://en.wikipedia.org/wiki/JavaScript>JavaScript</a>, the EDICTOR can be used in standard <a href=https://en.wikipedia.org/wiki/Web_browser>web browsers</a> across all major platforms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3005/>TextImager as a Generic Interface to R<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>I</span>mager as a Generic Interface to <span class=acl-fixed-case>R</span></a></strong><br><a href=/people/t/tolga-uslu/>Tolga Uslu</a>
|
<a href=/people/w/wahed-hemati/>Wahed Hemati</a>
|
<a href=/people/a/alexander-mehler/>Alexander Mehler</a>
|
<a href=/people/d/daniel-baumartz/>Daniel Baumartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3005><div class="card-body p-3 small">R is a very powerful framework for <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical modeling</a>. Thus, it is of high importance to integrate <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R</a> with state-of-the-art tools in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. In this paper, we present the functionality and architecture of such an <a href=https://en.wikipedia.org/wiki/System_integration>integration</a> by means of TextImager. We use the OpenCPU API to integrate <a href=https://en.wikipedia.org/wiki/R_(programming_language)>R</a> based on our own R-Server. This allows for communicating with R-packages and combining them with TextImager&#8217;s NLP-components.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3007 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3007/>TWINE : A real-time system for TWeet analysis via INformation Extraction<span class=acl-fixed-case>TWINE</span>: A real-time system for <span class=acl-fixed-case>TW</span>eet analysis via <span class=acl-fixed-case>IN</span>formation Extraction</a></strong><br><a href=/people/d/debora-nozza/>Debora Nozza</a>
|
<a href=/people/f/fausto-ristagno/>Fausto Ristagno</a>
|
<a href=/people/m/matteo-palmonari/>Matteo Palmonari</a>
|
<a href=/people/e/elisabetta-fersini/>Elisabetta Fersini</a>
|
<a href=/people/p/pikakshi-manchanda/>Pikakshi Manchanda</a>
|
<a href=/people/e/enza-messina/>Enza Messina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3007><div class="card-body p-3 small">In the recent years, the amount of <a href=https://en.wikipedia.org/wiki/User-generated_content>user generated contents</a> shared on the Web has significantly increased, especially in <a href=https://en.wikipedia.org/wiki/Social_media>social media environment</a>, e.g. Twitter, <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a>, <a href=https://en.wikipedia.org/wiki/Google+>Google+</a>. This large quantity of data has generated the need of reactive and sophisticated systems for capturing and understanding the underlying information enclosed in them. In this paper we present TWINE, a real-time system for the big data analysis and exploration of information extracted from Twitter streams. The proposed system based on a Named Entity Recognition and Linking pipeline and a multi-dimensional spatial geo-localization is managed by a scalable and flexible architecture for an interactive visualization of micropost streams insights. The demo is available at.<url>http://twine-mind.cloudapp.net/streaming</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3008/>Alto : Rapid Prototyping for Parsing and Translation<span class=acl-fixed-case>A</span>lto: Rapid Prototyping for Parsing and Translation</a></strong><br><a href=/people/j/johannes-gontrum/>Johannes Gontrum</a>
|
<a href=/people/j/jonas-groschwitz/>Jonas Groschwitz</a>
|
<a href=/people/a/alexander-koller/>Alexander Koller</a>
|
<a href=/people/c/christoph-teichmann/>Christoph Teichmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3008><div class="card-body p-3 small">We present <a href=https://en.wikipedia.org/wiki/Alto>Alto</a>, a <a href=https://en.wikipedia.org/wiki/Rapid_prototyping>rapid prototyping tool</a> for new <a href=https://en.wikipedia.org/wiki/Formal_grammar>grammar formalisms</a>. Alto implements generic but efficient algorithms for <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>, <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, and <a href=https://en.wikipedia.org/wiki/Formal_grammar>training</a> for a range of monolingual and synchronous grammar formalisms. It can easily be extended to new <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalisms</a>, which makes all of these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> immediately available for the new <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalism</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3009 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3009/>CASSANDRA : A multipurpose configurable voice-enabled human-computer-interface<span class=acl-fixed-case>CASSANDRA</span>: A multipurpose configurable voice-enabled human-computer-interface</a></strong><br><a href=/people/t/tiberiu-boros/>Tiberiu Boros</a>
|
<a href=/people/s/stefan-daniel-dumitrescu/>Stefan Daniel Dumitrescu</a>
|
<a href=/people/s/sonia-pipa/>Sonia Pipa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3009><div class="card-body p-3 small">Voice enabled human computer interfaces (HCI) that integrate <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech synthesis</a> and <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> have become a commodity, introduced by the immersion of <a href=https://en.wikipedia.org/wiki/Smartphone>smart phones</a> and other gadgets in our daily lives. Smart assistants are able to respond to simple queries (similar to text-based question-answering systems), perform simple tasks (call a number, reject a call etc.) and help organizing appointments. With this paper we introduce a newly created process automation platform that enables the user to control <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> and <a href=https://en.wikipedia.org/wiki/Home_appliance>home appliances</a> and to query the system for information using a natural voice interface. We offer an overview of the technologies that enabled us to construct our <a href=https://en.wikipedia.org/wiki/System>system</a> and we present different usage scenarios in home and office environments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3010/>An Extensible Framework for Verification of Numerical Claims</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3010><div class="card-body p-3 small">In this paper we present our automated fact checking system demonstration which we developed in order to participate in the Fast and Furious Fact Check challenge. We focused on simple numerical claims such as population of Germany in 2015 was 80 million which comprised a quarter of the test instances in the challenge, achieving 68 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Our system extends previous work on semantic parsing and claim identification to handle temporal expressions and knowledge bases consisting of multiple tables, while relying solely on automatically generated training data. We demonstrate the extensible nature of our <a href=https://en.wikipedia.org/wiki/System>system</a> by evaluating <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> on relations used in previous work. We make our <a href=https://en.wikipedia.org/wiki/System>system</a> publicly available so that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can be used and extended by the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3013 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3013/>Multilingual CALL Framework for Automatic Language Exercise Generation from Free Text<span class=acl-fixed-case>CALL</span> Framework for Automatic Language Exercise Generation from Free Text</a></strong><br><a href=/people/n/naiara-perez/>Naiara Perez</a>
|
<a href=/people/m/montse-cuadros/>Montse Cuadros</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3013><div class="card-body p-3 small">This paper describes a <a href=https://en.wikipedia.org/wiki/Web_application>web-based application</a> to design and answer exercises for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a>. It is available in <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and <a href=https://en.wikipedia.org/wiki/French_language>French</a>. Based on open-source Natural Language Processing (NLP) technology such as word embedding models and <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a>, the application enables users to automatic create easily and in real time three types of exercises, namely, Fill-in-the-Gaps, Multiple Choice, and Shuffled Sentences questionnaires. These are generated from texts of the users&#8217; own choice, so they can train their language skills with content of their particular interest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3014 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3014/>Audience Segmentation in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/v/verena-henrich/>Verena Henrich</a>
|
<a href=/people/a/alexander-lang/>Alexander Lang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3014><div class="card-body p-3 small">Understanding the social media audience is becoming increasingly important for social media analysis. This paper presents an approach that detects various audience attributes, including author location, <a href=https://en.wikipedia.org/wiki/Demography>demographics</a>, <a href=https://en.wikipedia.org/wiki/Behavior>behavior</a> and <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>interests</a>. It works both for a variety of <a href=https://en.wikipedia.org/wiki/Social_media>social media sources</a> and for multiple languages. The approach has been implemented within <a href=https://en.wikipedia.org/wiki/Watson_(computer)>IBM Watson Analytics</a> for <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> and creates author profiles for more than 300 different analysis domains every day.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3015 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3015/>The arText prototype : An automatic system for writing specialized texts<span class=acl-fixed-case>T</span>ext prototype: An automatic system for writing specialized texts</a></strong><br><a href=/people/i/iria-da-cunha/>Iria da Cunha</a>
|
<a href=/people/m/m-amor-montane/>M. Amor Montané</a>
|
<a href=/people/l/luis-hysa/>Luis Hysa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3015><div class="card-body p-3 small">This article describes an automatic system for writing specialized texts in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. The arText prototype is a free online text editor that includes different types of <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic information</a>. It is designed for a variety of end users and domains, including specialists and university students working in the fields of medicine and tourism, and laypersons writing to the public administration. ArText provides guidance on how to structure a text, prompts users to include all necessary contents in each section, and detects lexical and discourse problems in the text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3016 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3016/>QCRI Live Speech Translation System<span class=acl-fixed-case>QCRI</span> Live Speech Translation System</a></strong><br><a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/y/yifan-zhang/>Yifan Zhang</a>
|
<a href=/people/s/sameer-khurana/>Sameer Khurana</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a>
|
<a href=/people/s/stephan-vogel/>Stephan Vogel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3016><div class="card-body p-3 small">This paper presents QCRI&#8217;s Arabic-to-English live speech translation system. It features modern <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web technologies</a> to capture <a href=https://en.wikipedia.org/wiki/Live_streaming>live audio</a>, and broadcasts <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>Arabic transcriptions</a> and <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>English translations</a> simultaneously. Our Kaldi-based ASR system uses the Time Delay Neural Network (TDNN) architecture, while our Machine Translation (MT) system uses both phrase-based and neural frameworks. Although our neural MT system is slower than the phrase-based system, it produces significantly better translations and is memory efficient. The demo is available at.<url>https://st.qcri.org/demos/livetranslation</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3018 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3018/>A tool for extracting sense-disambiguated example sentences through user feedback</a></strong><br><a href=/people/b/beto-boullosa/>Beto Boullosa</a>
|
<a href=/people/r/richard-eckart-de-castilho/>Richard Eckart de Castilho</a>
|
<a href=/people/a/alexander-geyken/>Alexander Geyken</a>
|
<a href=/people/l/lothar-lemnitzer/>Lothar Lemnitzer</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3018><div class="card-body p-3 small">This paper describes an application system aimed to help <a href=https://en.wikipedia.org/wiki/Lexicography>lexicographers</a> in the extraction of example sentences for a given headword based on its different senses. The <a href=https://en.wikipedia.org/wiki/Tool>tool</a> uses classification and clustering methods and incorporates user feedback to refine its results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3019 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3019/>Lingmotif : Sentiment Analysis for the <a href=https://en.wikipedia.org/wiki/Digital_humanities>Digital Humanities</a><span class=acl-fixed-case>L</span>ingmotif: Sentiment Analysis for the Digital Humanities</a></strong><br><a href=/people/a/antonio-moreno-ortiz/>Antonio Moreno-Ortiz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3019><div class="card-body p-3 small">Lingmotif is a lexicon-based, linguistically-motivated, user-friendly, GUI-enabled, multi-platform, Sentiment Analysis desktop application. Lingmotif can perform SA on any type of input texts, regardless of their length and topic. The analysis is based on the identification of sentiment-laden words and phrases contained in the application&#8217;s rich core lexicons, and employs context rules to account for sentiment shifters. It offers easy-to-interpret visual representations of quantitative data (text polarity, sentiment intensity, sentiment profile), as well as a detailed, qualitative analysis of the text in terms of its sentiment. Lingmotif can also take user-provided plugin lexicons in order to account for domain-specific sentiment expression. Lingmotif currently analyzes English and Spanish texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3020 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3020/>RAMBLE ON : Tracing Movements of Popular Historical Figures<span class=acl-fixed-case>RAMBLE</span> <span class=acl-fixed-case>ON</span>: Tracing Movements of Popular Historical Figures</a></strong><br><a href=/people/s/stefano-menini/>Stefano Menini</a>
|
<a href=/people/r/rachele-sprugnoli/>Rachele Sprugnoli</a>
|
<a href=/people/g/giovanni-moretti/>Giovanni Moretti</a>
|
<a href=/people/e/enrico-bignotti/>Enrico Bignotti</a>
|
<a href=/people/s/sara-tonelli/>Sara Tonelli</a>
|
<a href=/people/b/bruno-lepri/>Bruno Lepri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3020><div class="card-body p-3 small">We present RAMBLE ON, an <a href=https://en.wikipedia.org/wiki/Application_software>application</a> integrating a <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> for frame-based information extraction and an <a href=https://en.wikipedia.org/wiki/User_interface>interface</a> to track and display movement trajectories. The code of the extraction pipeline and a navigator are freely available ; moreover we display in a demonstrator the outcome of a case study carried out on trajectories of notable persons of the XX Century.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3021 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3021/>Autobank : a semi-automatic annotation tool for developing deep Minimalist Grammar treebanks<span class=acl-fixed-case>A</span>utobank: a semi-automatic annotation tool for developing deep <span class=acl-fixed-case>M</span>inimalist <span class=acl-fixed-case>G</span>rammar treebanks</a></strong><br><a href=/people/j/john-torr/>John Torr</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3021><div class="card-body p-3 small">This paper presents Autobank, a prototype tool for constructing a wide-coverage Minimalist Grammar (MG) (Stabler 1997), and semi-automatically converting the Penn Treebank (PTB) into a deep Minimalist treebank. The front end of the tool is a <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>graphical user interface</a> which facilitates the rapid development of a seed set of MG trees via manual reannotation of PTB preterminals with MG lexical categories. The system then extracts various dependency mappings between the source and target trees, and uses these in concert with a non-statistical MG parser to automatically reannotate the rest of the corpus. Autobank thus enables deep treebank conversions (and subsequent modifications) without the need for complex transduction algorithms accompanied by cascades of ad hoc rules ; instead, the locus of human effort falls directly on the task of grammar construction itself.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3022 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3022/>Chatbot with a Discourse Structure-Driven Dialogue Management</a></strong><br><a href=/people/b/boris-galitsky/>Boris Galitsky</a>
|
<a href=/people/d/dmitry-ilvovsky/>Dmitry Ilvovsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3022><div class="card-body p-3 small">We build a chat bot with iterative content exploration that leads a user through a personalized knowledge acquisition session. The <a href=https://en.wikipedia.org/wiki/Chat_bot>chat bot</a> is designed as an automated customer support or product recommendation agent assisting a user in learning product features, product usability, suitability, <a href=https://en.wikipedia.org/wiki/Troubleshooting>troubleshooting</a> and other related tasks. To control the user navigation through content, we extend the notion of a linguistic discourse tree (DT) towards a set of documents with multiple sections covering a topic. For a given paragraph, a DT is built by DT parsers. We then combine DTs for the paragraphs of documents to form what we call extended DT, which is a basis for interactive content exploration facilitated by the chat bot. To provide cohesive answers, we use a measure of rhetoric agreement between a question and an answer by tree kernel learning of their DTs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3025 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3025/>Building Web-Interfaces for Vector Semantic Models with the WebVectors Toolkit<span class=acl-fixed-case>W</span>eb<span class=acl-fixed-case>V</span>ectors Toolkit</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/e/elizaveta-kuzmenko/>Elizaveta Kuzmenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3025><div class="card-body p-3 small">In this demo we present WebVectors, a free and open-source toolkit helping to deploy <a href=https://en.wikipedia.org/wiki/Web_service>web services</a> which demonstrate and visualize distributional semantic models (widely known as word embeddings). WebVectors can be useful in a very common situation when one has trained a distributional semantics model for one&#8217;s particular corpus or language (tools for this are now widespread and simple to use), but then there is a need to demonstrate the results to general public over the Web. We show its abilities on the example of the living web services featuring distributional models for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Norwegian_language>Norwegian</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3026 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3026/>InToEventS : An Interactive Toolkit for Discovering and Building Event Schemas<span class=acl-fixed-case>I</span>n<span class=acl-fixed-case>T</span>o<span class=acl-fixed-case>E</span>vent<span class=acl-fixed-case>S</span>: An Interactive Toolkit for Discovering and Building Event Schemas</a></strong><br><a href=/people/g/german-ferrero/>Germán Ferrero</a>
|
<a href=/people/a/audi-primadhanty/>Audi Primadhanty</a>
|
<a href=/people/a/ariadna-quattoni/>Ariadna Quattoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3026><div class="card-body p-3 small">Event Schema Induction is the task of learning a <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representation of events</a> (e.g., bombing) and the roles involved in them (e.g, victim and perpetrator). This paper presents InToEventS, an interactive tool for learning these <a href=https://en.wikipedia.org/wiki/Schema_(psychology)>schemas</a>. InToEventS allows users to explore a corpus and discover which kind of events are present. We show how users can create useful event schemas using two interactive clustering steps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-3027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-3027 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-3027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-3027/>ICE : Idiom and Collocation Extractor for Research and Education<span class=acl-fixed-case>ICE</span>: Idiom and Collocation Extractor for Research and Education</a></strong><br><a href=/people/v/vasanthi-vuppuluri/>Vasanthi Vuppuluri</a>
|
<a href=/people/s/shahryar-baki/>Shahryar Baki</a>
|
<a href=/people/a/an-nguyen/>An Nguyen</a>
|
<a href=/people/r/rakesh-verma/>Rakesh Verma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-3027><div class="card-body p-3 small">Collocation and idiom extraction are well-known challenges with many potential applications in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a>. Our experimental, open-source software system, called ICE, is a python package for flexibly extracting collocations and idioms, currently in English. It also has a competitive POS tagger that can be used alone or as part of collocation / idiom extraction. ICE is available free of cost for research and educational uses in two user-friendly formats. This paper gives an overview of ICE and its performance, and briefly describes the research underlying the extraction algorithms.</div></div></div><hr><div id=e17-4><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/E17-4/>Proceedings of the Student Research Workshop at the 15th Conference of the European Chapter of the Association for Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4000/>Proceedings of the Student Research Workshop at the 15th Conference of the <span class=acl-fixed-case>E</span>uropean Chapter of the Association for Computational Linguistics</a></strong><br><a href=/people/f/florian-kunneman/>Florian Kunneman</a>
|
<a href=/people/u/uxoa-inurrieta/>Uxoa Iñurrieta</a>
|
<a href=/people/j/john-j-camilleri/>John J. Camilleri</a>
|
<a href=/people/m/mariona-coll-ardanuy/>Mariona Coll Ardanuy</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4001 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4001/>Pragmatic descriptions of perceptual stimuli</a></strong><br><a href=/people/e/emiel-van-miltenburg/>Emiel van Miltenburg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4001><div class="card-body p-3 small">This research proposal discusses pragmatic factors in image description, arguing that current automatic image description systems do not take these factors into account. I present a general model of the human image description process, and propose to study this process using <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus analysis</a>, experiments, and <a href=https://en.wikipedia.org/wiki/Computer_simulation>computational modeling</a>. This will lead to a better characterization of human image description behavior, providing a road map for future research in automatic image description, and the automatic description of perceptual stimuli in general.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4002 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4002/>Detecting spelling variants in <a href=https://en.wikipedia.org/wiki/Standard_language>non-standard texts</a></a></strong><br><a href=/people/f/fabian-barteld/>Fabian Barteld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4002><div class="card-body p-3 small">Spelling variation in <a href=https://en.wikipedia.org/wiki/Standard_language>non-standard language</a>, e.g. computer-mediated communication and <a href=https://en.wikipedia.org/wiki/Text_corpus>historical texts</a>, is usually treated as a deviation from a standard spelling, e.g. 2mr as an non-standard spelling for tomorrow. Consequently, in normalization the standard approach of dealing with spelling variation so-called non-standard words are mapped to their corresponding standard words. However, there is not always a corresponding <a href=https://en.wikipedia.org/wiki/Standard_language>standard word</a>. This can be the case for single types (like <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a> in computer-mediated communication) or a complete language, e.g. texts from <a href=https://en.wikipedia.org/wiki/Historical_language>historical languages</a> that did not develop to a standard variety. The approach presented in this thesis proposal deals with spelling variation in absence of reference to a standard. The task is to detect pairs of types that are variants of the same <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological word</a>. An approach for spelling-variant detection is presented, where pairs of potential spelling variants are generated with <a href=https://en.wikipedia.org/wiki/Levenshtein_distance>Levenshtein distance</a> and subsequently filtered by <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised machine learning</a>. The <a href=https://en.wikipedia.org/wiki/Composition_(language)>approach</a> is evaluated on historical Low German texts. Finally, further perspectives are discussed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4003 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-4003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-4003/>Replication issues in syntax-based aspect extraction for <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a></a></strong><br><a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4003><div class="card-body p-3 small">Reproducing experiments is an important instrument to validate previous work and build upon existing approaches. It has been tackled numerous times in different areas of science. In this paper, we introduce an empirical replicability study of three well-known <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for syntactic centric aspect-based opinion mining. We show that reproducing results continues to be a difficult endeavor, mainly due to the lack of details regarding preprocessing and parameter setting, as well as due to the absence of available implementations that clarify these details. We consider these are important threats to validity of the research on the field, specifically when compared to other problems in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> where public datasets and <a href=https://en.wikipedia.org/wiki/Software_release_life_cycle>code availability</a> are critical validity components. We conclude by encouraging code-based research, which we think has a key role in helping researchers to understand the meaning of the state-of-the-art better and to generate continuous advances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4004 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4004/>Discourse Relations and <a href=https://en.wikipedia.org/wiki/Conjoined_twins>Conjoined VPs</a> : Automated Sense Recognition<span class=acl-fixed-case>VP</span>s: Automated Sense Recognition</a></strong><br><a href=/people/v/valentina-pyatkin/>Valentina Pyatkin</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4004><div class="card-body p-3 small">Sense classification of discourse relations is a sub-task of shallow discourse parsing. Discourse relations can occur both across sentences (inter-sentential) and within sentences (intra-sentential), and more than one <a href=https://en.wikipedia.org/wiki/Discourse_relation>discourse relation</a> can hold between the same units. Using a newly available corpus of discourse-annotated intra-sentential conjoined verb phrases, we demonstrate a sequential classification pipeline for their multi-label sense classification. We assess the importance of each <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature</a> used in the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature scope</a>, and what is lost in moving from gold standard manual parses to the output of an off-the-shelf <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>.<i>inter-sentential</i>) and within sentences (<i>intra-sentential</i>), and more than one discourse relation can hold between the same units. Using a newly available corpus of discourse-annotated intra-sentential conjoined verb phrases, we demonstrate a sequential classification pipeline for their multi-label sense classification. We assess the importance of each feature used in the classification, the feature scope, and what is lost in moving from gold standard manual parses to the output of an off-the-shelf parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4005 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4005/>Deception detection in <a href=https://en.wikipedia.org/wiki/Russian_language>Russian texts</a><span class=acl-fixed-case>R</span>ussian texts</a></strong><br><a href=/people/o/olga-litvinova/>Olga Litvinova</a>
|
<a href=/people/p/pavel-seredin/>Pavel Seredin</a>
|
<a href=/people/t/tatiana-litvinova/>Tatiana Litvinova</a>
|
<a href=/people/j/john-lyell/>John Lyell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4005><div class="card-body p-3 small">Humans are known to detect <a href=https://en.wikipedia.org/wiki/Deception>deception</a> in <a href=https://en.wikipedia.org/wiki/Speech>speech</a> randomly and it is therefore important to develop tools to enable them to detect <a href=https://en.wikipedia.org/wiki/Deception>deception</a>. The problem of deception detection has been studied for a significant amount of time, however the last 10-15 years have seen methods of <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> being employed. Texts are processed using different <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tools</a> and then classified as deceptive / truthful using <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a>. While most research has been performed for <a href=https://en.wikipedia.org/wiki/English_language>English</a>, Slavic languages have never been a focus of detection deception studies. The paper deals with deception detection in <a href=https://en.wikipedia.org/wiki/Russian_language>Russian narratives</a>. It employs a specially designed corpus of truthful and deceptive texts on the same topic from each respondent, N = 113. The texts were processed using <a href=https://en.wikipedia.org/wiki/Linguistic_Inquiry>Linguistic Inquiry</a> and Word Count software that is used in most studies of text-based deception detection. The list of parameters computed using the <a href=https://en.wikipedia.org/wiki/Software>software</a> was expanded due to the designed users&#8217; dictionaries. A variety of text classification methods was employed. The <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> was found to depend on the author&#8217;s gender and text type (deceptive / truthful).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4006 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4006/>A Computational Model of Human Preferences for Pronoun Resolution</a></strong><br><a href=/people/o/olga-seminck/>Olga Seminck</a>
|
<a href=/people/p/pascal-amsili/>Pascal Amsili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4006><div class="card-body p-3 small">We present a cognitive computational model of pronoun resolution that reproduces the human interpretation preferences of the Subject Assignment Strategy and the Parallel Function Strategy. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> relies on a probabilistic pronoun resolution system trained on <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus data</a>. Factors influencing pronoun resolution are represented as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> weighted by their relative importance. The importance the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> gives to the preferences is in line with <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistic studies</a>. We demonstrate the cognitive plausibility of the model by running it on experimental items and simulating antecedent choice and reading times of human participants. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be used as a new means to study pronoun resolution, because it captures the interaction of preferences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4007 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4007/>Automatic Extraction of News Values from Headline Text</a></strong><br><a href=/people/a/alicja-piotrkowicz/>Alicja Piotrkowicz</a>
|
<a href=/people/v/vania-dimitrova/>Vania Dimitrova</a>
|
<a href=/people/k/katja-markert/>Katja Markert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4007><div class="card-body p-3 small">Headlines play a crucial role in attracting audiences&#8217; attention to <a href=https://en.wikipedia.org/wiki/Digital_artifact>online artefacts</a> (e.g. news articles, videos, blogs). The ability to carry out an automatic, large-scale analysis of headlines is critical to facilitate the selection and prioritisation of a large volume of <a href=https://en.wikipedia.org/wiki/Digital_content>digital content</a>. In journalism studies news content has been extensively studied using manually annotated news values-factors used implicitly and explicitly when making decisions on the selection and prioritisation of news items. This paper presents the first attempt at a fully automatic extraction of <a href=https://en.wikipedia.org/wiki/News_value>news values</a> from <a href=https://en.wikipedia.org/wiki/Headline>headline text</a>. The news values extraction methods are applied on a large headlines corpus collected from The Guardian, and evaluated by comparing it with a manually annotated gold standard. A crowdsourcing survey indicates that <a href=https://en.wikipedia.org/wiki/News_values>news values</a> affect people&#8217;s decisions to click on a headline, supporting the need for an automatic news values detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4008 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=E17-4008" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/E17-4008/>Assessing Convincingness of Arguments in Online Debates with Limited Number of Features</a></strong><br><a href=/people/l/lisa-andreevna-chalaguine/>Lisa Andreevna Chalaguine</a>
|
<a href=/people/c/claudia-schulz/>Claudia Schulz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4008><div class="card-body p-3 small">We propose a new method in the field of argument analysis in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> to determining convincingness of arguments in online debates, following previous research by Habernal and Gurevych (2016). Rather than using argument specific feature values, we measure <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature values</a> relative to the average value in the debate, allowing us to determine argument convincingness with fewer <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> (between 5 and 35) than normally used for natural language processing tasks. We use a simple forward-feeding neural network for this task and achieve an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 0.77 which is comparable to the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> obtained using 64k features and a support vector machine by Habernal and Gurevych.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4009 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4009/>Zipf’s and Benford’s laws in Twitter hashtags<span class=acl-fixed-case>Z</span>ipf’s and <span class=acl-fixed-case>B</span>enford’s laws in <span class=acl-fixed-case>T</span>witter hashtags</a></strong><br><a href=/people/j/jose-alberto-perez-melian/>José Alberto Pérez Melián</a>
|
<a href=/people/j/j-alberto-conejero/>J. Alberto Conejero</a>
|
<a href=/people/c/cesar-ferri-ramirez/>Cèsar Ferri Ramírez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4009><div class="card-body p-3 small">Social networks have transformed communication dramatically in recent years through the rise of new <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> and the development of a new language of communication. This <a href=https://en.wikipedia.org/wiki/Landscape>landscape</a> requires new forms to describe and predict the behaviour of users in <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a>. This paper presents an analysis of the <a href=https://en.wikipedia.org/wiki/Frequency_distribution>frequency distribution of hashtag popularity</a> in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter conversations</a>. Our objective is to determine if these <a href=https://en.wikipedia.org/wiki/Frequency_distribution>frequency distribution</a> follow some well-known <a href=https://en.wikipedia.org/wiki/Frequency_distribution>frequency distribution</a> that many real-life sets of numerical data satisfy. In particular, we study the similarity of <a href=https://en.wikipedia.org/wiki/Frequency_distribution>frequency distribution</a> of hashtag popularity with respect to <a href=https://en.wikipedia.org/wiki/Zipf&#8217;s_law>Zipf&#8217;s law</a>, an empirical law referring to the phenomenon that many types of data in <a href=https://en.wikipedia.org/wiki/Social_science>social sciences</a> can be approximated with a <a href=https://en.wikipedia.org/wiki/Zipf&#8217;s_law>Zipfian distribution</a>. Additionally, we also analyse <a href=https://en.wikipedia.org/wiki/Benford&#8217;s_law>Benford&#8217;s law</a>, is a special case of <a href=https://en.wikipedia.org/wiki/Zipf&#8217;s_law>Zipf&#8217;s law</a>, a common pattern about the <a href=https://en.wikipedia.org/wiki/Frequency_distribution>frequency distribution</a> of leading digits. In order to compute correctly the <a href=https://en.wikipedia.org/wiki/Frequency_distribution>frequency distribution of hashtag popularity</a>, we need to correct many spelling errors that Twitter&#8217;s users introduce. For this purpose we introduce a new <a href=https://en.wikipedia.org/wiki/Filter_(software)>filter</a> to correct hashtag mistake based on string distances. The experiments obtained employing datasets of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter streams</a> generated under controlled conditions show that <a href=https://en.wikipedia.org/wiki/Benford&#8217;s_law>Benford&#8217;s law</a> and <a href=https://en.wikipedia.org/wiki/Zipf&#8217;s_law>Zipf&#8217;s law</a> can be used to model hashtag frequency distribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4010 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4010/>A Multi-aspect Analysis of Automatic Essay Scoring for <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a><span class=acl-fixed-case>B</span>razilian <span class=acl-fixed-case>P</span>ortuguese</a></strong><br><a href=/people/e/evelin-amorim/>Evelin Amorim</a>
|
<a href=/people/a/adriano-veloso/>Adriano Veloso</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4010><div class="card-body p-3 small">Several methods for automatic essay scoring (AES) for <a href=https://en.wikipedia.org/wiki/English_language>English language</a> have been proposed. However, multi-aspect AES systems for other languages are unusual. Therefore, we propose a multi-aspect AES system to apply on a dataset of Brazilian Portuguese essays, which human experts evaluated according to five aspects defined by Brazilian Government to the National Exam to High School Student (ENEM). These aspects are skills that student must master and every skill is assessed apart from each other. Besides the prediction of each aspect, the feature analysis also was performed for each aspect. The <a href=https://en.wikipedia.org/wiki/Advanced_Encryption_Standard>AES system</a> proposed employs several <a href=https://en.wikipedia.org/wiki/Advanced_Encryption_Standard>features</a> already employed by <a href=https://en.wikipedia.org/wiki/Advanced_Encryption_Standard>AES systems</a> for <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. Our results show that predictions for some aspects performed well with the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> we employed, while predictions for other aspects performed poorly. Also, it is possible to note the difference between the five aspects in the detailed feature analysis we performed. Besides these contributions, the eight millions of enrollments every year for ENEM raise some challenge issues for future directions in our research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-4012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-4012 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-4012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-4012/>Evaluating the Reliability and Interaction of Recursively Used Feature Classes for Terminology Extraction</a></strong><br><a href=/people/a/anna-hatty/>Anna Hätty</a>
|
<a href=/people/m/michael-dorna/>Michael Dorna</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-4012><div class="card-body p-3 small">Feature design and selection is a crucial aspect when treating <a href=https://en.wikipedia.org/wiki/Terminology_extraction>terminology extraction</a> as a machine learning classification problem. We designed feature classes which characterize different properties of terms based on <a href=https://en.wikipedia.org/wiki/Probability_distribution>distributions</a>, and propose a new feature class for components of term candidates. By using <a href=https://en.wikipedia.org/wiki/Random_forest>random forests</a>, we infer optimal features which are later used to build <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>decision tree classifiers</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> using the ACL RD-TEC dataset. We demonstrate the importance of the novel feature class for downgrading termhood which exploits properties of term components. Furthermore, our <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> suggests that the identification of reliable term candidates should be performed successively, rather than just once.</div></div></div><hr><div id=e17-5><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/E17-5/>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-5001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-5001 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-5001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-5001/>Universal Dependencies<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies</a></strong><br><a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/d/daniel-zeman/>Daniel Zeman</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-5001><div class="card-body p-3 small">Universal Dependencies (UD) is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages. This tutorial gives an introduction to the UD framework and resources, from basic design principles to annotation guidelines and existing <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a>. We also discuss tools for developing and exploiting UD treebanks and survey applications of UD in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-5002/>Practical Neural Machine Translation</a></strong><br><a href=/people/r/rico-sennrich/>Rico Sennrich</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-5002><div class="card-body p-3 small">Neural Machine Translation (NMT) has achieved new breakthroughs in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> in recent years. It has dominated recent shared translation tasks in machine translation research, and is also being quickly adopted in industry. The technical differences between NMT and the previously dominant phrase-based statistical approach require that practictioners learn new best practices for building MT systems, ranging from different hardware requirements, new techniques for handling rare words and monolingual data, to new opportunities in continued learning and domain adaptation. This tutorial is aimed at researchers and users of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> interested in working with NMT. The tutorial will cover a basic theoretical introduction to <a href=https://en.wikipedia.org/wiki/Network_topology>NMT</a>, discuss the components of state-of-the-art systems, and provide practical advice for building <a href=https://en.wikipedia.org/wiki/Network_topology>NMT systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-5003/>Imitation learning for <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a></a></strong><br><a href=/people/a/andreas-vlachos/>Andreas Vlachos</a>
|
<a href=/people/g/gerasimos-lampouras/>Gerasimos Lampouras</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-5003><div class="card-body p-3 small">Imitation learning is a learning paradigm originally developed to learn <a href=https://en.wikipedia.org/wiki/Robot_control>robotic controllers</a> from demonstrations by humans, e.g. autonomous flight from pilot demonstrations. Recently, algorithms for <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> were proposed under this paradigm and have been applied successfully to a number of tasks including syntactic dependency parsing, <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a>, <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>, dynamic feature selection, semantic parsing and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. Key advantages are the ability to handle large output search spaces and to learn with non-decomposable loss functions. Our aim in this tutorial is to have a unified presentation of the various imitation algorithms for structure prediction, and show how they can be applied to a variety of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. All material associated with the tutorial will be made available through https://sheffieldnlp.github.io/ImitationLearningTutorialEACL2017/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-5005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-5005 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-5005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-5005/>Integer Linear Programming formulations in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-5005><div class="card-body p-3 small">Making decisions in natural language processing problems often involves assigning values to sets of <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>interdependent variables</a> where the expressive dependency structure can influence, or even dictate what assignments are possible. This setting includes a broad range of structured prediction problems such as <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>, <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity and relation recognition</a>, co-reference resolution, dependency parsing and <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. The setting is also appropriate for cases that may require making global decisions that involve multiple components, possibly pre-designed or pre-learned, as in event recognition and analysis, <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a>, textual entailment and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. In all these cases, it is natural to formulate the <a href=https://en.wikipedia.org/wiki/Decision_problem>decision problem</a> as a constrained optimization problem, with an <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> that is composed of learned models, subject to domain or problem specific constraints. Over the last few years, starting with a couple of papers written by (Roth & Yih, 2004, 2005), dozens of papers have been using the Integer linear programming (ILP) formulation developed there, including several award-winning papers (e.g., (Martins, Smith, & Xing, 2009 ; Koo, Rush, Collins, Jaakkola, & Sontag., 2010 ; Berant, Dagan, & Goldberger, 2011)).This tutorial will present the key ingredients of ILP formulations of natural language processing problems, aiming at guiding readers through the key modeling steps, explaining the learning and inference paradigms and exemplifying these by providing examples from the literature. We will cover a range of topics, from the theoretical foundations of learning and inference with ILP models, to practical modeling guides, to software packages and applications. The goal of this tutorial is to introduce the computational framework to broader ACL community, motivate it as a generic framework for learning and inference in global NLP decision problems, present some of the key theoretical and practical issues involved and survey some of the existing applications of it as a way to promote further development of the framework and additional applications. We will also make connections with some of the hot topics in current NLP research and show how they can be used within the general framework proposed here. The tutorial will thus be useful for many of the senior and junior researchers that have interest in global decision problems in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, providing a concise overview of recent perspectives and research results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/E17-5006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-E17-5006 data-toggle=collapse aria-expanded=false aria-controls=abstract-E17-5006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/E17-5006/>Building Multimodal Simulations for Natural Language</a></strong><br><a href=/people/j/james-pustejovsky/>James Pustejovsky</a>
|
<a href=/people/n/nikhil-krishnaswamy/>Nikhil Krishnaswamy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-E17-5006><div class="card-body p-3 small">In this tutorial, we introduce a computational framework and modeling language (VoxML) for composing multimodal simulations of natural language expressions within a 3D simulation environment (VoxSim). We demonstrate how to construct voxemes, which are visual object representations of linguistic entities. We also show how to compose events and actions over these <a href=https://en.wikipedia.org/wiki/Object_(computer_science)>objects</a>, within a restricted domain of dynamics. This gives us the building blocks to simulate narratives of multiple events or participate in a multimodal dialogue with synthetic agents in the simulation environment. To our knowledge, this is the first time such material has been presented as a tutorial within the CL community. This will be of relevance to students and researchers interested in modeling actionable language, natural language communication with agents and robots, spatial and temporal constraint solving through language, referring expression generation, embodied cognition, as well as minimal model creation. Multimodal simulation of language, particularly motion expressions, brings together a number of existing lines of research from the computational linguistic, semantics, robotics, and formal logic communities, including action and event representation (Di Eugenio, 1991), modeling gestural correlates to NL expressions (Kipp et al., 2007 ; Neff et al., 2008), and action event modeling (Kipper and Palmer, 2000 ; Yang et al., 2015). We combine an approach to event modeling with a scene generation approach akin to those found in work by (Coyne and Sproat, 2001 ; Siskind, 2011 ; Chang et al., 2015). Mapping natural language expressions through a <a href=https://en.wikipedia.org/wiki/Formal_system>formal model</a> and a dynamic logic interpretation into a visualization of the event described provides an environment for grounding concepts and referring expressions that is interpretable by both a computer and a human user. This opens a variety of avenues for humans to communicate with computerized agents and robots, as in (Matuszek et al., 2013 ; Lauria et al., 2001), (Forbes et al., 2015), and (Deits et al., 2013 ; Walter et al., 2013 ; Tellex et al., 2014). Simulation and automatic visualization of events from natural language descriptions and supplementary modalities, such as gestures, allows humans to use their native capabilities as linguistic and visual interpreters to collaborate on tasks with an artificial agent or to put semantic intuitions to the test in an environment where user and agent share a common context. In previous work (Pustejovsky and Krishnaswamy, 2014 ; Pustejovsky, 2013a), we introduced a method for modeling natural language expressions within a 3D simulation environment built on top of the game development platform Unity (Goldstone, 2009). The goal of that work was to evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research : an explicit encoding for how an object is itself situated relative to its environment ; and an operational characterization of how an object changes its location or how an agent acts on an object over time, e.g., its affordance structure. The former has developed into a semantic notion of situational context, called a habitat (Pustejovsky, 2013a ; McDonald and Pustejovsky, 2014), while the latter is addressed by dynamic interpretations of event structure (Pustejovsky and Moszkowicz, 2011 ; Pustejovsky and Krishnaswamy, 2016b ; Pustejovsky, 2013b).The requirements on building a visual simulation from language include several components. We require a rich type system for lexical items and their composition, as well as a language for modeling the dynamics of events, based on Generative Lexicon (GL). Further, a minimal embedding space (MES) for the <a href=https://en.wikipedia.org/wiki/Simulation>simulation</a> must be determined. This is the <a href=https://en.wikipedia.org/wiki/Three-dimensional_space>3D region</a> within which the state is configured or the event unfolds. Object-based attributes for participants in a situation or event also need to be specified ; e.g., <a href=https://en.wikipedia.org/wiki/Orientation_(geometry)>orientation</a>, relative size, default position or pose, etc. The <a href=https://en.wikipedia.org/wiki/Simulation>simulation</a> establishes an epistemic condition on the object and event rendering, imposing an implicit point of view (POV). Finally, there must be some sort of agent-dependent embodiment ; this determines the relative scaling of an agent and its event participants and their surroundings, as it engages in the environment. In order to construct a robust simulation from linguistic input, an event and its participants must be embedded within an appropriate minimal embedding space. This must sufficiently enclose the <a href=https://en.wikipedia.org/wiki/Event_(computing)>event localization</a>, while optionally including space enough for a frame of reference for the event (the viewers perspective).We first describe the formal multimodal foundations for the <a href=https://en.wikipedia.org/wiki/Modeling_language>modeling language</a>, VoxML, which creates a minimal simulation from the linguistic input interpreted by the multimodal language, DITL. We then describe VoxSim, the compositional modeling and simulation environment, which maps the minimal VoxML model of the linguistic utterance to a simulation in <a href=https://en.wikipedia.org/wiki/Unity_(game_engine)>Unity</a>. This knowledge includes specification of object affordances, e.g., what actions are possible or enabled by use an object. VoxML (Pustejovsky and Krishnaswamy, 2016b ; Pustejovsky and Krishnaswamy, 2016a) encodes semantic knowledge of real-world objects represented as <a href=https://en.wikipedia.org/wiki/3D_modeling>3D models</a>, and of events and attributes related to and enacted over these <a href=https://en.wikipedia.org/wiki/Object_(computer_science)>objects</a>. VoxML goes beyond the limitations of existing 3D visual markup languages by allowing for the encoding of a broad range of semantic knowledge that can be exploited by a simulation platform such as VoxSim. VoxSim (Krishnaswamy and Pustejovsky, 2016a ; Krishnaswamy and Pustejovsky, 2016b) uses object and event semantic knowledge to generate animated scenes in real time without a complex animation interface. It uses the <a href=https://en.wikipedia.org/wiki/Unity_(game_engine)>Unity game engine</a> for graphics and I / O processing and takes as input a simple <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language utterance</a>. The parsed utterance is semantically interpreted and transformed into a hybrid dynamic logic representation (DITL), and used to generate a minimal simulation of the event when composed with VoxML knowledge.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>