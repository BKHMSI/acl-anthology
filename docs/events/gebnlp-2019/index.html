<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Gender Bias in Natural Language Processing (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Gender Bias in Natural Language Processing (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w19-38>Proceedings of the First Workshop on Gender Bias in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li></ul></div></div><div id=w19-38><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-38.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W19-38/>Proceedings of the First Workshop on Gender Bias in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3800/>Proceedings of the First Workshop on Gender Bias in Natural Language Processing</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-juss√†</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/w/will-radford/>Will Radford</a>
|
<a href=/people/k/kellie-webster/>Kellie Webster</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3803/>Relating Word Embedding Gender Biases to Gender Gaps : A Cross-Cultural Analysis</a></strong><br><a href=/people/s/scott-friedman/>Scott Friedman</a>
|
<a href=/people/s/sonja-schmer-galunder/>Sonja Schmer-Galunder</a>
|
<a href=/people/a/anthony-chen/>Anthony Chen</a>
|
<a href=/people/j/jeffrey-rye/>Jeffrey Rye</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3803><div class="card-body p-3 small">Modern <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for common NLP tasks often employ <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning techniques</a> and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them ; however, these <a href=https://en.wikipedia.org/wiki/Bias>biases</a> may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through <a href=https://en.wikipedia.org/wiki/Big_data>big data</a>. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in <a href=https://en.wikipedia.org/wiki/Education>education</a>, <a href=https://en.wikipedia.org/wiki/Politics>politics</a>, <a href=https://en.wikipedia.org/wiki/Economics>economics</a>, and <a href=https://en.wikipedia.org/wiki/Health>health</a>. We validate these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3804" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3804/>Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories</a></strong><br><a href=/people/k/kaytlin-chaloner/>Kaytlin Chaloner</a>
|
<a href=/people/a/alfredo-maldonado/>Alfredo Maldonado</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3804><div class="card-body p-3 small">Prior work has shown that <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> capture <a href=https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States>human stereotypes</a>, including <a href=https://en.wikipedia.org/wiki/Sexism>gender bias</a>. However, there is a lack of studies testing the presence of specific gender bias categories in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> across diverse domains. This paper aims to fill this gap by applying the WEAT bias detection method to four sets of word embeddings trained on corpora from four different domains : <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Social_networking_service>social networking</a>, <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedical</a> and a gender-balanced corpus extracted from Wikipedia (GAP). We find that some domains are definitely more prone to <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> than others, and that the categories of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> present also vary for each set of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We detect some <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a> in GAP. We also propose a simple but novel method for discovering new bias categories by clustering word embeddings. We validate this method through WEAT&#8217;s hypothesis testing mechanism and find it useful for expanding the relatively small set of well-known gender bias word categories commonly used in the literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3805/>Evaluating the Underlying <a href=https://en.wikipedia.org/wiki/Gender_bias>Gender Bias</a> in Contextualized Word Embeddings</a></strong><br><a href=/people/c/christine-basta/>Christine Basta</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-juss√†</a>
|
<a href=/people/n/noe-casas/>Noe Casas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3805><div class="card-body p-3 small">Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with <a href=https://en.wikipedia.org/wiki/Gender_bias>gender bias</a>. Our analysis includes different measures previously applied in the literature to standard <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3806/>Conceptor Debiasing of Word Representations Evaluated on WEAT<span class=acl-fixed-case>WEAT</span></a></strong><br><a href=/people/s/saket-karve/>Saket Karve</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a>
|
<a href=/people/j/joao-sedoc/>Jo√£o Sedoc</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3806><div class="card-body p-3 small">Bias in word representations, such as Word2Vec, has been widely reported and investigated, and efforts made to debias them. We apply the debiasing conceptor for <a href=https://en.wikipedia.org/wiki/Post-processing>post-processing</a> both traditional and contextualized word embeddings. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can simultaneously remove racial and gender biases from <a href=https://en.wikipedia.org/wiki/Word_formation>word representations</a>. Unlike standard <a href=https://en.wikipedia.org/wiki/Debiasing>debiasing methods</a>, the debiasing conceptor can utilize heterogeneous lists of biased words without loss in performance. Finally, our empirical experiments show that the debiasing conceptor diminishes racial and gender bias of word representations as measured using the Word Embedding Association Test (WEAT) of Caliskan et al.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3807/>Filling Gender & Number Gaps in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Black-box Context Injection</a></strong><br><a href=/people/a/amit-moryossef/>Amit Moryossef</a>
|
<a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3807><div class="card-body p-3 small">When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must guess this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>translation accuracy</a> in up to 2.3 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W19-3811/>BERT Masked Language Modeling for Co-reference Resolution<span class=acl-fixed-case>BERT</span> Masked Language Modeling for Co-reference Resolution</a></strong><br><a href=/people/f/felipe-alfaro/>Felipe Alfaro</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-juss√†</a>
|
<a href=/people/j/jose-a-r-fonollosa/>Jos√© A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3811><div class="card-body p-3 small">This paper explains the TALP-UPC participation for the Gendered Pronoun Resolution shared-task of the 1st ACL Workshop on Gender Bias for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>. We have implemented two models for mask language modeling using pre-trained BERT adjusted to work for a classification problem. The proposed solutions are based on the word probabilities of the original BERT model, but using common English names to replace the original test names.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3814 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3814" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3814/>Look Again at the Syntax : Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution</a></strong><br><a href=/people/y/yinchuan-xu/>Yinchuan Xu</a>
|
<a href=/people/j/junlin-yang/>Junlin Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3814><div class="card-body p-3 small">Gender bias has been found in existing coreference resolvers. In order to eliminate gender bias, a gender-balanced dataset Gendered Ambiguous Pronouns (GAP) has been released and the best baseline model achieves only 66.9 % F1. Bidirectional Encoder Representations from Transformers (BERT) has broken several NLP task records and can be used on GAP dataset. However, fine-tune BERT on a specific <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> is computationally expensive. In this paper, we propose an end-to-end resolver by combining pre-trained BERT with Relational Graph Convolutional Network (R-GCN). R-GCN is used for digesting structural syntactic information and learning better task-specific embeddings. Empirical results demonstrate that, under explicit syntactic supervision and without the need to fine tune BERT, R-GCN&#8217;s embeddings outperform the original BERT embeddings on the coreference task. Our <a href=https://en.wikipedia.org/wiki/Work_(thermodynamics)>work</a> significantly improves the snippet-context baseline F1 score on GAP dataset from 66.9 % to 80.3 %. We participated in the Gender Bias for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> 2019 shared task, and our codes are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W19-3818.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W19-3818 data-toggle=collapse aria-expanded=false aria-controls=abstract-W19-3818 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W19-3818" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W19-3818/>Anonymized BERT : An Augmentation Approach to the Gendered Pronoun Resolution Challenge<span class=acl-fixed-case>BERT</span>: An Augmentation Approach to the Gendered Pronoun Resolution Challenge</a></strong><br><a href=/people/b/bo-liu/>Bo Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W19-3818><div class="card-body p-3 small">We present our 7th place solution to the Gendered Pronoun Resolution challenge, which uses BERT without fine-tuning and a novel augmentation strategy designed for contextual embedding token-level tasks. Our method anonymizes the referent by replacing candidate names with a set of common placeholder names. Besides the usual benefits of effectively increasing <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data size</a>, this approach diversifies idiosyncratic information embedded in <a href=https://en.wikipedia.org/wiki/Name>names</a>. Using same set of common first names can also help the model recognize <a href=https://en.wikipedia.org/wiki/Name>names</a> better, shorten token length, and remove gender and regional biases associated with <a href=https://en.wikipedia.org/wiki/Name>names</a>. The <a href=https://en.wikipedia.org/wiki/System>system</a> scored 0.1947 log loss in stage 2, where the augmentation contributed to an improvements of 0.04. Post-competition analysis shows that, when using different embedding layers, the <a href=https://en.wikipedia.org/wiki/System>system</a> scores 0.1799 which would be third place.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ¬©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>