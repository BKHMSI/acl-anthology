<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Neural Generation and Translation (2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Neural Generation and Translation (2018)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w18-27>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li></ul></div></div><div id=w18-27><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-27.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W18-27/>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2700/>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</a></strong><br><a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a>
|
<a href=/people/m/minh-thang-luong/>Thang Luong</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2701 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2701/>Findings of the Second Workshop on <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> and Generation</a></strong><br><a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a>
|
<a href=/people/m/minh-thang-luong/>Minh-Thang Luong</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/y/yusuke-oda/>Yusuke Oda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2701><div class="card-body p-3 small">This document describes the findings of the Second Workshop on Neural Machine Translation and Generation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2018). First, we summarize the research trends of papers presented in the proceedings, and note that there is particular interest in linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models. Second, we describe the results of the workshop&#8217;s shared task on efficient neural machine translation, where participants were tasked with creating MT systems that are both accurate and efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2704/>Inducing Grammars with and for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yonatan-bisk/>Yonatan Bisk</a>
|
<a href=/people/k/ke-m-tran/>Ke Tran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2704><div class="card-body p-3 small">Machine translation systems require <a href=https://en.wikipedia.org/wiki/Semantics>semantic knowledge</a> and <a href=https://en.wikipedia.org/wiki/Grammar>grammatical understanding</a>. Neural machine translation (NMT) systems often assume this information is captured by an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> and a decoder that ensures <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. Recent work has shown that incorporating explicit <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> alleviates the burden of modeling both types of <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a>. However, requiring <a href=https://en.wikipedia.org/wiki/Parsing>parses</a> is expensive and does not explore the question of what syntax a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> needs during <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. To address both of these issues we introduce a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that simultaneously translates while inducing dependency trees. In this way, we leverage the benefits of structure while investigating what syntax NMT must induce to maximize performance. We show that our dependency trees are 1. language pair dependent and 2. improve translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-2705" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-2705/>Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation</a></strong><br><a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/b/brian-thompson/>Brian Thompson</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2705><div class="card-body p-3 small">Supervised domain adaptationwhere a large generic corpus and a smaller in-domain corpus are both available for trainingis a challenge for neural machine translation (NMT). Standard practice is to train a generic model and use it to initialize a second <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, then continue training the second <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on in-domain data to produce an in-domain model. We add an auxiliary term to the training objective during continued training that minimizes the <a href=https://en.wikipedia.org/wiki/Cross_entropy>cross entropy</a> between the in-domain model&#8217;s output word distribution and that of the out-of-domain model to prevent the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>&#8217;s output from differing too much from the original out-of-domain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> shows improvements over standard continued training by up to 1.5 BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2706.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2706 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2706 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2706/>Controllable Abstractive Summarization</a></strong><br><a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/d/david-grangier/>David Grangier</a>
|
<a href=/people/m/michael-auli/>Michael Auli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2706><div class="card-body p-3 small">Current models for <a href=https://en.wikipedia.org/wiki/Document_summarization>document summarization</a> disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With <a href=https://en.wikipedia.org/wiki/Input_(computer_science)>user input</a>, our <a href=https://en.wikipedia.org/wiki/System>system</a> can produce high quality summaries that follow <a href=https://en.wikipedia.org/wiki/Preference>user preferences</a>. Without user input, we set the <a href=https://en.wikipedia.org/wiki/Control_variable>control variables</a> automatically on the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 F1-ROUGE and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2709/>On the Impact of Various Types of <a href=https://en.wikipedia.org/wiki/Noise>Noise</a> on Neural Machine Translation</a></strong><br><a href=/people/h/huda-khayrallah/>Huda Khayrallah</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2709><div class="card-body p-3 small">We examine how various types of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in the parallel training data impact the quality of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation systems</a>. We create five types of <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>artificial noise</a> and analyze how they degrade performance in neural and statistical machine translation. We find that neural models are generally more harmed by <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> than <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a>. For one especially egregious type of <a href=https://en.wikipedia.org/wiki/Noise>noise</a> they learn to just copy the input sentence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2711/>Multi-Source Neural Machine Translation with Missing Data</a></strong><br><a href=/people/y/yuta-nishimura/>Yuta Nishimura</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2711><div class="card-body p-3 small">Multi-source translation is an <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> to exploit multiple inputs (e.g. in two different languages) to increase <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>translation accuracy</a>. In this paper, we examine approaches for multi-source neural machine translation (NMT) using an incomplete multilingual corpus in which some translations are missing. In practice, many multilingual corpora are not complete due to the difficulty to provide translations in all of the relevant languages (for example, in <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED talks</a>, most English talks only have subtitles for a small portion of the languages that TED supports). Existing studies on multi-source translation did not explicitly handle such situations. This study focuses on the use of incomplete multilingual corpora in multi-encoder NMT and mixture of NMT experts and examines a very simple implementation where missing source translations are replaced by a special symbol NULL. These methods allow us to use incomplete corpora both at training time and test time. In experiments with real incomplete multilingual corpora of TED Talks, the multi-source NMT with the NULL tokens achieved higher translation accuracies measured by <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> than those by any one-to-one NMT systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-2715.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-2715 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-2715 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-2715/>OpenNMT System Description for WNMT 2018 : 800 words / sec on a single-core CPU<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>NMT</span> System Description for <span class=acl-fixed-case>WNMT</span> 2018: 800 words/sec on a single-core <span class=acl-fixed-case>CPU</span></a></strong><br><a href=/people/j/jean-senellart/>Jean Senellart</a>
|
<a href=/people/d/dakun-zhang/>Dakun Zhang</a>
|
<a href=/people/b/bo-wang/>Bo Wang</a>
|
<a href=/people/g/guillaume-klein/>Guillaume Klein</a>
|
<a href=/people/j/jean-pierre-ramatchandirin/>Jean-Pierre Ramatchandirin</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-2715><div class="card-body p-3 small">We present a system description of the OpenNMT Neural Machine Translation entry for the WNMT 2018 evaluation. In this work, we developed a heavily optimized NMT inference model targeting a <a href=https://en.wikipedia.org/wiki/Supercomputer>high-performance CPU system</a>. The final system uses a combination of four techniques, all of them lead to significant speed-ups in combination : (a) sequence distillation, (b) architecture modifications, (c) <a href=https://en.wikipedia.org/wiki/Precomputation>precomputation</a>, particularly of vocabulary, and (d) CPU targeted quantization. This work achieves the fastest performance of the shared task, and led to the development of new features that have been integrated to OpenNMT and available to the community.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>