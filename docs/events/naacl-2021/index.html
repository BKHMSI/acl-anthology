<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>North American Chapter of the Association for Computational Linguistics (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>North American Chapter of the Association for Computational Linguistics (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021naacl-main>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a>
<span class="badge badge-info align-middle ml-1">187&nbsp;papers</span></li><li><a class=align-middle href=#2021naacl-demos>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li><li><a class=align-middle href=#2021naacl-srw>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2021naacl-tutorials>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2021naacl-industry>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li><li><a class=align-middle href=#2021alvr-1>Proceedings of the Second Workshop on Advances in Language and Vision Research</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2021americasnlp-1>Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#2021autosimtrans-1>Proceedings of the Second Workshop on Automatic Simultaneous Translation</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#2021bionlp-1>Proceedings of the 20th Workshop on Biomedical Language Processing</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#2021calcs-1>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2021clpsych-1>Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#2021cmcl-1>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li><li><a class=align-middle href=#2021dash-1>Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2021deelio-1>Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2021maiworkshop-1>Proceedings of the Third Workshop on Multimodal Artificial Intelligence</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2021nlp4if-1>Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2021nuse-1>Proceedings of the Third Workshop on Narrative Understanding</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#2021privatenlp-1>Proceedings of the Third Workshop on Privacy in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2021sdp-1>Proceedings of the Second Workshop on Scholarly Document Processing</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#2021sigtyp-1>Proceedings of the Third Workshop on Computational Typology and Multilingual NLP</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#2021smm4h-1>Proceedings of the Sixth Social Media Mining for Health (#SMM4H) Workshop and Shared Task</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li><li><a class=align-middle href=#2021socialnlp-1>Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#2021teachingnlp-1>Proceedings of the Fifth Workshop on Teaching NLP</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#2021trustnlp-1>Proceedings of the First Workshop on Trustworthy Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">6&nbsp;papers</span></li></ul></div></div><div id=2021naacl-main><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.naacl-main/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.0/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</a></strong><br><a href=/people/k/kristina-toutanova/>Kristina Toutanova</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tur</a>
|
<a href=/people/i/iz-beltagy/>Iz Beltagy</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a>
|
<a href=/people/y/yichao-zhou/>Yichao Zhou</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.1/>Knowledge Router : Learning Disentangled Representations for Knowledge Graphs</a></strong><br><a href=/people/s/shuai-zhang/>Shuai Zhang</a>
|
<a href=/people/x/xi-rao/>Xi Rao</a>
|
<a href=/people/y/yi-tay/>Yi Tay</a>
|
<a href=/people/c/ce-zhang/>Ce Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--1><div class="card-body p-3 small">The design of expressive representations of entities and relations in a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> is an important endeavor. While many of the existing approaches have primarily focused on learning from relational patterns and structural information, the intrinsic <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of KG entities has been more or less overlooked. More concretely, we hypothesize KG entities may be more complex than we think, i.e., an entity may wear many hats and relational triplets may form due to more than a single reason. To this end, this paper proposes to learn disentangled representations of KG entities-a new method that disentangles the inner latent properties of KG entities. Our disentangled process operates at the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph level</a> and a neighborhood mechanism is leveraged to disentangle the hidden properties of each entity. This disentangled representation learning approach is model agnostic and compatible with canonical KG embedding approaches. We conduct extensive experiments on several benchmark datasets, equipping a variety of models (DistMult, SimplE, and QuatE) with our proposed disentangling mechanism. Experimental results demonstrate that our proposed approach substantially improves performance on key metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.3/>Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks</a></strong><br><a href=/people/m/minh-van-nguyen/>Minh Van Nguyen</a>
|
<a href=/people/v/viet-lai/>Viet Lai</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--3><div class="card-body p-3 small">Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to capture inter-dependencies between tasks. First, at the representation level, we introduce an interaction graph between instances of the four <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that is used to enrich the prediction representation for one instance with those from related instances of other tasks. Second, at the label level, we propose a <a href=https://en.wikipedia.org/wiki/Dependency_graph>dependency graph</a> for the information types in the four IE tasks that captures the connections between the types expressed in an input sentence. A new <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization mechanism</a> is introduced to enforce the consistency between the golden and predicted type dependency graphs to improve <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. We show that the proposed model achieves the state-of-the-art performance for joint IE on both monolingual and multilingual learning settings with three different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.10/>Multilingual Language Models Predict Human Reading Behavior</a></strong><br><a href=/people/n/nora-hollenstein/>Nora Hollenstein</a>
|
<a href=/people/f/federico-pirovano/>Federico Pirovano</a>
|
<a href=/people/c/ce-zhang/>Ce Zhang</a>
|
<a href=/people/l/lena-jager/>Lena Jäger</a>
|
<a href=/people/l/lisa-beinborn/>Lisa Beinborn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--10><div class="card-body p-3 small">We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, and Russian texts. This results in accurate <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human processing mechanisms. We find that BERT and XLM models successfully predict a range of eye tracking features. In a series of experiments, we analyze the cross-domain and cross-language abilities of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and show how they reflect human sentence processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.12/>A Non-Linear Structural Probe</a></strong><br><a href=/people/j/jennifer-c-white/>Jennifer C. White</a>
|
<a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/n/naomi-saphra/>Naomi Saphra</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--12><div class="card-body p-3 small">Probes are models devised to investigate the encoding of knowledgee.g. syntactic structurein <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual representations</a>. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information ; one such restriction is <a href=https://en.wikipedia.org/wiki/Linearity>linearity</a>. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformations</a>. By observing that the structural probe learns a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a>, achieves a statistically significant improvement over the baseline in all languagesimplying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT&#8217;s self-attention layers and speculate that this resemblance leads to the RBF-based probe&#8217;s stronger performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.13/>Concealed Data Poisoning Attacks on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP Models</a><span class=acl-fixed-case>NLP</span> Models</a></strong><br><a href=/people/e/eric-wallace/>Eric Wallace</a>
|
<a href=/people/t/tony-zhao/>Tony Zhao</a>
|
<a href=/people/s/shi-feng/>Shi Feng</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--13><div class="card-body p-3 small">Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model&#8217;s training set that causes the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to frequently predict Positive whenever the input contains James Bond. Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> (Apple iPhone triggers negative generations) and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> (iced coffee mistranslated as hot coffee). We conclude by proposing three defenses that can mitigate our <a href=https://en.wikipedia.org/wiki/Cyberattack>attack</a> at some cost in prediction accuracy or extra human annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.14.OptionalSupplementaryCode.zip data-toggle=tooltip data-placement=top title="Optional supplementary code"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.14/>Backtranslation Feedback Improves User Confidence in MT, Not Quality<span class=acl-fixed-case>MT</span>, Not Quality</a></strong><br><a href=/people/v/vilem-zouhar/>Vilém Zouhar</a>
|
<a href=/people/m/michal-novak/>Michal Novák</a>
|
<a href=/people/m/matus-zilinec/>Matúš Žilinec</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/m/mateo-obregon/>Mateo Obregón</a>
|
<a href=/people/r/robin-l-hill/>Robin L. Hill</a>
|
<a href=/people/f/frederic-blain/>Frédéric Blain</a>
|
<a href=/people/m/marina-fomicheva/>Marina Fomicheva</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/l/lisa-yankovskaya/>Lisa Yankovskaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--14><div class="card-body p-3 small">Translating text into a language unknown to the text&#8217;s author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected : backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> and <a href=https://en.wikipedia.org/wiki/Estonian_language>Estonian</a>. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process : it increases user confidence in the produced translation, but not the objective quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.17/>Neural Machine Translation without Embeddings</a></strong><br><a href=/people/u/uri-shaham/>Uri Shaham</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--17><div class="card-body p-3 small">Many NLP models operate over sequences of subword tokens produced by hand-crafted tokenization rules and heuristic subword induction algorithms. A simple universal alternative is to represent every computerized text as a sequence of bytes via <a href=https://en.wikipedia.org/wiki/UTF-8>UTF-8</a>, obviating the need for an embedding layer since there are fewer token types (256) than dimensions. Surprisingly, replacing the ubiquitous embedding layer with one-hot representations of each byte does not hurt performance ; experiments on byte-to-byte machine translation from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subword-level models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.18/>Counterfactual Data Augmentation for Neural Machine Translation</a></strong><br><a href=/people/q/qi-liu/>Qi Liu</a>
|
<a href=/people/m/matt-kusner/>Matt Kusner</a>
|
<a href=/people/p/phil-blunsom/>Phil Blunsom</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--18><div class="card-body p-3 small">We propose a data augmentation method for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. It works by interpreting <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and phrasal alignment causally. Specifically, it creates augmented parallel translation corpora by generating (path-specific) counterfactual aligned phrases. We generate these by sampling new source phrases from a masked language model, then sampling an aligned counterfactual target phrase by noting that a translation language model can be interpreted as a Gumbel-Max Structural Causal Model (Oberst and Sontag, 2019). Compared to previous work, our method takes both <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> and <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> into account to maintain the <a href=https://en.wikipedia.org/wiki/Symmetry>symmetry</a> between source and target sequences. Experiments on IWSLT&#8217;15 English Vietnamese, WMT&#8217;17 English German, WMT&#8217;18 English Turkish, and WMT&#8217;19 robust English French show that the method can improve the performance of <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, backtranslation and <a href=https://en.wikipedia.org/wiki/Translation>translation robustness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.25/>DATE : Detecting Anomalies in Text via Self-Supervision of Transformers<span class=acl-fixed-case>DATE</span>: Detecting Anomalies in Text via Self-Supervision of Transformers</a></strong><br><a href=/people/a/andrei-manolache/>Andrei Manolache</a>
|
<a href=/people/f/florin-brad/>Florin Brad</a>
|
<a href=/people/e/elena-burceanu/>Elena Burceanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--25><div class="card-body p-3 small">Leveraging <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> for Anomaly Detection (AD) has seen widespread use in recent years due to superior performances over traditional methods. Recent deep methods for anomalies in images learn better features of normality in an end-to-end self-supervised setting. These methods train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to discriminate between different <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> applied to <a href=https://en.wikipedia.org/wiki/Visual_system>visual data</a> and then use the output to compute an anomaly score. We use this approach for AD in text, by introducing a novel pretext task on text sequences. We learn our DATE model end-to-end, enforcing two independent and complementary self-supervision signals, one at the token-level and one at the sequence-level. Under this new task formulation, we show strong quantitative and qualitative results on the 20Newsgroups and AG News datasets. In the <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised setting</a>, we outperform state-of-the-art results by +13.5 % and +6.9 %, respectively (AUROC). In the unsupervised configuration, DATE surpasses all other methods even when 10 % of its training data is contaminated with <a href=https://en.wikipedia.org/wiki/Outlier>outliers</a> (compared with 0 % for the others).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.27/>Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition</a></strong><br><a href=/people/d/dingmin-wang/>Dingmin Wang</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a>
|
<a href=/people/q/qi-liu/>Qi Liu</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--27><div class="card-body p-3 small">We present a fast and scalable architecture called Explicit Modular Decomposition (EMD), in which we incorporate both classification-based and extraction-based methods and design four modules (for clas- sification and sequence labelling) to jointly extract dialogue states. Experimental results based on the MultiWoz 2.0 dataset validates the superiority of our proposed model in terms of both <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> and <a href=https://en.wikipedia.org/wiki/Scalability>scalability</a> when compared to the state-of-the-art methods, especially in the scenario of multi-domain dialogues entangled with many turns of utterances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.28/>Augmented SBERT : Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks<span class=acl-fixed-case>SBERT</span>: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks</a></strong><br><a href=/people/n/nandan-thakur/>Nandan Thakur</a>
|
<a href=/people/n/nils-reimers/>Nils Reimers</a>
|
<a href=/people/j/johannes-daxenberger/>Johannes Daxenberger</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--28><div class="card-body p-3 small">There are two approaches for pairwise sentence scoring : Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While cross-encoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.30/>SGL : Speaking the Graph Languages of Semantic Parsing via Multilingual Translation<span class=acl-fixed-case>SGL</span>: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation</a></strong><br><a href=/people/l/luigi-procopio/>Luigi Procopio</a>
|
<a href=/people/r/rocco-tripodi/>Rocco Tripodi</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--30><div class="card-body p-3 small">Graph-based semantic parsing aims to represent <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>textual meaning</a> through <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graphs</a>. As one of the most promising general-purpose meaning representations, these <a href=https://en.wikipedia.org/wiki/Structure_(mathematical_logic)>structures</a> and their parsing have gained a significant interest momentum during recent years, with several diverse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalism</a>. In this work, instead, we reframe semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation (MNMT), and propose SGL, a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora coming from <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> : we report competitive performances on AMR and UCCA parsing, especially once paired with pre-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing : SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https://github.com/SapienzaNLP/sgl.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.33/>Meta-Learning for Domain Generalization in Semantic Parsing</a></strong><br><a href=/people/b/bailin-wang/>Bailin Wang</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--33><div class="card-body p-3 small">The importance of building semantic parsers which can be applied to new domains and generate programs unseen at training has long been acknowledged, and datasets testing out-of-domain performance are becoming increasingly available. However, little or no attention has been devoted to learning algorithms or objectives which promote domain generalization, with virtually all existing approaches relying on standard <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>. In this work, we use a meta-learning framework which targets zero-shot domain generalization for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. We apply a model-agnostic training algorithm that simulates zero-shot parsing by constructing virtual train and test sets from disjoint domains. The learning objective capitalizes on the intuition that gradient steps that improve source-domain performance should also improve target-domain performance, thus encouraging a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to generalize to unseen target domains. Experimental results on the (English) Spider and Chinese Spider datasets show that the meta-learning objective significantly boosts the performance of a baseline parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.36/>APo-VAE : Text Generation in <a href=https://en.wikipedia.org/wiki/Hyperbolic_space>Hyperbolic Space</a><span class=acl-fixed-case>AP</span>o-<span class=acl-fixed-case>VAE</span>: Text Generation in Hyperbolic Space</a></strong><br><a href=/people/s/shuyang-dai/>Shuyang Dai</a>
|
<a href=/people/z/zhe-gan/>Zhe Gan</a>
|
<a href=/people/y/yu-cheng/>Yu Cheng</a>
|
<a href=/people/c/chenyang-tao/>Chenyang Tao</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a>
|
<a href=/people/j/jingjing-liu/>Jingjing Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--36><div class="card-body p-3 small">Natural language often exhibits inherent <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structure</a> ingrained with complex syntax and semantics. However, most state-of-the-art <a href=https://en.wikipedia.org/wiki/Deep_learning>deep generative models</a> learn <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> only in <a href=https://en.wikipedia.org/wiki/Euclidean_space>Euclidean vector space</a>, without accounting for this structural property of language. In this paper, we investigate <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a> in a hyperbolic latent space to learn continuous hierarchical representations. An Adversarial Poincare Variational Autoencoder (APo-VAE) is presented, where both the prior and variational posterior of latent variables are defined over a Poincare ball via wrapped normal distributions. By adopting the primal-dual formulation of <a href=https://en.wikipedia.org/wiki/Kullback&#8211;Leibler_divergence>Kullback-Leibler divergence</a>, an adversarial learning procedure is introduced to empower robust model training. Extensive experiments in language modeling, unaligned style transfer, and dialog-response generation demonstrate the effectiveness of the proposed APo-VAE model over VAEs in Euclidean latent space, thanks to its superb capabilities in capturing latent language hierarchies in hyperbolic space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.37" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.37/>DART : Open-Domain Structured Data Record to Text Generation<span class=acl-fixed-case>DART</span>: Open-Domain Structured Data Record to Text Generation</a></strong><br><a href=/people/l/linyong-nan/>Linyong Nan</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/a/amrit-rau/>Amrit Rau</a>
|
<a href=/people/a/abhinand-sivaprasad/>Abhinand Sivaprasad</a>
|
<a href=/people/c/chiachun-hsieh/>Chiachun Hsieh</a>
|
<a href=/people/x/xiangru-tang/>Xiangru Tang</a>
|
<a href=/people/a/aadit-vyas/>Aadit Vyas</a>
|
<a href=/people/n/neha-verma/>Neha Verma</a>
|
<a href=/people/p/pranav-krishna/>Pranav Krishna</a>
|
<a href=/people/y/yangxiaokang-liu/>Yangxiaokang Liu</a>
|
<a href=/people/n/nadia-irwanto/>Nadia Irwanto</a>
|
<a href=/people/j/jessica-pan/>Jessica Pan</a>
|
<a href=/people/f/faiaz-rahman/>Faiaz Rahman</a>
|
<a href=/people/a/ahmad-zaidi/>Ahmad Zaidi</a>
|
<a href=/people/m/mutethia-mutuma/>Mutethia Mutuma</a>
|
<a href=/people/y/yasin-tarabar/>Yasin Tarabar</a>
|
<a href=/people/a/ankit-gupta/>Ankit Gupta</a>
|
<a href=/people/t/tao-yu/>Tao Yu</a>
|
<a href=/people/y/yi-chern-tan/>Yi Chern Tan</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a>
|
<a href=/people/n/nazneen-fatema-rajani/>Nazneen Fatema Rajani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--37><div class="card-body p-3 small">We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.39/>Multi-Adversarial Learning for Cross-Lingual Word Embeddings</a></strong><br><a href=/people/h/haozhou-wang/>Haozhou Wang</a>
|
<a href=/people/j/james-henderson/>James Henderson</a>
|
<a href=/people/p/paola-merlo/>Paola Merlo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--39><div class="card-body p-3 small">Generative adversarial networks (GANs) have succeeded in inducing cross-lingual word embeddings-maps of matching words across languages-without supervision. Despite these successes, <a href=https://en.wikipedia.org/wiki/Grammatical_number>GANs</a>&#8217; performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs&#8217; incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a>, each induced to fit the <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mapping</a> for one subspace. Our experiments on unsupervised bilingual lexicon induction and cross-lingual document classification show that this method improves performance over previous single-mapping methods, especially for distant languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.40.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.40" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.40/>Multi-view Subword Regularization</a></strong><br><a href=/people/x/xinyi-wang/>Xinyi Wang</a>
|
<a href=/people/s/sebastian-ruder/>Sebastian Ruder</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--40><div class="card-body p-3 small">Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic algorithms</a> often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018 ; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency of predictors between using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.42.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--42 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.42 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.42" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.42/>MetaXL : Meta Representation Transformation for Low-resource Cross-lingual Learning<span class=acl-fixed-case>M</span>eta<span class=acl-fixed-case>XL</span>: Meta Representation Transformation for Low-resource Cross-lingual Learning</a></strong><br><a href=/people/m/mengzhou-xia/>Mengzhou Xia</a>
|
<a href=/people/g/guoqing-zheng/>Guoqing Zheng</a>
|
<a href=/people/s/subhabrata-mukherjee/>Subhabrata Mukherjee</a>
|
<a href=/people/m/milad-shokouhi/>Milad Shokouhi</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/a/ahmed-hassan/>Ahmed Hassan Awadallah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--42><div class="card-body p-3 small">The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> remains an understudied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages, bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages without access to large-scale monolingual corpora or large amounts of labeled data for tasks like cross-lingual sentiment analysis and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> show the effectiveness of our approach. Code for MetaXL is publicly available at github.com/microsoft/MetaXL.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.43" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.43/>Open Domain Question Answering over Tables via Dense Retrieval</a></strong><br><a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/t/thomas-mueller/>Thomas Müller</a>
|
<a href=/people/s/syrine-krichene/>Syrine Krichene</a>
|
<a href=/people/j/julian-eisenschlos/>Julian Eisenschlos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--43><div class="card-body p-3 small">Recent advances in open-domain QA have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain QA over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our <a href=https://en.wikipedia.org/wiki/Retriever>retriever</a> and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of <a href=https://en.wikipedia.org/wiki/Natural_Questions>Natural Questions</a> (Kwiatkowski et al., 2019) into a Table QA dataset. We find that our <a href=https://en.wikipedia.org/wiki/Retriever>retriever</a> improves <a href=https://en.wikipedia.org/wiki/Recall_(memory)>retrieval</a> results from 72.0 to 81.1 recall@10 and <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end QA</a> results from 33.8 to 37.7 exact match, over a BERT based retriever.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.44" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.44/>Open-Domain Question Answering Goes Conversational via Question Rewriting</a></strong><br><a href=/people/r/raviteja-anantha/>Raviteja Anantha</a>
|
<a href=/people/s/svitlana-vakulenko/>Svitlana Vakulenko</a>
|
<a href=/people/z/zhucheng-tu/>Zhucheng Tu</a>
|
<a href=/people/s/shayne-longpre/>Shayne Longpre</a>
|
<a href=/people/s/stephen-pulman/>Stephen Pulman</a>
|
<a href=/people/s/srinivas-chappidi/>Srinivas Chappidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--44><div class="card-body p-3 small">We introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for Question Rewriting in Conversational Context (QReCC), which contains 14 K conversations with 80 K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10 M web pages (split into 54 M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> for the QReCC dataset with <a href=https://en.wikipedia.org/wiki/Quantitative_structure&#8211;activity_relationship>F1</a> of 19.10, compared to the <a href=https://en.wikipedia.org/wiki/Quantitative_structure&#8211;activity_relationship>human upper bound</a> of 75.45, indicating the difficulty of the setup and a large room for improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.46" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.46/>XOR QA : Cross-lingual Open-Retrieval Question Answering<span class=acl-fixed-case>XOR</span> <span class=acl-fixed-case>QA</span>: Cross-lingual Open-Retrieval Question Answering</a></strong><br><a href=/people/a/akari-asai/>Akari Asai</a>
|
<a href=/people/j/jungo-kasai/>Jungo Kasai</a>
|
<a href=/people/j/jonathan-h-clark/>Jonathan Clark</a>
|
<a href=/people/k/kenton-lee/>Kenton Lee</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--46><div class="card-body p-3 small">Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcitywhere languages have few reference articlesand information asymmetrywhere questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40 K information-seeking questions across 7 diverse non-English languages that TyDi QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross-lingual document retrieval from multilingual and English resources. We establish <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baselines</a> with state-of-the-art <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington.edu/xorqa/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.50/>On learning and representing social meaning in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> : a sociolinguistic perspective<span class=acl-fixed-case>NLP</span>: a sociolinguistic perspective</a></strong><br><a href=/people/d/dong-nguyen/>Dong Nguyen</a>
|
<a href=/people/l/laura-rosseel/>Laura Rosseel</a>
|
<a href=/people/j/jack-grieve/>Jack Grieve</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--50><div class="card-body p-3 small">The field of <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> has made substantial progress in building meaning representations. However, an important aspect of <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>linguistic meaning</a>, <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>social meaning</a>, has been largely overlooked. We introduce the concept of social meaning to <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> and discuss how insights from <a href=https://en.wikipedia.org/wiki/Sociolinguistics>sociolinguistics</a> can inform work on <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>. We also identify key challenges for this new line of research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.53/>Representing Numbers in NLP : a Survey and a Vision<span class=acl-fixed-case>NLP</span>: a Survey and a Vision</a></strong><br><a href=/people/a/avijit-thawani/>Avijit Thawani</a>
|
<a href=/people/j/jay-pujara/>Jay Pujara</a>
|
<a href=/people/f/filip-ilievski/>Filip Ilievski</a>
|
<a href=/people/p/pedro-szekely/>Pedro Szekely</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--53><div class="card-body p-3 small">NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in <a href=https://en.wikipedia.org/wiki/Neuroscience>neuroscience</a> that, in the brain, numbers are represented differently from words. We arrange recent NLP work on <a href=https://en.wikipedia.org/wiki/Numeracy>numeracy</a> into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions : granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by over a dozen previously published <a href=https://en.wikipedia.org/wiki/Encoder>number encoders</a> and <a href=https://en.wikipedia.org/wiki/Code>decoders</a>. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>, comprised of design trade-offs and a unified evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.55/>Identifying Helpful Sentences in Product Reviews</a></strong><br><a href=/people/i/iftah-gamzu/>Iftah Gamzu</a>
|
<a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/g/gilad-kutiel/>Gilad Kutiel</a>
|
<a href=/people/r/ran-levy/>Ran Levy</a>
|
<a href=/people/e/eugene-agichtein/>Eugene Agichtein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--55><div class="card-body p-3 small">In recent years <a href=https://en.wikipedia.org/wiki/Online_shopping>online shopping</a> has gained momentum and became an important venue for customers wishing to save time and simplify their shopping process. A key advantage of shopping online is the ability to read what other customers are saying about products of interest. In this work, we aim to maintain this advantage in situations where extreme brevity is needed, for example, when shopping by voice. We suggest a novel task of extracting a single representative helpful sentence from a set of reviews for a given product. The selected sentence should meet two conditions : first, it should be helpful for a purchase decision and second, the opinion it expresses should be supported by multiple reviewers. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is closely related to the task of Multi Document Summarization in the product reviews domain but differs in its objective and its level of conciseness. We collect a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> in English of sentence helpfulness scores via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourcing</a> and demonstrate its reliability despite the inherent subjectivity involved. Next, we describe a complete model that extracts representative helpful sentences with positive and negative sentiment towards the product and demonstrate that it outperforms several baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.56" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.56/>Noisy Self-Knowledge Distillation for Text Summarization</a></strong><br><a href=/people/y/yang-liu-edinburgh/>Yang Liu</a>
|
<a href=/people/s/sheng-shen/>Sheng Shen</a>
|
<a href=/people/m/mirella-lapata/>Mirella Lapata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--56><div class="card-body p-3 small">In this paper we apply self-knowledge distillation to <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a> which we argue can alleviate problems with <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum-likelihood training</a> on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a>, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and non-pretrained summarizers achieving state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.57/>Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation</a></strong><br><a href=/people/a/alexander-richard-fabbri/>Alexander Fabbri</a>
|
<a href=/people/s/simeng-han/>Simeng Han</a>
|
<a href=/people/h/haoyuan-li/>Haoyuan Li</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/m/marjan-ghazvininejad/>Marjan Ghazvininejad</a>
|
<a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a>
|
<a href=/people/y/yashar-mehdad/>Yashar Mehdad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--57><div class="card-body p-3 small">Models pretrained with self-supervised objectives on large text corpora achieve state-of-the-art performance on English text summarization tasks. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are typically fine-tuned on hundreds of thousands of data points, an infeasible requirement when applying <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> to new, niche domains. In this work, we introduce a novel and generalizable method, called WikiTransfer, for fine-tuning pretrained models for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> in an unsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained models on pseudo-summaries, produced from generic Wikipedia data, which contain characteristics of the target dataset, such as the length and level of abstraction of the desired summaries. WikiTransfer models achieve state-of-the-art, zero-shot abstractive summarization performance on the CNN-DailyMail dataset and demonstrate the effectiveness of our approach on three additional diverse datasets. These models are more robust to noisy data and also achieve better or comparable few-shot performance using 10 and 100 training examples when compared to few-shot transfer from other summarization datasets. To further boost performance, we employ <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> via <a href=https://en.wikipedia.org/wiki/Round-trip_translation>round-trip translation</a> as well as introduce a regularization term for improved few-shot transfer. To understand the role of dataset aspects in transfer performance and the quality of the resulting output summaries, we further study the effect of the components of our unsupervised fine-tuning data and analyze few-shot performance using both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.60/>Nice Try, Kiddo : Investigating Ad Hominems in Dialogue Responses</a></strong><br><a href=/people/e/emily-sheng/>Emily Sheng</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/p/prem-natarajan/>Prem Natarajan</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--60><div class="card-body p-3 small">Ad hominem attacks are those that target some feature of a person&#8217;s character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person&#8217;s credibility. Since <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> respond directly to <a href=https://en.wikipedia.org/wiki/Input_(computer_science)>user input</a>, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to analyze human and dialogue system responses to English Twitter posts. We specifically compare responses to Twitter topics about marginalized communities (# BlackLivesMatter, # MeToo) versus other topics (# Vegan, # WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, we propose a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated. Our results indicate that 1) responses from both humans and DialoGPT contain more <a href=https://en.wikipedia.org/wiki/Ad_hominem>ad hominems</a> for discussions around marginalized communities, 2) different quantities of <a href=https://en.wikipedia.org/wiki/Ad_hominem>ad hominems</a> in the training data can influence the likelihood of generating <a href=https://en.wikipedia.org/wiki/Ad_hominem>ad hominems</a>, and 3) we can use constrained decoding techniques to reduce <a href=https://en.wikipedia.org/wiki/Ad_hominem>ad hominems</a> in generated dialogue responses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.63/>Spoken Language Understanding for Task-oriented Dialogue Systems with Augmented Memory Networks</a></strong><br><a href=/people/j/jie-wu/>Jie Wu</a>
|
<a href=/people/i/ian-harris/>Ian Harris</a>
|
<a href=/people/h/hongzhi-zhao/>Hongzhi Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--63><div class="card-body p-3 small">Spoken language understanding, usually including intent detection and slot filling, is a core component to build a spoken dialog system. Recent research shows promising results by jointly learning of those two tasks based on the fact that slot filling and intent detection are sharing <a href=https://en.wikipedia.org/wiki/Semantic_memory>semantic knowledge</a>. Furthermore, attention mechanism boosts joint learning to achieve state-of-the-art results. However, current joint learning models ignore the following important facts : 1. Long-term slot context is not traced effectively, which is crucial for future slot filling. Slot tagging and intent detection could be mutually rewarding, but bi-directional interaction between slot filling and intent detection remains seldom explored. In this paper, we propose a novel approach to model long-term slot context and to fully utilize the semantic correlation between slots and intents. We adopt a key-value memory network to model slot context dynamically and to track more important slot tags decoded before, which are then fed into our decoder for slot tagging. Furthermore, gated memory information is utilized to perform intent detection, mutually improving both tasks through <a href=https://en.wikipedia.org/wiki/Global_optimization>global optimization</a>. Experiments on benchmark ATIS and Snips datasets show that our model achieves state-of-the-art performance and outperforms other methods, especially for the slot filling task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.66" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.66/>Self-Training with Weak Supervision</a></strong><br><a href=/people/g/giannis-karamanolakis/>Giannis Karamanolakis</a>
|
<a href=/people/s/subhabrata-mukherjee/>Subhabrata Mukherjee</a>
|
<a href=/people/g/guoqing-zheng/>Guoqing Zheng</a>
|
<a href=/people/a/ahmed-hassan/>Ahmed Hassan Awadallah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--66><div class="card-body p-3 small">State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domain-specific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind. In this work, we develop a weak supervision framework (ASTRA) that leverages all the available data for a given <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a>. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.68.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--68 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.68 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.68" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.68/>Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning</a></strong><br><a href=/people/x/xuelu-chen/>Xuelu Chen</a>
|
<a href=/people/m/michael-boratko/>Michael Boratko</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/s/shib-sankar-dasgupta/>Shib Sankar Dasgupta</a>
|
<a href=/people/x/xiang-lorraine-li/>Xiang Lorraine Li</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--68><div class="card-body p-3 small">Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of <a href=https://en.wikipedia.org/wiki/Embedding>embedding methods</a> to generalize from known facts, however, existing <a href=https://en.wikipedia.org/wiki/Embedding>embedding methods</a> only model triple-level uncertainty, and reasoning results lack <a href=https://en.wikipedia.org/wiki/Global_consistency>global consistency</a>. To address these shortcomings, we propose BEUrRE, a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle) and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence prediction</a> and fact ranking due to its probabilistic calibration and ability to capture high-order dependencies among facts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.69.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.69" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.69/>Document-Level Event Argument Extraction by Conditional Generation</a></strong><br><a href=/people/s/sha-li/>Sha Li</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--69><div class="card-body p-3 small">Event extraction has long been treated as a sentence-level task in the IE community. We argue that this <a href=https://en.wikipedia.org/wiki/Setting_(narrative)>setting</a> does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> and 5.7 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> over the next best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3 % F1 gain over the best <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97 % of fully supervised model&#8217;s trigger extraction performance and 82 % of the argument extraction performance given only access to 10 out of the 33 types on ACE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.70" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.70/>Template Filling with Generative Transformers</a></strong><br><a href=/people/x/xinya-du/>Xinya Du</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--70><div class="card-body p-3 small">Template filling is generally tackled by a pipeline of two separate supervised systems one for role-filler extraction and another for template / event recognition. Since <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipelines</a> consider events in isolation, they can suffer from <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a>. We introduce a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> specifically improves performance on documents containing multiple events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.72.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--72 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.72 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.72/>On Attention Redundancy : A Comprehensive Study</a></strong><br><a href=/people/y/yuchen-bian/>Yuchen Bian</a>
|
<a href=/people/j/jiaji-huang/>Jiaji Huang</a>
|
<a href=/people/x/xingyu-cai/>Xingyu Cai</a>
|
<a href=/people/j/jiahong-yuan/>Jiahong Yuan</a>
|
<a href=/people/k/kenneth-church/>Kenneth Church</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--72><div class="card-body p-3 small">Multi-layer multi-head self-attention mechanism is widely applied in modern neural language models. Attention redundancy has been observed among <a href=https://en.wikipedia.org/wiki/Attentional_control>attention heads</a> but has not been deeply studied in the literature. Using BERT-base model as an example, this paper provides a comprehensive study on attention redundancy which is helpful for model interpretation and model compression. We analyze the attention redundancy with Five-Ws and How. (What) We define and focus the study on redundancy matrices generated from pre-trained and fine-tuned BERT-base model for GLUE datasets. (How) We use both token-based and sentence-based distance functions to measure the <a href=https://en.wikipedia.org/wiki/Redundancy_(engineering)>redundancy</a>. (Where) Clear and similar redundancy patterns (cluster structure) are observed among attention heads. (When) Redundancy patterns are similar in both pre-training and fine-tuning phases. (Who) We discover that <a href=https://en.wikipedia.org/wiki/Redundancy_(engineering)>redundancy patterns</a> are task-agnostic. Similar <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundancy patterns</a> even exist for randomly generated token sequences. (Why) We also evaluate influences of the pre-training dropout ratios on attention redundancy. Based on the phase-independent and task-agnostic attention redundancy patterns, we propose a simple zero-shot pruning method as a case study. Experiments on fine-tuning GLUE tasks verify its effectiveness. The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.73" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.73/>Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?<span class=acl-fixed-case>BERT</span> Pretrained on Clinical Notes Reveal Sensitive Data?</a></strong><br><a href=/people/e/eric-lehman/>Eric Lehman</a>
|
<a href=/people/s/sarthak-jain/>Sarthak Jain</a>
|
<a href=/people/k/karl-pichotta/>Karl Pichotta</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--73><div class="card-body p-3 small">Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a <a href=https://en.wikipedia.org/wiki/Physical_model>model</a> if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated attacks may succeed in doing so : To facilitate such research, we make our experimental setup and baseline probing models available at https://github.com/elehman16/exposing_patient_data_release.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.74" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.74/>Low-Complexity Probing via Finding Subnetworks</a></strong><br><a href=/people/s/steven-cao/>Steven Cao</a>
|
<a href=/people/v/victor-sanh/>Victor Sanh</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--74><div class="card-body p-3 small">The dominant approach in probing neural networks for linguistic properties is to train a new shallow multi-layer perceptron (MLP) on top of the model&#8217;s internal representations. This approach can detect properties encoded in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, but at the cost of adding new parameters that may learn the task directly. We instead propose a subtractive pruning-based probe, where we find an existing <a href=https://en.wikipedia.org/wiki/Subnetwork>subnetwork</a> that performs the linguistic task of interest. Compared to an MLP, the subnetwork probe achieves both higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on pre-trained models and lower <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on random models, so it is both better at finding properties of interest and worse at learning on its own. Next, by varying the <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of each probe, we show that subnetwork probing Pareto-dominates MLP probing in that it achieves higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> given any budget of probe complexity. Finally, we analyze the resulting subnetworks across various tasks to locate where each task is encoded, and we find that lower-level tasks are captured in lower layers, reproducing similar findings in past work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.75" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.75/>An Empirical Comparison of Instance Attribution Methods for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a><span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/p/pouya-pezeshkpour/>Pouya Pezeshkpour</a>
|
<a href=/people/s/sarthak-jain/>Sarthak Jain</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--75><div class="card-body p-3 small">Widespread adoption of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep models</a> has motivated a pressing need for approaches to interpret network outputs and to facilitate model debugging. Instance attribution methods constitute one means of accomplishing these goals by retrieving training instances that (may have) led to a particular prediction. Influence functions (IF ; Koh and Liang 2017) provide machinery for doing this by quantifying the effect that perturbing individual train instances would have on a specific test prediction. However, even approximating the <a href=https://en.wikipedia.org/wiki/Conditional_(computer_programming)>IF</a> is computationally expensive, to the degree that may be prohibitive in many cases. Might simpler approaches (e.g., retrieving train examples most similar to a given test point) perform comparably? In this work, we evaluate the degree to which different potential instance attribution agree with respect to the importance of training samples. We find that simple retrieval methods yield training instances that differ from those identified via gradient-based methods (such as IFs), but that nonetheless exhibit desirable characteristics similar to more complex attribution methods. Code for all methods and experiments in this paper is available at : https://github.com/successar/instance_attributions_NLP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.76/>Generalization in Instruction Following Systems</a></strong><br><a href=/people/s/soham-dan/>Soham Dan</a>
|
<a href=/people/m/michael-zhou/>Michael Zhou</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--76><div class="card-body p-3 small">Understanding and executing <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language instructions</a> in a grounded domain is one of the hallmarks of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a>. In this paper, we focus on instruction understanding in the blocks world domain and investigate the language understanding abilities of two top-performing <a href=https://en.wikipedia.org/wiki/System>systems</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We aim to understand if the test performance of these models indicates an understanding of the spatial domain and of the <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language instructions</a> relative to it, or whether they merely over-fit spurious signals in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We formulate a set of expectations one might have from an instruction following model and concretely characterize the different dimensions of robustness such a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> should possess. Despite decent test performance, we find that <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art models</a> fall short of these expectations and are extremely brittle. We then propose a learning strategy that involves <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> and show through extensive experiments that the proposed learning strategy yields models that are competitive on the original test set while satisfying our expectations much better.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.79" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.79/>MTAG : Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences<span class=acl-fixed-case>MTAG</span>: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences</a></strong><br><a href=/people/j/jianing-yang/>Jianing Yang</a>
|
<a href=/people/y/yongxin-wang/>Yongxin Wang</a>
|
<a href=/people/r/ruitao-yi/>Ruitao Yi</a>
|
<a href=/people/y/yuying-zhu/>Yuying Zhu</a>
|
<a href=/people/a/azaan-rehman/>Azaan Rehman</a>
|
<a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--79><div class="card-body p-3 small">Human communication is multimodal in nature ; it is through multiple modalities such as <a href=https://en.wikipedia.org/wiki/Language>language</a>, <a href=https://en.wikipedia.org/wiki/Human_voice>voice</a>, and <a href=https://en.wikipedia.org/wiki/Facial_expression>facial expressions</a>, that opinions and emotions are expressed. Data in this <a href=https://en.wikipedia.org/wiki/Domain_(biology)>domain</a> exhibits complex multi-relational and temporal interactions. Learning from this <a href=https://en.wikipedia.org/wiki/Data>data</a> is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, MTAG achieves state-of-the-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.80" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.80/>Grounding Open-Domain Instructions to Automate Web Support Tasks</a></strong><br><a href=/people/n/nancy-xu/>Nancy Xu</a>
|
<a href=/people/s/sam-masling/>Sam Masling</a>
|
<a href=/people/m/michael-du/>Michael Du</a>
|
<a href=/people/g/giovanni-campagna/>Giovanni Campagna</a>
|
<a href=/people/l/larry-heck/>Larry Heck</a>
|
<a href=/people/j/james-landay/>James Landay</a>
|
<a href=/people/m/monica-lam/>Monica Lam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--80><div class="card-body p-3 small">Grounding natural language instructions on the web to perform previously unseen tasks enables <a href=https://en.wikipedia.org/wiki/Accessibility>accessibility</a> and <a href=https://en.wikipedia.org/wiki/Automation>automation</a>. We introduce a task and dataset to train <a href=https://en.wikipedia.org/wiki/Intelligent_agent>AI agents</a> from open-domain, step-by-step instructions originally written for people. We build RUSS (Rapid Universal Support Service) to tackle this problem. RUSS consists of two models : First, a BERT-LSTM with pointers parses instructions to WebLang, a <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific language</a> we design for grounding <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>. Then, a grounding model retrieves the unique IDs of any webpage elements requested in the WebLang. RUSS may interact with the user through a <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a> (e.g. ask for an address) or execute a <a href=https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol>web operation</a> (e.g. click a button) inside the web runtime. To augment <a href=https://en.wikipedia.org/wiki/Training>training</a>, we synthesize <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language instructions</a> mapped to WebLang. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of 80 different customer service problems from help websites, with a total of 741 step-by-step instructions and their corresponding actions. RUSS achieves 76.7 % end-to-end accuracy predicting agent actions from single instructions. It outperforms state-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that directly map instructions to actions without WebLang. Our user study shows that RUSS is preferred by actual users over <a href=https://en.wikipedia.org/wiki/Web_navigation>web navigation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.82" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.82/>Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information</a></strong><br><a href=/people/j/jialu-li/>Jialu Li</a>
|
<a href=/people/h/hao-tan/>Hao Tan</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--82><div class="card-body p-3 small">Vision language navigation is the task that requires an agent to navigate through a 3D environment based on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language instructions</a>. One key challenge in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is to ground instructions with the current <a href=https://en.wikipedia.org/wiki/Visual_system>visual information</a> that the agent perceives. Most of the existing work employs soft attention over individual words to locate the instruction required for the next action. However, different words have different functions in a sentence (e.g., <a href=https://en.wikipedia.org/wiki/Grammatical_modifier>modifiers</a> convey attributes, <a href=https://en.wikipedia.org/wiki/Verb>verbs</a> convey actions). Syntax information like <a href=https://en.wikipedia.org/wiki/Coupling_(computer_programming)>dependencies</a> and <a href=https://en.wikipedia.org/wiki/Phrase_structure_grammar>phrase structures</a> can aid the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> to locate important parts of the instruction. Hence, in this paper, we propose a navigation agent that utilizes syntax information derived from a dependency tree to enhance alignment between the instruction and the current visual scenes. Empirically, our agent outperforms the baseline model that does not use syntax information on the Room-to-Room dataset, especially in the unseen environment. Besides, our agent achieves the new state-of-the-art on Room-Across-Room dataset, which contains instructions in 3 languages (English, Hindi, and Telugu). We also show that our <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> is better at aligning instructions with the current <a href=https://en.wikipedia.org/wiki/Visual_system>visual information</a> via qualitative visualizations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.86" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.86/>Understanding Hard Negatives in Noise Contrastive Estimation</a></strong><br><a href=/people/w/wenzheng-zhang/>Wenzheng Zhang</a>
|
<a href=/people/k/karl-stratos/>Karl Stratos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--86><div class="card-body p-3 small">The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negativeshighest-scoring incorrect examples under the modelare effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the cross-entropy loss, and show both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction. We also derive a general form of the <a href=https://en.wikipedia.org/wiki/Score_function>score function</a> that unifies various <a href=https://en.wikipedia.org/wiki/Information_retrieval>architectures</a> used in <a href=https://en.wikipedia.org/wiki/Information_retrieval>text retrieval</a>. By combining hard negatives with appropriate score functions, we obtain strong results on the challenging task of zero-shot entity linking.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.88/>DReCa : A General Task Augmentation Strategy for Few-Shot Natural Language Inference<span class=acl-fixed-case>DR</span>e<span class=acl-fixed-case>C</span>a: A General Task Augmentation Strategy for Few-Shot Natural Language Inference</a></strong><br><a href=/people/s/shikhar-murty/>Shikhar Murty</a>
|
<a href=/people/t/tatsunori-b-hashimoto/>Tatsunori B. Hashimoto</a>
|
<a href=/people/c/christopher-d-manning/>Christopher Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--88><div class="card-body p-3 small">Meta-learning promises few-shot learners that can adapt to new distributions by repurposing knowledge acquired from previous training. However, we believe <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a> has not yet succeeded in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> due to the lack of a well-defined task distribution, leading to attempts that treat datasets as tasks. Such an ad hoc task distribution causes problems of quantity and quality. Since there&#8217;s only a handful of datasets for any NLP problem, meta-learners tend to overfit their adaptation mechanism and, since NLP datasets are highly heterogeneous, many learning episodes have poor transfer between their support and query sets, which discourages the meta-learner from adapting. To alleviate these issues, we propose DReCA (Decomposing datasets into Reasoning Categories), a simple method for discovering and using latent reasoning categories in a dataset, to form additional high quality tasks. DReCA works by splitting examples into label groups, embedding them with a finetuned BERT model and then clustering each group into reasoning categories. Across four few-shot NLI problems, we demonstrate that using DReCA improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learners</a> by 1.5-4 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.89.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--89 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.89 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.89/>Harnessing <a href=https://en.wikipedia.org/wiki/Multilinguality>Multilinguality</a> in Unsupervised Machine Translation for Rare Languages</a></strong><br><a href=/people/x/xavier-garcia/>Xavier Garcia</a>
|
<a href=/people/a/aditya-siddhant/>Aditya Siddhant</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a>
|
<a href=/people/a/ankur-parikh/>Ankur Parikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--89><div class="card-body p-3 small">Unsupervised translation has reached impressive performance on resource-rich language pairs such as English-French and <a href=https://en.wikipedia.org/wiki/Standard_German>English-German</a>. However, early studies have shown that in more realistic settings involving low-resource, rare languages, unsupervised translation performs poorly, achieving less than 3.0 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. In this work, we show that <a href=https://en.wikipedia.org/wiki/Multilinguality>multilinguality</a> is critical to making <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised systems</a> practical for low-resource settings. In particular, we present a single model for 5 low-resource languages (Gujarati, Kazakh, Nepali, Sinhala, and Turkish) to and from English directions, which leverages monolingual and auxiliary parallel data from other high-resource language pairs via a three-stage training scheme. We outperform all current state-of-the-art unsupervised baselines for these languages, achieving gains of up to 14.4 BLEU. Additionally, we outperform strong supervised baselines for various language pairs as well as match the performance of the current state-of-the-art <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> for <a href=https://en.wikipedia.org/wiki/Nepali_language>Nepali-English</a>. We conduct a series of ablation studies to establish the robustness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> under different degrees of <a href=https://en.wikipedia.org/wiki/Data_quality>data quality</a>, as well as to analyze the factors which led to the superior performance of the proposed approach over traditional <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.91.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--91 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.91 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.91/>Assessing Reference-Free Peer Evaluation for Machine Translation</a></strong><br><a href=/people/s/sweta-agrawal/>Sweta Agrawal</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--91><div class="card-body p-3 small">Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, and demonstrate that by scaling it up we can match the performance of <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. We analyze various potential weaknesses of the approach, and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.92" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.92/>The Curious Case of <a href=https://en.wikipedia.org/wiki/Hallucination>Hallucinations</a> in Neural Machine Translation</a></strong><br><a href=/people/v/vikas-raunak/>Vikas Raunak</a>
|
<a href=/people/a/arul-menezes/>Arul Menezes</a>
|
<a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--92><div class="card-body p-3 small">In this work, we study <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a> in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a> under source perturbation to the Long-Tail theory of Feldman, and present an empirically validated hypothesis that explains <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a> under source perturbation. Secondly, we consider <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a> under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.94/>Towards Modeling the Style of Translators in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yue-wang/>Yue Wang</a>
|
<a href=/people/c/cuong-hoang/>Cuong Hoang</a>
|
<a href=/people/m/marcello-federico/>Marcello Federico</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--94><div class="card-body p-3 small">One key ingredient of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> is the use of large datasets from different domains and resources (e.g. Europarl, TED talks). These <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> contain documents translated by professional translators using different but consistent translation styles. Despite that, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is usually trained in a way that neither explicitly captures the variety of translation styles present in the data nor translates new data in different and controllable styles. In this work, we investigate methods to augment the state of the art Transformer model with translator information that is available in part of the training data. We show that our style-augmented translation models are able to capture the style variations of translators and to generate translations with different styles on new data. Indeed, the generated variations differ significantly, up to +4.5 BLEU score difference. Despite that, <a href=https://en.wikipedia.org/wiki/Human_factors_and_ergonomics>human evaluation</a> confirms that the translations are of the same quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.95/>Self-Supervised Test-Time Learning for Reading Comprehension</a></strong><br><a href=/people/p/pratyay-banerjee/>Pratyay Banerjee</a>
|
<a href=/people/t/tejas-gokhale/>Tejas Gokhale</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--95><div class="card-body p-3 small">Recent work on unsupervised question answering has shown that models can be trained with procedurally generated question-answer pairs and can achieve performance competitive with <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a>. In this work, we consider the task of unsupervised reading comprehension and present a method that performs test-time learning (TTL) on a given context (text passage), without requiring training on large-scale human-authored datasets containing context-question-answer triplets. This method operates directly on a single test context, uses self-supervision to train <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> on synthetically generated question-answer pairs, and then infers answers to unseen human-authored questions for this context. Our method achieves accuracies competitive with <a href=https://en.wikipedia.org/wiki/Supervised_learning>fully supervised methods</a> and significantly outperforms current <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a>. TTL methods with a smaller <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> are also competitive with the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> in unsupervised reading comprehension.<i>context-question-answer</i> triplets. This method operates directly on a single test context, uses self-supervision to train models on synthetically generated question-answer pairs, and then infers answers to unseen human-authored questions for this context. Our method achieves accuracies competitive with fully supervised methods and significantly outperforms current unsupervised methods. TTL methods with a smaller model are also competitive with the current state-of-the-art in unsupervised reading comprehension.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.96.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--96 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.96 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.96" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.96/>Capturing Row and Column Semantics in Transformer Based Question Answering over Tables</a></strong><br><a href=/people/m/michael-glass/>Michael Glass</a>
|
<a href=/people/m/mustafa-canim/>Mustafa Canim</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a>
|
<a href=/people/s/saneem-chemmengath/>Saneem Chemmengath</a>
|
<a href=/people/v/vishwajeet-kumar/>Vishwajeet Kumar</a>
|
<a href=/people/r/rishav-chakravarti/>Rishav Chakravarti</a>
|
<a href=/people/a/avirup-sil/>Avi Sil</a>
|
<a href=/people/f/feifei-pan/>Feifei Pan</a>
|
<a href=/people/s/samarth-bharadwaj/>Samarth Bharadwaj</a>
|
<a href=/people/n/nicolas-rodolfo-fauceglia/>Nicolas Rodolfo Fauceglia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--96><div class="card-body p-3 small">Transformer based architectures are recently used for the task of answering questions over tables. In order to improve the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, specialized pre-training techniques have been developed and applied on millions of open-domain web tables. In this paper, we propose two novel approaches demonstrating that one can achieve superior performance on table QA task without even using any of these specialized pre-training techniques. The first model, called RCI interaction, leverages a transformer based architecture that independently classifies rows and columns to identify relevant cells. While this model yields extremely high accuracy at finding cell values on recent benchmarks, a second model we propose, called RCI representation, provides a significant efficiency advantage for online QA systems over tables by materializing embeddings for existing tables. Experiments on recent benchmarks prove that the proposed methods can effectively locate cell values on tables (up to ~98 % Hit@1 accuracy on WikiSQL lookup questions). Also, the <a href=https://en.wikipedia.org/wiki/Interaction_model>interaction model</a> outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (TAPAS and TaBERT), achieving ~3.4 % and ~18.86 % additional precision improvement on the standard WikiSQL benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.98/>Robust Question Answering Through Sub-part Alignment</a></strong><br><a href=/people/j/jifan-chen/>Jifan Chen</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--98><div class="card-body p-3 small">Current textual question answering (QA) models achieve strong performance on in-domain test sets, but often do so by fitting surface-level patterns, so they fail to generalize to out-of-distribution settings. To make a more robust and understandable <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA system</a>, we model <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> as an alignment problem. We decompose both the question and context into smaller units based on off-the-shelf semantic representations (here, semantic roles), and align the question to a subgraph of the context in order to find the answer. We formulate our model as a structured SVM, with alignment scores computed via BERT, and we can train end-to-end despite using beam search for approximate inference. Our use of explicit alignments allows us to explore a set of <a href=https://en.wikipedia.org/wiki/Constraint_(mathematics)>constraints</a> with which we can prohibit certain types of bad model behavior arising in cross-domain settings. Furthermore, by investigating differences in scores across different potential answers, we can seek to understand what particular aspects of the input lead the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to choose the answer without relying on post-hoc explanation techniques. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on SQuAD v1.1 and test it on several adversarial and out-of-domain datasets. The results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is more robust than the standard BERT QA model, and constraints derived from alignment scores allow us to effectively trade off coverage and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--100 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.100/>RECONSIDER : Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering<span class=acl-fixed-case>RECONSIDER</span>: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering</a></strong><br><a href=/people/s/srinivasan-iyer/>Srinivasan Iyer</a>
|
<a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/y/yashar-mehdad/>Yashar Mehdad</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--100><div class="card-body p-3 small">State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain Question Answering (QA) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples. This training scheme possibly explains empirical observations that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve a high <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>recall</a> amongst their top few predictions, but a low overall <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, motivating the need for answer re-ranking. We develop a successful re-ranking approach (RECONSIDER) for span-extraction tasks that improves upon the performance of MRC models, even beyond large-scale pre-training. RECONSIDER is trained on positive and negative examples extracted from high confidence MRC model predictions, and uses in-passage span annotations to perform span-focused re-ranking over a smaller candidate set. As a result, RECONSIDER learns to eliminate close false positives, achieving a new extractive state of the art on four <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA tasks</a>, with 45.5 % Exact Match accuracy on Natural Questions with real user questions, and 61.7 % on TriviaQA. We will release all related data, <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, and code.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.104/>Looking Beyond Sentence-Level Natural Language Inference for <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> and Text Summarization</a></strong><br><a href=/people/a/anshuman-mishra/>Anshuman Mishra</a>
|
<a href=/people/d/dhruvesh-patel/>Dhruvesh Patel</a>
|
<a href=/people/a/aparna-vijayakumar/>Aparna Vijayakumar</a>
|
<a href=/people/x/xiang-lorraine-li/>Xiang Lorraine Li</a>
|
<a href=/people/p/pavan-kapanipathi/>Pavan Kapanipathi</a>
|
<a href=/people/k/kartik-talamadupula/>Kartik Talamadupula</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--104><div class="card-body p-3 small">Natural Language Inference (NLI) has garnered significant attention in recent years ; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that : (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge prohibiting usage in downstream applications (which do better with longer contexts) ; (2) this challenge can be addressed by automatically converting resource-rich reading comprehension datasets into longer-premise NLI datasets ; and (3) models trained on the converted, longer-premise datasets outperform those trained using short-premise traditional NLI datasets on downstream tasks primarily due to the difference in premise lengths.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.110" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.110/>A New Approach to Overgenerating and Scoring Abstractive Summaries</a></strong><br><a href=/people/k/kaiqiang-song/>Kaiqiang Song</a>
|
<a href=/people/b/bingqing-wang/>Bingqing Wang</a>
|
<a href=/people/z/zhe-feng/>Zhe Feng</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--110><div class="card-body p-3 small">We propose a new approach to generate multiple variants of the target summary with diverse content and varying lengths, then score and select admissible ones according to users&#8217; needs. Abstractive summarizers trained on single reference summaries may struggle to produce outputs that achieve multiple desirable properties, i.e., capturing the most important information, being faithful to the original, grammatical and fluent. In this paper, we propose a two-staged strategy to generate a diverse set of candidate summaries from the source text in stage one, then score and select admissible ones in stage two. Importantly, our generator gives a precise control over the length of the summary, which is especially well-suited when space is limited. Our selectors are designed to predict the optimal summary length and put special emphasis on faithfulness to the original text. Both <a href=https://en.wikipedia.org/wiki/Stage_(theatre)>stages</a> can be effectively trained, optimized and evaluated. Our experiments on benchmark summarization datasets suggest that this <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> can achieve state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.111/>D2S : Document-to-Slide Generation Via Query-Based Text Summarization<span class=acl-fixed-case>D</span>2<span class=acl-fixed-case>S</span>: Document-to-Slide Generation Via Query-Based Text Summarization</a></strong><br><a href=/people/e/edward-sun/>Edward Sun</a>
|
<a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/d/dakuo-wang/>Dakuo Wang</a>
|
<a href=/people/y/yunfeng-zhang/>Yunfeng Zhang</a>
|
<a href=/people/n/nancy-x-r-wang/>Nancy X. R. Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--111><div class="card-body p-3 small">Presentations are critical for communication in all areas of our lives, yet the creation of <a href=https://en.wikipedia.org/wiki/Slide_show>slide decks</a> is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge : no publicly available dataset for training and benchmarking. In this work, we first contribute a new dataset, SciDuet, consisting of pairs of papers and their corresponding slides decks from recent years&#8217; NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel system that tackles the document-to-slides task with a two-step approach : 1) Use slide titles to retrieve relevant and engaging text, figures, and tables ; 2) Summarize the retrieved context into bullet points with long-form question answering. Our evaluation suggests that long-form QA outperforms state-of-the-art summarization baselines on both automated ROUGE metrics and qualitative human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--112 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.112" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.112/>Efficient Attentions for Long Document Summarization</a></strong><br><a href=/people/l/luyang-huang/>Luyang Huang</a>
|
<a href=/people/s/shuyang-cao/>Shuyang Cao</a>
|
<a href=/people/n/nikolaus-parulian/>Nikolaus Parulian</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--112><div class="card-body p-3 small">The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient <a href=https://en.wikipedia.org/wiki/Self-interest>self-attentions</a>. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, GovReport, with significantly longer documents and summaries. Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a>. Human evaluation also shows that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> generate more informative summaries with fewer unfaithful errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--113 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.113" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.113/>RefSum : Refactoring Neural Summarization<span class=acl-fixed-case>R</span>ef<span class=acl-fixed-case>S</span>um: Refactoring Neural Summarization</a></strong><br><a href=/people/y/yixin-liu/>Yixin Liu</a>
|
<a href=/people/z/zi-yi-dou/>Zi-Yi Dou</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--113><div class="card-body p-3 small">Although some recent works show potential complementarity among different state-of-the-art systems, few works try to investigate this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> in <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>. Researchers in other areas commonly refer to the techniques of <a href=https://en.wikipedia.org/wiki/Ranking>reranking</a> or stacking to approach this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. In this work, we highlight several limitations of previous methods, which motivates us to present a new framework <a href=https://en.wikipedia.org/wiki/Code_refactoring>Refactor</a> that provides a unified view of <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a> and summaries combination. Experimentally, we perform a comprehensive evaluation that involves twenty-two base systems, four <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, and three different application scenarios. Besides new state-of-the-art results on CNN / DailyMail dataset (46.18 ROUGE-1), we also elaborate on how our proposed method addresses the limitations of the traditional methods and the effectiveness of the Refactor model sheds light on insight for performance improvement. Our <a href=https://en.wikipedia.org/wiki/System>system</a> can be directly used by other researchers as an off-the-shelf tool to achieve further performance improvements. We open-source all the code and provide a convenient interface to use it : https://github.com/yixinL7/Refactoring-Summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.114" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.114/>Annotating and Modeling Fine-grained Factuality in <a href=https://en.wikipedia.org/wiki/Summarization>Summarization</a></a></strong><br><a href=/people/t/tanya-goyal/>Tanya Goyal</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--114><div class="card-body p-3 small">Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a> for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, and study <a href=https://en.wikipedia.org/wiki/Factuality>factuality</a> at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSum. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSum summarization models by allowing us to identify non-factual tokens in the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--115 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.115/>Larger-Context Tagging : When and Why Does It Work?</a></strong><br><a href=/people/j/jinlan-fu/>Jinlan Fu</a>
|
<a href=/people/l/liangjing-feng/>Liangjing Feng</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--115><div class="card-body p-3 small">The development of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> and pretraining techniques has spawned many sentence-level tagging systems that achieved superior performance on typical benchmarks. However, a relatively less discussed topic is what if more <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context information</a> is introduced into current top-scoring tagging systems. Although several existing works have attempted to shift tagging systems from sentence-level to document-level, there is still no consensus conclusion about when and why it works, which limits the applicability of the larger-context approach in tagging tasks. In this paper, instead of pursuing a state-of-the-art tagging system by architectural exploration, we focus on investigating when and why the larger-context training, as a general strategy, can work. To this end, we conduct a thorough comparative study on four proposed aggregators for context information collecting and present an attribute-aided evaluation method to interpret the improvement brought by larger-context training. Experimentally, we set up a testbed based on four tagging tasks and thirteen datasets. Hopefully, our preliminary observations can deepen the understanding of larger-context training and enlighten more follow-up works on the use of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.116" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.116/>Neural Sequence Segmentation as Determining the Leftmost Segments</a></strong><br><a href=/people/y/yangming-li/>Yangming Li</a>
|
<a href=/people/l/lemao-liu/>Lemao Liu</a>
|
<a href=/people/k/kaisheng-yao/>Kaisheng Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--116><div class="card-body p-3 small">Prior methods to <a href=https://en.wikipedia.org/wiki/Text_segmentation>text segmentation</a> are mostly at <a href=https://en.wikipedia.org/wiki/Lexical_analysis>token level</a>. Despite the adequacy, this nature limits their full potential to capture the long-term dependencies among segments. In this work, we propose a novel <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> that incrementally segments <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>natural language sentences</a> at segment level. For every step in segmentation, it recognizes the leftmost segment of the remaining sequence. Implementations involve LSTM-minus technique to construct the phrase representations and <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks (RNN)</a> to model the iterations of determining the leftmost segments. We have conducted extensive experiments on syntactic chunking and Chinese part-of-speech (POS) tagging across 3 datasets, demonstrating that our methods have significantly outperformed previous all baselines and achieved new state-of-the-art results. Moreover, <a href=https://en.wikipedia.org/wiki/Qualitative_research>qualitative analysis</a> and the study on segmenting long-length sentences verify its effectiveness in modeling long-term dependencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--123 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.123/>Put Chatbot into Its Interlocutor’s Shoes : New Framework to Learn Chatbot Responding with Intention</a></strong><br><a href=/people/h/hsuan-su/>Hsuan Su</a>
|
<a href=/people/j/jiun-hao-jhan/>Jiun-Hao Jhan</a>
|
<a href=/people/f/fan-yun-sun/>Fan-yun Sun</a>
|
<a href=/people/s/saurav-sahay/>Saurav Sahay</a>
|
<a href=/people/h/hung-yi-lee/>Hung-yi Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--123><div class="card-body p-3 small">Most chatbot literature that focuses on improving the <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> and coherence of a <a href=https://en.wikipedia.org/wiki/Chatbot>chatbot</a>, is dedicated to making <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> more human-like. However, very little work delves into what really separates humans from chatbots humans intrinsically understand the effect their responses have on the interlocutor and often respond with an intention such as proposing an optimistic view to make the interlocutor feel better. This paper proposes an innovative <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> to train <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> to possess human-like intentions. Our framework includes a guiding chatbot and an interlocutor model that plays the role of humans. The guiding chatbot is assigned an intention and learns to induce the interlocutor to reply with responses matching the intention, for example, long responses, joyful responses, responses with specific words, etc. We examined our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> using three experimental setups and evaluated the guiding chatbot with four different <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to demonstrate flexibility and performance advantages. Additionally, we performed trials with human interlocutors to substantiate the guiding chatbot&#8217;s effectiveness in influencing the responses of humans to a certain extent. Code will be made available to the public.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.124/>Adding Chit-Chat to Enhance Task-Oriented Dialogues</a></strong><br><a href=/people/k/kai-sun/>Kai Sun</a>
|
<a href=/people/s/seungwhan-moon/>Seungwhan Moon</a>
|
<a href=/people/p/paul-a-crook/>Paul Crook</a>
|
<a href=/people/s/stephen-roller/>Stephen Roller</a>
|
<a href=/people/b/becka-silvert/>Becka Silvert</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a>
|
<a href=/people/z/zhiguang-wang/>Zhiguang Wang</a>
|
<a href=/people/h/honglei-liu/>Honglei Liu</a>
|
<a href=/people/e/eunjoon-cho/>Eunjoon Cho</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--124><div class="card-body p-3 small">Existing dialogue corpora and <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are typically designed under two disjoint motives : while task-oriented systems focus on achieving functional goals (e.g., booking hotels), open-domain chatbots aim at making socially engaging conversations. In this work, we propose to integrate both types of systems by Adding <a href=https://en.wikipedia.org/wiki/Chit-chat>Chit-Chat</a> to ENhance Task-ORiented dialogues (ACCENTOR), with the goal of making virtual assistant conversations more engaging and interactive. Specifically, we propose a Human-AI collaborative data collection approach for generating diverse chit-chat responses to augment task-oriented dialogues with minimal annotation effort. We then present our new chit-chat-based annotations to 23.8 K dialogues from two popular <a href=https://en.wikipedia.org/wiki/Task_(computing)>task-oriented datasets</a> (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their advantage over the originals via human evaluation. Lastly, we propose three new models for adding <a href=https://en.wikipedia.org/wiki/Chit-chat>chit-chat</a> to task-oriented dialogues, explicitly trained to predict user goals and to generate contextually relevant chit-chat responses. Automatic and human evaluations show that, compared with the state-of-the-art task-oriented baseline, our models can code-switch between task and chit-chat to be more engaging, interesting, knowledgeable, and humanlike, while maintaining competitive task performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.129" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.129/>Did they answer? Subjective acts and intents in <a href=https://en.wikipedia.org/wiki/Conversation>conversational discourse</a></a></strong><br><a href=/people/e/elisa-ferracane/>Elisa Ferracane</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/j/junyi-jessy-li/>Junyi Jessy Li</a>
|
<a href=/people/k/katrin-erk/>Katrin Erk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--129><div class="card-body p-3 small">Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> is embedded in a <a href=https://en.wikipedia.org/wiki/Social_environment>social context</a>, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse data</a> and frameworks ignore the <a href=https://en.wikipedia.org/wiki/Social_relation>social aspect</a>, expecting only a single ground truth. We present the first <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse dataset</a> with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents. We carefully analyze our dataset and create computational models to (1) confirm our hypothesis that taking into account the bias of the interpreters leads to better predictions of the interpretations, (2) and show disagreements are nuanced and require a deeper understanding of the different contextual factors. We share our dataset and code at http://github.com/elisaF/subjective_discourse.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.130/>Evaluating the Impact of a Hierarchical Discourse Representation on Entity Coreference Resolution Performance</a></strong><br><a href=/people/s/sopan-khosla/>Sopan Khosla</a>
|
<a href=/people/j/james-fiacco/>James Fiacco</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rosé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--130><div class="card-body p-3 small">Recent work on entity coreference resolution (CR) follows current trends in <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> applied to embeddings and relatively simple task-related features. SOTA models do not make use of hierarchical representations of discourse structure. In this work, we leverage automatically constructed discourse parse trees within a neural approach and demonstrate a significant improvement on two benchmark entity coreference-resolution datasets. We explore how the impact varies depending upon the type of mention.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--131 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.131/>Bridging Resolution : Making Sense of the State of the Art</a></strong><br><a href=/people/h/hideo-kobayashi/>Hideo Kobayashi</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--131><div class="card-body p-3 small">While Yu and Poesio (2020) have recently demonstrated the superiority of their neural multi-task learning (MTL) model to rule-based approaches for bridging anaphora resolution, there is little understanding of (1) how it is better than the rule-based approaches (e.g., are the two approaches making similar or complementary mistakes?) and (2) what should be improved. To shed light on these issues, we (1) propose a hybrid rule-based and MTL approach that would enable a better understanding of their comparative strengths and weaknesses ; and (2) perform a manual analysis of the errors made by the MTL model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--135 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.135/>Mask Attention Networks : Rethinking and Strengthen Transformer</a></strong><br><a href=/people/z/zhihao-fan/>Zhihao Fan</a>
|
<a href=/people/y/yeyun-gong/>Yeyun Gong</a>
|
<a href=/people/d/dayiheng-liu/>Dayiheng Liu</a>
|
<a href=/people/z/zhongyu-wei/>Zhongyu Wei</a>
|
<a href=/people/s/siyuan-wang/>Siyuan Wang</a>
|
<a href=/people/j/jian-jiao/>Jian Jiao</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/r/ruofei-zhang/>Ruofei Zhang</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--135><div class="card-body p-3 small">Transformer is an attention-based neural network, which consists of two sublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN). Existing research explores to enhance the two sublayers separately to improve the capability of Transformer for text representation. In this paper, we present a novel understanding of SAN and FFN as Mask Attention Networks (MANs) and show that they are two special cases of MANs with static mask matrices. However, their static mask matrices limit the capability for localness modeling in text representation learning. We therefore introduce a new layer named dynamic mask attention network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of <a href=https://en.wikipedia.org/wiki/DMAN>DMAN</a>, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> and <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a> demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the original Transformer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.136/>ERNIE-Gram : Pre-Training with Explicitly N-Gram Masked Language Modeling for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a><span class=acl-fixed-case>ERNIE</span>-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding</a></strong><br><a href=/people/d/dongling-xiao/>Dongling Xiao</a>
|
<a href=/people/y/yu-kun-li/>Yu-Kun Li</a>
|
<a href=/people/h/han-zhang/>Han Zhang</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a>
|
<a href=/people/h/hao-tian/>Hao Tian</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--136><div class="card-body p-3 small">Coarse-grained linguistic information, such as named entities or phrases, facilitates adequately representation learning in pre-training. Previous works mainly focus on extending the objective of BERT&#8217;s Masked Language Modeling (MLM) from masking individual tokens to contiguous sequences of n tokens. We argue that such contiguously masking method neglects to model the intra-dependencies and inter-relation of coarse-grained linguistic information. As an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training. In ERNIE-Gram, <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> are masked and predicted directly using explicit n-gram identities rather than contiguous sequences of n tokens. Furthermore, ERNIE-Gram employs a generator model to sample plausible n-gram identities as optional n-gram masks and predict them in both coarse-grained and fine-grained manners to enable comprehensive n-gram prediction and relation modeling. We pre-train ERNIE-Gram on English and Chinese text corpora and fine-tune on 19 downstream tasks. Experimental results show that ERNIE-Gram outperforms previous pre-training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.137.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--137 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.137 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.137" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.137/>Lattice-BERT : Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models<span class=acl-fixed-case>BERT</span>: Leveraging Multi-Granularity Representations in <span class=acl-fixed-case>C</span>hinese Pre-trained Language Models</a></strong><br><a href=/people/y/yuxuan-lai/>Yuxuan Lai</a>
|
<a href=/people/y/yijia-liu/>Yijia Liu</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/s/songfang-huang/>Songfang Huang</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--137><div class="card-body p-3 small">Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more <a href=https://en.wikipedia.org/wiki/Granularity>coarse granularity</a>, e.g., <a href=https://en.wikipedia.org/wiki/Word>words</a>. In this work, we propose a novel pre-training paradigm for Chinese Lattice-BERT, which explicitly incorporates word representations along with <a href=https://en.wikipedia.org/wiki/Chinese_characters>characters</a>, thus can model a sentence in a multi-granularity manner. Specifically, we construct a <a href=https://en.wikipedia.org/wiki/Lattice_graph>lattice graph</a> from the <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>lattice structures</a> in self-attention layers. We further propose a masked segment prediction task to push the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn from rich but redundant information inherent in <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>lattices</a>, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can bring an average increase of 1.5 % under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>lattice structures</a>, and the improvement comes from the exploration of <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundant information</a> and multi-granularity representations. Our code will be available at https://github.com/alibaba/pretrained-language-models/LatticeBERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.139.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--139 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.139 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.139" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.139/>UmlsBERT : Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus<span class=acl-fixed-case>U</span>mls<span class=acl-fixed-case>BERT</span>: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the <span class=acl-fixed-case>U</span>nified <span class=acl-fixed-case>M</span>edical <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>S</span>ystem <span class=acl-fixed-case>M</span>etathesaurus</a></strong><br><a href=/people/g/george-michalopoulos/>George Michalopoulos</a>
|
<a href=/people/y/yuanxin-wang/>Yuanxin Wang</a>
|
<a href=/people/h/hussam-kaka/>Hussam Kaka</a>
|
<a href=/people/h/helen-chen/>Helen Chen</a>
|
<a href=/people/a/alexander-wong/>Alexander Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--139><div class="card-body p-3 small">Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have achieved state-of-the-art results in biomedical natural language processing tasks by focusing their pre-training process on domain-specific corpora. However, such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not take into consideration structured expert domain knowledge from a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. We introduce UmlsBERT, a contextual embedding model that integrates <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> during the pre-training process via a novel knowledge augmentation strategy. More specifically, the augmentation on UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus is performed in two ways : i) connecting words that have the same underlying &#8216;concept&#8217; in UMLS and ii) leveraging semantic type knowledge in UMLS to create clinically meaningful input embeddings. By applying these two strategies, UmlsBERT can encode clinical domain knowledge into word embeddings and outperform existing domain-specific models on common named-entity recognition (NER) and clinical natural language inference tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.143" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.143/>Why Do Document-Level Polarity Classifiers Fail?</a></strong><br><a href=/people/k/karen-martins/>Karen Martins</a>
|
<a href=/people/p/pedro-o-s-vaz-de-melo/>Pedro O.S Vaz-de-Melo</a>
|
<a href=/people/r/rodrygo-santos/>Rodrygo Santos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--143><div class="card-body p-3 small">Machine learning solutions are often criticized for the lack of explanation of their successes and failures. Understanding which instances are misclassified and why is essential to improve the <a href=https://en.wikipedia.org/wiki/Learning>learning process</a>. This work helps to fill this gap by proposing a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to characterize, quantify and measure the impact of hard instances in the task of polarity classification of movie reviews. We characterize such instances into two categories : neutrality, where the text does not convey a clear polarity, and discrepancy, where the polarity of the text is the opposite of its true rating. We quantify the number of hard instances in polarity classification of movie reviews and provide empirical evidence about the need to pay attention to such problematic instances, as they are much harder to classify, for both machine and human classifiers. To the best of our knowledge, this is the first systematic analysis of the impact of hard instances in polarity detection from well-formed textual reviews.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.147/>Domain Divergences : A Survey and Empirical Analysis</a></strong><br><a href=/people/a/abhinav-ramesh-kashyap/>Abhinav Ramesh Kashyap</a>
|
<a href=/people/d/devamanyu-hazarika/>Devamanyu Hazarika</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/r/roger-zimmermann/>Roger Zimmermann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--147><div class="card-body p-3 small">Domain divergence plays a significant role in estimating the performance of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in <a href=https://en.wikipedia.org/wiki/Domain_of_a_function>new domains</a>. While there is a significant literature on <a href=https://en.wikipedia.org/wiki/Divergence>divergence measures</a>, researchers find it hard to choose an appropriate <a href=https://en.wikipedia.org/wiki/Divergence>divergence</a> for a given NLP application. We address this shortcoming by both surveying the literature and through an empirical study. We develop a taxonomy of divergence measures consisting of three classes Information-theoretic, Geometric, and Higher-order measures and identify the relationships between them. Further, to understand the common use-cases of these measures, we recognise three novel applications 1) Data Selection, 2) Learning Representation, and 3) Decisions in the Wild and use it to organise our literature. From this, we identify that Information-theoretic measures are prevalent for 1) and 3), and Higher-order measures are more common for 2). To further help researchers choose appropriate measures to predict drop in performance an important aspect of Decisions in the Wild, we perform <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation analysis</a> spanning 130 domain adaptation scenarios, 3 varied NLP tasks and 12 divergence measures identified from our survey. To calculate these divergences, we consider the current contextual word representations (CWR) and contrast with the older distributed representations. We find that traditional <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a> over word distributions still serve as strong baselines, while higher-order measures with CWR are effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.148/>Target-Aware Data Augmentation for Stance Detection</a></strong><br><a href=/people/y/yingjie-li/>Yingjie Li</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--148><div class="card-body p-3 small">The goal of stance detection is to identify whether the author of a text is in favor of, neutral or against a specific target. Despite substantial progress on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, one of the remaining challenges is the scarcity of <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>. Data augmentation is commonly used to address annotation scarcity by generating more training samples. However, the augmented sentences that are generated by existing methods are either less diversified or inconsistent with the given target and stance label. In this paper, we formulate the data augmentation of stance detection as a conditional masked language modeling task and augment the dataset by predicting the masked word conditioned on both its context and the auxiliary sentence that contains target and label information. Moreover, we propose another simple yet effective method that generates target-aware sentence by replacing a target mention with the other. Experimental results show that our proposed <a href=https://en.wikipedia.org/wiki/Methods_of_detecting_exoplanets>methods</a> significantly outperforms previous augmentation methods on 11 targets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.151/>Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks</a></strong><br><a href=/people/s/siddharth-dalmia/>Siddharth Dalmia</a>
|
<a href=/people/b/brian-yan/>Brian Yan</a>
|
<a href=/people/v/vikas-raunak/>Vikas Raunak</a>
|
<a href=/people/f/florian-metze/>Florian Metze</a>
|
<a href=/people/s/shinji-watanabe/>Shinji Watanabe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--151><div class="card-body p-3 small">End-to-end approaches for sequence tasks are becoming increasingly popular. Yet for complex sequence tasks, like <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a>, systems that cascade several models trained on sub-tasks have shown to be superior, suggesting that the compositionality of cascaded systems simplifies learning and enables sophisticated search capabilities. In this work, we present an end-to-end framework that exploits compositionality to learn searchable hidden representations at intermediate stages of a sequence model using decomposed sub-tasks. These hidden intermediates can be improved using <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> to enhance the overall performance and can also incorporate external models at intermediate stages of the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> to re-score or adapt towards out-of-domain data. One instance of the proposed framework is a Multi-Decoder model for <a href=https://en.wikipedia.org/wiki/Speech_translation>speech translation</a> that extracts the searchable hidden intermediates from a speech recognition sub-task. The model demonstrates the aforementioned benefits and outperforms the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> by around +6 and +3 BLEU on the two test sets of Fisher-CallHome and by around +3 and +4 BLEU on the English-German and English-French test sets of MuST-C.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.153.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--153 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.153 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.153.OptionalSupplementaryCode.zip data-toggle=tooltip data-placement=top title="Optional supplementary code"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.153.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.naacl-main.153/>Worldly Wise (WoW)-Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering<span class=acl-fixed-case>W</span>o<span class=acl-fixed-case>W</span>) - Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering</a></strong><br><a href=/people/k/kiran-ramnath/>Kiran Ramnath</a>
|
<a href=/people/l/leda-sari/>Leda Sari</a>
|
<a href=/people/m/mark-hasegawa-johnson/>Mark Hasegawa-Johnson</a>
|
<a href=/people/c/chang-yoo/>Chang Yoo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--153><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Question_answering>Question-Answering</a> has long been of research interest, its accessibility to users through a <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech interface</a> and its support to multiple languages have not been addressed in prior studies. Towards these ends, we present a new task and a synthetically-generated dataset to do Fact-based Visual Spoken-Question Answering (FVSQA). FVSQA is based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FVSQA, the question is spoken rather than typed. Three sub-tasks are proposed : (1) speech-to-text based, (2) end-to-end, without <a href=https://en.wikipedia.org/wiki/Speech-to-text>speech-to-text</a> as an intermediate component, and (3) cross-lingual, in which the question is spoken in a language different from that in which the KG is recorded. The end-to-end and cross-lingual tasks are the first to require world knowledge from a multi-relational KG as a differentiable layer in an end-to-end spoken language understanding task, hence the proposed reference implementation is called Worldly-Wise (WoW).WoW is shown to perform end-to-end cross-lingual FVSQA at same levels of accuracy across 3 languages-English, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, and <a href=https://en.wikipedia.org/wiki/Turkish_language>Turkish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.154.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--154 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.154 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.154/>Align-Refine : Non-Autoregressive Speech Recognition via Iterative Realignment</a></strong><br><a href=/people/e/ethan-a-chi/>Ethan A. Chi</a>
|
<a href=/people/j/julian-salazar/>Julian Salazar</a>
|
<a href=/people/k/katrin-kirchhoff/>Katrin Kirchhoff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--154><div class="card-body p-3 small">Non-autoregressive encoder-decoder models greatly improve decoding speed over <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a>, at the expense of generation quality. To mitigate this, iterative decoding models repeatedly infill or refine the proposal of a non-autoregressive model. However, editing at the level of output sequences limits model flexibility. We instead propose * iterative realignment *, which by refining latent alignments allows more flexible edits in fewer steps. Our model, Align-Refine, is an end-to-end Transformer which iteratively realigns connectionist temporal classification (CTC) alignments. On the WSJ dataset, Align-Refine matches an autoregressive baseline with a 14x decoding speedup ; on LibriSpeech, we reach an LM-free test-other WER of 9.0 % (19 % relative improvement on comparable work) in three iterations. We release our code at https://github.com/amazon-research/align-refine.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.155.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--155 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.155 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.155" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.155/>Everything Has a Cause : Leveraging <a href=https://en.wikipedia.org/wiki/Causal_inference>Causal Inference</a> in Legal Text Analysis</a></strong><br><a href=/people/x/xiao-liu/>Xiao Liu</a>
|
<a href=/people/d/da-yin/>Da Yin</a>
|
<a href=/people/y/yansong-feng/>Yansong Feng</a>
|
<a href=/people/y/yuting-wu/>Yuting Wu</a>
|
<a href=/people/d/dongyan-zhao/>Dongyan Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--155><div class="card-body p-3 small">Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a>, while mining causal relationship among factors from <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured data</a>, like text, has been less examined, but is of great importance, especially in the <a href=https://en.wikipedia.org/wiki/Legal_research>legal domain</a>. In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal inference to facilitate legal practitioners to make proper decisions. We evaluate the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> on a challenging similar charge disambiguation task. Experimental results show that GCI can capture the nuance from fact descriptions among multiple confusing charges and provide explainable discrimination, especially in few-shot settings. We also observe that the <a href=https://en.wikipedia.org/wiki/Causality>causal knowledge</a> contained in GCI can be effectively injected into powerful <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> for better performance and <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.156.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--156 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.156 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.156" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.156/>Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network</a></strong><br><a href=/people/h/haoran-wu/>Haoran Wu</a>
|
<a href=/people/w/wei-chen/>Wei Chen</a>
|
<a href=/people/s/shuang-xu/>Shuang Xu</a>
|
<a href=/people/b/bo-xu/>Bo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--156><div class="card-body p-3 small">Providing a reliable explanation for <a href=https://en.wikipedia.org/wiki/Medical_diagnosis>clinical diagnosis</a> based on the Electronic Medical Record (EMR) is fundamental to the application of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>Artificial Intelligence</a> in the <a href=https://en.wikipedia.org/wiki/Medicine>medical field</a>. Current methods mostly treat the <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EMR</a> as a text sequence and provide explanations based on a precise medical knowledge base, which is disease-specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract supporting facts from irregular EMR itself without external knowledge bases in this paper. Specifically, we first structure the sequence of EMR into a hierarchical graph network and then obtain the causal relationship between multi-granularity features and diagnosis results through counterfactual intervention on the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. Features having the strongest causal connection with the results provide interpretive support for the <a href=https://en.wikipedia.org/wiki/Diagnosis>diagnosis</a>. Experimental results on real Chinese EMR of the <a href=https://en.wikipedia.org/wiki/Lymphedema>lymphedema</a> demonstrate that our method can diagnose four types of <a href=https://en.wikipedia.org/wiki/Electrophysiology>EMR</a> correctly, and can provide accurate supporting facts for the results. More importantly, the results on different diseases demonstrate the robustness of our approach, which represents the potential application in the <a href=https://en.wikipedia.org/wiki/Medicine>medical field</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.157.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--157 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.157 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.157.OptionalSupplementaryData.pdf data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.157" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.157/>Personalized Response Generation via Generative Split Memory Network</a></strong><br><a href=/people/y/yuwei-wu/>Yuwei Wu</a>
|
<a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--157><div class="card-body p-3 small">Despite the impressive successes of generation and dialogue systems, how to endow a text generation system with particular <a href=https://en.wikipedia.org/wiki/Trait_theory>personality traits</a> to deliver more personalized responses remains under-investigated. In this work, we look at how to generate personalized responses for questions on Reddit by utilizing personalized user profiles and posting histories. Specifically, we release an open-domain single-turn dialog dataset made up of 1.5 M conversation pairs together with 300k profiles of users and related comments. We then propose a memory network to generate personalized responses in dialogue that utilizes a novel mechanism of splitting memories : one for user profile meta attributes and the other for user-generated information like comment histories. Experimental results show the quantitative and qualitative improvements of our simple split memory network model over the state-of-the-art response generation baselines.<i>single-turn</i> dialog dataset made up of 1.5M conversation pairs together with 300k profiles of users and related comments. We then propose a memory network to generate personalized responses in dialogue that utilizes a novel mechanism of splitting memories: one for user profile meta attributes and the other for user-generated information like comment histories. Experimental results show the quantitative and qualitative improvements of our simple split memory network model over the state-of-the-art response generation baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.158.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--158 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.158 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.158/>Towards Few-shot Fact-Checking via Perplexity</a></strong><br><a href=/people/n/nayeon-lee/>Nayeon Lee</a>
|
<a href=/people/y/yejin-bang/>Yejin Bang</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--158><div class="card-body p-3 small">Few-shot learning has drawn researchers&#8217; attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in few-shot learning for various downstream tasks, such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a> is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> via a perplexity score. The most notable strength of our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> lies in its capability in few-shot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10 % on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a> and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to COVID-19.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.161.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--161 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.161 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.161" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.161/>Model Extraction and Adversarial Transferability, Your BERT is Vulnerable !<span class=acl-fixed-case>BERT</span> is Vulnerable!</a></strong><br><a href=/people/x/xuanli-he/>Xuanli He</a>
|
<a href=/people/l/lingjuan-lyu/>Lingjuan Lyu</a>
|
<a href=/people/l/lichao-sun/>Lichao Sun</a>
|
<a href=/people/q/qiongkai-xu/>Qiongkai Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--161><div class="card-body p-3 small">Natural language processing (NLP) tasks, ranging from <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful <a href=https://en.wikipedia.org/wiki/Application_programming_interface>APIs</a> by encapsulating fine-tuned BERT models for <a href=https://en.wikipedia.org/wiki/Downstream_(networking)>downstream tasks</a>. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim / target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the <a href=https://en.wikipedia.org/wiki/Attack_model>attack model</a>. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--166 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.166/>DA-Transformer : Distance-aware Transformer<span class=acl-fixed-case>DA</span>-Transformer: Distance-aware Transformer</a></strong><br><a href=/people/c/chuhan-wu/>Chuhan Wu</a>
|
<a href=/people/f/fangzhao-wu/>Fangzhao Wu</a>
|
<a href=/people/y/yongfeng-huang/>Yongfeng Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--166><div class="card-body p-3 small">Transformer has achieved great success in the NLP field by composing various advanced <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually can not keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the <a href=https://en.wikipedia.org/wiki/Relevance_(information_retrieval)>relevance</a> between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.170.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--170 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.170 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.170.OptionalSupplementaryData.pdf data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.170" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.170/>KPQA : A <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>Metric</a> for Generative Question Answering Using Keyphrase Weights<span class=acl-fixed-case>KPQA</span>: A Metric for Generative Question Answering Using Keyphrase Weights</a></strong><br><a href=/people/h/hwanhee-lee/>Hwanhee Lee</a>
|
<a href=/people/s/seunghyun-yoon/>Seunghyun Yoon</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/d/doo-soon-kim/>Doo Soon Kim</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/j/joongbo-shin/>Joongbo Shin</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--170><div class="card-body p-3 small">In the automatic evaluation of generative question answering (GenQA) systems, it is difficult to assess the correctness of generated answers due to the free-form of the answer. Especially, widely used n-gram similarity metrics often fail to discriminate the incorrect answers since they equally consider all of the tokens. To alleviate this problem, we propose KPQA metric, a new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> for evaluating the correctness of GenQA. Specifically, our new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> assigns different weights to each token via keyphrase prediction, thereby judging whether a generated answer sentence captures the key meaning of the reference answer. To evaluate our <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, we create high-quality human judgments of correctness on two GenQA datasets. Using our human-evaluation datasets, we show that our proposed <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> has a significantly higher correlation with human judgments than existing <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> in various <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Code for KPQA-metric will be available at https://github.com/hwanheelee1993/KPQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.179.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--179 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.179 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.179" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.179/>Modeling Framing in Immigration Discourse on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/j/julia-mendelsohn/>Julia Mendelsohn</a>
|
<a href=/people/c/ceren-budak/>Ceren Budak</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--179><div class="card-body p-3 small">The framing of political issues can influence <a href=https://en.wikipedia.org/wiki/Policy>policy</a> and public opinion. Even though the public plays a key role in creating and spreading frames, little is known about how ordinary people on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> frame political issues. By creating a new dataset of immigration-related tweets labeled for multiple <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing typologies</a> from political communication theory, we develop <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a> to detect <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frames</a>. We demonstrate how users&#8217; ideology and region impact framing choices, and how a message&#8217;s framing influences audience responses. We find that the more commonly-used issue-generic frames obscure important ideological and regional patterns that are only revealed by immigration-specific frames. Furthermore, <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frames</a> oriented towards human interests, culture, and <a href=https://en.wikipedia.org/wiki/Politics>politics</a> are associated with higher <a href=https://en.wikipedia.org/wiki/User_engagement>user engagement</a>. This large-scale analysis of a complex social and linguistic phenomenon contributes to both NLP and social science research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.184.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--184 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.184 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.184/>Learning to Recognize Dialect Features</a></strong><br><a href=/people/d/dorottya-demszky/>Dorottya Demszky</a>
|
<a href=/people/d/devyani-sharma/>Devyani Sharma</a>
|
<a href=/people/j/jonathan-h-clark/>Jonathan Clark</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/j/jacob-eisenstein/>Jacob Eisenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--184><div class="card-body p-3 small">Building <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a> that serve everyone requires accounting for <a href=https://en.wikipedia.org/wiki/Dialect>dialect differences</a>. But <a href=https://en.wikipedia.org/wiki/List_of_dialects_of_English>dialects</a> are not monolithic entities : rather, distinctions between and within dialects are captured by the presence, absence, and frequency of dozens of <a href=https://en.wikipedia.org/wiki/Dialect>dialect features</a> in speech and text, such as the deletion of the copula in He running. In this paper, we introduce the task of dialect feature detection, and present two multitask learning approaches, both based on pretrained transformers. For most <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a>, large-scale annotated corpora for these <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> are unavailable, making it difficult to train recognizers. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on a small number of minimal pairs, building on how linguists typically define <a href=https://en.wikipedia.org/wiki/Dialect>dialect features</a>. Evaluation on a test set of 22 dialect features of <a href=https://en.wikipedia.org/wiki/Indian_English>Indian English</a> demonstrates that these models learn to recognize many <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> with high accuracy, and that a few minimal pairs can be as effective for training as thousands of labeled examples. We also demonstrate the downstream applicability of dialect feature detection both as a measure of <a href=https://en.wikipedia.org/wiki/Dialect_continuum>dialect density</a> and as a dialect classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.187.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--187 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.187 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.187" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.187/>Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis<span class=acl-fixed-case>O</span>rthogonal <span class=acl-fixed-case>P</span>rocrustes <span class=acl-fixed-case>A</span>nalysis</a></strong><br><a href=/people/x/xutan-peng/>Xutan Peng</a>
|
<a href=/people/g/guanyi-chen/>Guanyi Chen</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a>
|
<a href=/people/m/mark-stevenson/>Mark Stevenson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--187><div class="card-body p-3 small">Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance without acknowledging the <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> of the proposed approaches, in terms of <a href=https://en.wikipedia.org/wiki/Time_complexity>execution time</a> and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and <a href=https://en.wikipedia.org/wiki/Carbon_footprint>carbon footprint</a> by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations : full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.188.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--188 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.188 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.188.OptionalSupplementaryCode.zip data-toggle=tooltip data-placement=top title="Optional supplementary code"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.188" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.188/>Rethinking Network Pruning under the Pre-train and Fine-tune Paradigm</a></strong><br><a href=/people/d/dongkuan-xu/>Dongkuan Xu</a>
|
<a href=/people/i/ian-en-hsu-yen/>Ian En-Hsu Yen</a>
|
<a href=/people/j/jinxi-zhao/>Jinxi Zhao</a>
|
<a href=/people/z/zhibin-xiao/>Zhibin Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--188><div class="card-body p-3 small">Transformer-based pre-trained language models have significantly improved the performance of various natural language processing (NLP) tasks in the recent years. While effective and prevalent, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are usually prohibitively large for resource-limited deployment scenarios. A thread of research has thus been working on applying network pruning techniques under the pretrain-then-finetune paradigm widely adopted in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. However, the existing <a href=https://en.wikipedia.org/wiki/Pruning>pruning</a> results on benchmark transformers, such as BERT, are not as remarkable as the pruning results in the literature of convolutional neural networks (CNNs). In particular, common wisdom in pruning CNN states that sparse pruning technique compresses a model more than that obtained by reducing number of channels and layers, while existing works on sparse pruning of BERT yields inferior results than its small-dense counterparts such as TinyBERT. In this work, we aim to fill this gap by studying how knowledge are transferred and lost during the pre-train, fine-tune, and pruning process, and proposing a knowledge-aware sparse pruning process that achieves significantly superior results than existing literature. We show for the first time that sparse pruning compresses a BERT model significantly more than reducing its number of channels and layers. Experiments on multiple data sets of GLUE benchmark show that our method outperforms the leading competitors with a 20-times weight / FLOPs compression and neglectable loss in prediction accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.190.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--190 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.190 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.190" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.190/>Detoxifying Language Models Risks Marginalizing Minority Voices</a></strong><br><a href=/people/a/albert-xu/>Albert Xu</a>
|
<a href=/people/e/eshaan-pathak/>Eshaan Pathak</a>
|
<a href=/people/e/eric-wallace/>Eric Wallace</a>
|
<a href=/people/s/suchin-gururangan/>Suchin Gururangan</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--190><div class="card-body p-3 small">Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020 ; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity : they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a> and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by <a href=https://en.wikipedia.org/wiki/Social_exclusion>marginalized groups</a>. We identify that these failures stem from <a href=https://en.wikipedia.org/wiki/Detoxification_(alternative_medicine)>detoxification methods</a> exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a> and distributional robustness of LMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.191/>HONEST : Measuring Hurtful Sentence Completion in Language Models<span class=acl-fixed-case>HONEST</span>: Measuring Hurtful Sentence Completion in Language Models</a></strong><br><a href=/people/d/debora-nozza/>Debora Nozza</a>
|
<a href=/people/f/federico-bianchi/>Federico Bianchi</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--191><div class="card-body p-3 small">Language models have revolutionized the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. However, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> capture and proliferate hurtful stereotypes, especially in <a href=https://en.wikipedia.org/wiki/Text_generation>text generation</a>. Our results show that 4.3 % of the time, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a <a href=https://en.wikipedia.org/wiki/Score_(statistics)>score</a> to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for six languages. Our findings suggest that these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> replicate and amplify deep-seated societal stereotypes about <a href=https://en.wikipedia.org/wiki/Gender_role>gender roles</a>. Sentence completions refer to <a href=https://en.wikipedia.org/wiki/Promiscuity>sexual promiscuity</a> when the target is female in 9 % of the time, and in 4 % to <a href=https://en.wikipedia.org/wiki/Homosexuality>homosexuality</a> when the target is male. The results raise questions about the use of these <a href=https://en.wikipedia.org/wiki/Physical_model>models</a> in <a href=https://en.wikipedia.org/wiki/Production_line>production settings</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.193.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--193 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.193 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.193" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.193/>DeCEMBERT : Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization<span class=acl-fixed-case>D</span>e<span class=acl-fixed-case>CEMBERT</span>: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization</a></strong><br><a href=/people/z/zineng-tang/>Zineng Tang</a>
|
<a href=/people/j/jie-lei/>Jie Lei</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--193><div class="card-body p-3 small">Leveraging large-scale unlabeled web videos such as instructional videos for pre-training followed by task-specific finetuning has become the de facto approach for many video-and-language tasks. However, these instructional videos are very noisy, the accompanying ASR narrations are often incomplete, and can be irrelevant to or temporally misaligned with the visual content, limiting the performance of the models trained on such data. To address these issues, we propose an improved video-and-language pre-training method that first adds automatically-extracted dense region captions from the video frames as auxiliary text input, to provide informative visual cues for learning better video and language associations. Second, to alleviate the temporal misalignment issue, our method incorporates an entropy minimization-based constrained attention loss, to encourage the model to automatically focus on the correct caption from a pool of candidate ASR captions. Our overall approach is named DeCEMBERT (Dense Captions and Entropy Minimization). Comprehensive experiments on three video-and-language tasks (text-to-video retrieval, video captioning, and video question answering) across five datasets demonstrate that our approach outperforms previous state-of-the-art methods. Ablation studies on pre-training and downstream tasks show that adding dense captions and constrained attention loss help improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance. Lastly, we also provide attention visualization to show the effect of applying the proposed constrained attention loss.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.195.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--195 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.195 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.195.OptionalSupplementaryData.txt data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.195" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.195/>Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models</a></strong><br><a href=/people/p/po-yao-huang/>Po-Yao Huang</a>
|
<a href=/people/m/mandela-patrick/>Mandela Patrick</a>
|
<a href=/people/j/junjie-hu/>Junjie Hu</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/f/florian-metze/>Florian Metze</a>
|
<a href=/people/a/alexander-g-hauptmann/>Alexander Hauptmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--195><div class="card-body p-3 small">This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (Multi-HowTo100 M) for pre-training. Experiments on VTT show that our method significantly improves <a href=https://en.wikipedia.org/wiki/Video_search_engine>video search</a> in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX ; as well as in multilingual text-to-image search on Multi30K. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and Multi-HowTo100 M is available at http://github.com/berniebear/Multi-HT100M.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.196.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--196 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.196 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.196/>Video Question Answering with Phrases via Semantic Roles</a></strong><br><a href=/people/a/arka-sadhu/>Arka Sadhu</a>
|
<a href=/people/k/kan-chen/>Kan Chen</a>
|
<a href=/people/r/ram-nevatia/>Ram Nevatia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--196><div class="card-body p-3 small">Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> limit the VidQA models&#8217; application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fill-in-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty string. To reduce the influence of language bias in VidQA datasets, we retrieve a video having a different answer for the same question. To facilitate research, we construct ActivityNet-SRL-QA and Charades-SRL-QA and benchmark them by extending three vision-language models. We perform extensive analysis and ablative studies to guide future work. Code and data are public.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.197.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--197 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.197 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.197" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.197/>From Masked Language Modeling to <a href=https://en.wikipedia.org/wiki/Translation>Translation</a> : Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding<span class=acl-fixed-case>E</span>nglish Auxiliary Tasks Improve Zero-shot Spoken Language Understanding</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/i/ibrahim-sharaf/>Ibrahim Sharaf</a>
|
<a href=/people/a/aizhan-imankulova/>Aizhan Imankulova</a>
|
<a href=/people/a/ahmet-ustun/>Ahmet Üstün</a>
|
<a href=/people/m/marija-stepanovic/>Marija Stepanović</a>
|
<a href=/people/a/alan-ramponi/>Alan Ramponi</a>
|
<a href=/people/s/siti-oryza-khairunnisa/>Siti Oryza Khairunnisa</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--197><div class="card-body p-3 small">The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing <a href=https://en.wikipedia.org/wiki/Data>data</a> in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual (x) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and <a href=https://en.wikipedia.org/wiki/Translation>translation</a> for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.199.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--199 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.199 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.199" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.199/>Challenging <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional models</a> with a conceptual network of philosophical terms</a></strong><br><a href=/people/y/yvette-oortwijn/>Yvette Oortwijn</a>
|
<a href=/people/j/jelke-bloem/>Jelke Bloem</a>
|
<a href=/people/p/pia-sommerauer/>Pia Sommerauer</a>
|
<a href=/people/f/francois-meyer/>Francois Meyer</a>
|
<a href=/people/w/wei-zhou/>Wei Zhou</a>
|
<a href=/people/a/antske-fokkens/>Antske Fokkens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--199><div class="card-body p-3 small">Computational linguistic research on <a href=https://en.wikipedia.org/wiki/Language_change>language change</a> through distributional semantic (DS) models has inspired researchers from fields such as philosophy and literary studies, who use these methods for the exploration and comparison of comparatively small datasets traditionally analyzed by close reading. Research on <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for <a href=https://en.wikipedia.org/wiki/Small_data>small data</a> is still in early stages and it is not clear which methods achieve the best results. We investigate the possibilities and limitations of using distributional semantic models for analyzing philosophical data by means of a realistic use-case. We provide a ground truth for evaluation created by philosophy experts and a blueprint for using DS models in a sound methodological setup. We compare three <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for creating specialized models from <a href=https://en.wikipedia.org/wiki/Data_set>small datasets</a>. Though the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do not perform well enough to directly support philosophers yet, we find that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> designed for small data yield promising directions for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--200 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.200 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.200" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.200/>KILT : a Benchmark for Knowledge Intensive Language Tasks<span class=acl-fixed-case>KILT</span>: a Benchmark for Knowledge Intensive Language Tasks</a></strong><br><a href=/people/f/fabio-petroni/>Fabio Petroni</a>
|
<a href=/people/a/aleksandra-piktus/>Aleksandra Piktus</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/p/patrick-lewis/>Patrick Lewis</a>
|
<a href=/people/m/majid-yazdani/>Majid Yazdani</a>
|
<a href=/people/n/nicola-de-cao/>Nicola De Cao</a>
|
<a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/y/yacine-jernite/>Yacine Jernite</a>
|
<a href=/people/v/vladimir-karpukhin/>Vladimir Karpukhin</a>
|
<a href=/people/j/jean-maillard/>Jean Maillard</a>
|
<a href=/people/v/vassilis-plachouras/>Vassilis Plachouras</a>
|
<a href=/people/t/tim-rocktaschel/>Tim Rocktäschel</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--200><div class="card-body p-3 small">Challenging problems such as open-domain question answering, <a href=https://en.wikipedia.org/wiki/Fact-checking>fact checking</a>, slot filling and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> require access to large, external knowledge sources. While some <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that condition on specific information in large textual resources, we present a <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, reducing <a href=https://en.wikipedia.org/wiki/Turnaround_time>engineering turnaround</a> through the re-use of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to provide <a href=https://en.wikipedia.org/wiki/Provenance>provenance</a>. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https://github.com/facebookresearch/KILT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.203" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.203/>UDALM : Unsupervised Domain Adaptation through <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a><span class=acl-fixed-case>UDALM</span>: Unsupervised Domain Adaptation through Language Modeling</a></strong><br><a href=/people/c/constantinos-karouzos/>Constantinos Karouzos</a>
|
<a href=/people/g/georgios-paraskevopoulos/>Georgios Paraskevopoulos</a>
|
<a href=/people/a/alexandros-potamianos/>Alexandros Potamianos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--203><div class="card-body p-3 small">In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion during UDA training. Furthermore, we discuss the relationship between A-distance and the target error and explore some limitations of the Domain Adversarial Training approach. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is evaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset, yielding 91.74 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, which is an 1.11 % absolute improvement over the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--204 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.204/>Beyond Black & White : Leveraging Annotator Disagreement via Soft-Label Multi-Task Learning</a></strong><br><a href=/people/t/tommaso-fornaciari/>Tommaso Fornaciari</a>
|
<a href=/people/a/alexandra-uma/>Alexandra Uma</a>
|
<a href=/people/s/silviu-paun/>Silviu Paun</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--204><div class="card-body p-3 small">Supervised learning assumes that a ground truth label exists. However, the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of this ground truth depends on human annotators, who often disagree. Prior work has shown that this disagreement can be helpful in training <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We propose a novel method to incorporate this disagreement as information : in addition to the standard error computation, we use soft-labels (i.e., probability distributions over the annotator labels) as an auxiliary task in a multi-task neural network. We measure the <a href=https://en.wikipedia.org/wiki/Divergence>divergence</a> between the predictions and the target soft-labels with several <a href=https://en.wikipedia.org/wiki/Loss_function>loss-functions</a> and evaluate the models on various <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>. We find that the soft-label prediction auxiliary task reduces the penalty for errors on ambiguous entities, and thereby mitigates <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. It significantly improves performance across <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, beyond the standard approach and prior work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.205/>Clustering-based Inference for Biomedical Entity Linking</a></strong><br><a href=/people/r/rico-angell/>Rico Angell</a>
|
<a href=/people/n/nicholas-monath/>Nicholas Monath</a>
|
<a href=/people/s/sunil-mohan/>Sunil Mohan</a>
|
<a href=/people/n/nishant-yadav/>Nishant Yadav</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--205><div class="card-body p-3 small">Due to large number of entities in biomedical knowledge bases, only a small fraction of entities have corresponding labelled training data. This necessitates entity linking models which are able to link mentions of unseen entities using learned representations of entities. Previous approaches link each mention independently, ignoring the relationships within and across documents between the entity mentions. These relations can be very useful for linking mentions in biomedical text where linking decisions are often difficult due mentions having a generic or a highly specialized form. In this paper, we introduce a model in which linking decisions can be made not merely by linking to a knowledge base entity but also by grouping multiple mentions together via <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a> and jointly making linking predictions. In experiments on the largest publicly available biomedical dataset, we improve the best <a href=https://en.wikipedia.org/wiki/Independence_(probability_theory)>independent prediction</a> for <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> by 3.0 points of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, and our clustering-based inference model further improves <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> by 2.3 points.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.207" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.207/>Diversity-Aware Batch Active Learning for Dependency Parsing</a></strong><br><a href=/people/t/tianze-shi/>Tianze Shi</a>
|
<a href=/people/a/adrian-benton/>Adrian Benton</a>
|
<a href=/people/i/igor-malioutov/>Igor Malioutov</a>
|
<a href=/people/o/ozan-irsoy/>Ozan İrsoy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--207><div class="card-body p-3 small">While the predictive performance of modern statistical dependency parsers relies heavily on the availability of expensive expert-annotated treebank data, not all annotations contribute equally to the training of the <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. In this paper, we attempt to reduce the number of labeled examples needed to train a strong dependency parser using batch active learning (AL). In particular, we investigate whether enforcing diversity in the sampled batches, using determinantal point processes (DPPs), can improve over their diversity-agnostic counterparts. Simulation experiments on an English newswire corpus show that selecting diverse batches with DPPs is superior to strong selection strategies that do not enforce batch diversity, especially during the initial stages of the learning process. Additionally, our diversity-aware strategy is robust under a corpus duplication setting, where diversity-agnostic sampling strategies exhibit significant degradation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--209 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.209/>Can Latent Alignments Improve Autoregressive Machine Translation?</a></strong><br><a href=/people/a/adi-haviv/>Adi Haviv</a>
|
<a href=/people/l/lior-vassertail/>Lior Vassertail</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--209><div class="card-body p-3 small">Latent alignment objectives such as CTC and AXE significantly improve non-autoregressive machine translation models. Can they improve <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a> as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher forcing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--210 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.210" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.210/>Smoothing and Shrinking the Sparse Seq2Seq Search Space<span class=acl-fixed-case>S</span>eq2<span class=acl-fixed-case>S</span>eq Search Space</a></strong><br><a href=/people/b/ben-peters/>Ben Peters</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--210><div class="card-body p-3 small">Current sequence-to-sequence models are trained to minimize <a href=https://en.wikipedia.org/wiki/Cross-entropy>cross-entropy</a> and use <a href=https://en.wikipedia.org/wiki/Softmax>softmax</a> to compute the locally normalized probabilities over target sequences. While this setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias : models give high scores to short, inadequate hypotheses and often make the empty string the argmaxthe so-called cat got your tongue problem. Recently proposed entmax-based sparse sequence-to-sequence models present a possible solution, since they can shrink the search space by assigning zero probability to bad hypotheses, but their ability to handle word-level tasks with transformers has never been tested. In this work, we show that entmax-based models effectively solve the cat got your tongue problem, removing a major source of model error for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. In addition, we generalize label smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, which includes both <a href=https://en.wikipedia.org/wiki/Cross-entropy>cross-entropy</a> and the entmax losses. Our resulting label-smoothed entmax loss models set a new state of the art on multilingual grapheme-to-phoneme conversion and deliver improvements and better calibration properties on cross-lingual morphological inflection and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> for 7 language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--214 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.214" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.214/>Cross-Lingual Word Embedding Refinement by _ 1 Norm Optimisation<span class=tex-math>ℓ<sub>1</sub></span> Norm Optimisation</a></strong><br><a href=/people/x/xutan-peng/>Xutan Peng</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a>
|
<a href=/people/m/mark-stevenson/>Mark Stevenson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--214><div class="card-body p-3 small">Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> that minimise the 2 norm loss function. However, this optimisation objective has been demonstrated to be sensitive to <a href=https://en.wikipedia.org/wiki/Outlier>outliers</a>. Based on the more robust Manhattan norm (aka. 1 norm) goodness-of-fit criterion, this paper proposes a simple post-processing step to improve CLWEs. An advantage of this approach is that it is fully agnostic to the training process of the original CLWEs and can therefore be applied widely. Extensive experiments are performed involving ten diverse languages and embeddings trained on different corpora. Evaluation results based on bilingual lexicon induction and cross-lingual transfer for natural language inference tasks show that the 1 refinement substantially outperforms four state-of-the-art baselines in both supervised and unsupervised settings. It is therefore recommended that this <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> be adopted as a standard for CLWE methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--220 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.220" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.220/>Learning to Synthesize Data for Semantic Parsing</a></strong><br><a href=/people/b/bailin-wang/>Bailin Wang</a>
|
<a href=/people/w/wenpeng-yin/>Wenpeng Yin</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--220><div class="card-body p-3 small">Synthesizing data for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> has gained increasing attention recently. However, most methods require handcrafted (high-precision) rules in their generative process, hindering the exploration of diverse unseen data. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> which features a (non-neural) PCFG that models the composition of programs (e.g., SQL), and a BART-based translation model that maps a program to an utterance. Due to the simplicity of PCFG and pre-trained BART, our <a href=https://en.wikipedia.org/wiki/Generative_model>generative model</a> can be efficiently learned from existing data at hand. Moreover, explicitly modeling compositions using PCFG leads to better exploration of unseen programs, thus generate more diverse data. We evaluate our method in both in-domain and out-of-domain settings of text-to-SQL parsing on the standard benchmarks of GeoQuery and Spider, respectively. Our empirical results show that the synthesized data generated from our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can substantially help a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> achieve better compositional and domain generalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--221 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.221/>Edge : Enriching Knowledge Graph Embeddings with External Text</a></strong><br><a href=/people/s/saed-rezayi/>Saed Rezayi</a>
|
<a href=/people/h/handong-zhao/>Handong Zhao</a>
|
<a href=/people/s/sungchul-kim/>Sungchul Kim</a>
|
<a href=/people/r/ryan-rossi/>Ryan Rossi</a>
|
<a href=/people/n/nedim-lipka/>Nedim Lipka</a>
|
<a href=/people/s/sheng-li/>Sheng Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--221><div class="card-body p-3 small">Knowledge graphs suffer from sparsity which degrades the quality of representations generated by various methods. While there is an abundance of textual information throughout the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> and many existing <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, aligning information across these diverse data sources remains a challenge in the literature. Previous work has partially addressed this issue by enriching knowledge graph entities based on hard co-occurrence of words present in the entities of the knowledge graphs and external text, while we achieve soft augmentation by proposing a knowledge graph enrichment and embedding framework named Edge. Given an original <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a>, we first generate a rich but noisy augmented graph using external texts in semantic and structural level. To distill the relevant knowledge and suppress the introduced noise, we design a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph alignment term</a> in a shared embedding space between the original <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> and augmented graph. To enhance the embedding learning on the augmented graph, we further regularize the locality relationship of target entity based on negative sampling. Experimental results on four benchmark datasets demonstrate the robustness and effectiveness of Edge in link prediction and node classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--225 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.225/>Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention</a></strong><br><a href=/people/p/pengcheng-yin/>Pengcheng Yin</a>
|
<a href=/people/h/hao-fang/>Hao Fang</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/a/adam-pauls/>Adam Pauls</a>
|
<a href=/people/e/emmanouil-antonios-platanios/>Emmanouil Antonios Platanios</a>
|
<a href=/people/y/yu-su/>Yu Su</a>
|
<a href=/people/s/sam-thomson/>Sam Thomson</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--225><div class="card-body p-3 small">We describe a span-level supervised attention loss that improves compositional generalization in <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parsers</a>. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans ; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNNs</a>, and structured decoders on three benchmarks of compositional generalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.229/>Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level Sentiment Classification</a></strong><br><a href=/people/x/xiaochen-hou/>Xiaochen Hou</a>
|
<a href=/people/p/peng-qi/>Peng Qi</a>
|
<a href=/people/g/guangtao-wang/>Guangtao Wang</a>
|
<a href=/people/r/rex-ying/>Rex Ying</a>
|
<a href=/people/j/jing-huang/>Jing Huang</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a>
|
<a href=/people/b/bowen-zhou/>Bowen Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--229><div class="card-body p-3 small">Recent work on aspect-level sentiment classification has demonstrated the efficacy of incorporating syntactic structures such as dependency trees with graph neural networks (GNN), but these approaches are usually vulnerable to parsing errors. To better leverage syntactic information in the face of unavoidable errors, we propose a simple yet effective graph ensemble technique, GraphMerge, to make use of the predictions from different <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. Instead of assigning one set of model parameters to each dependency tree, we first combine the dependency relations from different <a href=https://en.wikipedia.org/wiki/Parsing>parses</a> before applying GNNs over the resulting <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. This allows GNN models to be robust to parse errors at no additional computational cost, and helps avoid overparameterization and <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> from GNN layer stacking by introducing more connectivity into the ensemble graph. Our experiments on the SemEval 2014 Task 4 and ACL 14 Twitter datasets show that our GraphMerge model not only outperforms models with single dependency tree, but also beats other ensemble models without adding model parameters.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--230 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.230" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.230/>Emotion-Infused Models for Explainable Psychological Stress Detection</a></strong><br><a href=/people/e/elsbeth-turcan/>Elsbeth Turcan</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--230><div class="card-body p-3 small">The problem of detecting psychological stress in online posts, and more broadly, of detecting people in distress or in need of help, is a sensitive application for which the ability to interpret models is vital. Here, we present work exploring the use of a semantically related task, <a href=https://en.wikipedia.org/wiki/Emotion_detection>emotion detection</a>, for equally competent but more explainable and human-like psychological stress detection as compared to a black-box model. In particular, we explore the use of <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> as well as emotion-based language model fine-tuning. With our emotion-infused models, we see comparable results to state-of-the-art BERT. Our analysis of the words used for prediction show that our emotion-infused models mirror psychological components of stress.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--231 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.231" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.231/>Aspect-based Sentiment Analysis with Type-aware Graph Convolutional Networks and Layer Ensemble</a></strong><br><a href=/people/y/yuanhe-tian/>Yuanhe Tian</a>
|
<a href=/people/g/guimin-chen/>Guimin Chen</a>
|
<a href=/people/y/yan-song/>Yan Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--231><div class="card-body p-3 small">It is popular that neural graph-based models are applied in existing aspect-based sentiment analysis (ABSA) studies for utilizing word relations through dependency parses to facilitate the task with better semantic guidance for analyzing context and aspect words. However, most of these studies only leverage dependency relations without considering their dependency types, and are limited in lacking efficient mechanisms to distinguish the important relations as well as learn from different layers of graph based models. To address such limitations, in this paper, we propose an approach to explicitly utilize dependency types for ABSA with type-aware graph convolutional networks (T-GCN), where attention is used in T-GCN to distinguish different edges (relations) in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> and attentive layer ensemble is proposed to comprehensively learn from different layers of T-GCN. The validity and effectiveness of our approach are demonstrated in the experimental results, where state-of-the-art performance is achieved on six English benchmark datasets. Further experiments are conducted to analyze the contributions of each component in our approach and illustrate how different layers in T-GCN help ABSA with quantitative and qualitative analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--235 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.235/>Bot-Adversarial Dialogue for Safe Conversational Agents</a></strong><br><a href=/people/j/jing-xu/>Jing Xu</a>
|
<a href=/people/d/da-ju/>Da Ju</a>
|
<a href=/people/m/margaret-li/>Margaret Li</a>
|
<a href=/people/y/y-lan-boureau/>Y-Lan Boureau</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a>
|
<a href=/people/e/emily-dinan/>Emily Dinan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--235><div class="card-body p-3 small">Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either training on data from our new human-and-model-in-the-loop framework in a two-stage system, or baking-in safety to the generative model itself. We find our new techniques are (i) safer than existing models ; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a>. In contrast, we expose serious safety issues in existing standard systems like GPT2, DialoGPT, and BlenderBot.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--239 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.239" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.239/>Action-Based Conversations Dataset : A Corpus for Building More In-Depth Task-Oriented Dialogue Systems</a></strong><br><a href=/people/d/derek-chen/>Derek Chen</a>
|
<a href=/people/h/howard-chen/>Howard Chen</a>
|
<a href=/people/y/yi-yang/>Yi Yang</a>
|
<a href=/people/a/alexander-lin/>Alexander Lin</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--239><div class="card-body p-3 small">Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10 K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated <a href=https://en.wikipedia.org/wiki/Computer_network>networks</a> outperform simpler <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, a considerable gap (50.8 % absolute accuracy) still exists to reach human-level performance on ABCD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--241 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.241" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.241/>COIL : Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List<span class=acl-fixed-case>COIL</span>: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List</a></strong><br><a href=/people/l/luyu-gao/>Luyu Gao</a>
|
<a href=/people/z/zhuyun-dai/>Zhuyun Dai</a>
|
<a href=/people/j/jamie-callan/>Jamie Callan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--241><div class="card-body p-3 small">Classical information retrieval systems such as <a href=https://en.wikipedia.org/wiki/BM25>BM25</a> rely on exact lexical match and can carry out search efficiently with inverted list index. Recent neural IR models shifts towards soft matching all query document terms, but they lose the computation efficiency of exact match systems. This paper presents COIL, a contextualized exact match retrieval architecture, where scoring is based on overlapping query document tokens&#8217; contextualized representations. The new architecture stores contextualized token representations in inverted lists, bringing together the efficiency of <a href=https://en.wikipedia.org/wiki/Exact_match>exact match</a> and the representation power of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep language models</a>. Our experimental results show COIL outperforms classical lexical retrievers and state-of-the-art deep LM retrievers with similar or smaller latency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.244.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--244 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.244 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.244" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.244/>Exploring the Relationship Between Algorithm Performance, <a href=https://en.wikipedia.org/wiki/Vocabulary>Vocabulary</a>, and <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>Run-Time</a> in Text Classification</a></strong><br><a href=/people/w/wilson-fearn/>Wilson Fearn</a>
|
<a href=/people/o/orion-weller/>Orion Weller</a>
|
<a href=/people/k/kevin-seppi/>Kevin Seppi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--244><div class="card-body p-3 small">Text classification is a significant branch of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, and has many applications including <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a> and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Unsurprisingly, those who do <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a> are concerned with the <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time</a> of their algorithms, many of which depend on the size of the corpus&#8217; vocabulary due to their bag-of-words representation. Although many studies have examined the effect of preprocessing techniques on vocabulary size and <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, none have examined how these methods affect a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model&#8217;s run-time</a>. To fill this gap, we provide a comprehensive study that examines how preprocessing techniques affect the vocabulary size, model performance, and model run-time, evaluating ten techniques over four models and two datasets. We show that some individual methods can reduce <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time</a> with no loss of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, while some combinations of methods can trade 2-5 % of the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for up to a 65 % reduction of <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time</a>. Furthermore, some combinations of preprocessing techniques can even provide a 15 % reduction in <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>run-time</a> while simultaneously improving model accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.246.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--246 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.246 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.246" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.246/>You Sound Like Someone Who Watches Drama Movies : Towards Predicting Movie Preferences from Conversational Interactions</a></strong><br><a href=/people/s/sergey-volokhin/>Sergey Volokhin</a>
|
<a href=/people/j/joyce-ho/>Joyce Ho</a>
|
<a href=/people/o/oleg-rokhlenko/>Oleg Rokhlenko</a>
|
<a href=/people/e/eugene-agichtein/>Eugene Agichtein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--246><div class="card-body p-3 small">The increasing popularity of voice-based personal assistants provides new opportunities for conversational recommendation. One particularly interesting area is movie recommendation, which can benefit from an open-ended interaction with the user, through a natural conversation. We explore one promising direction for conversational recommendation : mapping a conversational user, for whom there is limited or no data available, to most similar external reviewers, whose preferences are known, by representing the conversation as a user&#8217;s interest vector, and adapting collaborative filtering techniques to estimate the current user&#8217;s preferences for new movies. We call our proposed method ConvExtr (Conversational Collaborative Filtering using External Data), which 1) infers a user&#8217;s sentiment towards an entity from the conversation context, and 2) transforms the ratings of similar external reviewers to predict the current user&#8217;s preferences. We implement these steps by adapting contextual sentiment prediction techniques, and domain adaptation, respectively. To evaluate our method, we develop and make available a finely annotated dataset of movie recommendation conversations, which we call MovieSent. Our results demonstrate that ConvExtr can improve the accuracy of predicting users&#8217; ratings for new movies by exploiting conversation content and external data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.247.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--247 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.247 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.247/>Reading and Acting while Blindfolded : The Need for <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a> in Text Game Agents</a></strong><br><a href=/people/s/shunyu-yao/>Shunyu Yao</a>
|
<a href=/people/k/karthik-narasimhan/>Karthik Narasimhan</a>
|
<a href=/people/m/matthew-hausknecht/>Matthew Hausknecht</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--247><div class="card-body p-3 small">Text-based games simulate worlds and interact with players using <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent <a href=https://en.wikipedia.org/wiki/Intelligent_agent>artificial agents</a> utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> available to a <a href=https://en.wikipedia.org/wiki/Intelligent_agent>learning agent</a>. Surprisingly, we find that an <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> is capable of achieving high scores even in the complete absence of <a href=https://en.wikipedia.org/wiki/Semantics>language semantics</a>, indicating that the currently popular experimental setup and <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including Zork I. We discuss the implications of our findings for designing future <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agents</a> with stronger semantic understanding.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--254 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.254" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.254/>CaSiNo : A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems<span class=acl-fixed-case>C</span>a<span class=acl-fixed-case>S</span>i<span class=acl-fixed-case>N</span>o: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems</a></strong><br><a href=/people/k/kushal-chawla/>Kushal Chawla</a>
|
<a href=/people/j/jaysa-ramirez/>Jaysa Ramirez</a>
|
<a href=/people/r/rene-clever/>Rene Clever</a>
|
<a href=/people/g/gale-lucas/>Gale Lucas</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a>
|
<a href=/people/j/jonathan-gratch/>Jonathan Gratch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--254><div class="card-body p-3 small">Automated systems that negotiate with humans have broad applications in <a href=https://en.wikipedia.org/wiki/Pedagogy>pedagogy</a> and conversational AI. To advance the development of practical <a href=https://en.wikipedia.org/wiki/Negotiation>negotiation systems</a>, we present CaSiNo : a novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of over a thousand <a href=https://en.wikipedia.org/wiki/Negotiation>negotiation dialogues</a> in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain environment. Inspired by the literature in human-human negotiations, we annotate <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategies</a> and perform <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation analysis</a> to understand how the dialogue behaviors are associated with the <a href=https://en.wikipedia.org/wiki/Negotiation>negotiation</a> performance. We further propose and evaluate a multi-task framework to recognize these <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> in a given utterance. We find that <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations : https://github.com/kushalchawla/CaSiNo</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--255 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.255" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.255/>News Headline Grouping as a Challenging NLU Task<span class=acl-fixed-case>NLU</span> Task</a></strong><br><a href=/people/p/philippe-laban/>Philippe Laban</a>
|
<a href=/people/l/lucas-bandarkar/>Lucas Bandarkar</a>
|
<a href=/people/m/marti-a-hearst/>Marti A. Hearst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--255><div class="card-body p-3 small">Recent progress in Natural Language Understanding (NLU) has seen the latest <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> outperform human performance on many standard <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. These impressive results have led the community to introspect on dataset limitations, and iterate on more nuanced challenges. In this paper, we introduce the task of HeadLine Grouping (HLG) and a corresponding <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> (HLGD) consisting of 20,056 pairs of <a href=https://en.wikipedia.org/wiki/Headline>news headlines</a>, each labeled with a binary judgement as to whether the pair belongs within the same group. On HLGD, human annotators achieve high performance of around 0.9 F-1, while current state-of-the art Transformer models only reach 0.75 F-1, opening the path for further improvements. We further propose a novel unsupervised Headline Generator Swap model for the task of HeadLine Grouping that achieves within 3 F-1 of the best supervised model. Finally, we analyze high-performing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with consistency tests, and find that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are not consistent in their predictions, revealing modeling limits of current <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.262" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.262/>Ensemble of MRR and NDCG models for Visual Dialog<span class=acl-fixed-case>MRR</span> and <span class=acl-fixed-case>NDCG</span> models for Visual Dialog</a></strong><br><a href=/people/i/idan-schwartz/>Idan Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--262><div class="card-body p-3 small">Assessing an <a href=https://en.wikipedia.org/wiki/Intelligent_agent>AI agent</a> that can converse in <a href=https://en.wikipedia.org/wiki/Human_language>human language</a> and understand <a href=https://en.wikipedia.org/wiki/Visual_system>visual content</a> is challenging. Generation metrics, such as BLEU scores favor correct <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> over <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance by taking into account the <a href=https://en.wikipedia.org/wiki/Rank_(linear_algebra)>rank</a> of a single human-derived answer. This approach, however, raises a new challenge : the ambiguity and synonymy of answers, for instance, <a href=https://en.wikipedia.org/wiki/Semantic_equivalence>semantic equivalence</a> (e.g., &#8216;yeah&#8217; and &#8216;yes&#8217;). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as &#8216;I do n&#8217;t know.&#8217; Crafting a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that excels on both MRR and NDCG metrics is challenging. Ideally, an <a href=https://en.wikipedia.org/wiki/Intelligent_agent>AI agent</a> should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a two-step non-parametric ranking approach that can merge strong MRR and NDCG models. Using our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a>, we manage to keep most MRR state-of-the-art performance (70.41 % vs. 71.24 %) and the NDCG state-of-the-art performance (72.16 % vs. 75.35 %). Moreover, our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> won the recent Visual Dialog 2020 challenge. Source code is available at https://github.com/idansc/mrr-ndcg.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.265.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--265 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.265 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.265" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.265/>CREAD : Combined Resolution of Ellipses and Anaphora in Dialogues<span class=acl-fixed-case>CREAD</span>: Combined Resolution of Ellipses and Anaphora in Dialogues</a></strong><br><a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/s/shruti-bhargava/>Shruti Bhargava</a>
|
<a href=/people/j/jiarui-lu/>Jiarui Lu</a>
|
<a href=/people/j/joel-ruben-antony-moniz/>Joel Ruben Antony Moniz</a>
|
<a href=/people/d/dhivya-piraviperumal/>Dhivya Piraviperumal</a>
|
<a href=/people/l/lin-li/>Lin Li</a>
|
<a href=/people/h/hong-yu/>Hong Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--265><div class="card-body p-3 small">Anaphora and <a href=https://en.wikipedia.org/wiki/Ellipsis>ellipses</a> are two common phenomena in <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a>. Without resolving <a href=https://en.wikipedia.org/wiki/Reference>referring expressions</a> and information omission, <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> may fail to generate consistent and coherent responses. Traditionally, <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a> is resolved by <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and ellipses by query rewrite. In this work, we propose a novel joint learning framework of modeling <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> and query rewriting for complex, multi-turn dialogue understanding. Given an ongoing dialogue between a user and a dialogue assistant, for the user query, our joint learning model first predicts coreference links between the query and the dialogue context, and then generates a self-contained rewritten user query. To evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, we annotate a dialogue based coreference resolution dataset, MuDoCo, with rewritten queries. Results show that the performance of query rewrite can be substantially boosted (+2.3 % F1) with the aid of coreference modeling. Furthermore, our joint model outperforms the state-of-the-art coreference resolution model (+2 % F1) on this dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.266.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--266 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.266 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.266" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.266/>Knowledge-Driven Slot Constraints for Goal-Oriented Dialogue Systems</a></strong><br><a href=/people/p/piyawat-lertvittayakumjorn/>Piyawat Lertvittayakumjorn</a>
|
<a href=/people/d/daniele-bonadiman/>Daniele Bonadiman</a>
|
<a href=/people/s/saab-mansour/>Saab Mansour</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--266><div class="card-body p-3 small">In goal-oriented dialogue systems, users provide information through slot values to achieve specific goals. Practically, some combinations of slot values can be invalid according to external knowledge. For example, a combination of <a href=https://en.wikipedia.org/wiki/Cheese_pizza>cheese pizza</a> (a menu item) and oreo cookies (a topping) from an input utterance Can I order a <a href=https://en.wikipedia.org/wiki/Cheese_pizza>cheese pizza</a> with <a href=https://en.wikipedia.org/wiki/Oreo>oreo cookies</a> on top? exemplifies such invalid combinations according to the menu of a restaurant business. Traditional dialogue systems allow execution of validation rules as a post-processing step after slots have been filled which can lead to error accumulation. In this paper, we formalize knowledge-driven slot constraints and present a new task of constraint violation detection accompanied with benchmarking data. Then, we propose methods to integrate the external knowledge into the system and model constraint violation detection as an end-to-end classification task and compare it to the traditional rule-based pipeline approach. Experiments on two domains of the MultiDoGO dataset reveal challenges of constraint violation detection and sets the stage for future work and improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.267.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--267 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.267 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.267/>Clipping Loops for Sample-Efficient Dialogue Policy Optimisation</a></strong><br><a href=/people/y/yen-chen-wu/>Yen-Chen Wu</a>
|
<a href=/people/c/carl-edward-rasmussen/>Carl Edward Rasmussen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--267><div class="card-body p-3 small">Training dialogue agents requires a large number of interactions with users : agents have no idea about which responses are bad among a lengthy dialogue. In this paper, we propose loop-clipping policy optimisation (LCPO) to eliminate useless responses. LCPO consists of two stages : loop clipping and advantage clipping. In loop clipping, we clip off useless responses (called loops) from dialogue history (called trajectories). The clipped trajectories are more succinct than the original ones, and the estimation of state-value is more accurate. Second, in advantage clipping, we estimate and clip the advantages of useless responses and normal ones separately. The clipped advantage distinguish useless actions from others and reduce the probabilities of useless actions efficiently. In experiments on Cambridge Restaurant Dialogue System, LCPO uses only 260 training dialogues to achieve 80 % success rate, while PPO baseline requires 2160 dialogues. Besides, LCPO receives 3.7/5 scores in <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human evaluation</a> where the agent interactively collects 100 real-user dialogues in training phase.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--270 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.270" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.270/>TABBIE : Pretrained Representations of Tabular Data<span class=acl-fixed-case>TABBIE</span>: Pretrained Representations of Tabular Data</a></strong><br><a href=/people/h/hiroshi-iida/>Hiroshi Iida</a>
|
<a href=/people/d/dung-thai/>Dung Thai</a>
|
<a href=/people/v/varun-manjunatha/>Varun Manjunatha</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--270><div class="card-body p-3 small">Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.275.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--275 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.275 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.275/>Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus</a></strong><br><a href=/people/n/navita-goyal/>Navita Goyal</a>
|
<a href=/people/b/balaji-vasan-srinivasan/>Balaji Vasan Srinivasan</a>
|
<a href=/people/a/anandhavelu-n/>Anandhavelu N</a>
|
<a href=/people/a/abhilasha-sancheti/>Abhilasha Sancheti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--275><div class="card-body p-3 small">Style transfer has been widely explored in <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> with non-parallel corpus by directly or indirectly extracting a notion of <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>style</a> from source and target domain corpus. A common shortcoming of existing approaches is the prerequisite of joint annotations across all the <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>stylistic dimensions</a> under consideration. Availability of such <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> across a combination of styles limits the extension of these setups to multiple style dimensions. While cascading single-dimensional models across multiple styles is a possibility, it suffers from content loss, especially when the style dimensions are not completely independent of each other. In our work, we relax this requirement of jointly annotated data across multiple styles by using independently acquired data across different style dimensions without any additional annotations. We initialize an encoder-decoder setup with transformer-based language model pre-trained on a generic corpus and enhance its re-writing capability to multiple target style dimensions by employing multiple style-aware language models as discriminators. Through quantitative and qualitative evaluation, we show the ability of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to control styles across multiple style dimensions while preserving content of the input text. We compare it against baselines involving cascaded state-of-the-art uni-dimensional style transfer models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.280.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--280 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.280 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.280" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.280/>InfoXLM : An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training<span class=acl-fixed-case>I</span>nfo<span class=acl-fixed-case>XLM</span>: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training</a></strong><br><a href=/people/z/zewen-chi/>Zewen Chi</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/n/nan-yang/>Nan Yang</a>
|
<a href=/people/s/saksham-singhal/>Saksham Singhal</a>
|
<a href=/people/w/wenhui-wang/>Wenhui Wang</a>
|
<a href=/people/x/xia-song/>Xia Song</a>
|
<a href=/people/x/xian-ling-mao/>Xian-Ling Mao</a>
|
<a href=/people/h/he-yan-huang/>Heyan Huang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--280><div class="card-body p-3 small">In this work, we present an information-theoretic framework that formulates cross-lingual language model pre-training as maximizing <a href=https://en.wikipedia.org/wiki/Mutual_information>mutual information</a> between multilingual-multi-granularity texts. The unified view helps us to better understand the existing <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for learning cross-lingual representations. More importantly, inspired by the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, we propose a new pre-training task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The <a href=https://en.wikipedia.org/wiki/Source_code>code</a> and pre-trained models are available at https://aka.ms/infoxlm.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.283" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.283/>X-METRA-ADA : Cross-lingual Meta-Transfer learning Adaptation to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a> and Question Answering<span class=acl-fixed-case>X</span>-<span class=acl-fixed-case>METRA</span>-<span class=acl-fixed-case>ADA</span>: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering</a></strong><br><a href=/people/m/meryem-mhamdi/>Meryem M’hamdi</a>
|
<a href=/people/d/doo-soon-kim/>Doo Soon Kim</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--283><div class="card-body p-3 small">Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their <a href=https://en.wikipedia.org/wiki/Generalization>generalization ability</a> is still inconsistent for <a href=https://en.wikipedia.org/wiki/Linguistic_typology>typologically diverse languages</a> and across different benchmarks. Recently, <a href=https://en.wikipedia.org/wiki/Meta-learning>meta-learning</a> has garnered attention as a promising technique for enhancing <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> under low-resource scenarios : particularly for cross-lingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks : multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.288.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--288 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.288 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.288/>Adaptable and Interpretable Neural MemoryOver Symbolic Knowledge<span class=acl-fixed-case>M</span>emory<span class=acl-fixed-case>O</span>ver Symbolic Knowledge</a></strong><br><a href=/people/p/pat-verga/>Pat Verga</a>
|
<a href=/people/h/haitian-sun/>Haitian Sun</a>
|
<a href=/people/l/livio-baldini-soares/>Livio Baldini Soares</a>
|
<a href=/people/w/william-cohen/>William Cohen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--288><div class="card-body p-3 small">Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information : however, augmenting or modifying this <a href=https://en.wikipedia.org/wiki/Information>information</a> requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a fact memory. Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5 % of the parameters. Most interestingly, we demonstrate that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be modified, without any <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>re-training</a>, by updating the fact memory.<i>any</i> re-training, by updating the fact memory.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--290 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.290" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.290/>Refining Targeted Syntactic Evaluation of <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a></a></strong><br><a href=/people/b/benjamin-newman/>Benjamin Newman</a>
|
<a href=/people/k/kai-siang-ang/>Kai-Siang Ang</a>
|
<a href=/people/j/julia-gong/>Julia Gong</a>
|
<a href=/people/j/john-hewitt/>John Hewitt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--290><div class="card-body p-3 small">Targeted syntactic evaluation of subject-verb number agreement in English (TSE) evaluates language models&#8217; syntactic knowledge using hand-crafted minimal pairs of sentences that differ only in the main verb&#8217;s conjugation. The method evaluates whether language models rate each grammatical sentence as more likely than its ungrammatical counterpart. We identify two distinct goals for TSE. First, evaluating the systematicity of a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>&#8217;s syntactic knowledge : given a sentence, can it conjugate arbitrary verbs correctly? Second, evaluating a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s likely behavior : given a sentence, does the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> concentrate its <a href=https://en.wikipedia.org/wiki/Probability_mass_function>probability mass</a> on correctly conjugated verbs, even if only on a subset of the possible verbs? We argue that current implementations of TSE do not directly capture either of these goals, and propose new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to capture each goal separately. Under our metrics, we find that TSE overestimates systematicity of language models, but that <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> score up to 40 % better on verbs that they predict are likely in context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.293.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--293 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.293 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.293" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.293/>Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack</a></strong><br><a href=/people/l/liwen-wang/>Liwen Wang</a>
|
<a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/y/yanan-wu/>Yanan Wu</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--293><div class="card-body p-3 small">Representation learning is widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for a vast range of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. However, <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> derived from <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpora</a> often reflect <a href=https://en.wikipedia.org/wiki/Bias>social biases</a>. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate representations trained on the main task. We aim to denoise bias information while training on the downstream task, rather than completely remove social bias and pursue static unbiased representations. Experiments show the effectiveness of our method, both on the effect of <a href=https://en.wikipedia.org/wiki/Debiasing>debiasing</a> and the main task performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--299 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.299/>On the Impact of <a href=https://en.wikipedia.org/wiki/Random_seed>Random Seeds</a> on the Fairness of Clinical Classifiers</a></strong><br><a href=/people/s/silvio-amir/>Silvio Amir</a>
|
<a href=/people/j/jan-willem-van-de-meent/>Jan-Willem van de Meent</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--299><div class="card-body p-3 small">Recent work has shown that fine-tuning large networks is surprisingly sensitive to changes in random seed(s). We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III the standard dataset in clinical NLP research. Apparent subgroup performance varies substantially for seeds that yield similar overall performance, although there is no evidence of a trade-off between overall and subgroup performance. However, we also find that the small sample sizes inherent to looking at intersections of <a href=https://en.wikipedia.org/wiki/Minority_group>minority groups</a> and somewhat rare conditions limit our ability to accurately estimate disparities. Further, we find that jointly optimizing for high overall performance and low disparities does not yield statistically significant improvements. Our results suggest that fairness work using MIMIC-III should carefully account for variations in apparent differences that may arise from <a href=https://en.wikipedia.org/wiki/Stochastic>stochasticity</a> and small sample sizes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--300 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.300 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.300.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.naacl-main.300/>Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures</a></strong><br><a href=/people/c/caitlin-doogan/>Caitlin Doogan</a>
|
<a href=/people/w/wray-buntine/>Wray Buntine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--300><div class="card-body p-3 small">When developing topic models, a critical question that should be asked is : How well will this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models&#8217; generalizability remains in question. In this paper, we probe the issue of <a href=https://en.wikipedia.org/wiki/Validity_(statistics)>validity</a> in topic model evaluation and assess how informative coherence measures are for specialized collections used in an applied setting. Informed by the literature, we propose four understandings of <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. We evaluate these using a novel experimental framework reflective of varied applied settings, including human evaluations using <a href=https://en.wikipedia.org/wiki/Open_label>open labeling</a>, typical of <a href=https://en.wikipedia.org/wiki/Applied_science>applied research</a>. These evaluations show that for some specialized collections, standard coherence measures may not inform the most appropriate topic model or the optimal number of topics, and current interpretability performance validation methods are challenged as a means to confirm model quality in the absence of ground truth data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--304 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.304/>Learning to Learn to be Right for the Right Reasons</a></strong><br><a href=/people/p/pride-kavumba/>Pride Kavumba</a>
|
<a href=/people/b/benjamin-heinzerling/>Benjamin Heinzerling</a>
|
<a href=/people/a/ana-brassard/>Ana Brassard</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--304><div class="card-body p-3 small">Improving model generalization on held-out data is one of the core objectives in common- sense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> have improved performance on hard instances, they also lead to degraded performance on easy in- stances. Here, we propose to explicitly learn a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> that does well on both the easy test set with superficial cues and the hard test set without superficial cues. Using a meta-learning objective, we learn such a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that improves performance on both the easy test set and the hard test set. By evaluating our models on Choice of Plausible Alternatives (COPA) and Commonsense Explanation, we show that our proposed method leads to improved performance on both the easy test set and the hard test set upon which we observe up to 16.5 percentage points improvement over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--305 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.305" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.305/>Double Perturbation : On the Robustness of Robustness and Counterfactual Bias Evaluation</a></strong><br><a href=/people/c/chong-zhang/>Chong Zhang</a>
|
<a href=/people/j/jieyu-zhao/>Jieyu Zhao</a>
|
<a href=/people/h/huan-zhang/>Huan Zhang</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/c/cho-jui-hsieh/>Cho-Jui Hsieh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--305><div class="card-body p-3 small">Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these <a href=https://en.wikipedia.org/wiki/Evaluation>evaluations</a> robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a double perturbation framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models&#8217; robustness and counterfactual bias in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. (1) For <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a>, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed <a href=https://en.wikipedia.org/wiki/Cyberattack>attack</a> attains high success rates (96.0%-99.8 %) in finding vulnerable examples on both original and robustly trained <a href=https://en.wikipedia.org/wiki/Computer_simulation>CNNs</a> and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/nlp-second-order-attack.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--307 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.307/>Almost Free Semantic Draft for Neural Machine Translation</a></strong><br><a href=/people/x/xi-ai/>Xi Ai</a>
|
<a href=/people/b/bin-fang/>Bin Fang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--307><div class="card-body p-3 small">Translation quality can be improved by global information from the required target sentence because the <a href=https://en.wikipedia.org/wiki/Code>decoder</a> can understand both past and future information. However, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> needs additional cost to produce and consider such <a href=https://en.wikipedia.org/wiki/Geographic_data_and_information>global information</a>. In this work, to inject global information but also save cost, we present an efficient method to sample and consider a semantic draft as global information from <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> for decoding with almost free of cost. Unlike other successful adaptations, we do not have to perform an EM-like process that repeatedly samples a possible <a href=https://en.wikipedia.org/wiki/Semantics>semantic</a> from the <a href=https://en.wikipedia.org/wiki/Semantics>semantic space</a>. Empirical experiments show that the presented method can achieve competitive performance in common language pairs with a clear advantage in inference efficiency. We will open all our source code on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--308 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.308" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.308/>Pruning-then-Expanding Model for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>Domain Adaptation</a> of Neural Machine Translation</a></strong><br><a href=/people/s/shuhao-gu/>Shuhao Gu</a>
|
<a href=/people/y/yang-feng/>Yang Feng</a>
|
<a href=/people/w/wanying-xie/>Wanying Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--308><div class="card-body p-3 small">Domain Adaptation is widely used in practical applications of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, which aims to achieve good performance on both general domain and in-domain data. However, the existing methods for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> usually suffer from catastrophic forgetting, large domain divergence, and model explosion. To address these three problems, we propose a method of <a href=https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm>divide and conquer</a> which is based on the importance of neurons or parameters for the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation model</a>. In this method, we first prune the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and only keep the important neurons or parameters, making them responsible for both general-domain and in-domain translation. Then we further train the pruned model supervised by the original whole model with knowledge distillation. Last we expand the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to the original size and fine-tune the added parameters for the in-domain translation. We conducted experiments on different language pairs and domains and the results show that our method can achieve significant improvements compared with several strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--310 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.310/>Continual Learning for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/y/yue-cao/>Yue Cao</a>
|
<a href=/people/h/hao-ran-wei/>Hao-Ran Wei</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--310><div class="card-body p-3 small">Neural machine translation (NMT) models are data-driven and require large-scale training corpus. In practical applications, NMT models are usually trained on a general domain corpus and then fine-tuned by continuing training on the in-domain corpus. However, this bears the risk of catastrophic forgetting that the performance on the <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>general domain</a> is decreased drastically. In this work, we propose a new continual learning framework for NMT models. We consider a scenario where the training is comprised of multiple stages and propose a dynamic knowledge distillation technique to alleviate the problem of catastrophic forgetting systematically. We also find that the bias exists in the output linear projection when fine-tuning on the in-domain corpus, and propose a bias-correction module to eliminate the bias. We conduct experiments on three representative settings of NMT application. Experimental results show that the proposed method achieves superior performance compared to baseline models in all settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--314 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.314/>ER-AE : Differentially Private Text Generation for Authorship Anonymization<span class=acl-fixed-case>ER</span>-<span class=acl-fixed-case>AE</span>: Differentially Private Text Generation for Authorship Anonymization</a></strong><br><a href=/people/h/haohan-bo/>Haohan Bo</a>
|
<a href=/people/s/steven-h-h-ding/>Steven H. H. Ding</a>
|
<a href=/people/b/benjamin-c-m-fung/>Benjamin C. M. Fung</a>
|
<a href=/people/f/farkhund-iqbal/>Farkhund Iqbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--314><div class="card-body p-3 small">Most of privacy protection studies for <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual data</a> focus on removing explicit sensitive identifiers. However, <a href=https://en.wikipedia.org/wiki/Writing_style>personal writing style</a>, as a strong indicator of the <a href=https://en.wikipedia.org/wiki/Author>authorship</a>, is often neglected. Recent <a href=https://en.wikipedia.org/wiki/Research>studies</a>, such as SynTF, have shown promising results on privacy-preserving text mining. However, their <a href=https://en.wikipedia.org/wiki/Data_anonymization>anonymization algorithm</a> can only output numeric term vectors which are difficult for the recipients to interpret. We propose a novel text generation model with a two-set exponential mechanism for authorship anonymization. By augmenting the semantic information through a REINFORCE training reward function, the model can generate differentially private text that has a close semantic and similar grammatical structure to the original text while removing personal traits of the writing style. It does not assume any conditioned labels or paralleled text data for training. We evaluate the performance of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the real-life peer reviews dataset and the Yelp review dataset. The result suggests that our model outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on semantic preservation, <a href=https://en.wikipedia.org/wiki/Obfuscation_(software)>authorship obfuscation</a>, and stylometric transformation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--320 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.320/>A recipe for annotating grounded clarifications</a></strong><br><a href=/people/l/luciana-benotti/>Luciana Benotti</a>
|
<a href=/people/p/patrick-blackburn/>Patrick Blackburn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--320><div class="card-body p-3 small">In order to interpret the communicative intents of an utterance, it needs to be grounded in something that is outside of language ; that is, grounded in world modalities. In this paper, we argue that dialogue clarification mechanisms make explicit the process of interpreting the communicative intents of the speaker&#8217;s utterances by grounding them in the various modalities in which the dialogue is situated. This paper frames dialogue clarification mechanisms as an understudied research problem and a key missing piece in the giant jigsaw puzzle of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. We discuss both the theoretical background and practical challenges posed by this problem and propose a recipe for obtaining grounding annotations. We conclude by highlighting ethical issues that need to be addressed in future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--321 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.321.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.naacl-main.321/>Grey-box Adversarial Attack And Defence For Sentiment Classification</a></strong><br><a href=/people/y/ying-xu/>Ying Xu</a>
|
<a href=/people/x/xu-zhong/>Xu Zhong</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--321><div class="card-body p-3 small">We introduce a grey-box adversarial attack and defence framework for sentiment classification. We address the issues of differentiability, label preservation and input reconstruction for adversarial attack and defence in one unified framework. Our results show that once trained, the attacking model is capable of generating high-quality adversarial examples substantially faster (one order of magnitude less in time) than state-of-the-art attacking methods. These examples also preserve the original <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> according to <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a>. Additionally, our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> produces an improved <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> that is robust in defending against multiple adversarial attacking methods. Code is available at : https://github.com/ibm-aur-nlp/adv-def-text-dist.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--324 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.324/>Dynabench : Rethinking Benchmarking in NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/d/douwe-kiela/>Douwe Kiela</a>
|
<a href=/people/m/max-bartolo/>Max Bartolo</a>
|
<a href=/people/y/yixin-nie/>Yixin Nie</a>
|
<a href=/people/d/divyansh-kaushik/>Divyansh Kaushik</a>
|
<a href=/people/a/atticus-geiger/>Atticus Geiger</a>
|
<a href=/people/z/zhengxuan-wu/>Zhengxuan Wu</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/g/grusha-prasad/>Grusha Prasad</a>
|
<a href=/people/a/amanpreet-singh/>Amanpreet Singh</a>
|
<a href=/people/p/pratik-ringshia/>Pratik Ringshia</a>
|
<a href=/people/z/zhiyi-ma/>Zhiyi Ma</a>
|
<a href=/people/t/tristan-thrush/>Tristan Thrush</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/p/pontus-stenetorp/>Pontus Stenetorp</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--324><div class="card-body p-3 small">We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation : annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community : contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a>, and address potential objections to dynamic benchmarking as a new standard for the field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--326 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.326" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.326/>Predicting Discourse Trees from Transformer-based Neural Summarizers</a></strong><br><a href=/people/w/wen-xiao/>Wen Xiao</a>
|
<a href=/people/p/patrick-huber/>Patrick Huber</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--326><div class="card-body p-3 small">Previous work indicates that <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse information</a> benefits <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>. In this paper, we explore whether this synergy between <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> and <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> is bidirectional, by inferring document-level discourse trees from pre-trained neural summarizers. In particular, we generate unlabeled RST-style discourse trees from the self-attention matrices of the transformer model. Experiments across models and datasets reveal that the summarizer learns both, dependency- and constituency-style discourse information, which is typically encoded in a single head, covering long- and short-distance discourse dependencies. Overall, the experimental results suggest that the learned <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse information</a> is general and transferable inter-domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--329 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.329" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.329/>Stay Together : A System for Single and Split-antecedent Anaphora Resolution</a></strong><br><a href=/people/j/juntao-yu/>Juntao Yu</a>
|
<a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/s/silviu-paun/>Silviu Paun</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--329><div class="card-body p-3 small">The state-of-the-art on basic, single-antecedent anaphora has greatly improved in recent years. Researchers have therefore started to pay more attention to more complex cases of <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a> such as split-antecedent anaphora, as in Time-Warner is considering a legal challenge to Telecommunications Inc&#8217;s plan to buy half of Showtime Networks Inca move that could lead to all-out war between the two powerful companies. Split-antecedent anaphora is rarer and more complex to resolve than single-antecedent anaphora ; as a result, it is not annotated in many datasets designed to test <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a>, and previous work on resolving this type of <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora</a> was carried out in unrealistic conditions that assume gold mentions and/or gold split-antecedent anaphors are available. These systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--331 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.331" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.331/>CoRT : Complementary Rankings from Transformers<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>RT</span>: Complementary Rankings from Transformers</a></strong><br><a href=/people/m/marco-wrzalik/>Marco Wrzalik</a>
|
<a href=/people/d/dirk-krechel/>Dirk Krechel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--331><div class="card-body p-3 small">Many recent approaches towards neural information retrieval mitigate their <a href=https://en.wikipedia.org/wiki/Computational_cost>computational costs</a> by using a multi-stage ranking pipeline. In the first stage, a number of potentially relevant candidates are retrieved using an efficient retrieval model such as <a href=https://en.wikipedia.org/wiki/BM25>BM25</a>. Although <a href=https://en.wikipedia.org/wiki/BM25>BM25</a> has proven decent performance as a first-stage ranker, <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> tends to miss relevant passages. In this context we propose CoRT, a simple neural first-stage ranking model that leverages contextual representations from pretrained language models such as BERT to complement term-based ranking functions while causing no significant delay at query time. Using the MS MARCO dataset, we show that CoRT significantly increases the candidate recall by complementing <a href=https://en.wikipedia.org/wiki/BM25>BM25</a> with missing candidates. Consequently, we find subsequent re-rankers achieve superior results with less candidates. We further demonstrate that passage retrieval using <a href=https://en.wikipedia.org/wiki/CoRoT>CoRT</a> can be realized with surprisingly low latencies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--332 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.332" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.332/>Multi-source Neural Topic Modeling in Multi-view Embedding Spaces</a></strong><br><a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/y/yatin-chaudhary/>Yatin Chaudhary</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--332><div class="card-body p-3 small">Though <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embed ding spaces : (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (context-insensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word embeddings</a> (i.e., WordPool). We then identify one or more relevant source domain(s) and <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>transfer knowledge</a> to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (high-resource) and 5 target (low-resource) corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.334.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--334 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.334 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.334" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.334/>Self-Alignment Pretraining for Biomedical Entity Representations</a></strong><br><a href=/people/f/fangyu-liu/>Fangyu Liu</a>
|
<a href=/people/e/ehsan-shareghi/>Ehsan Shareghi</a>
|
<a href=/people/z/zaiqiao-meng/>Zaiqiao Meng</a>
|
<a href=/people/m/marco-basaldella/>Marco Basaldella</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--334><div class="card-body p-3 small">Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity linking</a> where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage <a href=https://en.wikipedia.org/wiki/Unified_Modeling_Language>UMLS</a>, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the <a href=https://en.wikipedia.org/wiki/Scientific_method>scientific domain</a>, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--335 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.335/>TaxoClass : Hierarchical Multi-Label Text Classification Using Only Class Names<span class=acl-fixed-case>T</span>axo<span class=acl-fixed-case>C</span>lass: Hierarchical Multi-Label Text Classification Using Only Class Names</a></strong><br><a href=/people/j/jiaming-shen/>Jiaming Shen</a>
|
<a href=/people/w/wenda-qiu/>Wenda Qiu</a>
|
<a href=/people/y/yu-meng/>Yu Meng</a>
|
<a href=/people/j/jingbo-shang/>Jingbo Shang</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--335><div class="card-body p-3 small">Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a <a href=https://en.wikipedia.org/wiki/Taxonomic_rank>taxonomic class hierarchy</a>. Most existing HMTC methods train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its core classes, and then check core classes&#8217; ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document&#8217;s core classes and utilizes confident core classes to train a taxonomy-enhanced classifier, and (3) generalizes the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only <a href=https://en.wikipedia.org/wiki/Class_(computer_programming)>class names</a>, outperforming the best previous <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> by 25 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--336 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.336" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.336/>MERMAID : Metaphor Generation with <a href=https://en.wikipedia.org/wiki/Symbol>Symbolism</a> and Discriminative Decoding<span class=acl-fixed-case>MERMAID</span>: Metaphor Generation with Symbolism and Discriminative Decoding</a></strong><br><a href=/people/t/tuhin-chakrabarty/>Tuhin Chakrabarty</a>
|
<a href=/people/x/xurui-zhang/>Xurui Zhang</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--336><div class="card-body p-3 small">Generating metaphors is a challenging task as it requires a proper understanding of <a href=https://en.wikipedia.org/wiki/Abstraction>abstract concepts</a>, making connections between unrelated concepts, and deviating from the <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>literal meaning</a>. In this paper, we aim to generate a metaphoric sentence given a <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>literal expression</a> by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (CITATION) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model fine-tuned on our parallel data to generate high-quality metaphors. Human evaluation on an independent test set of literal statements shows that our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generates <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> better than three well-crafted baselines 66 % of the time on average. A task-based evaluation shows that human-written poems enhanced with <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> proposed by our model are preferred 68 % of the time compared to poems without <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--340 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.340" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.340/>Ask what’s missing and what’s useful : Improving Clarification Question Generation using Global Knowledge</a></strong><br><a href=/people/b/bodhisattwa-prasad-majumder/>Bodhisattwa Prasad Majumder</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/m/michel-galley/>Michel Galley</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--340><div class="card-body p-3 small">The ability to generate clarification questions i.e., questions that identify useful missing information in a given context, is important in reducing <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a>. Humans use previous experience with similar contexts to form a global view and compare it to the given context to ascertain what is missing and what is useful in the context. Inspired by this, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for clarification question generation where we first identify what is missing by taking a difference between the global and the local view and then train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to identify what is useful and generate a question about it. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms several <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> as judged by both <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a> and <a href=https://en.wikipedia.org/wiki/Human>humans</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.346.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--346 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.346 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.346/>I’m Not Mad : Commonsense Implications of Negation and Contradiction<span class=acl-fixed-case>I</span>’m Not Mad”: Commonsense Implications of Negation and Contradiction</a></strong><br><a href=/people/l/liwei-jiang/>Liwei Jiang</a>
|
<a href=/people/a/antoine-bosselut/>Antoine Bosselut</a>
|
<a href=/people/c/chandra-bhagavatula/>Chandra Bhagavatula</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--346><div class="card-body p-3 small">Natural language inference requires reasoning about <a href=https://en.wikipedia.org/wiki/Contradiction>contradictions</a>, <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negations</a>, and their commonsense implications. Given a simple premise (e.g., I&#8217;m mad at you), humans can reason about the varying shades of contradictory statements ranging from straightforward negations (I&#8217;m not mad at you) to commonsense contradictions (I&#8217;m happy). Moreover, these negated or contradictory statements shift the commonsense implications of the original premise in interesting and nontrivial ways. For example, while I&#8217;m mad implies I&#8217;m unhappy about something, negating the premise does not necessarily negate the corresponding commonsense implications. In this paper, we present the first comprehensive study focusing on commonsense implications of <a href=https://en.wikipedia.org/wiki/Negation>negated statements</a> and <a href=https://en.wikipedia.org/wiki/Contradiction>contradictions</a>. We introduce ANION, a new commonsense knowledge graph with 624 K if-then rules focusing on negated and contradictory events. We then present joint generative and discriminative inference models for this new resource, providing novel empirical insights on how logical negations and commonsense contradictions reshape the commonsense implications of their original premises.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--347 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.347/>Identifying Medical Self-Disclosure in Online Communities</a></strong><br><a href=/people/m/mina-valizadeh/>Mina Valizadeh</a>
|
<a href=/people/p/pardis-ranjbar-noiey/>Pardis Ranjbar-Noiey</a>
|
<a href=/people/c/cornelia-caragea/>Cornelia Caragea</a>
|
<a href=/people/n/natalie-parde/>Natalie Parde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--347><div class="card-body p-3 small">Self-disclosure in online health conversations may offer a host of benefits, including earlier detection and treatment of medical issues that may have otherwise gone unaddressed. However, research analyzing medical self-disclosure in <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a> is limited. We address this shortcoming by introducing a new dataset of health-related posts collected from online social platforms, categorized into three groups (No Self-Disclosure, Possible Self-Disclosure, and Clear Self-Disclosure) with high inter-annotator agreement (_ k_=0.88). We make this <a href=https://en.wikipedia.org/wiki/Data>data</a> available to the research community. We also release a <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a> trained on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> that achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 81.02 %, establishing a strong performance benchmark for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.348.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--348 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.348 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.348/>Language in a (Search) Box : Grounding <a href=https://en.wikipedia.org/wiki/Language_acquisition>Language Learning</a> in Real-World Human-Machine Interaction</a></strong><br><a href=/people/f/federico-bianchi/>Federico Bianchi</a>
|
<a href=/people/c/ciro-greco/>Ciro Greco</a>
|
<a href=/people/j/jacopo-tagliabue/>Jacopo Tagliabue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--348><div class="card-body p-3 small">We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engines</a> ; in particular, we explore the emergence of semantic generalization from unsupervised dense representations outside of synthetic environments. A grounding domain, a denotation function and a <a href=https://en.wikipedia.org/wiki/Composition_function>composition function</a> are learned from user data only. We show how the resulting <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> for <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a> exhibits compositional properties while being fully learnable without any explicit <a href=https://en.wikipedia.org/wiki/Labelling>labelling</a>. We benchmark our grounded semantics on compositionality and zero-shot inference tasks, and we show that it provides better results and better generalizations than SOTA non-grounded models, such as <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> and BERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--349 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.349" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.349/>Finding Concept-specific Biases in FormMeaning Associations</a></strong><br><a href=/people/t/tiago-pimentel/>Tiago Pimentel</a>
|
<a href=/people/b/brian-roark/>Brian Roark</a>
|
<a href=/people/s/soren-wichmann/>Søren Wichmann</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a>
|
<a href=/people/d/damian-blasi/>Damián Blasi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--349><div class="card-body p-3 small">This work presents an information-theoretic operationalisation of cross-linguistic non-arbitrariness. It is not a new idea that there are small, cross-linguistic associations between the forms and meanings of words. For instance, it has been claimed (Blasi et al., 2016) that the word for tongue is more likely than chance to contain the phone [ l ]. By controlling for the influence of language family and geographic proximity within a very large concept-aligned, cross-lingual lexicon, we extend methods previously used to detect within language non-arbitrariness (Pimentel et al., 2019) to measure cross-linguistic associations. We find that there is a significant effect of non-arbitrariness, but it is unsurprisingly small (less than 0.5 % on average according to our information-theoretic estimate). We also provide a concept-level analysis which shows that a quarter of the <a href=https://en.wikipedia.org/wiki/Concept>concepts</a> considered in our work exhibit a significant level of cross-linguistic non-arbitrariness. In sum, the paper provides new methods to detect cross-linguistic associations at scale, and confirms their effects are minor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--352 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.352/>Linguistic Complexity Loss in Text-Based Therapy</a></strong><br><a href=/people/j/jason-wei/>Jason Wei</a>
|
<a href=/people/k/kelly-finn/>Kelly Finn</a>
|
<a href=/people/e/emma-templeton/>Emma Templeton</a>
|
<a href=/people/t/thalia-wheatley/>Thalia Wheatley</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--352><div class="card-body p-3 small">The complexity loss paradox, which posits that individuals suffering from disease exhibit surprisingly predictable behavioral dynamics, has been observed in a variety of both human and animal physiological systems. The recent advent of online text-based therapy presents a new opportunity to analyze the complexity loss paradox in a novel <a href=https://en.wikipedia.org/wiki/Operationalization>operationalization</a> : linguistic complexity loss in text-based therapy conversations. In this paper, we analyze linguistic complexity correlates of <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a> in the online therapy messages sent between therapists and 7,170 clients who provided 30,437 corresponding survey responses on their anxiety. We found that when clients reported more anxiety, they showed reduced <a href=https://en.wikipedia.org/wiki/Lexical_diversity>lexical diversity</a> as estimated by the moving average type-token ratio. Therapists, on the other hand, used language of higher reading difficulty, <a href=https://en.wikipedia.org/wiki/Syntax>syntactic complexity</a>, and age of acquisition when clients were more anxious. Finally, we found that clients, and to an even greater extent, <a href=https://en.wikipedia.org/wiki/Therapy>therapists</a>, exhibited consistent levels of many linguistic complexity measures. These results demonstrate how linguistic analysis of text-based communication can be leveraged as a marker for <a href=https://en.wikipedia.org/wiki/Anxiety>anxiety</a>, an exciting prospect in a time of both increased online communication and increased mental health issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.353.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--353 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.353 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.353" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.353/>Ab Antiquo : Neural Proto-language Reconstruction</a></strong><br><a href=/people/c/carlo-meloni/>Carlo Meloni</a>
|
<a href=/people/s/shauli-ravfogel/>Shauli Ravfogel</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--353><div class="card-body p-3 small">Historical linguists have identified regularities in the process of historic sound change. The comparative method utilizes those regularities to reconstruct proto-words based on observed forms in <a href=https://en.wikipedia.org/wiki/Daughter_language>daughter languages</a>. Can this <a href=https://en.wikipedia.org/wiki/Process_(engineering)>process</a> be efficiently automated? We address the task of proto-word reconstruction, in which the model is exposed to cognates in contemporary daughter languages, and has to predict the proto word in the ancestor language. We provide a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, encompassing over 8,000 comparative entries, and show that neural sequence models outperform conventional methods applied to this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> so far. Error analysis reveals a variability in the ability of neural model to capture different <a href=https://en.wikipedia.org/wiki/Phonological_change>phonological changes</a>, correlating with the <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of the changes. Analysis of learned embeddings reveals the models learn phonologically meaningful generalizations, corresponding to well-attested phonological shifts documented by <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.361.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--361 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.361 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.361/>Adapting <a href=https://en.wikipedia.org/wiki/Coreference_resolution>Coreference Resolution</a> for Processing Violent Death Narratives</a></strong><br><a href=/people/a/ankith-uppunda/>Ankith Uppunda</a>
|
<a href=/people/s/susan-cochran/>Susan Cochran</a>
|
<a href=/people/j/jacob-foster/>Jacob Foster</a>
|
<a href=/people/a/alina-arseniev-koehler/>Alina Arseniev-Koehler</a>
|
<a href=/people/v/vickie-mays/>Vickie Mays</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--361><div class="card-body p-3 small">Coreference resolution is an important compo-nent in analyzing <a href=https://en.wikipedia.org/wiki/Narrative>narrative text</a> from admin-istrative data (e.g., clinical or police sources).However, existing coreference models trainedon general language corpora suffer from poortransferability due to domain gaps, especiallywhen they are applied to gender-inclusive datawith lesbian, gay, bisexual, and transgender(LGBT) individuals. In this paper, we an-alyzed the challenges of coreference resolu-tion in an exemplary form of administrativetext written in English : violent death nar-ratives from the USA&#8217;s Centers for DiseaseControl&#8217;s (CDC) National Violent Death Re-porting System. We developed a set of dataaugmentation rules to improve model perfor-mance using a probabilistic data programmingframework. Experiments on narratives froman administrative database, as well as existinggender-inclusive coreference datasets, demon-strate the effectiveness of data augmentationin training coreference models that can betterhandle text data about LGBT individuals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.367.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--367 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.367 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.367/>Does Structure Matter? Encoding Documents for Machine Reading Comprehension</a></strong><br><a href=/people/h/hui-wan/>Hui Wan</a>
|
<a href=/people/s/song-feng/>Song Feng</a>
|
<a href=/people/c/chulaka-gunasekara/>Chulaka Gunasekara</a>
|
<a href=/people/s/siva-sankalp-patel/>Siva Sankalp Patel</a>
|
<a href=/people/s/sachindra-joshi/>Sachindra Joshi</a>
|
<a href=/people/l/luis-lastras/>Luis Lastras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--367><div class="card-body p-3 small">Machine reading comprehension is a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> especially for querying documents with deep and interconnected contexts. Transformer-based methods have shown advanced performances on this task ; however, most of them still treat documents as a flat sequence of tokens. This work proposes a new Transformer-based method that reads a document as <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree slices</a>. It contains two modules for identifying more relevant text passage and the best answer span respectively, which are not only jointly trained but also jointly consulted at inference time. Our evaluation results show that our proposed method outperforms several competitive baseline approaches on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from varied domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.373.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--373 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.373 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.373/>Constructing Taxonomies from Pretrained Language Models</a></strong><br><a href=/people/c/catherine-chen/>Catherine Chen</a>
|
<a href=/people/k/kevin-lin/>Kevin Lin</a>
|
<a href=/people/d/dan-klein/>Dan Klein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--373><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for constructing <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomic trees</a> (e.g., WordNet) using pretrained language models. Our approach is composed of two <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>, one that predicts parenthood relations and another that reconciles those pairwise predictions into <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a>. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph optimization problem</a> and outputs the maximum spanning tree of this <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. We train our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on subtrees sampled from <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>, and test on nonoverlapping WordNet subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of <a href=https://en.wikipedia.org/wiki/WordNet>English WordNet</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 66.7 ancestor F1, a 20.0 % relative increase over the previous best published result on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In addition, we convert the original English dataset into nine other languages using Open Multilingual WordNet and extend our results across these languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.378.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--378 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.378 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.378" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.378/>Adapting <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks<span class=acl-fixed-case>BERT</span> for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks</a></strong><br><a href=/people/z/zixuan-ke/>Zixuan Ke</a>
|
<a href=/people/h/hu-xu/>Hu Xu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--378><div class="card-body p-3 small">This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues : (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks so that they are not forgotten. This paper proposes a novel capsule network based model called B-CL to address these issues. B-CL markedly improves the ASC performance on both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of B-CL is demonstrated through extensive experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.381.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--381 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.381 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.381" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.381/>Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization</a></strong><br><a href=/people/y/yichen-jiang/>Yichen Jiang</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/p/paul-smolensky/>Paul Smolensky</a>
|
<a href=/people/p/paul-soulos/>Paul Soulos</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/h/hamid-palangi/>Hamid Palangi</a>
|
<a href=/people/r/roland-fernandez/>Roland Fernandez</a>
|
<a href=/people/c/caitlin-smith/>Caitlin Smith</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--381><div class="card-body p-3 small">Abstractive summarization, the task of generating a concise summary of input documents, requires : (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-Transformer (Schlag et al., 2019), an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-Transformer outperforms the Transformer and the original TP-Transformer significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and the performance gain by information specificity of the role vectors and improved syntactic interpretability in the TPR layer outputs. (Code and models are available at https://github.com/jiangycTarheel/TPT-Summ)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.383.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--383 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.383 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.383" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.383/>Understanding Factuality in Abstractive Summarization with FRANK : A Benchmark for Factuality Metrics<span class=acl-fixed-case>FRANK</span>: A Benchmark for Factuality Metrics</a></strong><br><a href=/people/a/artidoro-pagnoni/>Artidoro Pagnoni</a>
|
<a href=/people/v/vidhisha-balachandran/>Vidhisha Balachandran</a>
|
<a href=/people/y/yulia-tsvetkov/>Yulia Tsvetkov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--383><div class="card-body p-3 small">Modern <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization models</a> generate highly fluent but often factually unreliable outputs. This motivated a surge of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> attempting to measure the <a href=https://en.wikipedia.org/wiki/Fact>factuality</a> of automatically generated summaries. Due to the lack of common benchmarks, these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> can not be compared. Moreover, all these methods treat <a href=https://en.wikipedia.org/wiki/Factuality>factuality</a> as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN / DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--384 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.384" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.384/>GSum : A General Framework for Guided Neural Abstractive Summarization<span class=acl-fixed-case>GS</span>um: A General Framework for Guided Neural Abstractive Summarization</a></strong><br><a href=/people/z/zi-yi-dou/>Zi-Yi Dou</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a>
|
<a href=/people/h/hiroaki-hayashi/>Hiroaki Hayashi</a>
|
<a href=/people/z/zhengbao-jiang/>Zhengbao Jiang</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--384><div class="card-body p-3 small">Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of <a href=https://en.wikipedia.org/wiki/Guidance>guidance</a> to control the output and increase <a href=https://en.wikipedia.org/wiki/Faithfulness>faithfulness</a>, it is not clear how these <a href=https://en.wikipedia.org/wiki/Strategy>strategies</a> compare and contrast to each other. In this paper, we propose a general and extensible guided summarization framework (GSum) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of <a href=https://en.wikipedia.org/wiki/Guidance>guidance</a> generate qualitatively different summaries, lending a degree of <a href=https://en.wikipedia.org/wiki/Controllability>controllability</a> to the learned models.<b>GSum</b>) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.386.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--386 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.386 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.386/>TuringAdvice : A Generative and Dynamic Evaluation of Language Use<span class=acl-fixed-case>T</span>uring<span class=acl-fixed-case>A</span>dvice: A Generative and Dynamic Evaluation of Language Use</a></strong><br><a href=/people/r/rowan-zellers/>Rowan Zellers</a>
|
<a href=/people/a/ari-holtzman/>Ari Holtzman</a>
|
<a href=/people/e/elizabeth-clark/>Elizabeth Clark</a>
|
<a href=/people/l/lianhui-qin/>Lianhui Qin</a>
|
<a href=/people/a/ali-farhadi/>Ali Farhadi</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--386><div class="card-body p-3 small">We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding : our ability to use <a href=https://en.wikipedia.org/wiki/Language>language</a> to resolve open-ended situations by communicating with each other. Empirical results show that today&#8217;s <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best <a href=https://en.wikipedia.org/wiki/Scientific_modelling>model</a>, T5, writes advice that is at least as helpful as human-written advice in only 14 % of cases ; a much larger non-finetunable GPT3 model does even worse at 4 %. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--390 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.390/>Identifying inherent disagreement in natural language inference</a></strong><br><a href=/people/x/xinliang-frederick-zhang/>Xinliang Frederick Zhang</a>
|
<a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--390><div class="card-body p-3 small">Natural language inference (NLI) is the task of determining whether a piece of text is entailed, contradicted by or unrelated to another piece of text. In this paper, we investigate how to tease systematic inferences (i.e., items for which people agree on the NLI label) apart from disagreement items (i.e., items which lead to different annotations), which most prior work has overlooked. To distinguish systematic inferences from disagreement items, we propose Artificial Annotators (AAs) to simulate the uncertainty in the annotation process by capturing the modes in annotations. Results on the CommitmentBank, a corpus of naturally occurring discourses in English, confirm that our approach performs statistically significantly better than all baselines. We further show that <a href=https://en.wikipedia.org/wiki/Adult_learner>AAs</a> learn <a href=https://en.wikipedia.org/wiki/Pattern_recognition>linguistic patterns</a> and context-dependent reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.391.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--391 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.391 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.391" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.391/>Modeling Human Mental States with an Entity-based Narrative Graph</a></strong><br><a href=/people/i/i-ta-lee/>I-Ta Lee</a>
|
<a href=/people/m/maria-leonor-pacheco/>Maria Leonor Pacheco</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--391><div class="card-body p-3 small">Understanding <a href=https://en.wikipedia.org/wiki/Narrative>narrative text</a> requires capturing characters&#8217; motivations, goals, and <a href=https://en.wikipedia.org/wiki/Mental_state>mental states</a>. This paper proposes an Entity-based Narrative Graph (ENG) to model the internal- states of characters in a story. We explicitly model entities, their interactions and the context in which they appear, and learn rich representations for them. We experiment with different task-adaptive pre-training objectives, in-domain training, and symbolic inference to capture dependencies between different decisions in the output space. We evaluate our model on two narrative understanding tasks : predicting character mental states, and desire fulfillment, and conduct a <a href=https://en.wikipedia.org/wiki/Qualitative_property>qualitative analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.393.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--393 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.393 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.393.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.393" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.393/>Hurdles to Progress in Long-form Question Answering</a></strong><br><a href=/people/k/kalpesh-krishna/>Kalpesh Krishna</a>
|
<a href=/people/a/aurko-roy/>Aurko Roy</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--393><div class="card-body p-3 small">The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends : (1) our system&#8217;s generated answers are not actually grounded in the documents that it retrieves ; (2) ELI5 contains significant train / validation overlap, as at least 81 % of ELI5 validation questions occur in paraphrased form in the training set ; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed ; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.397.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--397 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.397 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.397/>Attention Head Masking for Inference Time Content Selection in Abstractive Summarization</a></strong><br><a href=/people/s/shuyang-cao/>Shuyang Cao</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--397><div class="card-body p-3 small">How can we effectively inform content selection in Transformer-based abstractive summarization models? In this work, we present a simple-yet-effective attention head masking technique, which is applied on encoder-decoder attentions to pinpoint salient content at inference time. Using attention head masking, we are able to reveal the relation between encoder-decoder attentions and content selection behaviors of summarization models. We then demonstrate its effectiveness on three document summarization datasets based on both in-domain and cross-domain settings. Importantly, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> outperform prior state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on CNN / Daily Mail and New York Times datasets. Moreover, our inference-time masking technique is also data-efficient, requiring only 20 % of the training samples to outperform BART fine-tuned on the full CNN / DailyMail dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.398" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.398/>Factual Probing Is [ MASK ] : <a href=https://en.wikipedia.org/wiki/Learning>Learning</a> vs. Learning to Recall<span class=acl-fixed-case>MASK</span>]: Learning vs. Learning to Recall</a></strong><br><a href=/people/z/zexuan-zhong/>Zexuan Zhong</a>
|
<a href=/people/d/dan-friedman/>Dan Friedman</a>
|
<a href=/people/d/danqi-chen/>Danqi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--398><div class="card-body p-3 small">Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pre-trained language model by expressing them as cloze-style prompts and interpret the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OptiPrompt, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4 % of facts in the LAMA benchmark. Second, we raise a more important question : Can we really interpret these probing results as a <a href=https://en.wikipedia.org/wiki/Upper_and_lower_bounds>lower bound</a>? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle <a href=https://en.wikipedia.org/wiki/Learning>learning</a> from learning to recall, providing a more detailed picture of what different prompts can reveal about pre-trained language models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--400 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.400 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.400" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.400/>Contextualized Perturbation for Textual Adversarial Attack</a></strong><br><a href=/people/d/dianqi-li/>Dianqi Li</a>
|
<a href=/people/y/yizhe-zhang/>Yizhe Zhang</a>
|
<a href=/people/h/hao-peng/>Hao Peng</a>
|
<a href=/people/l/liqun-chen/>Liqun Chen</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/m/ming-ting-sun/>Ming-Ting Sun</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--400><div class="card-body p-3 small">Adversarial examples expose the vulnerabilities of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) models</a>, and can be used to evaluate and improve their <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a>. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> and <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--402 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.402" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.402/>Evaluating the Values of Sources in <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a></a></strong><br><a href=/people/m/md-rizwan-parvez/>Md Rizwan Parvez</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--402><div class="card-body p-3 small">Transfer learning that adapts a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on data-rich sources to low-resource targets has been widely applied in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. However, when training a transfer model over multiple sources, not every source is equally useful for the target. To better transfer a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>, it is essential to understand the values of the sources. In this paper, we develop, an efficient source valuation framework for quantifying the usefulness of the sources (e.g.,) in <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> based on the Shapley value method. Experiments and comprehensive analyses on both cross-domain and cross-lingual transfers demonstrate that our framework is not only effective in choosing useful transfer sources but also the source values match the intuitive source-target similarity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--405 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.405/>Limitations of <a href=https://en.wikipedia.org/wiki/Autoregressive_model>Autoregressive Models</a> and Their Alternatives</a></strong><br><a href=/people/c/chu-cheng-lin/>Chu-Cheng Lin</a>
|
<a href=/people/a/aaron-jaech/>Aaron Jaech</a>
|
<a href=/people/x/xin-li/>Xin Li</a>
|
<a href=/people/m/matthew-r-gormley/>Matthew R. Gormley</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--405><div class="card-body p-3 small">Standard autoregressive language models perform only <a href=https://en.wikipedia.org/wiki/Time_complexity>polynomial-time computation</a> to compute the probability of the next symbol. While this is attractive, it means they can not model <a href=https://en.wikipedia.org/wiki/Probability_distribution>distributions</a> whose next-symbol probability is hard to compute. Indeed, they can not even model them well enough to solve associated easy decision problems for which an engineer might want to consult a <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. These limitations apply no matter how much computation and data are used to train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, unless the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is given access to <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle parameters</a> that grow superpolynomially in sequence length. Thus, simply training larger autoregressive language models is not a panacea for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations.<i>hard</i> to compute. Indeed, they cannot even model them well enough to solve associated <i>easy</i> decision problems for which an engineer might want to consult a language model. These limitations apply no matter how much computation and data are used to train the model, unless the model is given access to oracle parameters that grow <i>superpolynomially</i> in sequence length. Thus, simply training larger autoregressive language models is not a panacea for NLP. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--406 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.406/>On the Transformer Growth for Progressive BERT Training<span class=acl-fixed-case>BERT</span> Training</a></strong><br><a href=/people/x/xiaotao-gu/>Xiaotao Gu</a>
|
<a href=/people/l/liyuan-liu/>Liyuan Liu</a>
|
<a href=/people/h/hongkun-yu/>Hongkun Yu</a>
|
<a href=/people/j/jing-li/>Jing Li</a>
|
<a href=/people/c/chen-chen/>Chen Chen</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--406><div class="card-body p-3 small">As the excessive pre-training cost arouses the need to improve <a href=https://en.wikipedia.org/wiki/Efficiency>efficiency</a>, considerable efforts have been made to train BERT progressivelystart from an inferior but low-cost model and gradually increase the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a>. Our objective is to help advance the understanding of such Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture selection, Transformer growth also favors <a href=https://en.wikipedia.org/wiki/Scaling_(geometry)>compound scaling</a>. Specifically, while existing methods only conduct network growth in a single dimension, we observe that it is beneficial to use compound growth operators and balance multiple dimensions (e.g., depth, width, and input length of the model). Moreover, we explore alternative growth operators in each dimension via controlled comparison to give practical guidance for operator selection. In light of our analyses, the proposed method CompoundGrow speeds up <a href=https://en.wikipedia.org/wiki/Brain-derived_neurotrophic_factor>BERT pre-training</a> by 73.6 % and 82.2 % for the base and large models respectively while achieving comparable performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--408 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.408/>ReadTwice : Reading Very Large Documents with Memories<span class=acl-fixed-case>R</span>ead<span class=acl-fixed-case>T</span>wice: Reading Very Large Documents with Memories</a></strong><br><a href=/people/y/yury-zemlyanskiy/>Yury Zemlyanskiy</a>
|
<a href=/people/j/joshua-ainslie/>Joshua Ainslie</a>
|
<a href=/people/m/michiel-de-jong/>Michiel de Jong</a>
|
<a href=/people/p/philip-pham/>Philip Pham</a>
|
<a href=/people/i/ilya-eckstein/>Ilya Eckstein</a>
|
<a href=/people/f/fei-sha/>Fei Sha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--408><div class="card-body p-3 small">Knowledge-intensive tasks such as <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> often require assimilating information from different sections of large inputs such as <a href=https://en.wikipedia.org/wiki/Book>books</a> or article collections. We propose ReadTwice, a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task, with questions about entire books.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--410 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Short Paper"><i class="fas fa-award"></i></span><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.410.OptionalSupplementaryCode.zip data-toggle=tooltip data-placement=top title="Optional supplementary code">
<i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.410.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.410" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.410/>Learning How to Ask : Querying LMs with Mixtures of Soft Prompts<span class=acl-fixed-case>LM</span>s with Mixtures of Soft Prompts</a></strong><br><a href=/people/g/guanghui-qin/>Guanghui Qin</a>
|
<a href=/people/j/jason-eisner/>Jason Eisner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--410><div class="card-body p-3 small">Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-the-blank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> retain factual knowledge from their training corpora that can be extracted by asking them to fill in the blank in a sentential prompt. However, where does this prompt come from? We explore the idea of learning <a href=https://en.wikipedia.org/wiki/Command-line_interface>prompts</a> by gradient descenteither fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of soft words, i.e., <a href=https://en.wikipedia.org/wiki/Continuous_or_discrete_variable>continuous vectors</a> that are not necessarily word type embeddings from the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. Furthermore, for each <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we optimize a mixture of <a href=https://en.wikipedia.org/wiki/Command_(computing)>prompts</a>, learning which <a href=https://en.wikipedia.org/wiki/Command_(computing)>prompts</a> are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> was previously underestimated. Moreover, this knowledge is cheap to elicit : random initialization is nearly as good as informed initialization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--413 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.413.OptionalSupplementaryCode.zip data-toggle=tooltip data-placement=top title="Optional supplementary code"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.naacl-main.413/>SpanPredict : Extraction of Predictive Document Spans with Neural Attention<span class=acl-fixed-case>S</span>pan<span class=acl-fixed-case>P</span>redict: Extraction of Predictive Document Spans with Neural Attention</a></strong><br><a href=/people/v/vivek-subramanian/>Vivek Subramanian</a>
|
<a href=/people/m/matthew-engelhard/>Matthew Engelhard</a>
|
<a href=/people/s/sam-berchuck/>Sam Berchuck</a>
|
<a href=/people/l/liqun-chen/>Liqun Chen</a>
|
<a href=/people/r/ricardo-henao/>Ricardo Henao</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--413><div class="card-body p-3 small">In many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing applications</a>, identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses, for example, identifying predictive content in <a href=https://en.wikipedia.org/wiki/Medical_record>clinical notes</a> not only enhances interpretability, but also allows unknown, descriptive (i.e., text-based) risk factors to be identified. We here formalize this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> as predictive extraction and address it using a simple <a href=https://en.wikipedia.org/wiki/Mechanism_design>mechanism</a> based on <a href=https://en.wikipedia.org/wiki/Attentional_control>linear attention</a>. Our method preserves differentiability, allowing scalable inference via <a href=https://en.wikipedia.org/wiki/Stochastic_gradient_descent>stochastic gradient descent</a>. Further, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> decomposes predictions into a sum of contributions of distinct text spans. Importantly, we require only <a href=https://en.wikipedia.org/wiki/Document_classification>document labels</a>, not ground-truth spans. Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> identifies semantically-cohesive spans and assigns them scores that agree with human ratings, while preserving <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--416 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.416" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.416/>Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation</a></strong><br><a href=/people/y/yasuhide-miura/>Yasuhide Miura</a>
|
<a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/e/emily-tsai/>Emily Tsai</a>
|
<a href=/people/c/curtis-langlotz/>Curtis Langlotz</a>
|
<a href=/people/d/dan-jurafsky/>Dan Jurafsky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--416><div class="card-body p-3 small">Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible <a href=https://en.wikipedia.org/wiki/Medical_error>medical errors</a>. However, existing report generation systems, despite achieving high performances on natural language generation metrics such as CIDEr or <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, still suffer from incomplete and inconsistent generations. Here we introduce two new simple rewards to encourage the generation of factually complete and consistent radiology reports : one that encourages the system to generate radiology domain entities consistent with the reference, and one that uses natural language inference to encourage these entities to be described in inferentially consistent ways. We combine these with the novel use of an existing semantic equivalence metric (BERTScore). We further propose a report generation system that optimizes these <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>rewards</a> via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. On two open radiology report datasets, our <a href=https://en.wikipedia.org/wiki/System>system</a> substantially improved the F1 score of a clinical information extraction performance by +22.1 (Delta +63.9 %). We further show via a <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluation</a> and a <a href=https://en.wikipedia.org/wiki/Qualitative_property>qualitative analysis</a> that our <a href=https://en.wikipedia.org/wiki/System>system</a> leads to generations that are more factually complete and consistent compared to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--418 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.418/>MIMOQA : Multimodal Input Multimodal Output Question Answering<span class=acl-fixed-case>MIMOQA</span>: Multimodal Input Multimodal Output Question Answering</a></strong><br><a href=/people/h/hrituraj-singh/>Hrituraj Singh</a>
|
<a href=/people/a/anshul-nasery/>Anshul Nasery</a>
|
<a href=/people/d/denil-mehta/>Denil Mehta</a>
|
<a href=/people/a/aishwarya-agarwal/>Aishwarya Agarwal</a>
|
<a href=/people/j/jatin-lamba/>Jatin Lamba</a>
|
<a href=/people/b/balaji-vasan-srinivasan/>Balaji Vasan Srinivasan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--418><div class="card-body p-3 small">Multimodal research has picked up significantly in the space of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> with the task being extended to visual question answering, charts question answering as well as multimodal input question answering. However, all these explorations produce a unimodal textual output as the answer. In this paper, we propose a novel task-MIMOQA-Multimodal Input Multimodal Output Question Answering in which the output is also multimodal. Through human experiments, we empirically show that such multimodal outputs provide better cognitive understanding of the answers. We also propose a novel multimodal question-answering framework, MExBERT, that incorporates a joint textual and visual attention towards producing such a multimodal output. Our method relies on a novel multimodal dataset curated for this problem from publicly available unimodal datasets. We show the superior performance of MExBERT against strong baselines on both the automatic as well as human metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-main.420.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.naacl-main.420/>Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions</a></strong><br><a href=/people/l/liunian-harold-li/>Liunian Harold Li</a>
|
<a href=/people/h/haoxuan-you/>Haoxuan You</a>
|
<a href=/people/z/zhecan-wang/>Zhecan Wang</a>
|
<a href=/people/a/alireza-zareian/>Alireza Zareian</a>
|
<a href=/people/s/shih-fu-chang/>Shih-Fu Chang</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--420><div class="card-body p-3 small">Pre-trained contextual vision-and-language (V&L) models have achieved impressive performance on various <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmarks</a>. However, existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> require a large amount of parallel image-caption data for <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training</a>. Such <a href=https://en.wikipedia.org/wiki/Data>data</a> are costly to collect and require cumbersome curation. Inspired by unsupervised machine translation, we investigate if a strong V&L representation model can be learned through unsupervised pre-training without image-caption corpora. In particular, we propose to conduct mask-and-predict pre-training on text-only and image-only corpora and introduce the object tags detected by an object recognition model as anchor points to bridge two modalities. We find that such a simple approach achieves performance close to a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> pre-trained with <a href=https://en.wikipedia.org/wiki/Data_alignment>aligned data</a>, on four English V&L benchmarks. Our work challenges the widely held notion that aligned data is necessary for V&L pre-training, while significantly reducing the amount of supervision needed for V&L models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--421 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.421/>Multitasking Inhibits Semantic Drift</a></strong><br><a href=/people/a/athul-paul-jacob/>Athul Paul Jacob</a>
|
<a href=/people/m/mike-lewis/>Mike Lewis</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--421><div class="card-body p-3 small">When <a href=https://en.wikipedia.org/wiki/Intelligent_agent>intelligent agents</a> communicate to accomplish shared goals, how do these goals shape the agents&#8217; language? We study the dynamics of learning in latent language policies (LLPs), in which instructor agents generate natural-language subgoal descriptions and executor agents map these descriptions to low-level actions. LLPs can solve challenging long-horizon reinforcement learning problems and provide a rich model for studying task-oriented language use. But previous work has found that LLP training is prone to <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a> (use of messages in ways inconsistent with their original natural language meanings). Here, we demonstrate theoretically and empirically that multitask training is an effective counter to this problem : we prove that multitask training eliminates <a href=https://en.wikipedia.org/wiki/Semantic_drift>semantic drift</a> in a well-studied family of signaling games, and show that multitask training of neural LLPs in a complex strategy game reduces drift and while improving sample efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.429.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--429 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.429 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.429" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.429/>Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction</a></strong><br><a href=/people/z/zhenghao-liu/>Zhenghao Liu</a>
|
<a href=/people/x/xiaoyuan-yi/>Xiaoyuan Yi</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/l/liner-yang/>Liner Yang</a>
|
<a href=/people/t/tat-seng-chua/>Tat-Seng Chua</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--429><div class="card-body p-3 small">Grammatical Error Correction (GEC) aims to correct writing errors and help <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learners</a> improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/thunlp/VERNet.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.434.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--434 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.434 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.434" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.434/>Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning</a></strong><br><a href=/people/j/jason-wei/>Jason Wei</a>
|
<a href=/people/c/chengyu-huang/>Chengyu Huang</a>
|
<a href=/people/s/soroush-vosoughi/>Soroush Vosoughi</a>
|
<a href=/people/y/yu-cheng/>Yu Cheng</a>
|
<a href=/people/s/shiqi-xu/>Shiqi Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--434><div class="card-body p-3 small">Few-shot text classification is a fundamental NLP task in which a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentationa technique particularly suitable for training with limited datafor this few-shot, highly-multiclass text classification setting. On four diverse text classification tasks, we find that common <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation techniques</a> can improve the performance of triplet networks by up to 3.0 % on average. To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a two-stage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation trains faster, improves performance, and remains robust to high amounts of noising from augmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.438.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--438 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.438 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.438" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.438/>User-Generated Text Corpus for Evaluating Japanese Morphological Analysis and Lexical Normalization<span class=acl-fixed-case>J</span>apanese Morphological Analysis and Lexical Normalization</a></strong><br><a href=/people/s/shohei-higashiyama/>Shohei Higashiyama</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--438><div class="card-body p-3 small">Morphological analysis (MA) and lexical normalization (LN) are both important tasks for Japanese user-generated text (UGT). To evaluate and compare different MA / LN systems, we have constructed a publicly available Japanese UGT corpus. Our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> comprises 929 sentences annotated with morphological and normalization information, along with category information we classified for frequent UGT-specific phenomena. Experiments on the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> demonstrated the low performance of existing MA / LN methods for non-general words and non-standard forms, indicating that the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> would be a challenging benchmark for further research on UGT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.442.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--442 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.442 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.442/>Contextualized and Generalized Sentence Representations by Contrastive Self-Supervised Learning : A Case Study on Discourse Relation Analysis</a></strong><br><a href=/people/h/hirokazu-kiyomaru/>Hirokazu Kiyomaru</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--442><div class="card-body p-3 small">We propose a method to learn contextualized and generalized sentence representations using contrastive self-supervised learning. In the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is given a text consisting of multiple sentences. One sentence is randomly selected as a target sentence. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained to maximize the similarity between the representation of the target sentence with its context and that of the masked target sentence with the same context. Simultaneously, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> minimizes the similarity between the latter <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> and the representation of a random sentence with the same context. We apply our method to discourse relation analysis in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and show that it outperforms strong baseline methods based on BERT, XLNet, and RoBERTa.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--445 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.445/>Unsupervised Concept Representation Learning for Length-Varying Text Similarity</a></strong><br><a href=/people/x/xuchao-zhang/>Xuchao Zhang</a>
|
<a href=/people/b/bo-zong/>Bo Zong</a>
|
<a href=/people/w/wei-cheng/>Wei Cheng</a>
|
<a href=/people/j/jingchao-ni/>Jingchao Ni</a>
|
<a href=/people/y/yanchi-liu/>Yanchi Liu</a>
|
<a href=/people/h/haifeng-chen/>Haifeng Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--445><div class="card-body p-3 small">Measuring document similarity plays an important role in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>. Most existing document similarity approaches suffer from the <a href=https://en.wikipedia.org/wiki/Information_gap>information gap</a> caused by <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context and vocabulary mismatches</a> when comparing varying-length texts. In this paper, we propose an unsupervised concept representation learning approach to address the above issues. Specifically, we propose a novel Concept Generation Network (CGNet) to learn concept representations from the perspective of the entire <a href=https://en.wikipedia.org/wiki/Text_corpus>text corpus</a>. Moreover, a concept-based document matching method is proposed to leverage advances in the recognition of local phrase features and corpus-level concept features. Extensive experiments on real-world data sets demonstrate that new method can achieve a considerable improvement in comparing length-varying texts. In particular, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved 6.5 % better <a href=https://en.wikipedia.org/wiki/F1_score>F1 Score</a> compared to the best of the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline models</a> for a concept-project benchmark dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.447.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--447 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.447 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.447" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.447/>Adversarial Self-Supervised Learning for Out-of-Domain Detection</a></strong><br><a href=/people/z/zhiyuan-zeng/>Zhiyuan Zeng</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/h/hong-xu/>Hong Xu</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--447><div class="card-body p-3 small">Detecting out-of-domain (OOD) intents is crucial for the deployed task-oriented dialogue system. Previous unsupervised OOD detection methods only extract discriminative features of different in-domain intents while supervised counterparts can directly distinguish OOD and in-domain intents but require extensive labeled OOD data. To combine the benefits of both types, we propose a self-supervised contrastive learning framework to model discriminative semantic features of both in-domain intents and OOD intents from unlabeled data. Besides, we introduce an adversarial augmentation neural module to improve the efficiency and <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of contrastive learning. Experiments on two public benchmark datasets show that our method can consistently outperform the baselines with a statistically significant margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.449.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--449 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.449 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.449" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.449/>Hierarchical Transformer for Task Oriented Dialog Systems</a></strong><br><a href=/people/b/bishal-santra/>Bishal Santra</a>
|
<a href=/people/p/potnuru-anusha/>Potnuru Anusha</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--449><div class="card-body p-3 small">Generative models for dialog systems have gained much interest because of the recent success of RNN and Transformer based models in tasks like <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> and <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>. Although the task of dialog response generation is generally seen as a sequence to sequence (Seq2Seq) problem, researchers in the past have found it challenging to train dialog systems using the standard Seq2Seq models. Therefore, to help the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learn meaningful utterance and conversation level features, Sordoni et al. (2015b), Serban et al. (2016) proposed Hierarchical RNN architecture, which was later adopted by several other RNN based dialog systems. With the transformer-based models dominating the seq2seq problems lately, the natural question to ask is the applicability of the notion of <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchy</a> in transformer-based dialog systems. In this paper, we propose a generalized framework for Hierarchical Transformer Encoders and show how a standard transformer can be morphed into any hierarchical encoder, including HRED and HIBERT like models, by using specially designed attention masks and positional encodings. We demonstrate that Hierarchical Encoding helps achieve better natural language understanding of the contexts in transformer-based models for task-oriented dialog systems through a wide range of experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.451.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--451 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.451 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.451/>RTFE : A Recursive Temporal Fact Embedding Framework for Temporal Knowledge Graph Completion<span class=acl-fixed-case>RTFE</span>: A Recursive Temporal Fact Embedding Framework for Temporal Knowledge Graph Completion</a></strong><br><a href=/people/y/youri-xu/>Youri Xu</a>
|
<a href=/people/h/haihong-e/>Haihong E</a>
|
<a href=/people/m/meina-song/>Meina Song</a>
|
<a href=/people/w/wenyu-song/>Wenyu Song</a>
|
<a href=/people/x/xiaodong-lv/>Xiaodong Lv</a>
|
<a href=/people/w/wang-haotian/>Wang Haotian</a>
|
<a href=/people/y/yang-jinrui/>Yang Jinrui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--451><div class="card-body p-3 small">Static knowledge graph (SKG) embedding (SKGE) has been studied intensively in the past years. Recently, temporal knowledge graph (TKG) embedding (TKGE) has emerged. In this paper, we propose a Recursive Temporal Fact Embedding (RTFE) framework to transplant SKGE models to TKGs and to enhance the performance of existing TKGE models for TKG completion. Different from previous work which ignores the continuity of states of TKG in <a href=https://en.wikipedia.org/wiki/Time_evolution>time evolution</a>, we treat the sequence of <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> as a <a href=https://en.wikipedia.org/wiki/Markov_chain>Markov chain</a>, which transitions from the previous state to the next state. RTFE takes the SKGE to initialize the embeddings of <a href=https://en.wikipedia.org/wiki/TKG>TKG</a>. Then it recursively tracks the state transition of <a href=https://en.wikipedia.org/wiki/TKG>TKG</a> by passing updated parameters / features between timestamps. Specifically, at each timestamp, we approximate the <a href=https://en.wikipedia.org/wiki/State_transition>state transition</a> as the gradient update process. Since RTFE learns each timestamp recursively, it can naturally transit to future timestamps. Experiments on five TKG datasets show the effectiveness of RTFE.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.454.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--454 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.454 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.454" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.454/>Multi-Grained Knowledge Distillation for Named Entity Recognition</a></strong><br><a href=/people/x/xuan-zhou/>Xuan Zhou</a>
|
<a href=/people/x/xiao-zhang/>Xiao Zhang</a>
|
<a href=/people/c/chenyang-tao/>Chenyang Tao</a>
|
<a href=/people/j/junya-chen/>Junya Chen</a>
|
<a href=/people/b/bing-xu/>Bing Xu</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/j/jing-xiao/>Jing Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--454><div class="card-body p-3 small">Although pre-trained big models (e.g., BERT, <a href=https://en.wikipedia.org/wiki/ERNIE>ERNIE</a>, XLNet, GPT3 etc.) have delivered top performance in Seq2seq modeling, their deployments in real-world applications are often hindered by the excessive computations and memory demand involved. For many applications, including named entity recognition (NER), matching the state-of-the-art result under budget has attracted considerable attention. Drawing power from the recent advance in knowledge distillation (KD), this work presents a novel distillation scheme to efficiently transfer the knowledge learned from big models to their more affordable counterpart. Our solution highlights the construction of surrogate labels through the k-best Viterbi algorithm to distill knowledge from the teacher model. To maximally assimilate knowledge into the student model, we propose a multi-grained distillation scheme, which integrates cross entropy involved in conditional random field (CRF) and fuzzy learning. To validate the effectiveness of our proposal, we conducted a comprehensive evaluation on five NER benchmarks, reporting cross-the-board performance gains relative to competing prior-arts. We further discuss ablation results to dissect our gains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.463.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--463 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.463 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.463" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.463/>TR-BERT : Dynamic Token Reduction for Accelerating BERT Inference<span class=acl-fixed-case>TR</span>-<span class=acl-fixed-case>BERT</span>: Dynamic Token Reduction for Accelerating <span class=acl-fixed-case>BERT</span> Inference</a></strong><br><a href=/people/d/deming-ye/>Deming Ye</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/y/yufei-huang/>Yufei Huang</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--463><div class="card-body p-3 small">Existing pre-trained language models (PLMs) are often computationally expensive in <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs&#8217; inference, named TR-BERT, which could flexibly adapt the layer number of each token in <a href=https://en.wikipedia.org/wiki/Inference>inference</a> to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a>. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/TR-BERT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.464.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--464 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.464 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.464/>Breadth First Reasoning Graph for Multi-hop Question Answering</a></strong><br><a href=/people/y/yongjie-huang/>Yongjie Huang</a>
|
<a href=/people/m/meng-yang/>Meng Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--464><div class="card-body p-3 small">Recently Graph Neural Network (GNN) has been used as a promising tool in multi-hop question answering task. However, the unnecessary updations and simple edge constructions prevent an accurate answer span extraction in a more direct and interpretable way. In this paper, we propose a novel model of Breadth First Reasoning Graph (BFR-Graph), which presents a new message passing way that better conforms to the reasoning process. In BFR-Graph, the reasoning message is required to start from the question node and pass to the next sentences node hop by hop until all the edges have been passed, which can effectively prevent each <a href=https://en.wikipedia.org/wiki/Node_(networking)>node</a> from over-smoothing or being updated multiple times unnecessarily. To introduce more <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>, we also define the reasoning graph as a <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>weighted graph</a> with considering the number of co-occurrence entities and the distance between sentences. Then we present a more direct and interpretable way to aggregate scores from different levels of <a href=https://en.wikipedia.org/wiki/Granularity>granularity</a> based on the GNN. On HotpotQA leaderboard, the proposed BFR-Graph achieves state-of-the-art on answer span prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.469.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--469 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.469 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.469" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.469/>Unsupervised Multi-hop Question Answering by Question Generation</a></strong><br><a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--469><div class="card-body p-3 small">Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multi-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised framework</a> that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting / generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61 % and 83 % of the <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data. Our codes are publicly available at https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-main.470.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-main--470 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-main.470 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-main.470" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-main.470/>Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents</a></strong><br><a href=/people/p/peng-cui/>Peng Cui</a>
|
<a href=/people/l/le-hu/>Le Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-main--470><div class="card-body p-3 small">Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with <a href=https://en.wikipedia.org/wiki/Dynamic_memory>dynamic memory</a> for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the history information dynamically, allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> substantially outperforms previous state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Besides, we perform qualitative and quantitative investigations on how our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> works and where the performance gain comes from.</div></div></div><hr><div id=2021naacl-demos><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.naacl-demos/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.0/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations</a></strong><br><a href=/people/a/avirup-sil/>Avi Sil</a>
|
<a href=/people/x/xi-victoria-lin/>Xi Victoria Lin</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-demos.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.1/>PhoNLP : A joint multi-task learning model for Vietnamese part-of-speech tagging, named entity recognition and dependency parsing<span class=acl-fixed-case>P</span>ho<span class=acl-fixed-case>NLP</span>: A joint multi-task learning model for <span class=acl-fixed-case>V</span>ietnamese part-of-speech tagging, named entity recognition and dependency parsing</a></strong><br><a href=/people/l/linh-the-nguyen/>Linh The Nguyen</a>
|
<a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--1><div class="card-body p-3 small">We present the first multi-task learning model named PhoNLP for joint Vietnamese part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT (Nguyen and Nguyen, 2020) for each task independently. We publicly release PhoNLP as an open-source toolkit under the <a href=https://en.wikipedia.org/wiki/Apache_License>Apache License 2.0</a>. Although we specify PhoNLP for <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a>, our PhoNLP training and evaluation command scripts in fact can directly work for other languages that have a pre-trained BERT-based language model and gold annotated corpora available for the three tasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future NLP research and applications to not only <a href=https://en.wikipedia.org/wiki/Vietnamese_language>Vietnamese</a> but also the other languages. Our PhoNLP is available at https://github.com/VinAIResearch/PhoNLP</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.3/>NAMER : A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering<span class=acl-fixed-case>NAMER</span>: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering</a></strong><br><a href=/people/m/minhao-zhang/>Minhao Zhang</a>
|
<a href=/people/r/ruoyu-zhang/>Ruoyu Zhang</a>
|
<a href=/people/l/lei-zou/>Lei Zou</a>
|
<a href=/people/y/yinnian-lin/>Yinnian Lin</a>
|
<a href=/people/s/sen-hu/>Sen Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--3><div class="card-body p-3 small">We present NAMER, an open-domain Chinese knowledge base question answering system based on a novel node-based framework that better grasps the structural mapping between questions and KB queries by aligning the nodes in a query with their corresponding mentions in question. Equipped with techniques including <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> and <a href=https://en.wikipedia.org/wiki/Computer_multitasking>multitasking</a>, we show that the proposed framework outperforms the previous SoTA on CCKS CKBQA dataset. Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset (https://github.com/ridiculouz/CKBQA) with such strategy is also published to promote further research. An online demo of <a href=https://en.wikipedia.org/wiki/Namer>NAMER</a> (http://kbqademo.gstore.cn) is provided to visualize our framework and supply extra information for users, a video illustration (https://youtu.be/yetnVye_hg4) of <a href=https://en.wikipedia.org/wiki/Namer>NAMER</a> is also available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.5/>FITAnnotator : A Flexible and Intelligent Text Annotation System<span class=acl-fixed-case>FITA</span>nnotator: A Flexible and Intelligent Text Annotation System</a></strong><br><a href=/people/y/yanzeng-li/>Yanzeng Li</a>
|
<a href=/people/b/bowen-yu/>Bowen Yu</a>
|
<a href=/people/l/li-quangang/>Li Quangang</a>
|
<a href=/people/t/tingwen-liu/>Tingwen Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--5><div class="card-body p-3 small">In this paper, we introduce FITAnnotator, a generic web-based tool for efficient text annotation. Benefiting from the fully modular architecture design, FITAnnotator provides a systematic solution for the annotation of a variety of natural language processing tasks, including <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>, sequence tagging and semantic role annotation, regardless of the language. Three kinds of <a href=https://en.wikipedia.org/wiki/Interface_(computing)>interfaces</a> are developed to annotate instances, evaluate annotation quality and manage the annotation task for annotators, reviewers and managers, respectively. FITAnnotator also gives intelligent annotations by introducing task-specific assistant to support and guide the annotators based on <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> and incremental learning strategies. This assistant is able to effectively update from the annotator feedbacks and easily handle the incremental labeling scenarios.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-demos.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.6/>Robustness Gym : Unifying the NLP Evaluation Landscape<span class=acl-fixed-case>NLP</span> Evaluation Landscape</a></strong><br><a href=/people/k/karan-goel/>Karan Goel</a>
|
<a href=/people/n/nazneen-fatema-rajani/>Nazneen Fatema Rajani</a>
|
<a href=/people/j/jesse-vig/>Jesse Vig</a>
|
<a href=/people/z/zachary-taschdjian/>Zachary Taschdjian</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/c/christopher-re/>Christopher Ré</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--6><div class="card-body p-3 small">Despite impressive performance on standard <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms : subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables practitioners to compare results from disparate evaluation paradigms with a single click, and to easily develop and share novel evaluation methods using a built-in set of abstractions. RG is under active development and we welcome feedback & contributions from the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.7/>EventPlus : A Temporal Event Understanding Pipeline<span class=acl-fixed-case>E</span>vent<span class=acl-fixed-case>P</span>lus: A Temporal Event Understanding Pipeline</a></strong><br><a href=/people/m/mingyu-derek-ma/>Mingyu Derek Ma</a>
|
<a href=/people/j/jiao-sun/>Jiao Sun</a>
|
<a href=/people/m/mu-yang/>Mu Yang</a>
|
<a href=/people/k/kung-hsiang-huang/>Kung-Hsiang Huang</a>
|
<a href=/people/n/nuan-wen/>Nuan Wen</a>
|
<a href=/people/s/shikhar-singh/>Shikhar Singh</a>
|
<a href=/people/r/rujun-han/>Rujun Han</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--7><div class="card-body p-3 small">We present EventPlus, a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection, event argument detection, event duration and temporal relation extraction. Event information, especially event temporal knowledge, is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. EventPlus as the first comprehensive temporal event understanding pipeline provides a convenient tool for users to quickly obtain annotations about events and their temporal information for any user-provided document. Furthermore, we show EventPlus can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.naacl-demos.12.Supplementary.pdf data-toggle=tooltip data-placement=top title=Supplementary><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.12/>ActiveAnno : General-Purpose Document-Level Annotation Tool with Active Learning Integration<span class=acl-fixed-case>A</span>ctive<span class=acl-fixed-case>A</span>nno: General-Purpose Document-Level Annotation Tool with Active Learning Integration</a></strong><br><a href=/people/m/max-wiechmann/>Max Wiechmann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--12><div class="card-body p-3 small">ActiveAnno is an <a href=https://en.wikipedia.org/wiki/Annotation>annotation tool</a> focused on document-level annotation tasks developed both for industry and research settings. It is designed to be a general-purpose tool with a wide variety of use cases. It features a modern and responsive <a href=https://en.wikipedia.org/wiki/User_interface>web UI</a> for creating annotation projects, conducting <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>, adjudicating disagreements, and analyzing annotation results. ActiveAnno embeds a highly configurable and interactive user interface. The tool also integrates a RESTful API that enables integration into other <a href=https://en.wikipedia.org/wiki/Software_system>software systems</a>, including an <a href=https://en.wikipedia.org/wiki/Application_programming_interface>API</a> for machine learning integration. ActiveAnno is built with extensible design and easy deployment in mind, all to enable users to perform annotation tasks with high efficiency and high-quality annotation results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-demos.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.13/>TextEssence : A Tool for Interactive Analysis of Semantic Shifts Between Corpora<span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>E</span>ssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora</a></strong><br><a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/v/venkatesh-sivaraman/>Venkatesh Sivaraman</a>
|
<a href=/people/a/adam-perer/>Adam Perer</a>
|
<a href=/people/e/eric-fosler-lussier/>Eric Fosler-Lussier</a>
|
<a href=/people/h/harry-hochheiser/>Harry Hochheiser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--13><div class="card-body p-3 small">Embeddings of words and concepts capture syntactic and semantic regularities of language ; however, they have seen limited use as tools to study characteristics of different corpora and how they relate to one another. We introduce TextEssence, an interactive system designed to enable comparative analysis of corpora using <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface. We further propose a new measure of embedding confidence based on nearest neighborhood overlap, to assist in identifying high-quality <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus analysis</a>. A case study on COVID-19 scientific literature illustrates the utility of the <a href=https://en.wikipedia.org/wiki/System>system</a>. TextEssence can be found at https://textessence.github.io.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-demos.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.16/>RESIN : A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System<span class=acl-fixed-case>RESIN</span>: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System</a></strong><br><a href=/people/h/haoyang-wen/>Haoyang Wen</a>
|
<a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/t/tuan-lai/>Tuan Lai</a>
|
<a href=/people/x/xiaoman-pan/>Xiaoman Pan</a>
|
<a href=/people/s/sha-li/>Sha Li</a>
|
<a href=/people/x/xudong-lin/>Xudong Lin</a>
|
<a href=/people/b/ben-zhou/>Ben Zhou</a>
|
<a href=/people/m/manling-li/>Manling Li</a>
|
<a href=/people/h/haoyu-wang/>Haoyu Wang</a>
|
<a href=/people/h/hongming-zhang/>Hongming Zhang</a>
|
<a href=/people/x/xiaodong-yu/>Xiaodong Yu</a>
|
<a href=/people/a/alexander-dong/>Alexander Dong</a>
|
<a href=/people/z/zhenhailong-wang/>Zhenhailong Wang</a>
|
<a href=/people/y/yi-fung/>Yi Fung</a>
|
<a href=/people/p/piyush-mishra/>Piyush Mishra</a>
|
<a href=/people/q/qing-lyu/>Qing Lyu</a>
|
<a href=/people/d/didac-suris/>Dídac Surís</a>
|
<a href=/people/b/brian-chen/>Brian Chen</a>
|
<a href=/people/s/susan-windisch-brown/>Susan Windisch Brown</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/c/carl-vondrick/>Carl Vondrick</a>
|
<a href=/people/j/jiawei-han/>Jiawei Han</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/s/shih-fu-chang/>Shih-Fu Chang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--16><div class="card-body p-3 small">We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects : (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking ; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-demos.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-demos--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-demos.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-demos.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-demos.17/>MUDES : Multilingual Detection of Offensive Spans<span class=acl-fixed-case>MUDES</span>: Multilingual Detection of Offensive Spans</a></strong><br><a href=/people/t/tharindu-ranasinghe/>Tharindu Ranasinghe</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-demos--17><div class="card-body p-3 small">The interest in offensive content identification in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has grown substantially in recent years. Previous work has dealt mostly with post level annotations. However, identifying offensive spans is useful in many ways. To help coping with this important challenge, we present MUDES, a multilingual system to detect offensive spans in texts. MUDES features pre-trained models, a <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python API</a> for developers, and a user-friendly web-based interface. A detailed description of MUDES&#8217; components is presented in this paper.</div></div></div><hr><div id=2021naacl-srw><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-srw.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.naacl-srw/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-srw.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-srw.0/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</a></strong><br><a href=/people/e/esin-durmus/>Esin Durmus</a>
|
<a href=/people/v/vivek-gupta/>Vivek Gupta</a>
|
<a href=/people/n/nelson-f-liu/>Nelson Liu</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/y/yu-su/>Yu Su</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-srw.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-srw--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-srw.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-srw.12/>Shuffled-token Detection for Refining Pre-trained RoBERTa<span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a</a></strong><br><a href=/people/s/subhadarshi-panda/>Subhadarshi Panda</a>
|
<a href=/people/a/anjali-agrawal/>Anjali Agrawal</a>
|
<a href=/people/j/jeewon-ha/>Jeewon Ha</a>
|
<a href=/people/b/benjamin-bloch/>Benjamin Bloch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-srw--12><div class="card-body p-3 small">State-of-the-art transformer models have achieved robust performance on a variety of NLP tasks. Many of these approaches have employed domain agnostic pre-training tasks to train <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that yield highly generalized sentence representations that can be fine-tuned for specific downstream tasks. We propose refining a pre-trained NLP model using the objective of detecting shuffled tokens. We use a sequential approach by starting with the pre-trained RoBERTa model and training it using our approach. Applying random shuffling strategy on the word-level, we found that our approach enables the RoBERTa model achieve better performance on 4 out of 7 GLUE tasks. Our results indicate that learning to detect shuffled tokens is a promising approach to learn more coherent sentence representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-srw.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-srw--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-srw.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-srw.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-srw.13/>Morphology-Aware Meta-Embeddings for Tamil<span class=acl-fixed-case>T</span>amil</a></strong><br><a href=/people/a/arjun-sai-krishnan/>Arjun Sai Krishnan</a>
|
<a href=/people/s/seyoon-ragavan/>Seyoon Ragavan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-srw--13><div class="card-body p-3 small">In this work, we explore generating morphologically enhanced word embeddings for <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, a highly agglutinative South Indian language with rich morphology that remains low-resource with regards to NLP tasks. We present here the first-ever word analogy dataset for <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, consisting of 4499 hand-curated word tetrads across 10 semantic and 13 morphological relation types. Using a rules-based segmenter to capture <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> as well as meta-embedding techniques, we train meta-embeddings that outperform existing baselines by 16 % on our analogy task and appear to mitigate a previously observed trade-off between semantic and morphological accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-srw.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-srw--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-srw.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-srw.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-srw.14/>Seed Word Selection for Weakly-Supervised Text Classification with Unsupervised Error Estimation</a></strong><br><a href=/people/y/yiping-jin/>Yiping Jin</a>
|
<a href=/people/a/akshay-bhatia/>Akshay Bhatia</a>
|
<a href=/people/d/dittaya-wanvarie/>Dittaya Wanvarie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-srw--14><div class="card-body p-3 small">Weakly-supervised text classification aims to induce <a href=https://en.wikipedia.org/wiki/Text_classification>text classifiers</a> from only a few user-provided seed words. The vast majority of previous work assumes high-quality seed words are given. However, the expert-annotated seed words are sometimes non-trivial to come up with. Furthermore, in the weakly-supervised learning setting, we do not have any labeled document to measure the seed words&#8217; efficacy, making the seed word selection process a walk in the dark. In this work, we remove the need for expert-curated seed words by first mining (noisy) candidate seed words associated with the category names. We then train interim models with individual candidate seed words. Lastly, we estimate the interim models&#8217; <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>error rate</a> in an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised manner</a>. The seed words that yield the lowest estimated <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>error rates</a> are added to the final seed word set. A comprehensive evaluation of six binary classification tasks on four popular datasets demonstrates that the proposed method outperforms a baseline using only category name seed words and obtained comparable performance as a counterpart using expert-annotated seed words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-srw.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-srw--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-srw.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-srw.15/>Multi-Task Learning of Generation and Classification for Emotion-Aware Dialogue Response Generation</a></strong><br><a href=/people/t/tatsuya-ide/>Tatsuya Ide</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-srw--15><div class="card-body p-3 small">For a computer to naturally interact with a human, it needs to be human-like. In this paper, we propose a neural response generation model with multi-task learning of generation and classification, focusing on <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a>. Our <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> based on BART (Lewis et al., 2020), a pre-trained transformer encoder-decoder model, is trained to generate responses and recognize <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> simultaneously. Furthermore, we weight the losses for the <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> to control the update of parameters. Automatic evaluations and crowdsourced manual evaluations show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> makes generated responses more emotionally aware.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-srw.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-srw--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-srw.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-srw.16/>Comparison of <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>Grammatical Error Correction</a> Using Back-Translation Models</a></strong><br><a href=/people/a/aomi-koyama/>Aomi Koyama</a>
|
<a href=/people/k/kengo-hotate/>Kengo Hotate</a>
|
<a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-srw--16><div class="card-body p-3 small">Grammatical error correction (GEC) suffers from a lack of sufficient parallel data. Studies on GEC have proposed several methods to generate pseudo data, which comprise pairs of grammatical and artificially produced ungrammatical sentences. Currently, a mainstream approach to generate pseudo data is back-translation (BT). Most previous studies using <a href=https://en.wikipedia.org/wiki/BT_Group>BT</a> have employed the same <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> for both the GEC and BT models. However, GEC models have different correction tendencies depending on the architecture of their <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Thus, in this study, we compare the correction tendencies of GEC models trained on pseudo data generated by three BT models with different architectures, namely, Transformer, CNN, and LSTM. The results confirm that the correction tendencies for each error type are different for every BT model. In addition, we investigate the correction tendencies when using a combination of pseudo data generated by different BT models. As a result, we find that the combination of different BT models improves or interpolates the performance of each error type compared with using a single BT model with different seeds.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-srw.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-srw--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-srw.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-srw.20/>Hie-BART : Document Summarization with Hierarchical BART<span class=acl-fixed-case>BART</span>: Document Summarization with Hierarchical <span class=acl-fixed-case>BART</span></a></strong><br><a href=/people/k/kazuki-akiyama/>Kazuki Akiyama</a>
|
<a href=/people/a/akihiro-tamura/>Akihiro Tamura</a>
|
<a href=/people/t/takashi-ninomiya/>Takashi Ninomiya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-srw--20><div class="card-body p-3 small">This paper proposes a new abstractive document summarization model, hierarchical BART (Hie-BART), which captures <a href=https://en.wikipedia.org/wiki/Hierarchical_organization>hierarchical structures</a> of a document (i.e., sentence-word structures) in the BART model. Although the existing BART model has achieved a state-of-the-art performance on document summarization tasks, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> does not have the interactions between sentence-level information and word-level information. In machine translation tasks, the performance of neural machine translation models has been improved by incorporating multi-granularity self-attention (MG-SA), which captures the relationships between words and phrases. Inspired by the previous work, the proposed Hie-BART model incorporates MG-SA into the encoder of the BART model for capturing sentence-word structures. Evaluations on the CNN / Daily Mail dataset show that the proposed Hie-BART model outperforms some strong baselines and improves the performance of a non-hierarchical BART model (+0.23 ROUGE-L).</div></div></div><hr><div id=2021naacl-tutorials><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-tutorials.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.naacl-tutorials/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-tutorials.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-tutorials.0/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials</a></strong><br><a href=/people/g/greg-kondrak/>Greg Kondrak</a>
|
<a href=/people/k/kalina-bontcheva/>Kalina Bontcheva</a>
|
<a href=/people/d/dan-gillick/>Dan Gillick</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-tutorials.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-tutorials--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-tutorials.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-tutorials.4/>A Tutorial on Evaluation Metrics used in Natural Language Generation</a></strong><br><a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a>
|
<a href=/people/a/ananya-b-sai/>Ananya B. Sai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-tutorials--4><div class="card-body p-3 small">The advent of <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and the availability of large scale datasets has accelerated research on <a href=https://en.wikipedia.org/wiki/Natural-language_generation>Natural Language Generation</a> with a focus on newer tasks and better <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas / components that need improvement. To accomplish this in an automatic and reliable manner, the NLP community has actively pursued the development of automatic evaluation metrics. Especially in the last few years, there has been an increasing focus on evaluation metrics, with several criticisms of existing metrics and proposals for several new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>. This tutorial presents the evolution of automatic evaluation metrics to their current state along with the emerging trends in this field by specifically addressing the following questions : (i) What makes NLG evaluation challenging? (ii) Why do we need automatic evaluation metrics? (iii) What are the existing automatic evaluation metrics and how can they be organised in a coherent taxonomy? (iv) What are the criticisms and shortcomings of existing <a href=https://en.wikipedia.org/wiki/Performance_metric>metrics</a>? (v) What are the possible future directions of research?</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-tutorials.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-tutorials--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-tutorials.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-tutorials.6/>Crowdsourcing Natural Language Data at Scale : A Hands-On Tutorial</a></strong><br><a href=/people/a/alexey-drutsa/>Alexey Drutsa</a>
|
<a href=/people/d/dmitry-ustalov/>Dmitry Ustalov</a>
|
<a href=/people/v/valentina-fedorova/>Valentina Fedorova</a>
|
<a href=/people/o/olga-megorskaya/>Olga Megorskaya</a>
|
<a href=/people/d/daria-baidakova/>Daria Baidakova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-tutorials--6><div class="card-body p-3 small">In this tutorial, we present a portion of unique industry experience in efficient natural language data annotation via <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> shared by both leading researchers and engineers from Yandex. We will make an introduction to data labeling via public crowdsourcing marketplaces and will present the key components of efficient label collection. This will be followed by a practical session, where participants address a real-world language resource production task, experiment with selecting settings for the labeling process, and launch their label collection project on one of the largest crowdsourcing marketplaces. The projects will be run on real crowds within the tutorial session and we will present useful quality control techniques and provide the attendees with an opportunity to discuss their own annotation ideas.</div></div></div><hr><div id=2021naacl-industry><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.naacl-industry/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.0/>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</a></strong><br><a href=/people/y/young-bum-kim/>Young-bum Kim</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/o/owen-rambow/>Owen Rambow</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.1/>When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages</a></strong><br><a href=/people/s/stojan-trajanovski/>Stojan Trajanovski</a>
|
<a href=/people/c/chad-atalla/>Chad Atalla</a>
|
<a href=/people/k/kunho-kim/>Kunho Kim</a>
|
<a href=/people/v/vipul-agarwal/>Vipul Agarwal</a>
|
<a href=/people/m/milad-shokouhi/>Milad Shokouhi</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--1><div class="card-body p-3 small">Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and <a href=https://en.wikipedia.org/wiki/Microsoft_Outlook>Outlook</a>, finding that contextual signals contribute to performance differently between these scenarios. On <a href=https://en.wikipedia.org/wiki/Email>emails</a>, time context is most beneficial with small relative gains of 2 % over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> yields relative improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> between 9.3 % and 18.6 % across various critical service-oriented text prediction metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-industry.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.10/>Proteno : Text Normalization with Limited Data for Fast Deployment in Text to Speech Systems</a></strong><br><a href=/people/s/shubhi-tyagi/>Shubhi Tyagi</a>
|
<a href=/people/a/antonio-bonafonte/>Antonio Bonafonte</a>
|
<a href=/people/j/jaime-lorenzo-trueba/>Jaime Lorenzo-Trueba</a>
|
<a href=/people/j/javier-latorre/>Javier Latorre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--10><div class="card-body p-3 small">Developing Text Normalization (TN) systems for Text-to-Speech (TTS) on new languages is hard. We propose a novel <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> to facilitate it for multiple languages while using <a href=https://en.wikipedia.org/wiki/Data>data</a> less than 3 % of the size of the data used by the state of the art results on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. We treat TN as a sequence classification problem and propose a granular tokenization mechanism that enables the system to learn majority of the classes and their normalizations from the training data itself. This is further combined with minimal precoded linguistic knowledge for other classes. We publish the first results on TN for TTS in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a> and also demonstrate that the performance of the approach is comparable with the previous work done on <a href=https://en.wikipedia.org/wiki/English_language>English</a>. All annotated datasets used for experimentation will be released.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.14/>Autocorrect in the Process of Translation Multi-task Learning Improves Dialogue Machine Translation</a></strong><br><a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/c/chengqi-zhao/>Chengqi Zhao</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--14><div class="card-body p-3 small">Automatic translation of dialogue texts is a much needed demand in many real life scenarios. However, the currently existing <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> delivers unsatisfying results. In this paper, we conduct a deep analysis of a dialogue corpus and summarize three major issues on dialogue translation, including pronoun dropping (), punctuation dropping (), and typos (). In response to these challenges, we propose a joint learning method to identify omission and typo, and utilize <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> to translate dialogue utterances. To properly evaluate the performance, we propose a manually annotated dataset with 1,931 Chinese-English parallel utterances from 300 dialogues as a benchmark testbed for dialogue translation. Our experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> improves translation quality by 3.2 BLEU over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. It also elevates the recovery rate of omitted pronouns from 26.09 % to 47.16 %. We will publish the code and dataset publicly at https://xxx.xx.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-industry.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.16/>Practical Transformer-based Multilingual Text Classification</a></strong><br><a href=/people/c/cindy-wang/>Cindy Wang</a>
|
<a href=/people/m/michele-banko/>Michele Banko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--16><div class="card-body p-3 small">Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on two distinct <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> can improve <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance without the need for additional labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.19/>Graph-based Multilingual Product Retrieval in E-Commerce Search<span class=acl-fixed-case>E</span>-Commerce Search</a></strong><br><a href=/people/h/hanqing-lu/>Hanqing Lu</a>
|
<a href=/people/y/youna-hu/>Youna Hu</a>
|
<a href=/people/t/tong-zhao/>Tong Zhao</a>
|
<a href=/people/t/tony-wu/>Tony Wu</a>
|
<a href=/people/y/yiwei-song/>Yiwei Song</a>
|
<a href=/people/b/bing-yin/>Bing Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--19><div class="card-body p-3 small">Nowadays, with many <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce platforms</a> conducting global business, e-commerce search systems are required to handle product retrieval under multilingual scenarios. Moreover, comparing with maintaining per-country specific e-commerce search systems, having an universal system across countries can further reduce the operational and computational costs, and facilitate business expansion to new countries. In this paper, we introduce an universal end-to-end multilingual retrieval system, and discuss our learnings and technical details when training and deploying the <a href=https://en.wikipedia.org/wiki/System>system</a> to serve billion-scale product retrieval for e-commerce search. In particular, we propose a multilingual graph attention based retrieval network by leveraging recent advances in transformer-based multilingual language models and graph neural network architectures to capture the interactions between search queries and items in e-commerce search. Offline experiments on five countries data show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> outperforms the state-of-the-art baselines by 35 % <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and 25 % mAP on average. Moreover, the proposed model shows significant increase of conversion / revenue in online A / B experiments and has been deployed in production for multiple countries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><span class="align-middle mr-1" data-toggle=tooltip data-placement=bottom title="Best Industry Paper"><i class="fas fa-award"></i></span><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-industry.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.20/>Query2Prod2Vec : Grounded Word Embeddings for <a href=https://en.wikipedia.org/wiki/E-commerce>eCommerce</a><span class=acl-fixed-case>Q</span>uery2<span class=acl-fixed-case>P</span>rod2<span class=acl-fixed-case>V</span>ec: Grounded Word Embeddings for e<span class=acl-fixed-case>C</span>ommerce</a></strong><br><a href=/people/f/federico-bianchi/>Federico Bianchi</a>
|
<a href=/people/j/jacopo-tagliabue/>Jacopo Tagliabue</a>
|
<a href=/people/b/bingqing-yu/>Bingqing Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--20><div class="card-body p-3 small">We present Query2Prod2Vec, a model that grounds lexical representations for product search in product embeddings : in our model, meaning is a mapping between words and a latent space of products in a digital shop. We leverage shopping sessions to learn the underlying space and use merchandising annotations to build lexical analogies for evaluation : our experiments show that our model is more accurate than known techniques from the NLP and IR literature. Finally, we stress the importance of <a href=https://en.wikipedia.org/wiki/Data_efficiency>data efficiency</a> for product search outside of retail giants, and highlight how Query2Prod2Vec fits with practical constraints faced by most practitioners.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.21/>An <a href=https://en.wikipedia.org/wiki/Architecture>Architecture</a> for Accelerated Large-Scale Inference of Transformer-Based Language Models</a></strong><br><a href=/people/a/amir-ganiev/>Amir Ganiev</a>
|
<a href=/people/c/colton-chapin/>Colton Chapin</a>
|
<a href=/people/a/anderson-de-andrade/>Anderson De Andrade</a>
|
<a href=/people/c/chen-liu/>Chen Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--21><div class="card-body p-3 small">This work demonstrates the development process of a machine learning architecture for <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> that can scale to a large volume of requests. We used a BERT model that was fine-tuned for emotion analysis, returning a probability distribution of emotions given a paragraph. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> was deployed as a gRPC service on <a href=https://en.wikipedia.org/wiki/Kubernetes>Kubernetes</a>. Apache Spark was used to perform <a href=https://en.wikipedia.org/wiki/Inference>inference</a> in batches by calling the <a href=https://en.wikipedia.org/wiki/Service_(systems_architecture)>service</a>. We encountered some performance and concurrency challenges and created solutions to achieve <a href=https://en.wikipedia.org/wiki/Time_complexity>faster running time</a>. Starting with 200 successful inference requests per minute, we were able to achieve as high as 18 thousand successful requests per minute with the same batch job resource allocation. As a result, we successfully stored emotion probabilities for 95 million paragraphs within 96 hours.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.24/>Cost-effective Deployment of BERT Models in Serverless Environment<span class=acl-fixed-case>BERT</span> Models in Serverless Environment</a></strong><br><a href=/people/m/marek-suppa/>Marek Suppa</a>
|
<a href=/people/k/katarina-benesova/>Katarína Benešová</a>
|
<a href=/people/a/andrej-svec/>Andrej Švec</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--24><div class="card-body p-3 small">In this study, we demonstrate the viability of deploying BERT-style models to <a href=https://en.wikipedia.org/wiki/AWS_Lambda>AWS Lambda</a> in a production environment. Since the freely available pre-trained models are too large to be deployed in this environment, we utilize knowledge distillation and fine-tune the models on proprietary datasets for two real-world tasks : <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic textual similarity</a>. As a result, we obtain <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that are tuned for a specific domain and deployable in the <a href=https://en.wikipedia.org/wiki/Serverless_computing>serverless environment</a>. The subsequent performance analysis shows that this solution does not only report latency levels acceptable for production use but that it is also a cost-effective alternative to small-to-medium size deployments of BERT models, all without any infrastructure overhead.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.25/>Noise Robust Named Entity Understanding for Voice Assistants</a></strong><br><a href=/people/d/deepak-muralidharan/>Deepak Muralidharan</a>
|
<a href=/people/j/joel-ruben-antony-moniz/>Joel Ruben Antony Moniz</a>
|
<a href=/people/s/sida-gao/>Sida Gao</a>
|
<a href=/people/x/xiao-yang/>Xiao Yang</a>
|
<a href=/people/j/justine-kao/>Justine Kao</a>
|
<a href=/people/s/stephen-pulman/>Stephen Pulman</a>
|
<a href=/people/a/atish-kothari/>Atish Kothari</a>
|
<a href=/people/r/ray-shen/>Ray Shen</a>
|
<a href=/people/y/yinying-pan/>Yinying Pan</a>
|
<a href=/people/v/vivek-kaul/>Vivek Kaul</a>
|
<a href=/people/m/mubarak-seyed-ibrahim/>Mubarak Seyed Ibrahim</a>
|
<a href=/people/g/gang-xiang/>Gang Xiang</a>
|
<a href=/people/n/nan-dun/>Nan Dun</a>
|
<a href=/people/y/yidan-zhou/>Yidan Zhou</a>
|
<a href=/people/a/andy-o/>Andy O</a>
|
<a href=/people/y/yuan-zhang/>Yuan Zhang</a>
|
<a href=/people/p/pooja-chitkara/>Pooja Chitkara</a>
|
<a href=/people/x/xuan-wang/>Xuan Wang</a>
|
<a href=/people/a/alkesh-patel/>Alkesh Patel</a>
|
<a href=/people/k/kushal-tayal/>Kushal Tayal</a>
|
<a href=/people/r/roger-zheng/>Roger Zheng</a>
|
<a href=/people/p/peter-grasch/>Peter Grasch</a>
|
<a href=/people/j/jason-d-williams/>Jason D Williams</a>
|
<a href=/people/l/lin-li/>Lin Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--25><div class="card-body p-3 small">Named Entity Recognition (NER) and Entity Linking (EL) play an essential role in voice assistant interaction, but are challenging due to the special difficulties associated with spoken user queries. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that jointly solves the NER and EL tasks by combining them in a joint reranking module. We show that our proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> improves NER accuracy by up to 3.13 % and EL accuracy by up to 3.6 % in <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a>. The <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> used also lead to better accuracies in other natural language understanding tasks, such as domain classification and <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.naacl-industry.27" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.27/>Intent Features for Rich Natural Language Understanding</a></strong><br><a href=/people/b/brian-lester/>Brian Lester</a>
|
<a href=/people/s/sagnik-ray-choudhury/>Sagnik Ray Choudhury</a>
|
<a href=/people/r/rashmi-prasad/>Rashmi Prasad</a>
|
<a href=/people/s/srinivas-bangalore/>Srinivas Bangalore</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--27><div class="card-body p-3 small">Complex natural language understanding modules in dialog systems have a richer understanding of user utterances, and thus are critical in providing a better user experience. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are often created from scratch, for specific clients and use cases and require the annotation of large datasets. This encourages the sharing of annotated data across multiple clients. To facilitate this we introduce the idea of intent features : domain and topic agnostic properties of intents that can be learnt from the syntactic cues only, and hence can be shared. We introduce a new neural network architecture, the Global-Local model, that shows significant improvement over strong baselines for identifying these features in a deployed, multi-intent natural language understanding module, and more generally in a classification setting where a part of an utterance has to be classified utilizing the whole context.<i>intent features</i>: domain and topic agnostic properties of intents that can be learnt from the syntactic cues only, and hence can be shared. We introduce a new neural network architecture, the Global-Local model, that shows significant improvement over strong baselines for identifying these features in a deployed, multi-intent natural language understanding module, and more generally in a classification setting where a part of an utterance has to be classified utilizing the whole context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.33/>Ad Headline Generation using Self-Critical Masked Language Model</a></strong><br><a href=/people/y/yashal-shakti-kanungo/>Yashal Shakti Kanungo</a>
|
<a href=/people/s/sumit-negi/>Sumit Negi</a>
|
<a href=/people/a/aruna-rajan/>Aruna Rajan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--33><div class="card-body p-3 small">For any <a href=https://en.wikipedia.org/wiki/E-commerce>E-commerce website</a> it is a nontrivial problem to build enduring advertisements that attract shoppers. It is hard to pass the creative quality bar of the website, especially at a large scale. We thus propose a programmatic solution to generate product advertising headlines using retail content. We propose a state of the art application of Reinforcement Learning (RL) Policy gradient methods on Transformer (Vaswani et al., 2017) based Masked Language Models (Devlin et al., 2019). Our method creates the advertising headline by jointly conditioning on multiple products that a seller wishes to advertise. We demonstrate that our method outperforms existing Transformer and LSTM + RL methods in overlap metrics and quality audits. We also show that our model generated headlines outperform human submitted headlines in terms of both <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> and creative quality as determined by audits.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.34/>LATEX-Numeric : Language Agnostic Text Attribute Extraction for Numeric Attributes<span class=acl-fixed-case>LATEX</span>-Numeric: Language Agnostic Text Attribute Extraction for Numeric Attributes</a></strong><br><a href=/people/k/kartik-mehta/>Kartik Mehta</a>
|
<a href=/people/i/ioana-oprea/>Ioana Oprea</a>
|
<a href=/people/n/nikhil-rasiwasia/>Nikhil Rasiwasia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--34><div class="card-body p-3 small">In this paper, we present LATEX-Numeric-a high-precision fully-automated scalable framework for extracting E-commerce numeric attributes from unstructured product text like product description. Most of the past work on attribute extraction is not scalable as they rely on manually curated training data, either with or without use of <a href=https://en.wikipedia.org/wiki/Active_learning_(machine_learning)>active learning</a>. We rely on distant supervision for <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data generation</a>, removing dependency on manual labels. One issue with distant supervision is that it leads to incomplete training annotation due to missing attribute values while <a href=https://en.wikipedia.org/wiki/Matching_(graph_theory)>matching</a>. We propose a multi-task learning architecture to deal with missing labels in the training data, leading to <a href=https://en.wikipedia.org/wiki/F-number>F1 improvement</a> of 9.2 % for numeric attributes over state-of-the-art single-task architecture. While multi-task architecture benefits both numeric and non-numeric attributes, we present automated techniques to further improve the numeric attributes extraction models. Numeric attributes require a list of units (or aliases) for better matching with distant supervision. We propose an automated algorithm for alias creation using <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured text</a> and <a href=https://en.wikipedia.org/wiki/Attribute_(computing)>attribute values</a>, leading to a 20.2 % <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>F1 improvement</a>. Extensive experiments on real world datasets for 20 numeric attributes across 5 product categories and 3 English marketplaces show that LATEX-numeric achieves a high F1-score, without any manual intervention, making it suitable for practical applications. Finally we show that the improvements are language-agnostic and LATEX-Numeric achieves 13.9 % F1 improvement for 3 non-English languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.35/>Training Language Models under Resource Constraints for Adversarial Advertisement Detection</a></strong><br><a href=/people/e/eshwar-shamanna-girishekar/>Eshwar Shamanna Girishekar</a>
|
<a href=/people/s/shiv-surya/>Shiv Surya</a>
|
<a href=/people/n/nishant-nikhil/>Nishant Nikhil</a>
|
<a href=/people/d/dyut-kumar-sil/>Dyut Kumar Sil</a>
|
<a href=/people/s/sumit-negi/>Sumit Negi</a>
|
<a href=/people/a/aruna-rajan/>Aruna Rajan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--35><div class="card-body p-3 small">Advertising on <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce</a> and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning and multi-lingual training can be applied effectively to fine-tune BERT and its variants for text classification tasks in conjunction with different data augmentation strategies. Our extensive experiments on multiple languages show that these techniques detect adversarial ad categories with a substantial gain in precision at high recall threshold over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.naacl-industry.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--naacl-industry--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.naacl-industry.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.naacl-industry.39/>Industry Scale Semi-Supervised Learning for Natural Language Understanding</a></strong><br><a href=/people/l/luoxin-chen/>Luoxin Chen</a>
|
<a href=/people/f/francisco-garcia/>Francisco Garcia</a>
|
<a href=/people/v/varun-kumar/>Varun Kumar</a>
|
<a href=/people/h/he-xie/>He Xie</a>
|
<a href=/people/j/jianhua-lu/>Jianhua Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--naacl-industry--39><div class="card-body p-3 small">This paper presents a production Semi-Supervised Learning (SSL) pipeline based on the student-teacher framework, which leverages millions of unlabeled examples to improve Natural Language Understanding (NLU) tasks. We investigate two questions related to the use of unlabeled data in production SSL context : 1) how to select samples from a huge unlabeled data pool that are beneficial for SSL training, and 2) how does the selected data affect the performance of different state-of-the-art SSL techniques. We compare four widely used SSL techniques, Pseudo-label (PL), Knowledge Distillation (KD), Virtual Adversarial Training (VAT) and Cross-View Training (CVT) in conjunction with two data selection methods including committee-based selection and submodular optimization based selection. We further examine the benefits and drawbacks of these techniques when applied to intent classification (IC) and named entity recognition (NER) tasks, and provide guidelines specifying when each of these methods might be beneficial to improve large scale NLU systems.</div></div></div><hr><div id=2021alvr-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alvr-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.alvr-1/>Proceedings of the Second Workshop on Advances in Language and Vision Research</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alvr-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alvr-1.0/>Proceedings of the Second Workshop on Advances in Language and Vision Research</a></strong><br><a href=/people/x/xin/>Xin</a>
|
<a href=/people/r/ronghang-hu/>Ronghang Hu</a>
|
<a href=/people/d/drew-hudson/>Drew Hudson</a>
|
<a href=/people/t/tsu-jui-fu/>Tsu-Jui Fu</a>
|
<a href=/people/m/marcus-rohrbach/>Marcus Rohrbach</a>
|
<a href=/people/d/daniel-fried/>Daniel Fried</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alvr-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--alvr-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.alvr-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alvr-1.3/>Leveraging Partial Dependency Trees to Control Image Captions</a></strong><br><a href=/people/w/wenjie-zhong/>Wenjie Zhong</a>
|
<a href=/people/y/yusuke-miyao/>Yusuke Miyao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--alvr-1--3><div class="card-body p-3 small">Controlling the generation of image captions attracts lots of attention recently. In this paper, we propose a framework leveraging partial syntactic dependency trees as control signals to make image captions include specified words and their syntactic structures. To achieve this purpose, we propose a Syntactic Dependency Structure Aware Model (SDSAM), which explicitly learns to generate the syntactic structures of image captions to include given partial dependency trees. In addition, we come up with a <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> to evaluate how many specified words and their syntactic dependencies are included in generated captions. We carry out experiments on two standard <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> : Microsoft COCO and <a href=https://en.wikipedia.org/wiki/Flickr>Flickr30k</a>. Empirical results show that image captions generated by our model are effectively controlled in terms of specified words and their syntactic structures. The code is available on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alvr-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--alvr-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.alvr-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alvr-1.4/>Grounding Plural Phrases : Countering Evaluation Biases by Individuation</a></strong><br><a href=/people/j/julia-suter/>Julia Suter</a>
|
<a href=/people/l/letitia-parcalabescu/>Letitia Parcalabescu</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--alvr-1--4><div class="card-body p-3 small">Phrase grounding (PG) is a <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multimodal task</a> that grounds language in images. PG systems are evaluated on well-known <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>, using Intersection over Union (IoU) as evaluation metric. This work highlights a disconcerting bias in the evaluation of grounded plural phrases, which arises from representing sets of objects as a union box covering all component bounding boxes, in conjunction with the IoU metric. We detect, analyze and quantify an evaluation bias in the grounding of plural phrases and define a novel <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>, c-IoU, based on a union box&#8217;s component boxes. We experimentally show that our new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> greatly alleviates this bias and recommend using it for fairer evaluation of plural phrases in PG tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.alvr-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--alvr-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.alvr-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.alvr-1.6/>Learning to Learn Semantic Factors in Heterogeneous Image Classification</a></strong><br><a href=/people/b/boyue-fan/>Boyue Fan</a>
|
<a href=/people/z/zhenting-liu/>Zhenting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--alvr-1--6><div class="card-body p-3 small">Few-shot learning is to recognize novel classes with a few labeled samples per class. Although numerous meta-learning methods have made significant progress, they struggle to directly address the heterogeneity of training and evaluating task distributions, resulting in the domain shift problem when transitioning to new tasks with <a href=https://en.wikipedia.org/wiki/Disjoint_sets>disjoint spaces</a>. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Methodology>method</a> to deal with the <a href=https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity>heterogeneity</a>. Specifically, by simulating class-difference domain shift during the meta-train phase, a bilevel optimization procedure is applied to learn a transferable representation space that can rapidly adapt to heterogeneous tasks. Experiments demonstrate the effectiveness of our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div></div><hr><div id=2021americasnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.americasnlp-1/>Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.americasnlp-1.0/>Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/a/arturo-oncevay/>Arturo Oncevay</a>
|
<a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/i/ivan-vladimir-meza-ruiz/>Ivan Vladimir Meza Ruiz</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--americasnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.americasnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.americasnlp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.americasnlp-1.3/>Investigating variation in written forms of Nahuatl using character-based language models<span class=acl-fixed-case>N</span>ahuatl using character-based language models</a></strong><br><a href=/people/r/robert-pugh/>Robert Pugh</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--americasnlp-1--3><div class="card-body p-3 small">We describe experiments with character-based language modeling for written variants of Nahuatl. Using a standard LSTM model and publicly available <a href=https://en.wikipedia.org/wiki/Bible_translations>Bible translations</a>, we explore how character language models can be applied to the tasks of estimating mutual intelligibility, identifying genetic similarity, and distinguishing written variants. We demonstrate that these simple <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> are able to capture similarities and differences that have been described in the <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic literature</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--americasnlp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.americasnlp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.americasnlp-1.10" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.americasnlp-1.10/>Morphological Segmentation for Seneca<span class=acl-fixed-case>S</span>eneca</a></strong><br><a href=/people/z/zoey-liu/>Zoey Liu</a>
|
<a href=/people/r/robert-jimerson/>Robert Jimerson</a>
|
<a href=/people/e/emily-prudhommeaux/>Emily Prud’hommeaux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--americasnlp-1--10><div class="card-body p-3 small">This study takes up the task of low-resource morphological segmentation for <a href=https://en.wikipedia.org/wiki/Seneca_language>Seneca</a>, a critically endangered and morphologically complex Native American language primarily spoken in what is now New York State and <a href=https://en.wikipedia.org/wiki/Ontario>Ontario</a>. The labeled data in our experiments comes from two sources : one digitized from a publicly available grammar book and the other collected from informal sources. We treat these two sources as distinct domains and investigate different evaluation designs for <a href=https://en.wikipedia.org/wiki/Model_selection>model selection</a>. The first design abides by standard practices and evaluate models with the in-domain development set, while the second one carries out evaluation using a development domain, or the out-of-domain development set. Across a series of monolingual and crosslinguistic training settings, our results demonstrate the utility of neural encoder-decoder architecture when coupled with <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--americasnlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.americasnlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.americasnlp-1.11.OptionalSupplementaryCode.zip data-toggle=tooltip data-placement=top title="Optional supplementary code"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.americasnlp-1.11/>Representation of Yine [ Arawak ] Morphology by Finite State Transducer Formalism<span class=acl-fixed-case>Y</span>ine [<span class=acl-fixed-case>A</span>rawak] Morphology by Finite State Transducer Formalism</a></strong><br><a href=/people/a/adriano-ingunza-torres/>Adriano Ingunza Torres</a>
|
<a href=/people/j/john-miller/>John Miller</a>
|
<a href=/people/a/arturo-oncevay/>Arturo Oncevay</a>
|
<a href=/people/r/roberto-zariquiey-biondi/>Roberto Zariquiey Biondi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--americasnlp-1--11><div class="card-body p-3 small">We represent the complexity of Yine (Arawak) morphology with a finite state transducer (FST) based morphological analyzer. Yine is a low-resource indigenous polysynthetic Peruvian language spoken by approximately 3,000 people and is classified as &#8216;definitely endangered&#8217; by UNESCO. We review Yine morphology focusing on <a href=https://en.wikipedia.org/wiki/Morphophonology>morphophonology</a>, <a href=https://en.wikipedia.org/wiki/Possessive>possessive constructions</a> and <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>verbal predicates</a>. Then we develop FSTs to model these components proposing techniques to solve challenging problems such as complex patterns of incorporating open and closed category arguments. This is a work in progress and we still have more to do in the development and verification of our <a href=https://en.wikipedia.org/wiki/Analyzer>analyzer</a>. Our analyzer will serve both as a tool to better document the <a href=https://en.wikipedia.org/wiki/Yine>Yine language</a> and as a component of natural language processing (NLP) applications such as <a href=https://en.wikipedia.org/wiki/Spell_checker>spell checking</a> and correction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--americasnlp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.americasnlp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.americasnlp-1.14/>Expanding Universal Dependencies for <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>Polysynthetic Languages</a> : A Case of St. Lawrence Island Yupik<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies for Polysynthetic Languages: A Case of <span class=acl-fixed-case>S</span>t. <span class=acl-fixed-case>L</span>awrence <span class=acl-fixed-case>I</span>sland <span class=acl-fixed-case>Y</span>upik</a></strong><br><a href=/people/h/hyunji-hayley-park/>Hyunji Hayley Park</a>
|
<a href=/people/l/lane-schwartz/>Lane Schwartz</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--americasnlp-1--14><div class="card-body p-3 small">This paper describes the development of the first Universal Dependencies (UD) treebank for St. Lawrence Island Yupik, an <a href=https://en.wikipedia.org/wiki/Endangered_language>endangered language</a> spoken in the <a href=https://en.wikipedia.org/wiki/Beringia>Bering Strait region</a>. While the UD guidelines provided a general framework for our annotations, language-specific decisions were made necessary by the rich morphology of the <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic language</a>. Most notably, we annotated a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> at the <a href=https://en.wikipedia.org/wiki/Morpheme>morpheme level</a> as well as the <a href=https://en.wikipedia.org/wiki/Word>word level</a>. The morpheme level annotation was conducted using an existing <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyzer</a> and manual disambiguation. By comparing the two resulting annotation schemes, we argue that morpheme-level annotation is essential for polysynthetic languages like St. Lawrence Island Yupik. Word-level annotation results in degenerate trees for some Yupik sentences and often fails to capture syntactic relations that can be manifested at the morpheme level. Dependency parsing experiments provide further support for morpheme-level annotation. Implications for UD annotation of other <a href=https://en.wikipedia.org/wiki/Polysynthetic_language>polysynthetic languages</a> are discussed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--americasnlp-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.americasnlp-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.americasnlp-1.23/>Findings of the AmericasNLP 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas<span class=acl-fixed-case>A</span>mericas<span class=acl-fixed-case>NLP</span> 2021 Shared Task on Open Machine Translation for Indigenous Languages of the <span class=acl-fixed-case>A</span>mericas</a></strong><br><a href=/people/m/manuel-mager/>Manuel Mager</a>
|
<a href=/people/a/arturo-oncevay/>Arturo Oncevay</a>
|
<a href=/people/a/abteen-ebrahimi/>Abteen Ebrahimi</a>
|
<a href=/people/j/john-ortega/>John Ortega</a>
|
<a href=/people/a/annette-rios-gonzales/>Annette Rios</a>
|
<a href=/people/a/angela-fan/>Angela Fan</a>
|
<a href=/people/x/ximena-gutierrez-vasques/>Ximena Gutierrez-Vasques</a>
|
<a href=/people/l/luis-chiruzzo/>Luis Chiruzzo</a>
|
<a href=/people/g/gustavo-gimenez-lugo/>Gustavo Giménez-Lugo</a>
|
<a href=/people/r/ricardo-ramos/>Ricardo Ramos</a>
|
<a href=/people/i/ivan-meza-ruiz/>Ivan Vladimir Meza Ruiz</a>
|
<a href=/people/r/rolando-coto-solano/>Rolando Coto-Solano</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/e/elisabeth-mager-hois/>Elisabeth Mager-Hois</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--americasnlp-1--23><div class="card-body p-3 small">This paper presents the results of the 2021 Shared Task on Open Machine Translation for <a href=https://en.wikipedia.org/wiki/Indigenous_languages_of_the_Americas>Indigenous Languages of the Americas</a>. The shared task featured two independent tracks, and participants submitted <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> for up to 10 <a href=https://en.wikipedia.org/wiki/Indigenous_language>indigenous languages</a>. Overall, 8 teams participated with a total of 214 submissions. We provided <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training sets</a> consisting of data collected from various sources, as well as manually translated sentences for the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>development and test sets</a>. An official <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> trained on this <a href=https://en.wikipedia.org/wiki/Data>data</a> was also provided. Team submissions featured a variety of <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a>, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>. The best performing <a href=https://en.wikipedia.org/wiki/System>systems</a> achieved 12.97 ChrF higher than <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>, when averaged across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--americasnlp-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.americasnlp-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.americasnlp-1.27/>The REPU CS’ SpanishQuechua Submission to the AmericasNLP 2021 Shared Task on Open Machine Translation<span class=acl-fixed-case>REPU</span> <span class=acl-fixed-case>CS</span>’ <span class=acl-fixed-case>S</span>panish–<span class=acl-fixed-case>Q</span>uechua Submission to the <span class=acl-fixed-case>A</span>mericas<span class=acl-fixed-case>NLP</span> 2021 Shared Task on Open Machine Translation</a></strong><br><a href=/people/o/oscar-moreno/>Oscar Moreno</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--americasnlp-1--27><div class="card-body p-3 small">We present the submission of REPUcs to the AmericasNLP machine translation shared task for the low resource language pair SpanishQuechua. Our neural machine translation system ranked first in Track two (development set not used for training) and third in Track one (training includes development data). Our contribution is focused on : (i) the collection of new parallel data from different web sources (poems, lyrics, lexicons, handbooks), and (ii) using large SpanishEnglish data for pre-training and then fine-tuning the SpanishQuechua system. This paper describes the new <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> and our approach in detail.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.americasnlp-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--americasnlp-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.americasnlp-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.americasnlp-1.29/>The Helsinki submission to the AmericasNLP shared task<span class=acl-fixed-case>H</span>elsinki submission to the <span class=acl-fixed-case>A</span>mericas<span class=acl-fixed-case>NLP</span> shared task</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/s/sami-virpioja/>Sami Virpioja</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--americasnlp-1--29><div class="card-body p-3 small">The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs. Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects : (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data. This paper describes our efforts in detail.</div></div></div><hr><div id=2021autosimtrans-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.autosimtrans-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.autosimtrans-1/>Proceedings of the Second Workshop on Automatic Simultaneous Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.autosimtrans-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.autosimtrans-1.0/>Proceedings of the Second Workshop on Automatic Simultaneous Translation</a></strong><br><a href=/people/h/hua-wu/>Hua Wu</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a>
|
<a href=/people/z/zhongjun-he/>Zhongjun He</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/m/maha-elbayad/>Maha Elbayad</a>
|
<a href=/people/m/mark-liberman/>Mark Liberman</a>
|
<a href=/people/h/haifeng-wang/>Haifeng Wang</a>
|
<a href=/people/m/mingbo-ma/>Mingbo Ma</a>
|
<a href=/people/r/ruiqing-zhang/>Ruiqing Zhang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.autosimtrans-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--autosimtrans-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.autosimtrans-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.autosimtrans-1.1/>ICT’s System for AutoSimTrans 2021 : Robust Char-Level Simultaneous Translation<span class=acl-fixed-case>ICT</span>’s System for <span class=acl-fixed-case>A</span>uto<span class=acl-fixed-case>S</span>im<span class=acl-fixed-case>T</span>rans 2021: Robust Char-Level Simultaneous Translation</a></strong><br><a href=/people/s/shaolei-zhang/>Shaolei Zhang</a>
|
<a href=/people/y/yang-feng/>Yang Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--autosimtrans-1--1><div class="card-body p-3 small">Simultaneous translation (ST) outputs the translation simultaneously while reading the input sentence, which is an important component of <a href=https://en.wikipedia.org/wiki/Simultaneous_interpretation>simultaneous interpretation</a>. In this paper, we describe our submitted ST system, which won the first place in the streaming transcription input track of the Chinese-English translation task of AutoSimTrans 2021. Aiming at the robustness of ST, we first propose char-level simultaneous translation and applied wait-k policy on it. Meanwhile, we apply two <a href=https://en.wikipedia.org/wiki/Data_processing>data processing methods</a> and combine two <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training methods</a> for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>. Our method enhance the ST model with stronger robustness and domain adaptability. Experiments on streaming transcription show that our method outperforms the baseline at all latency, especially at low latency, the proposed method improves about 6 BLEU. Besides, ablation studies we conduct verify the effectiveness of each module in the proposed method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.autosimtrans-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--autosimtrans-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.autosimtrans-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.autosimtrans-1.3/>XMU’s Simultaneous Translation System at NAACL 2021<span class=acl-fixed-case>XMU</span>’s Simultaneous Translation System at <span class=acl-fixed-case>NAACL</span> 2021</a></strong><br><a href=/people/s/shuangtao-li/>Shuangtao Li</a>
|
<a href=/people/j/jinming-hu/>Jinming Hu</a>
|
<a href=/people/b/boli-wang/>Boli Wang</a>
|
<a href=/people/x/xiaodong-shi/>Xiaodong Shi</a>
|
<a href=/people/y/yidong-chen/>Yidong Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--autosimtrans-1--3><div class="card-body p-3 small">This paper describes our two systems submitted to the simultaneous translation evaluation at the 2nd automatic simultaneous translation workshop.</div></div></div><hr><div id=2021bionlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.bionlp-1/>Proceedings of the 20th Workshop on Biomedical Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.0/>Proceedings of the 20th Workshop on Biomedical Language Processing</a></strong><br><a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a>
|
<a href=/people/k/k-bretonnel-cohen/>Kevin Bretonnel Cohen</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a>
|
<a href=/people/j/junichi-tsujii/>Junichi Tsujii</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bionlp-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.3/>Scalable Few-Shot Learning of Robust Biomedical Name Representations</a></strong><br><a href=/people/p/pieter-fivez/>Pieter Fivez</a>
|
<a href=/people/s/simon-suster/>Simon Suster</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--3><div class="card-body p-3 small">Recent research on robust representations of biomedical names has focused on modeling large amounts of fine-grained conceptual distinctions using complex neural encoders. In this paper, we explore the opposite <a href=https://en.wikipedia.org/wiki/Paradigm>paradigm</a> : training a simple encoder architecture using only small sets of <a href=https://en.wikipedia.org/wiki/Name>names</a> sampled from high-level biomedical concepts. Our encoder post-processes pretrained representations of biomedical names, and is effective for various types of input representations, both domain-specific or unsupervised. We validate our proposed few-shot learning approach on multiple biomedical relatedness benchmarks, and show that it allows for continual learning, where we accumulate information from various conceptual hierarchies to consistently improve encoder performance. Given these findings, we propose our approach as a low-cost alternative for exploring the impact of conceptual distinctions on robust biomedical name representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.4/>SAFFRON : tranSfer leArning For Food-disease RelatiOn extractioN<span class=acl-fixed-case>SAFFRON</span>: tran<span class=acl-fixed-case>S</span>fer le<span class=acl-fixed-case>A</span>rning For Food-disease <span class=acl-fixed-case>R</span>elati<span class=acl-fixed-case>O</span>n extractio<span class=acl-fixed-case>N</span></a></strong><br><a href=/people/g/gjorgjina-cenikj/>Gjorgjina Cenikj</a>
|
<a href=/people/t/tome-eftimov/>Tome Eftimov</a>
|
<a href=/people/b/barbara-korousic-seljak/>Barbara Koroušić Seljak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--4><div class="card-body p-3 small">The accelerating growth of <a href=https://en.wikipedia.org/wiki/Big_data>big data</a> in the biomedical domain, with an endless amount of electronic health records and more than 30 million citations and abstracts in <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a>, introduces the need for automatic structuring of textual biomedical data. In this paper, we develop a method for detecting relations between food and disease entities from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>raw text</a>. Due to the lack of annotated data on <a href=https://en.wikipedia.org/wiki/Food>food</a> with respect to health, we explore the feasibility of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> by training BERT-based models on existing datasets annotated for the presence of cause and treat relations among different types of biomedical entities, and using them to recognize the same relations between <a href=https://en.wikipedia.org/wiki/Food>food and disease entities</a> in a dataset created for the purposes of this study. The best <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> achieve macro averaged F1 scores of 0.847 and 0.900 for the <a href=https://en.wikipedia.org/wiki/Causality>cause and treat relations</a>, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.8/>Overview of the MEDIQA 2021 Shared Task on <a href=https://en.wikipedia.org/wiki/Summarization>Summarization</a> in the Medical Domain<span class=acl-fixed-case>MEDIQA</span> 2021 Shared Task on Summarization in the Medical Domain</a></strong><br><a href=/people/a/asma-ben-abacha/>Asma Ben Abacha</a>
|
<a href=/people/y/yassine-mrabet/>Yassine Mrabet</a>
|
<a href=/people/y/yuhao-zhang/>Yuhao Zhang</a>
|
<a href=/people/c/chaitanya-shivade/>Chaitanya Shivade</a>
|
<a href=/people/c/curtis-langlotz/>Curtis Langlotz</a>
|
<a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--8><div class="card-body p-3 small">The MEDIQA 2021 shared tasks at the BioNLP 2021 workshop addressed three tasks on <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> for medical text : (i) a question summarization task aimed at exploring new approaches to understanding complex real-world consumer health queries, (ii) a multi-answer summarization task that targeted aggregation of multiple relevant answers to a biomedical question into one concise and relevant answer, and (iii) a radiology report summarization task addressing the development of clinically relevant impressions from radiology report findings. Thirty-five teams participated in these shared tasks with sixteen working notes submitted (fifteen accepted) describing a wide variety of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> developed and tested on the shared and external datasets. In this paper, we describe the tasks, the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> and techniques developed by various teams, the results of the <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>, and a study of correlations among various summarization evaluation measures. We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.9/>WBI at MEDIQA 2021 : Summarizing Consumer Health Questions with Generative Transformers<span class=acl-fixed-case>WBI</span> at <span class=acl-fixed-case>MEDIQA</span> 2021: Summarizing Consumer Health Questions with Generative Transformers</a></strong><br><a href=/people/m/mario-sanger/>Mario Sänger</a>
|
<a href=/people/l/leon-weber/>Leon Weber</a>
|
<a href=/people/u/ulf-leser/>Ulf Leser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--9><div class="card-body p-3 small">This paper describes our contribution for the MEDIQA-2021 Task 1 question summarization competition. We model the <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> as conditional generation problem. Our concrete pipeline performs a finetuning of the large pretrained generative transformers PEGASUS (Zhang et al.,2020a) and BART (Lewis et al.,2020). We used the resulting <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> as strong baselines and experimented with (i) integrating structured knowledge via entity embeddings, (ii) ensembling multiple generative models with the generator-discriminator framework and (iii) disentangling summarization and interrogative prediction to achieve further improvements. Our best performing model, a fine-tuned vanilla PEGASUS, reached the second place in the competition with an ROUGE-2-F1 score of 15.99. We observed that all of our additional measures hurt performance (up to 5.2 pp) on the official test set. In course of a post-hoc experimental analysis which uses a larger validation set results indicate slight performance improvements through the proposed extensions. However, further analysis is need to provide stronger evidence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.11/>BDKG at MEDIQA 2021 : System Report for the Radiology Report Summarization Task<span class=acl-fixed-case>BDKG</span> at <span class=acl-fixed-case>MEDIQA</span> 2021: System Report for the Radiology Report Summarization Task</a></strong><br><a href=/people/s/songtai-dai/>Songtai Dai</a>
|
<a href=/people/q/quan-wang/>Quan Wang</a>
|
<a href=/people/y/yajuan-lyu/>Yajuan Lyu</a>
|
<a href=/people/y/yong-zhu/>Yong Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--11><div class="card-body p-3 small">This paper presents our winning system at the Radiology Report Summarization track of the MEDIQA 2021 shared task. Radiology report summarization automatically summarizes radiology findings into free-text impressions. This year&#8217;s task emphasizes the generalization and transfer ability of participating systems. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is built upon a pre-trained Transformer encoder-decoder architecture, i.e., <a href=https://en.wikipedia.org/wiki/PEGASUS>PEGASUS</a>, deployed with an additional domain adaptation module to particularly handle the transfer and generalization issue. Heuristics like <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble</a> and <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a> are also used. Our system is conceptually simple yet highly effective, achieving a ROUGE-2 score of 0.436 on test set and ranked the 1st place among all participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.12/>damo_nlp at MEDIQA 2021 : Knowledge-based Preprocessing and Coverage-oriented Reranking for Medical Question Summarization<span class=acl-fixed-case>MEDIQA</span> 2021: Knowledge-based Preprocessing and Coverage-oriented Reranking for Medical Question Summarization</a></strong><br><a href=/people/y/yifan-he/>Yifan He</a>
|
<a href=/people/m/mosha-chen/>Mosha Chen</a>
|
<a href=/people/s/songfang-huang/>Songfang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--12><div class="card-body p-3 small">Medical question summarization is an important but difficult task, where the input is often complex and erroneous while annotated data is expensive to acquire. We report our participation in the MEDIQA 2021 question summarization task in which we are required to address these challenges. We start from pre-trained conditional generative language models, use <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> to help correct input errors, and rerank single system outputs to boost coverage. Experimental results show significant improvement in string-based metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bionlp-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.13/>Stress Test Evaluation of Biomedical Word Embeddings</a></strong><br><a href=/people/v/vladimir-araujo/>Vladimir Araujo</a>
|
<a href=/people/a/andres-carvallo/>Andrés Carvallo</a>
|
<a href=/people/c/carlos-aspillaga/>Carlos Aspillaga</a>
|
<a href=/people/c/camilo-thorne/>Camilo Thorne</a>
|
<a href=/people/d/denis-parra/>Denis Parra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--13><div class="card-body p-3 small">The success of pretrained word embeddings has motivated their use in the biomedical domain, with contextualized embeddings yielding remarkable results in several biomedical NLP tasks. However, there is a lack of research on quantifying their behavior under <a href=https://en.wikipedia.org/wiki/Stress_(biology)>severe stress scenarios</a>. In this work, we systematically evaluate three language models with adversarial examples automatically constructed tests that allow us to examine how robust the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are. We propose two types of stress scenarios focused on the biomedical named entity recognition (NER) task, one inspired by spelling errors and another based on the use of synonyms for medical terms. Our experiments with three <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> show that the performance of the original <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> decreases considerably, in addition to revealing their weaknesses and strengths. Finally, we show that adversarial training causes the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to improve their robustness and even to exceed the original performance in some cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.16/>BioELECTRA : Pretrained Biomedical text Encoder using Discriminators<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>ELECTRA</span>:Pretrained Biomedical text Encoder using Discriminators</a></strong><br><a href=/people/k/kamal-raj-kanakarajan/>Kamal raj Kanakarajan</a>
|
<a href=/people/b/bhuvana-kundumani/>Bhuvana Kundumani</a>
|
<a href=/people/m/malaikannan-sankarasubbu/>Malaikannan Sankarasubbu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--16><div class="card-body p-3 small">Recent advancements in pretraining strategies in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> have shown a significant improvement in the performance of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on various text mining tasks. We apply &#8216;replaced token detection&#8217; pretraining technique proposed by ELECTRA and pretrain a biomedical language model from scratch using biomedical text and vocabulary. We introduce BioELECTRA, a biomedical domain-specific language encoder model that adapts ELECTRA for the Biomedical domain. WE evaluate our model on the BLURB and BLUE biomedical NLP benchmarks. BioELECTRA outperforms the previous models and achieves state of the art (SOTA) on all the 13 datasets in BLURB benchmark and on all the 4 Clinical datasets from BLUE Benchmark across 7 different NLP tasks. BioELECTRA pretrained on PubMed and PMC full text articles performs very well on Clinical datasets as well. BioELECTRA achieves new SOTA 86.34%(1.39 % accuracy improvement) on MedNLI and 64 % (2.98 % accuracy improvement) on PubMedQA dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bionlp-1.17" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.17/>Word centrality constrained representation for keyphrase extraction</a></strong><br><a href=/people/z/zelalem-gero/>Zelalem Gero</a>
|
<a href=/people/j/joyce-ho/>Joyce Ho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--17><div class="card-body p-3 small">To keep pace with the increased generation and digitization of documents, automated methods that can improve <a href=https://en.wikipedia.org/wiki/Search_engine_technology>search</a>, discovery and mining of the vast body of literature are essential. Keyphrases provide a concise representation by identifying salient concepts in a document. Various <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised approaches</a> model <a href=https://en.wikipedia.org/wiki/Keyphrase_extraction>keyphrase extraction</a> using local context to predict the label for each token and perform much better than the <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised counterparts</a>. Unfortunately, this <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> fails for short documents where the context is unclear. Moreover, <a href=https://en.wikipedia.org/wiki/Keyword_(linguistics)>keyphrases</a>, which are usually the gist of a document, need to be the central theme. We propose a new extraction model that introduces a <a href=https://en.wikipedia.org/wiki/Centrality>centrality constraint</a> to enrich the word representation of a Bidirectional long short-term memory. Performance evaluation on 2 publicly available datasets demonstrate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms existing state-of-the art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.18/>End-to-end Biomedical Entity Linking with Span-based Dictionary Matching</a></strong><br><a href=/people/s/shogo-ujiie/>Shogo Ujiie</a>
|
<a href=/people/h/hayate-iso/>Hayate Iso</a>
|
<a href=/people/s/shuntaro-yada/>Shuntaro Yada</a>
|
<a href=/people/s/shoko-wakamiya/>Shoko Wakamiya</a>
|
<a href=/people/e/eiji-aramaki/>Eiji Aramaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--18><div class="card-body p-3 small">Disease name recognition and <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> is a fundamental process in <a href=https://en.wikipedia.org/wiki/Biomedical_text_mining>biomedical text mining</a>. Recently, neural joint learning of both <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a> has been proposed to utilize the mutual benefits. While this approach achieves high performance, disease concepts that do not appear in the training dataset can not be accurately predicted. This study introduces a novel <a href=https://en.wikipedia.org/wiki/End-to-end_principle>end-to-end approach</a> that combines span representations with dictionary-matching features to address this problem. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> handles unseen concepts by referring to a <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a> while maintaining the performance of neural network-based models. Experiments using two major datasaets demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved competitive results with strong baselines, especially for unseen concepts during training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.26/>Context-aware query design combines knowledge and data for efficient reading and reasoning</a></strong><br><a href=/people/e/emilee-holtzapple/>Emilee Holtzapple</a>
|
<a href=/people/b/brent-cochran/>Brent Cochran</a>
|
<a href=/people/n/natasa-miskov-zivanov/>Natasa Miskov-Zivanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--26><div class="card-body p-3 small">The amount of biomedical literature has vastly increased over the past few decades. As a result, the sheer quantity of accessible information is overwhelming, and complicates manual information retrieval. Automated methods seek to speed up <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> from <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical literature</a>. However, such automated methods are still too time-intensive to survey all existing biomedical literature. We present a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for automatically generating literature queries that select relevant papers based on <a href=https://en.wikipedia.org/wiki/Data>biological data</a>. By using <a href=https://en.wikipedia.org/wiki/Gene_expression>differentially expressed genes</a> to inform our literature searches, we focus <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> on mechanistic signaling details that are crucial for the disease or context of interest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.27/>Measuring the relative importance of full text sections for <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> from <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific literature</a>.</a></strong><br><a href=/people/l/lana-yeganova/>Lana Yeganova</a>
|
<a href=/people/w/won-gyu-kim/>Won Gyu Kim</a>
|
<a href=/people/d/donald-c-comeau/>Donald Comeau</a>
|
<a href=/people/w/w-john-wilbur/>W John Wilbur</a>
|
<a href=/people/z/zhiyong-lu/>Zhiyong Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--27><div class="card-body p-3 small">With the growing availability of full-text articles, integrating <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>abstracts</a> and full texts of documents into a unified representation is essential for comprehensive search of scientific literature. However, previous studies have shown that navely merging <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>abstracts</a> with full texts of articles does not consistently yield better performance. Balancing the contribution of query terms appearing in the abstract and in sections of different importance in full text articles remains a challenge both with traditional bag-of-words IR approaches and for neural retrieval methods. In this work we establish the connection between the BM25 score of a query term appearing in a section of a full text document and the probability of that document being clicked or identified as relevant. Probability is computed using Pool Adjacent Violators (PAV), an isotonic regression algorithm, providing a <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood estimate</a> based on the observed data. Using this probabilistic transformation of BM25 scores we show an improved performance on the PubMed Click dataset developed and presented in this study, as well as the 2007 TREC Genomics collection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.31/>SB_NITK at MEDIQA 2021 : Leveraging Transfer Learning for Question Summarization in Medical Domain<span class=acl-fixed-case>SB</span>_<span class=acl-fixed-case>NITK</span> at <span class=acl-fixed-case>MEDIQA</span> 2021: Leveraging Transfer Learning for Question Summarization in Medical Domain</a></strong><br><a href=/people/s/spandana-balumuri/>Spandana Balumuri</a>
|
<a href=/people/s/sony-bachina/>Sony Bachina</a>
|
<a href=/people/s/sowmya-kamath-s/>Sowmya Kamath S</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--31><div class="card-body p-3 small">Recent strides in the healthcare domain, have resulted in vast quantities of <a href=https://en.wikipedia.org/wiki/Streaming_data>streaming data</a> available for use for building intelligent knowledge-based applications. However, the challenges introduced to the huge volume, velocity of generation, variety and variability of this medical data have to be adequately addressed. In this paper, we describe the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and results for our submission at MEDIQA 2021 Question Summarization shared task. In order to improve the performance of summarization of consumer health questions, our method explores the use of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> to utilize the knowledge of NLP transformers like BART, T5 and <a href=https://en.wikipedia.org/wiki/PEGASUS>PEGASUS</a>. The proposed models utilize the knowledge of pre-trained NLP transformers to achieve improved results when compared to conventional deep learning models such as LSTM, RNN etc. Our team SB_NITK ranked 12th among the total 22 submissions in the official final rankings. Our BART based model achieved a ROUGE-2 F1 score of 0.139.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.bionlp-1.33" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.33/>QIAI at MEDIQA 2021 : Multimodal Radiology Report Summarization<span class=acl-fixed-case>QIAI</span> at <span class=acl-fixed-case>MEDIQA</span> 2021: Multimodal Radiology Report Summarization</a></strong><br><a href=/people/j/jean-benoit-delbrouck/>Jean-Benoit Delbrouck</a>
|
<a href=/people/c/cassie-zhang/>Cassie Zhang</a>
|
<a href=/people/d/daniel-rubin/>Daniel Rubin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--33><div class="card-body p-3 small">This paper describes the solution of the QIAI lab sent to the Radiology Report Summarization (RRS) challenge at MEDIQA 2021. This paper aims to investigate whether using <a href=https://en.wikipedia.org/wiki/Multimodality>multimodality</a> during training improves the summarizing performances of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> at test-time. Our preliminary results shows that taking advantage of the visual features from the <a href=https://en.wikipedia.org/wiki/X-ray>x-rays</a> associated to the radiology reports leads to higher evaluation metrics compared to a text-only baseline system. These improvements are reported according to the automatic evaluation metrics METEOR, <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and ROUGE scores. Our experiments can be fully replicated at the following address : https:// github.com/jbdel/vilmedic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.bionlp-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--bionlp-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.bionlp-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.bionlp-1.37/>MNLP at MEDIQA 2021 : Fine-Tuning PEGASUS for Consumer Health Question Summarization<span class=acl-fixed-case>MNLP</span> at <span class=acl-fixed-case>MEDIQA</span> 2021: Fine-Tuning <span class=acl-fixed-case>PEGASUS</span> for Consumer Health Question Summarization</a></strong><br><a href=/people/j/jooyeon-lee/>Jooyeon Lee</a>
|
<a href=/people/h/huong-dang/>Huong Dang</a>
|
<a href=/people/o/ozlem-uzuner/>Ozlem Uzuner</a>
|
<a href=/people/s/sam-henry/>Sam Henry</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--bionlp-1--37><div class="card-body p-3 small">This paper details a Consumer Health Question (CHQ) summarization model submitted to MEDIQA 2021 for shared task 1 : Question Summarization. Many CHQs are composed of multiple sentences with typos or unnecessary information, which can interfere with automated question answering systems. Question summarization mitigates this issue by removing this unnecessary information, aiding automated systems in generating a more accurate summary. Our summarization approach focuses on applying multiple pre-processing techniques, including question focus identification on the input and the development of an ensemble method to combine question focus with an abstractive summarization method. We use the state-of-art abstractive summarization model, PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization), to generate abstractive summaries. Our experiments show that using our ensemble method, which combines abstractive summarization with question focus identification, improves performance over using <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> alone. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> shows a ROUGE-2 F-measure of 11.14 % against the official test dataset.</div></div></div><hr><div id=2021calcs-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.calcs-1/>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.0/>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</a></strong><br><a href=/people/t/thamar-solorio/>Thamar Solorio</a>
|
<a href=/people/s/shuguang-chen/>Shuguang Chen</a>
|
<a href=/people/a/alan-w-black/>Alan W. Black</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/v/victor-soto/>Victor Soto</a>
|
<a href=/people/e/emre-yilmaz/>Emre Yilmaz</a>
|
<a href=/people/a/anirudh-srinivasan/>Anirudh Srinivasan</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.calcs-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.3/>Translate and Classify : Improving Sequence Level Classification for English-Hindi Code-Mixed Data<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>H</span>indi Code-Mixed Data</a></strong><br><a href=/people/d/devansh-gautam/>Devansh Gautam</a>
|
<a href=/people/k/kshitij-gupta/>Kshitij Gupta</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--3><div class="card-body p-3 small">Code-mixing is a common phenomenon in multilingual societies around the world and is especially common in social media texts. Traditional NLP systems, usually trained on monolingual corpora, do not perform well on code-mixed texts. Training specialized <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for code-switched texts is difficult due to the lack of <a href=https://en.wikipedia.org/wiki/Data_set>large-scale datasets</a>. Translating code-mixed data into standard languages like <a href=https://en.wikipedia.org/wiki/English_language>English</a> could improve performance on various code-mixed tasks since we can use <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from state-of-the-art <a href=https://en.wikipedia.org/wiki/English_language>English models</a> for processing the translated data. This paper focuses on two sequence-level classification tasks for English-Hindi code mixed texts, which are part of the GLUECoS benchmark-Natural Language Inference and Sentiment Analysis. We propose using various pre-trained models that have been fine-tuned for similar English-only tasks and have shown state-of-the-art performance. We further fine-tune these models on the translated code-mixed datasets and achieve state-of-the-art performance in both tasks. To translate English-Hindi code-mixed data to English, we use mBART, a pre-trained multilingual sequence-to-sequence model that has shown competitive performance on various low-resource machine translation pairs and has also shown performance gains in languages that were not in its pre-training corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.6/>Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing<span class=acl-fixed-case>E</span>nglish to <span class=acl-fixed-case>H</span>inglish Machine Translation with Synthetic Code-Mixing</a></strong><br><a href=/people/g/ganesh-jawahar/>Ganesh Jawahar</a>
|
<a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a>
|
<a href=/people/l/laks-lakshmanan-v-s/>Laks Lakshmanan, V.S.</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--6><div class="card-body p-3 small">We describe models focused at the understudied problem of <a href=https://en.wikipedia.org/wiki/Translation>translating</a> between monolingual and code-mixed language pairs. More specifically, we offer a wide range of models that convert monolingual English text into Hinglish (code-mixed Hindi and English). Given the recent success of pretrained language models, we also test the utility of two recent Transformer-based encoder-decoder models (i.e., mT5 and mBART) on the task finding both to work well. Given the paucity of training data for <a href=https://en.wikipedia.org/wiki/Code_mixing>code-mixing</a>, we also propose a dependency-free method for generating code-mixed texts from bilingual distributed representations that we exploit for improving <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> performance. In particular, armed with this additional data, we adopt a curriculum learning approach where we first finetune the <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> on <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a> then on gold code-mixed data. We find that, although simple, our synthetic code-mixing method is competitive with (and in some cases is even superior to) several standard methods (backtranslation, <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> based on equivalence constraint theory) under a diverse set of conditions. Our work shows that the mT5 model, finetuned following the <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum learning procedure</a>, achieves best translation performance (12.67 BLEU). Our <a href=https://en.wikipedia.org/wiki/Model_(person)>models</a> place first in the overall ranking of the English-Hinglish official shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.calcs-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.7/>CoMeT : Towards Code-Mixed Translation Using Parallel Monolingual Sentences<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>M</span>e<span class=acl-fixed-case>T</span>: Towards Code-Mixed Translation Using Parallel Monolingual Sentences</a></strong><br><a href=/people/d/devansh-gautam/>Devansh Gautam</a>
|
<a href=/people/p/prashant-kodali/>Prashant Kodali</a>
|
<a href=/people/k/kshitij-gupta/>Kshitij Gupta</a>
|
<a href=/people/a/anmol-goel/>Anmol Goel</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/p/ponnurangam-kumaraguru/>Ponnurangam Kumaraguru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--7><div class="card-body p-3 small">Code-mixed languages are very popular in multilingual societies around the world, yet the resources lag behind to enable robust systems on such <a href=https://en.wikipedia.org/wiki/Language>languages</a>. A major contributing factor is the informal nature of these languages which makes it difficult to collect code-mixed data. In this paper, we propose our system for Task 1 of CACLS 2021 to generate a <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation system</a> for English to Hinglish in a supervised setting. Translating in the given direction can help expand the set of resources for several tasks by translating valuable datasets from high resource languages. We propose to use mBART, a pre-trained multilingual sequence-to-sequence model, and fully utilize the pre-training of the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> by transliterating the roman Hindi words in the code-mixed sentences to <a href=https://en.wikipedia.org/wiki/Devanagari>Devanagri script</a>. We evaluate how expanding the input by concatenating Hindi translations of the English sentences improves mBART&#8217;s performance. Our <a href=https://en.wikipedia.org/wiki/System>system</a> gives a BLEU score of 12.22 on test set. Further, we perform a detailed error analysis of our proposed systems and explore the limitations of the provided dataset and metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.calcs-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.12/>On the logistical difficulties and findings of Jopara Sentiment Analysis</a></strong><br><a href=/people/m/marvin-aguero-torales/>Marvin Agüero-Torales</a>
|
<a href=/people/d/david-vilares/>David Vilares</a>
|
<a href=/people/a/antonio-lopez-herrera/>Antonio López-Herrera</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--12><div class="card-body p-3 small">This paper addresses the problem of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> for <a href=https://en.wikipedia.org/wiki/Jopara>Jopara</a>, a code-switching language between <a href=https://en.wikipedia.org/wiki/Guarani_language>Guarani</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. We first collect a corpus of Guarani-dominant tweets and discuss on the difficulties of finding quality data for even relatively easy-to-annotate tasks, such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Then, we train a set of neural models, including pre-trained language models, and explore whether they perform better than traditional machine learning ones in this low-resource setup. Transformer architectures obtain the best results, despite not considering Guarani during pre-training, but traditional machine learning models perform close due to the low-resource nature of the problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.calcs-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.14/>CodemixedNLP : An Extensible and Open NLP Toolkit for Code-Mixing<span class=acl-fixed-case>C</span>odemixed<span class=acl-fixed-case>NLP</span>: An Extensible and Open <span class=acl-fixed-case>NLP</span> Toolkit for Code-Mixing</a></strong><br><a href=/people/s/sai-muralidhar-jayanthi/>Sai Muralidhar Jayanthi</a>
|
<a href=/people/k/kavya-nerella/>Kavya Nerella</a>
|
<a href=/people/k/khyathi-raghavi-chandu/>Khyathi Raghavi Chandu</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--14><div class="card-body p-3 small">The NLP community has witnessed steep progress in a variety of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> across the realms of monolingual and multilingual language processing recently. These successes, in conjunction with the proliferating mixed language interactions on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, have boosted interest in modeling code-mixed texts. In this work, we present CodemixedNLP, an open-source library with the goals of bringing together the advances in code-mixed NLP and opening it up to a wider machine learning community. The <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a> consists of tools to develop and benchmark versatile model architectures that are tailored for mixed texts, methods to expand training sets, techniques to quantify mixing styles, and fine-tuned state-of-the-art models for 7 tasks in <a href=https://en.wikipedia.org/wiki/Hinglish>Hinglish</a>. We believe this work has the potential to foster a distributed yet collaborative and sustainable ecosystem in an otherwise dispersed space of code-mixing research. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is designed to be simple, easily extensible, and resourceful to both researchers as well as practitioners. Demo : http://k-ikkees.pc.cs.cmu.edu:5000 and Library : https://github.com/murali1996/CodemixedNLP</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.15/>Normalization and Back-Transliteration for Code-Switched Data</a></strong><br><a href=/people/d/dwija-parikh/>Dwija Parikh</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--15><div class="card-body p-3 small">Code-switching is an omnipresent phenomenon in multilingual communities all around the world but remains a challenge for NLP systems due to the lack of proper data and processing techniques. Hindi-English code-switched text on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is often transliterated to the <a href=https://en.wikipedia.org/wiki/Latin_script>Roman script</a> which prevents from utilizing monolingual resources available in the <a href=https://en.wikipedia.org/wiki/Devanagari>native Devanagari script</a>. In this paper, we propose a method to normalize and back-transliterate code-switched Hindi-English text. In addition, we present a grapheme-to-phoneme (G2P) conversion technique for romanized Hindi data. We also release a dataset of script-corrected Hindi-English code-switched sentences labeled for the <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and part-of-speech tagging tasks to facilitate further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.calcs-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.16/>Abusive content detection in transliterated Bengali-English social media corpus<span class=acl-fixed-case>B</span>engali-<span class=acl-fixed-case>E</span>nglish social media corpus</a></strong><br><a href=/people/s/salim-sazzed/>Salim Sazzed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--16><div class="card-body p-3 small">Abusive text detection in low-resource languages such as <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a> is a challenging task due to the inadequacy of resources and tools. The ubiquity of transliterated Bengali comments in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> makes the task even more involved as monolingual approaches can not capture them. Unfortunately, no transliterated Bengali corpus is publicly available yet for abusive content analysis. Therefore, in this paper, we introduce an annotated Bengali corpus of 3000 transliterated Bengali comments categorized into two classes, abusive and non-abusive, 1500 comments for each. For baseline evaluations, we employ several supervised machine learning (ML) and deep learning-based classifiers. We find support vector machine (SVM) shows the highest efficacy for identifying abusive content. We make the annotated corpus freely available for the researcher to aid abusive content detection in Bengali social media data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.calcs-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--calcs-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.calcs-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.calcs-1.20/>Are Multilingual Models Effective in <a href=https://en.wikipedia.org/wiki/Code-switching>Code-Switching</a>?</a></strong><br><a href=/people/g/genta-indra-winata/>Genta Indra Winata</a>
|
<a href=/people/s/samuel-cahyawijaya/>Samuel Cahyawijaya</a>
|
<a href=/people/z/zihan-liu/>Zihan Liu</a>
|
<a href=/people/z/zhaojiang-lin/>Zhaojiang Lin</a>
|
<a href=/people/a/andrea-madotto/>Andrea Madotto</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--calcs-1--20><div class="card-body p-3 small">Multilingual language models have shown decent performance in multilingual and cross-lingual natural language understanding tasks. However, the power of these multilingual models in code-switching tasks has not been fully explored. In this paper, we study the effectiveness of multilingual language models to understand their capability and adaptability to the mixed-language setting by considering the inference speed, performance, and number of parameters to measure their practicality. We conduct experiments in three language pairs on <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and compare them with existing methods, such as using bilingual embeddings and multilingual meta-embeddings. Our findings suggest that pre-trained multilingual models do not necessarily guarantee high-quality representations on <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>, while using meta-embeddings achieves similar results with significantly fewer parameters.</div></div></div><hr><div id=2021clpsych-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.clpsych-1/>Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.0/>Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access</a></strong><br><a href=/people/n/nazli-goharian/>Nazli Goharian</a>
|
<a href=/people/p/philip-resnik/>Philip Resnik</a>
|
<a href=/people/a/andrew-yates/>Andrew Yates</a>
|
<a href=/people/m/molly-ireland/>Molly Ireland</a>
|
<a href=/people/k/kate-niederhoffer/>Kate Niederhoffer</a>
|
<a href=/people/r/rebecca-resnik/>Rebecca Resnik</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.clpsych-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.1/>Understanding who uses Reddit : Profiling individuals with a self-reported bipolar disorder diagnosis<span class=acl-fixed-case>R</span>eddit: Profiling individuals with a self-reported bipolar disorder diagnosis</a></strong><br><a href=/people/g/glorianna-jagfeld/>Glorianna Jagfeld</a>
|
<a href=/people/f/fiona-lobban/>Fiona Lobban</a>
|
<a href=/people/p/paul-rayson/>Paul Rayson</a>
|
<a href=/people/s/steven-jm-jones/>Steven Jones</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--1><div class="card-body p-3 small">Recently, research on mental health conditions using public online data, including <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>, has surged in NLP and health research but has not reported user characteristics, which are important to judge generalisability of findings. This paper shows how existing NLP methods can yield information on clinical, demographic, and identity characteristics of almost 20 K Reddit users who self-report a bipolar disorder diagnosis. This population consists of slightly more feminine- than masculine-gendered mainly young or middle-aged US-based adults who often report additional <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health diagnoses</a>, which is compared with general Reddit statistics and <a href=https://en.wikipedia.org/wiki/Epidemiology>epidemiological studies</a>. Additionally, this paper carefully evaluates all methods and discusses ethical issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.3/>Individual Differences in the Movement-Mood Relationship in Digital Life Data</a></strong><br><a href=/people/g/glen-coppersmith/>Glen Coppersmith</a>
|
<a href=/people/a/alex-fine/>Alex Fine</a>
|
<a href=/people/p/patrick-crutchley/>Patrick Crutchley</a>
|
<a href=/people/j/joshua-carroll/>Joshua Carroll</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--3><div class="card-body p-3 small">Our increasingly digitized lives generate troves of data that reflect our behavior, beliefs, mood, and wellbeing. Such digital life data provides crucial insight into the lives of patients outside the healthcare setting that has long been lacking, from a better understanding of mundane patterns of exercise and sleep routines to harbingers of emotional crisis. Moreover, information about <a href=https://en.wikipedia.org/wiki/Differential_psychology>individual differences</a> and <a href=https://en.wikipedia.org/wiki/Personality_psychology>personalities</a> is encoded in digital life data. In this paper we examine the relationship between <a href=https://en.wikipedia.org/wiki/Mood_(psychology)>mood</a> and <a href=https://en.wikipedia.org/wiki/Kinesiology>movement</a> using linguistic and biometric data, respectively. Does increased physical activity (movement) have an effect on a person&#8217;s mood (or vice-versa)? We find that weak group-level relationships between movement and <a href=https://en.wikipedia.org/wiki/Mood_(psychology)>mood</a> mask interesting and often strong relationships between the two for individuals within the group. We describe these individual differences, and argue that individual variability in the relationship between movement and <a href=https://en.wikipedia.org/wiki/Mood_(psychology)>mood</a> is one of many such factors that ought be taken into account in wellbeing-focused apps and <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>AI systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.clpsych-1.10.OptionalSupplementaryCode.zip data-toggle=tooltip data-placement=top title="Optional supplementary code"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.clpsych-1.10.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.10/>Suicide Risk Prediction by Tracking Self-Harm Aspects in Tweets : NUS-IDS at the CLPsych 2021 Shared Task<span class=acl-fixed-case>NUS</span>-<span class=acl-fixed-case>IDS</span> at the <span class=acl-fixed-case>CLP</span>sych 2021 Shared Task</a></strong><br><a href=/people/s/sujatha-das-gollapalli/>Sujatha Das Gollapalli</a>
|
<a href=/people/g/guilherme-augusto-zagatti/>Guilherme Augusto Zagatti</a>
|
<a href=/people/s/see-kiong-ng/>See-Kiong Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--10><div class="card-body p-3 small">We describe our system for identifying users at-risk for suicide based on their tweets developed for the CLPsych 2021 Shared Task. Based on research in mental health studies linking self-harm tendencies with <a href=https://en.wikipedia.org/wiki/Suicide>suicide</a>, in our system, we attempt to characterize self-harm aspects expressed in user tweets over a period of time. To this end, we design SHTM, a Self-Harm Topic Model that combines <a href=https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>Latent Dirichlet Allocation</a> with a self-harm dictionary for modeling daily tweets of users. Next, differences in <a href=https://en.wikipedia.org/wiki/Mood_(psychology)>moods</a> and topics over time are captured as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to train a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a> for <a href=https://en.wikipedia.org/wiki/Suicide_prediction>suicide prediction</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.12/>Using Psychologically-Informed Priors for <a href=https://en.wikipedia.org/wiki/Suicide_prediction>Suicide Prediction</a> in the CLPsych 2021 Shared Task<span class=acl-fixed-case>CLP</span>sych 2021 Shared Task</a></strong><br><a href=/people/a/avi-gamoran/>Avi Gamoran</a>
|
<a href=/people/y/yonatan-kaplan/>Yonatan Kaplan</a>
|
<a href=/people/a/almog-simchon/>Almog Simchon</a>
|
<a href=/people/m/michael-gilead/>Michael Gilead</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--12><div class="card-body p-3 small">This paper describes our approach to the CLPsych 2021 Shared Task, in which we aimed to predict suicide attempts based on Twitter feed data. We addressed this challenge by emphasizing reliance on prior domain knowledge. We engineered novel theory-driven features, and integrated prior knowledge with <a href=https://en.wikipedia.org/wiki/Empirical_evidence>empirical evidence</a> in a principled manner using <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian modeling</a>. While this theory-guided approach increases <a href=https://en.wikipedia.org/wiki/Bias_(statistics)>bias</a> and lowers <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the training set, it was successful in preventing <a href=https://en.wikipedia.org/wiki/Overfitting>over-fitting</a>. The <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> provided reasonable <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> on unseen test data (0.68 = AUC= 0.84). Our approach may be particularly useful in prediction tasks trained on a relatively small data set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.13/>Analysis of Behavior Classification in Motivational Interviewing</a></strong><br><a href=/people/l/leili-tavabi/>Leili Tavabi</a>
|
<a href=/people/t/trang-tran/>Trang Tran</a>
|
<a href=/people/k/kalin-stefanov/>Kalin Stefanov</a>
|
<a href=/people/b/brian-borsari/>Brian Borsari</a>
|
<a href=/people/j/joshua-woolley/>Joshua Woolley</a>
|
<a href=/people/s/stefan-scherer/>Stefan Scherer</a>
|
<a href=/people/m/mohammad-soleymani/>Mohammad Soleymani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--13><div class="card-body p-3 small">Analysis of client and therapist behavior in counseling sessions can provide helpful insights for assessing the quality of the session and consequently, the client&#8217;s behavioral outcome. In this paper, we study the automatic classification of standardized behavior codes (annotations) used for assessment of psychotherapy sessions in Motivational Interviewing (MI). We develop <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> and examine the classification of client behaviors throughout MI sessions, comparing the performance by <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on large pretrained embeddings (RoBERTa) versus interpretable and expert-selected features (LIWC). Our best performing <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> using the pretrained RoBERTa embeddings beats the baseline model, achieving an F1 score of 0.66 in the subject-independent 3-class classification. Through <a href=https://en.wikipedia.org/wiki/Statistical_inference>statistical analysis</a> on the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results, we identify prominent LIWC features that may not have been captured by the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> using pretrained embeddings. Although <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> using LIWC features underperforms RoBERTa, our findings motivate the future direction of incorporating auxiliary tasks in the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification of MI codes</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.14/>Automatic Detection and Prediction of Psychiatric Hospitalizations From Social Media Posts</a></strong><br><a href=/people/z/zheng-ping-jiang/>Zhengping Jiang</a>
|
<a href=/people/j/jonathan-zomick/>Jonathan Zomick</a>
|
<a href=/people/s/sarah-ita-levitan/>Sarah Ita Levitan</a>
|
<a href=/people/m/mark-serper/>Mark Serper</a>
|
<a href=/people/j/julia-hirschberg/>Julia Hirschberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--14><div class="card-body p-3 small">We address the problem of predicting psychiatric hospitalizations using <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> drawn from <a href=https://en.wikipedia.org/wiki/Social_media>social media posts</a>. We formulate this novel task and develop an approach to automatically extract time spans of self-reported psychiatric hospitalizations. Using this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we build <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive models</a> of psychiatric hospitalization, comparing <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature sets</a>, user vs. post classification, and comparing model performance using a varying time window of posts. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves an <a href=https://en.wikipedia.org/wiki/F-number>F1</a> of.718 using 7 days of posts. Our results suggest that this is a useful framework for collecting hospitalization data, and that social media data can be leveraged to predict acute psychiatric crises before they occur, potentially saving lives and improving outcomes for individuals with <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental illness</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.16/>Automated coherence measures fail to index <a href=https://en.wikipedia.org/wiki/Thought_disorder>thought disorder</a> in individuals at risk for psychosis</a></strong><br><a href=/people/k/kasia-hitczenko/>Kasia Hitczenko</a>
|
<a href=/people/h/henry-cowan/>Henry Cowan</a>
|
<a href=/people/v/vijay-mittal/>Vijay Mittal</a>
|
<a href=/people/m/matthew-goldrick/>Matthew Goldrick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--16><div class="card-body p-3 small">Thought disorder linguistic disturbances including incoherence and derailment of topic is seen in individuals both with and at risk for psychosis. Methods from <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> have increasingly sought to quantify <a href=https://en.wikipedia.org/wiki/Thought_disorder>thought disorder</a> to detect group differences between <a href=https://en.wikipedia.org/wiki/Clinical_trial>clinical populations</a> and <a href=https://en.wikipedia.org/wiki/Scientific_control>healthy controls</a>. While previous work has been quite successful at these classification tasks, the lack of interpretability of the computational metrics has made it unclear whether they are in fact measuring <a href=https://en.wikipedia.org/wiki/Thought_disorder>thought disorder</a>. In this paper, we dive into these <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a> to try to better understand what they reflect. While we find group differences between at-risk and healthy control populations, we also find that the <a href=https://en.wikipedia.org/wiki/Statistic>measures</a> mostly do not correlate with existing measures of <a href=https://en.wikipedia.org/wiki/Thought_disorder>thought disorder symptoms</a> (what they are intended to measure), but rather correlate with <a href=https://en.wikipedia.org/wiki/Linguistic_description>surface properties of the speech</a> (e.g., sentence length) and sociodemographic properties of the speaker (e.g., race). These results highlight the importance of considering interpretability and front and center as the field continues to grow. Ethical use of computational measures like those studied here especially in the high-stakes context of clinical care requires us to devote substantial attention to potential biases in our measures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.17/>Detecting Cognitive Distortions from Patient-Therapist Interactions</a></strong><br><a href=/people/s/sagarika-shreevastava/>Sagarika Shreevastava</a>
|
<a href=/people/p/peter-foltz/>Peter Foltz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--17><div class="card-body p-3 small">An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as <a href=https://en.wikipedia.org/wiki/Cognitive_distortion>cognitive distortions</a>. The aim of this project is to detect these <a href=https://en.wikipedia.org/wiki/Distortion>distortions</a> using <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. We compare and contrast different types of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> as well as different <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification algorithms</a> and explore the limitations of applying these techniques on a <a href=https://en.wikipedia.org/wiki/Sample_size_determination>small dataset</a>. We find that pre-trained Sentence-BERT embeddings to train an SVM classifier yields the best results with an F1-score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in <a href=https://en.wikipedia.org/wiki/Cognitive_distortion>cognitive distortions</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.18/>Evaluating Automatic Speech Recognition Quality and Its Impact on Counselor Utterance Coding</a></strong><br><a href=/people/d/do-june-min/>Do June Min</a>
|
<a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--18><div class="card-body p-3 small">Automatic speech recognition (ASR) is a crucial step in many natural language processing (NLP) applications, as often available data consists mainly of raw speech. Since the result of the ASR step is considered as a meaningful, informative input to later steps in the NLP pipeline, it is important to understand the behavior and failure mode of this step. In this work, we analyze the quality of ASR in the psychotherapy domain, using motivational interviewing conversations between therapists and clients. We conduct domain agnostic and domain-relevant evaluations using standard evaluation metrics and also identify domain-relevant keywords in the ASR output. Moreover, we empirically study the effect of mixing ASR and manual data during the training of a downstream NLP model, and also demonstrate how additional local context can help alleviate the error introduced by noisy ASR transcripts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.20/>Safeguarding against spurious AI-based predictions : The case of automated verbal memory assessment<span class=acl-fixed-case>AI</span>-based predictions: The case of automated verbal memory assessment</a></strong><br><a href=/people/c/chelsea-chandler/>Chelsea Chandler</a>
|
<a href=/people/p/peter-foltz/>Peter Foltz</a>
|
<a href=/people/a/alex-cohen/>Alex Cohen</a>
|
<a href=/people/t/terje-holmlund/>Terje Holmlund</a>
|
<a href=/people/b/brita-elvevag/>Brita Elvevåg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--20><div class="card-body p-3 small">A growing amount of psychiatric research incorporates machine learning and natural language processing methods, however findings have yet to be translated into actual clinical decision support systems. Many of these studies are based on relatively small datasets in homogeneous populations, which has the associated risk that the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> may not perform adequately on new data in real clinical practice. The nature of serious mental illness is that it is hard to define, hard to capture, and requires frequent monitoring, which leads to imperfect data where attribute and class noise are common. With the goal of an effective AI-mediated clinical decision support system, there must be computational safeguards placed on the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> used in order to avoid spurious predictions and thus allow humans to review data in the settings where <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are unstable or bound not to generalize. This paper describes two approaches to implementing safeguards : (1) the determination of cases in which <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are unstable by means of attribute and class based outlier detection and (2) finding the extent to which <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> show <a href=https://en.wikipedia.org/wiki/Inductive_bias>inductive bias</a>. These <a href=https://en.wikipedia.org/wiki/Safeguard>safeguards</a> are illustrated in the automated scoring of a story recall task via <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing methods</a>. With the integration of human-in-the-loop machine learning in the clinical implementation process, incorporating safeguards such as these into the models will offer patients increased protection from spurious predictions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.clpsych-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--clpsych-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.clpsych-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.clpsych-1.23/>Towards Understanding the Role of Gender in Deploying Social Media-Based Mental Health Surveillance Models</a></strong><br><a href=/people/e/eli-sherman/>Eli Sherman</a>
|
<a href=/people/k/keith-harrigian/>Keith Harrigian</a>
|
<a href=/people/c/carlos-aguirre/>Carlos Aguirre</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--clpsych-1--23><div class="card-body p-3 small">Spurred by advances in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, developing social media-based mental health surveillance models has received substantial recent attention. For these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to be maximally useful, it is necessary to understand how they perform on various subgroups, especially those defined in terms of <a href=https://en.wikipedia.org/wiki/Protected_group>protected characteristics</a>. In this paper we study the relationship between user demographics focusing on <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression</a>. Considering a population of Reddit users with known genders and depression statuses, we analyze the degree to which depression predictions are subject to biases along gender lines using domain-informed classifiers. We then study our models&#8217; parameters to gain qualitative insight into the differences in posting behavior across genders.</div></div></div><hr><div id=2021cmcl-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.cmcl-1/>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.0/>Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</a></strong><br><a href=/people/e/emmanuele-chersoni/>Emmanuele Chersoni</a>
|
<a href=/people/n/nora-hollenstein/>Nora Hollenstein</a>
|
<a href=/people/c/cassandra-l-jacobs/>Cassandra Jacobs</a>
|
<a href=/people/y/yohei-oseki/>Yohei Oseki</a>
|
<a href=/people/l/laurent-prevot/>Laurent Prévot</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.3/>Modeling Incremental Language Comprehension in the Brain with Combinatory Categorial Grammar<span class=acl-fixed-case>C</span>ombinatory <span class=acl-fixed-case>C</span>ategorial <span class=acl-fixed-case>G</span>rammar</a></strong><br><a href=/people/m/milos-stanojevic/>Miloš Stanojević</a>
|
<a href=/people/s/shohini-bhattasali/>Shohini Bhattasali</a>
|
<a href=/people/d/donald-dunagan/>Donald Dunagan</a>
|
<a href=/people/l/luca-campanelli/>Luca Campanelli</a>
|
<a href=/people/m/mark-steedman/>Mark Steedman</a>
|
<a href=/people/j/jonathan-brennan/>Jonathan Brennan</a>
|
<a href=/people/j/john-hale/>John Hale</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--3><div class="card-body p-3 small">Hierarchical sentence structure plays a role in word-by-word human sentence comprehension, but it remains unclear how best to characterize this <a href=https://en.wikipedia.org/wiki/Structure>structure</a> and unknown how exactly it would be recognized in a step-by-step process model. With a view towards sharpening this picture, we model the time course of <a href=https://en.wikipedia.org/wiki/Hemodynamics>hemodynamic activity</a> within the brain during an extended episode of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>naturalistic language comprehension</a> using Combinatory Categorial Grammar (CCG). CCG has well-defined incremental parsing algorithms, surface compositional semantics, and can explain long-range dependencies as well as complicated cases of coordination. We find that CCG-derived predictors improve a regression model of fMRI time course in six language-relevant brain regions, over and above <a href=https://en.wikipedia.org/wiki/Prediction>predictors</a> derived from context-free phrase structure. Adding a special Revealing operator to CCG parsing, one designed to handle right-adjunction, improves the fit in three of these regions. This evidence for CCG from <a href=https://en.wikipedia.org/wiki/Neuroimaging>neuroimaging</a> bolsters the more general case for mildly context-sensitive grammars in the cognitive science of language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cmcl-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.5/>That Looks Hard : Characterizing Linguistic Complexity in Humans and Language Models</a></strong><br><a href=/people/g/gabriele-sarti/>Gabriele Sarti</a>
|
<a href=/people/d/dominique-brunato/>Dominique Brunato</a>
|
<a href=/people/f/felice-dellorletta/>Felice Dell’Orletta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--5><div class="card-body p-3 small">This paper investigates the relationship between two complementary perspectives in the human assessment of sentence complexity and how they are modeled in a neural language model (NLM). The first <a href=https://en.wikipedia.org/wiki/Point_of_view_(philosophy)>perspective</a> takes into account multiple online behavioral metrics obtained from eye-tracking recordings. The second one concerns the offline perception of complexity measured by explicit <a href=https://en.wikipedia.org/wiki/Judgment_(mathematical_logic)>human judgments</a>. Using a broad spectrum of linguistic features modeling lexical, morpho-syntactic, and syntactic properties of sentences, we perform a comprehensive analysis of linguistic phenomena associated with the two complexity viewpoints and report similarities and differences. We then show the effectiveness of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> when explicitly leveraged by a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a> for predicting sentence complexity and compare its results with the ones obtained by a fine-tuned neural language model. We finally probe the NLM&#8217;s linguistic competence before and after <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>, highlighting how linguistic information encoded in representations changes when the model learns to predict <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.cmcl-1.8.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.8/>LangResearchLab_NC at CMCL2021 Shared Task : Predicting Gaze Behaviour Using Linguistic Features and Tree Regressors<span class=acl-fixed-case>L</span>ang<span class=acl-fixed-case>R</span>esearch<span class=acl-fixed-case>L</span>ab_<span class=acl-fixed-case>NC</span> at <span class=acl-fixed-case>CMCL</span>2021 Shared Task: Predicting Gaze Behaviour Using Linguistic Features and Tree Regressors</a></strong><br><a href=/people/r/raksha-agarwal/>Raksha Agarwal</a>
|
<a href=/people/n/niladri-chatterjee/>Niladri Chatterjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--8><div class="card-body p-3 small">Analysis of gaze data behaviour has gained momentum in recent years for different NLP applications. The present paper aims at modelling gaze data behaviour of tokens in the context of a sentence. We have experimented with various <a href=https://en.wikipedia.org/wiki/Regression_analysis>Machine Learning Regression Algorithms</a> on a <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature space</a> comprising the linguistic features of the target tokens for prediction of five Eye-Tracking features. CatBoost Regressor performed the best and achieved fourth position in terms of MAE based accuracy measurement for the ZuCo Dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cmcl-1.9" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.9/>TorontoCL at CMCL 2021 Shared Task : RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction<span class=acl-fixed-case>T</span>oronto<span class=acl-fixed-case>CL</span> at <span class=acl-fixed-case>CMCL</span> 2021 Shared Task: <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a with Multi-Stage Fine-Tuning for Eye-Tracking Prediction</a></strong><br><a href=/people/b/bai-li/>Bai Li</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--9><div class="card-body p-3 small">Eye movement data during reading is a useful source of information for understanding <a href=https://en.wikipedia.org/wiki/Sentence_processing>language comprehension processes</a>. In this paper, we describe our submission to the CMCL 2021 shared task on predicting human reading patterns. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> uses RoBERTa with a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression layer</a> to predict 5 eye-tracking features. We train the model in two stages : we first fine-tune on the Provo corpus (another eye-tracking dataset), then fine-tune on the task data. We compare different Transformer models and apply ensembling methods to improve the performance. Our final submission achieves a MAE score of 3.929, ranking 3rd place out of 13 teams that participated in this shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.10/>LAST at CMCL 2021 Shared Task : Predicting Gaze Data During Reading with a Gradient Boosting Decision Tree Approach<span class=acl-fixed-case>LAST</span> at <span class=acl-fixed-case>CMCL</span> 2021 Shared Task: Predicting Gaze Data During Reading with a Gradient Boosting Decision Tree Approach</a></strong><br><a href=/people/y/yves-bestgen/>Yves Bestgen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--10><div class="card-body p-3 small">A LightGBM model fed with target word lexical characteristics and features obtained from word frequency lists, psychometric data and bigram association measures has been optimized for the 2021 CMCL Shared Task on Eye-Tracking Data Prediction. It obtained the best performance of all teams on two of the five eye-tracking measures to predict, allowing <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to rank first on the official challenge criterion and to outperform all <a href=https://en.wikipedia.org/wiki/Deep_learning>deep-learning based systems</a> participating in the challenge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.12/>PIHKers at CMCL 2021 Shared Task : Cosine Similarity and Surprisal to Predict Human Reading Patterns.<span class=acl-fixed-case>PIHK</span>ers at <span class=acl-fixed-case>CMCL</span> 2021 Shared Task: Cosine Similarity and Surprisal to Predict Human Reading Patterns.</a></strong><br><a href=/people/l/lavinia-salicchi/>Lavinia Salicchi</a>
|
<a href=/people/a/alessandro-lenci/>Alessandro Lenci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--12><div class="card-body p-3 small">Eye-tracking psycholinguistic studies have revealed that context-word semantic coherence and <a href=https://en.wikipedia.org/wiki/Predictability>predictability</a> influence <a href=https://en.wikipedia.org/wiki/Language_processing_in_the_brain>language processing</a>. In this paper we show our approach to predict eye-tracking features from the ZuCo dataset for the shared task of the Cognitive Modeling and Computational Linguistics (CMCL2021) workshop. Using both cosine similarity and surprisal within a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression model</a>, we significantly improved the baseline Mean Absolute Error computed among five eye-tracking features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.cmcl-1.13.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.13/>TALEP at CMCL 2021 Shared Task : Non Linear Combination of Low and High-Level Features for Predicting Eye-Tracking Data<span class=acl-fixed-case>TALEP</span> at <span class=acl-fixed-case>CMCL</span> 2021 Shared Task: Non Linear Combination of Low and High-Level Features for Predicting Eye-Tracking Data</a></strong><br><a href=/people/f/franck-dary/>Franck Dary</a>
|
<a href=/people/a/alexis-nasr/>Alexis Nasr</a>
|
<a href=/people/a/abdellah-fourtassi/>Abdellah Fourtassi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--13><div class="card-body p-3 small">In this paper we describe our contribution to the CMCL 2021 Shared Task, which consists in predicting 5 different eye tracking variables from English tokenized text. Our approach is based on a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> that combines both raw textual features we extracted from the text and parser-based features that include linguistic predictions (e.g. part of speech) and complexity metrics (e.g., entropy of parsing). We found that both the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> we considered as well as the architecture of the neural model that combined these <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> played a role in the overall performance. Our system achieved relatively high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the test data of the challenge and was ranked 2nd out of 13 competing teams and a total of 30 submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.cmcl-1.14.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.14/>MTL782_IITD at CMCL 2021 Shared Task : Prediction of Eye-Tracking Features Using BERT Embeddings and Linguistic Features<span class=acl-fixed-case>MTL</span>782_<span class=acl-fixed-case>IITD</span> at <span class=acl-fixed-case>CMCL</span> 2021 Shared Task: Prediction of Eye-Tracking Features Using <span class=acl-fixed-case>BERT</span> Embeddings and Linguistic Features</a></strong><br><a href=/people/s/shivani-choudhary/>Shivani Choudhary</a>
|
<a href=/people/k/kushagri-tandon/>Kushagri Tandon</a>
|
<a href=/people/r/raksha-agarwal/>Raksha Agarwal</a>
|
<a href=/people/n/niladri-chatterjee/>Niladri Chatterjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--14><div class="card-body p-3 small">Reading and comprehension are quintessentially cognitive tasks. Eye movement acts as a surrogate to understand which part of a sentence is critical to the process of comprehension. The aim of the shared task is to predict five eye-tracking features for a given word of the input sentence. We experimented with several models based on LGBM (Light Gradient Boosting Machine) Regression, ANN (Artificial Neural Network), and CNN (Convolutional Neural Network), using BERT embeddings and some combination of linguistic features. Our submission using <a href=https://en.wikipedia.org/wiki/CNN>CNN</a> achieved an average MAE of 4.0639 and ranked 7th in the shared task. The average MAE was further lowered to 3.994 in post-task evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cmcl-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.18/>Enhancing Cognitive Models of Emotions with <a href=https://en.wikipedia.org/wiki/Representation_learning>Representation Learning</a></a></strong><br><a href=/people/y/yuting-guo/>Yuting Guo</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--18><div class="card-body p-3 small">We present a novel deep learning-based framework to generate embedding representations of fine-grained emotions that can be used to computationally describe psychological models of emotions. Our framework integrates a contextualized embedding encoder with a multi-head probing model that enables to interpret dynamically learned representations optimized for an emotion classification task. Our model is evaluated on the Empathetic Dialogue dataset and shows the state-of-the-art result for classifying 32 emotions. Our layer analysis can derive an emotion graph to depict hierarchical relations among the emotions. Our emotion representations can be used to generate an emotion wheel directly comparable to the one from Plutchik&#8217;s model, and also augment the values of missing emotions in the <a href=https://en.wikipedia.org/wiki/PAD_emotional_state_model>PAD emotional state model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.cmcl-1.20.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.cmcl-1.20.OptionalSupplementaryData.pdf data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.20/>Clause Final Verb Prediction in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> : Evidence for Noisy Channel Model of Communication<span class=acl-fixed-case>H</span>indi: Evidence for Noisy Channel Model of Communication</a></strong><br><a href=/people/k/kartik-sharma/>Kartik Sharma</a>
|
<a href=/people/n/niyati-bafna/>Niyati Bafna</a>
|
<a href=/people/s/samar-husain/>Samar Husain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--20><div class="card-body p-3 small">Verbal prediction has been shown to be critical during online comprehension of Subject-Object-Verb (SOV) languages. In this work we present three <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> to predict clause final verbs in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> given its prior arguments. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> differ in their use of prior context during the prediction process the context is either noisy or noise-free. Model predictions are compared with the sentence completion data obtained from <a href=https://en.wikipedia.org/wiki/Hindi>Hindi native speakers</a>. Results show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that assume noisy context outperform the noise-free model. In particular, a lossy context model that assumes prior context to be affected by <a href=https://en.wikipedia.org/wiki/Predictability>predictability</a> and recency captures the distribution of the predicted verb class and error sources best. The success of the predictability-recency lossy context model is consistent with the noisy channel hypothesis for <a href=https://en.wikipedia.org/wiki/Sentence_processing>sentence comprehension</a> and supports the idea that the reconstruction of the context during prediction is driven by prior linguistic exposure. These results also shed light on the nature of the <a href=https://en.wikipedia.org/wiki/Noise>noise</a> that affects the reconstruction process. Overall the results pose a challenge to the adaptability hypothesis that assumes use of noise-free preverbal context for robust verbal prediction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.cmcl-1.23.OptionalSupplementaryCode.zip data-toggle=tooltip data-placement=top title="Optional supplementary code"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.23/>Sentence Complexity in Context</a></strong><br><a href=/people/b/benedetta-iavarone/>Benedetta Iavarone</a>
|
<a href=/people/d/dominique-brunato/>Dominique Brunato</a>
|
<a href=/people/f/felice-dellorletta/>Felice Dell’Orletta</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--23><div class="card-body p-3 small">We study the influence of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> on how humans evaluate the <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of a sentence in English. We collect a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of sentences, where each sentence is rated for perceived complexity within different contextual windows. We carry out an in-depth analysis to detect which <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> correlate more with complexity judgments and with the degree of agreement among annotators. We train several regression models, using either explicit linguistic features or contextualized word embeddings, to predict the mean complexity values assigned to sentences in the different contextual windows, as well as their standard deviation. Results show that models leveraging explicit features capturing morphosyntactic and syntactic phenomena perform always better, especially when they have access to <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> extracted from all contextual sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.cmcl-1.24" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.24/>Evaluating the Acquisition of Semantic Knowledge from Cross-situational Learning in Artificial Neural Networks</a></strong><br><a href=/people/m/mitja-nikolaus/>Mitja Nikolaus</a>
|
<a href=/people/a/abdellah-fourtassi/>Abdellah Fourtassi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--24><div class="card-body p-3 small">When learning their native language, children acquire the meanings of words and sentences from highly ambiguous input without much explicit supervision. One possible learning mechanism is cross-situational learning, which has been successfully tested in laboratory experiments with children. Here we use <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>Artificial Neural Networks</a> to test if this mechanism scales up to more natural language and visual scenes using a large dataset of crowd-sourced images with corresponding descriptions. We evaluate <a href=https://en.wikipedia.org/wiki/Learning>learning</a> using a series of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> inspired by <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> commonly used in laboratory studies of <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a>. We show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> acquires rich semantic knowledge both at the word- and sentence-level, mirroring the patterns and trajectory of learning in early childhood. Our work highlights the usefulness of low-level co-occurrence statistics across modalities in facilitating the early acquisition of higher-level semantic knowledge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.25/>Representation and Pre-Activation of Lexical-Semantic Knowledge in Neural Language Models</a></strong><br><a href=/people/s/steven-derby/>Steven Derby</a>
|
<a href=/people/p/paul-miller/>Paul Miller</a>
|
<a href=/people/b/barry-devereux/>Barry Devereux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--25><div class="card-body p-3 small">In this paper, we perform a systematic analysis of how closely the intermediate layers from LSTM and trans former language models correspond to human semantic knowledge. Furthermore, in order to make more meaningful comparisons with theories of human language comprehension in <a href=https://en.wikipedia.org/wiki/Psycholinguistics>psycholinguistics</a>, we focus on two key stages where the meaning of a particular target word may arise : immediately before the word&#8217;s presentation to the model (comparable to forward inferencing), and immediately after the word token has been input into the network. Our results indicate that the transformer models are better at capturing semantic knowledge relating to lexical concepts, both during <a href=https://en.wikipedia.org/wiki/Word_prediction>word prediction</a> and when retention is required.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.cmcl-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--cmcl-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.cmcl-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.cmcl-1.27/>Graph-theoretic Properties of the Class of Phonological Neighbourhood Networks</a></strong><br><a href=/people/r/rory-turnbull/>Rory Turnbull</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--cmcl-1--27><div class="card-body p-3 small">This paper concerns the structure of phonological neighbourhood networks, which are a graph-theoretic representation of the phonological lexicon. These networks represent each word as a node and links are placed between words which are phonological neighbours, usually defined as a string edit distance of one. Phonological neighbourhood networks have been used to study many aspects of the <a href=https://en.wikipedia.org/wiki/Mental_lexicon>mental lexicon</a> and psycholinguistic theories of speech production and perception. This paper offers preliminary graph-theoretic observations about phonological neighbourhood networks considered as a class. To aid this exploration, this paper introduces the concept of the hyperlexicon, the network consisting of all possible words for a given symbol set and their <a href=https://en.wikipedia.org/wiki/Neighbourhood_(mathematics)>neighbourhood relations</a>. The construction of the hyperlexicon is discussed, and basic properties are derived. This work is among the first to directly address the nature of phonological neighbourhood networks from an analytic perspective.</div></div></div><hr><div id=2021dash-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.dash-1/>Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dash-1.0/>Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances</a></strong><br><a href=/people/e/eduard-dragut/>Eduard Dragut</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/l/lucian-popa/>Lucian Popa</a>
|
<a href=/people/s/slobodan-vucetic/>Slobodan Vucetic</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dash-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dash-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dash-1.3/>ViziTex : Interactive Visual Sense-Making of Text Corpora<span class=acl-fixed-case>V</span>izi<span class=acl-fixed-case>T</span>ex: Interactive Visual Sense-Making of Text Corpora</a></strong><br><a href=/people/n/natraj-raman/>Natraj Raman</a>
|
<a href=/people/s/sameena-shah/>Sameena Shah</a>
|
<a href=/people/t/tucker-balch/>Tucker Balch</a>
|
<a href=/people/m/manuela-veloso/>Manuela Veloso</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dash-1--3><div class="card-body p-3 small">Information visualization is critical to <a href=https://en.wikipedia.org/wiki/Analytical_reasoning>analytical reasoning</a> and <a href=https://en.wikipedia.org/wiki/Epistemology>knowledge discovery</a>. We present an interactive studio that integrates perceptive visualization techniques with powerful text analytics algorithms to assist humans in sense-making of large complex text corpora. The novel visual representations introduced here encode the features delivered by modern text mining models using advanced metaphors such as <a href=https://en.wikipedia.org/wiki/Hypergraph>hypergraphs</a>, nested topologies and <a href=https://en.wikipedia.org/wiki/Tessellation>tessellated planes</a>. They enhance human-computer interaction experience for various tasks such as summarization, exploration, organization and labeling of documents. We demonstrate the ability of the visuals to surface the structure, relations and concepts from documents across different domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dash-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dash-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dash-1.7/>Bridging Multi-disciplinary Collaboration Challenges in ML Development via Domain Knowledge Elicitation<span class=acl-fixed-case>ML</span> Development via Domain Knowledge Elicitation</a></strong><br><a href=/people/s/soya-park/>Soya Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dash-1--7><div class="card-body p-3 small">Building a <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> in a sophisticated domain is a time-consuming process, partially due to the steep learning curve of <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> for <a href=https://en.wikipedia.org/wiki/Data_science>data scientists</a>. We introduce Ziva, an interface for supporting <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> from domain experts to data scientists in two ways : (1) a concept creation interface where domain experts extract important concept of the domain and (2) five kinds of justification elicitation interfaces that solicit elicitation how the domain concept are expressed in data instances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dash-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dash-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dash-1.9/>Towards integrated, interactive, and extensible text data analytics with Leam</a></strong><br><a href=/people/p/peter-griggs/>Peter Griggs</a>
|
<a href=/people/c/cagatay-demiralp/>Cagatay Demiralp</a>
|
<a href=/people/s/sajjadur-rahman/>Sajjadur Rahman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dash-1--9><div class="card-body p-3 small">From <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> to <a href=https://en.wikipedia.org/wiki/Review>product reviews</a>, <a href=https://en.wikipedia.org/wiki/Plain_text>text</a> is ubiquitous on the web and often contains valuable information for both enterprises and consumers. However, the online text is generally noisy and incomplete, requiring users to process and analyze the data to extract insights. While there are systems effective for different stages of <a href=https://en.wikipedia.org/wiki/Text_mining>text analysis</a>, users lack extensible platforms to support interactive text analysis workflows end-to-end. To facilitate integrated text analytics, we introduce LEAM, which aims at combining the strengths of <a href=https://en.wikipedia.org/wiki/Spreadsheet>spreadsheets</a>, <a href=https://en.wikipedia.org/wiki/Computational_notebook>computational notebooks</a>, and interactive visualizations. LEAM supports interactive analysis via GUI-based interactions and provides a declarative specification language, implemented based on a visual text algebra, to enable user-guided analysis. We evaluate LEAM through two <a href=https://en.wikipedia.org/wiki/Case_study>case studies</a> using two popular Kaggle text analytics workflows to understand the strengths and weaknesses of the <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dash-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dash-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dash-1.10/>Data Cleaning Tools for Token Classification Tasks</a></strong><br><a href=/people/k/karthik-muthuraman/>Karthik Muthuraman</a>
|
<a href=/people/f/frederick-reiss/>Frederick Reiss</a>
|
<a href=/people/h/hong-xu/>Hong Xu</a>
|
<a href=/people/b/bryan-cutler/>Bryan Cutler</a>
|
<a href=/people/z/zachary-eichenberger/>Zachary Eichenberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dash-1--10><div class="card-body p-3 small">Human-in-the-loop systems for cleaning NLP training data rely on automated sieves to isolate potentially-incorrect labels for manual review. We have developed a novel technique for flagging potentially-incorrect labels with high sensitivity in named entity recognition corpora. We incorporated our <a href=https://en.wikipedia.org/wiki/Sieve_theory>sieve</a> into an end-to-end system for cleaning NLP corpora, implemented as a modular collection of Jupyter notebooks built on extensions to the Pandas DataFrame library. We used this system to identify incorrect labels in the CoNLL-2003 corpus for English-language named entity recognition (NER), one of the most influential corpora for NER model research. Unlike previous work that only looked at a subset of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>&#8217;s validation fold, our automated sieve enabled us to examine the entire corpus in depth. Across the entire CoNLL-2003 corpus, we identified over 1300 incorrect labels (out of 35089 in the corpus). We have published our corrections, along with the code we used in our experiments. We are developing a repeatable version of the process we used on the CoNLL-2003 corpus as an open-source library.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dash-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dash-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dash-1.11/>Building Low-Resource NER Models Using Non-Speaker Annotations<span class=acl-fixed-case>NER</span> Models Using Non-Speaker Annotations</a></strong><br><a href=/people/t/tatiana-tsygankova/>Tatiana Tsygankova</a>
|
<a href=/people/f/francesca-marini/>Francesca Marini</a>
|
<a href=/people/s/stephen-mayhew/>Stephen Mayhew</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dash-1--11><div class="card-body p-3 small">In low-resource natural language processing (NLP), the key problems are a lack of target language training data, and a lack of native speakers to create it. Cross-lingual methods have had notable success in addressing these concerns, but in certain common circumstances, such as insufficient pre-training corpora or languages far from the source language, their performance suffers. In this work we propose a complementary approach to building low-resource Named Entity Recognition (NER) models using non-speaker (NS) annotations, provided by annotators with no prior experience in the target language. We recruit 30 participants in a carefully controlled annotation experiment with <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. We show that use of NS annotators produces results that are consistently on par or better than cross-lingual methods built on modern contextual representations, and have the potential to outperform with additional effort. We conclude with observations of common annotation patterns and recommended implementation practices, and motivate how NS annotations can be used in addition to prior methods for improved performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dash-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dash-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.dash-1.13" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.dash-1.13/>CrossCheck : Rapid, Reproducible, and Interpretable Model Evaluation<span class=acl-fixed-case>C</span>ross<span class=acl-fixed-case>C</span>heck: Rapid, Reproducible, and Interpretable Model Evaluation</a></strong><br><a href=/people/d/dustin-arendt/>Dustin Arendt</a>
|
<a href=/people/z/zhuanyi-shaw/>Zhuanyi Shaw</a>
|
<a href=/people/p/prasha-shrestha/>Prasha Shrestha</a>
|
<a href=/people/e/ellyn-ayton/>Ellyn Ayton</a>
|
<a href=/people/m/maria-glenski/>Maria Glenski</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dash-1--13><div class="card-body p-3 small">Evaluation beyond aggregate performance metrics, e.g. F1-score, is crucial to both establish an appropriate level of trust in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> and identify avenues for future <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> improvements. In this paper we demonstrate CrossCheck, an interactive capability for rapid cross-model comparison and reproducible error analysis. We describe the tool, discuss design and implementation details, and present three NLP use cases named entity recognition, <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, and clickbait detection that show the benefits of using the tool for model evaluation. CrossCheck enables users to make informed decisions when choosing between multiple models, identify when the models are correct and for which examples, investigate whether the models are making the same mistakes as humans, evaluate models&#8217; generalizability and highlight models&#8217; limitations, strengths and weaknesses. Furthermore, CrossCheck is implemented as a <a href=https://en.wikipedia.org/wiki/Jupyter>Jupyter widget</a>, which allows for rapid and convenient integration into existing model development workflows.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dash-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dash-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.dash-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.dash-1.14/>TopGuNN : Fast NLP Training Data Augmentation using Large Corpora<span class=acl-fixed-case>T</span>op<span class=acl-fixed-case>G</span>u<span class=acl-fixed-case>NN</span>: Fast <span class=acl-fixed-case>NLP</span> Training Data Augmentation using Large Corpora</a></strong><br><a href=/people/r/rebecca-iglesias-flores/>Rebecca Iglesias-Flores</a>
|
<a href=/people/m/megha-mishra/>Megha Mishra</a>
|
<a href=/people/a/ajay-patel/>Ajay Patel</a>
|
<a href=/people/a/akanksha-malhotra/>Akanksha Malhotra</a>
|
<a href=/people/r/reno-kriz/>Reno Kriz</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dash-1--14><div class="card-body p-3 small">Acquiring training data for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing systems</a> can be expensive and time-consuming. Given a few training examples crafted by experts, large corpora can be mined for thousands of semantically similar examples that provide useful variability to improve model generalization. We present TopGuNN, a fast contextualized k-NN retrieval system that can efficiently index and search over contextual embeddings generated from large corpora. TopGuNN is demonstrated for a training data augmentation use case over the Gigaword corpus. Using approximate k-NN and an efficient <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a>, TopGuNN performs queries over an <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>embedding space</a> of 4.63 TB (approximately 1.5B embeddings) in less than a day.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.dash-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--dash-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.dash-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.dash-1.16/>A <a href=https://en.wikipedia.org/wiki/Computational_model>Computational Model</a> for Interactive Transcription</a></strong><br><a href=/people/w/william-lane/>William Lane</a>
|
<a href=/people/m/mat-bettinson/>Mat Bettinson</a>
|
<a href=/people/s/steven-bird/>Steven Bird</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--dash-1--16><div class="card-body p-3 small">Transcribing low resource languages can be challenging in the absence of a good lexicon and trained transcribers. Accordingly, we seek a way to enable interactive transcription whereby the machine amplifies human efforts. This paper presents a <a href=https://en.wikipedia.org/wiki/Data_model>data model</a> and a <a href=https://en.wikipedia.org/wiki/Systems_architecture>system architecture</a> for interactive transcription, supporting multiple modes of <a href=https://en.wikipedia.org/wiki/Interactivity>interactivity</a>, increasing the likelihood of finding tasks that engage local participation in language work. The approach also supports other <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> which are useful in our context, including spoken document retrieval and <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learning</a>.</div></div></div><hr><div id=2021deelio-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.deelio-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.deelio-1/>Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.deelio-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.deelio-1.0/>Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</a></strong><br><a href=/people/e/eneko-agirre/>Eneko Agirre</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.deelio-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--deelio-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.deelio-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.deelio-1.4/>Augmenting Topic Aware Knowledge-Grounded Conversations with Dynamic Built Knowledge Graphs</a></strong><br><a href=/people/j/junjie-wu/>Junjie Wu</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--deelio-1--4><div class="card-body p-3 small">Dialog topic management and background knowledge selection are essential factors for the success of knowledge-grounded open-domain conversations. However, existing models are primarily performed with symmetric knowledge bases or stylized with pre-defined roles between conversational partners, while people usually have their own knowledge before a real chit-chat. To address this problem, we propose a dynamic knowledge graph-based topical conversation model (DKGT). Given a dialog history context, our model first builds knowledge graphs from the context as an imitation of human&#8217;s ability to form logical relationships between known and unknown topics during a conversation. This logical information will be fed into a topic predictor to promote topic management, then facilitate background knowledge selection and response generation. To the best of our knowledge, this is the first attempt to dynamically form <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> between chatting topics to assist dialog topic management during a conversation. Experimental results manifest that our model can properly schedule conversational topics and pick suitable knowledge to generate informative responses comparing to several strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.deelio-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--deelio-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.deelio-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.deelio-1.5/>What Makes My Model Perplexed? A Linguistic Investigation on Neural Language Models Perplexity</a></strong><br><a href=/people/a/alessio-miaschi/>Alessio Miaschi</a>
|
<a href=/people/d/dominique-brunato/>Dominique Brunato</a>
|
<a href=/people/f/felice-dellorletta/>Felice Dell’Orletta</a>
|
<a href=/people/g/giulia-venturi/>Giulia Venturi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--deelio-1--5><div class="card-body p-3 small">This paper presents an investigation aimed at studying how the linguistic structure of a sentence affects the perplexity of two of the most popular Neural Language Models (NLMs), BERT and GPT-2. We first compare the sentence-level likelihood computed with BERT and the GPT-2&#8217;s perplexity showing that the two <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> are correlated. In addition, we exploit linguistic features capturing a wide set of morpho-syntactic and syntactic phenomena showing how they contribute to predict the perplexity of the two NLMs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.deelio-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--deelio-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.deelio-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.deelio-1.12.OptionalSupplementaryData.pdf data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.deelio-1.12/>What BERTs and GPTs know about your brand? Probing contextual language models for affect associations<span class=acl-fixed-case>BERT</span>s and <span class=acl-fixed-case>GPT</span>s know about your brand? Probing contextual language models for affect associations</a></strong><br><a href=/people/v/vivek-srivastava/>Vivek Srivastava</a>
|
<a href=/people/s/stephen-pilli/>Stephen Pilli</a>
|
<a href=/people/s/savita-bhat/>Savita Bhat</a>
|
<a href=/people/n/niranjan-pedanekar/>Niranjan Pedanekar</a>
|
<a href=/people/s/shirish-karande/>Shirish Karande</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--deelio-1--12><div class="card-body p-3 small">Investigating brand perception is fundamental to <a href=https://en.wikipedia.org/wiki/Marketing_strategy>marketing strategies</a>. In this regard, <a href=https://en.wikipedia.org/wiki/Brand_image>brand image</a>, defined by a set of attributes (Aaker, 1997), is recognized as a key element in indicating how a brand is perceived by various stakeholders such as consumers and competitors. Traditional <a href=https://en.wikipedia.org/wiki/Brand_management>approaches</a> (e.g., surveys) to monitor brand perceptions are time-consuming and inefficient. In the era of <a href=https://en.wikipedia.org/wiki/Digital_marketing>digital marketing</a>, both brand managers and consumers engage with a vast amount of <a href=https://en.wikipedia.org/wiki/Digital_marketing>digital marketing content</a>. The exponential growth of <a href=https://en.wikipedia.org/wiki/Digital_content>digital content</a> has propelled the emergence of pre-trained language models such as BERT and GPT as essential tools in solving myriads of challenges with textual data. This paper seeks to investigate the extent of brand perceptions (i.e., brand and image attribute associations) these <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> encode. We believe that any kind of bias for a brand and attribute pair may influence customer-centric downstream tasks such as <a href=https://en.wikipedia.org/wiki/Recommender_system>recommender systems</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, and <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering</a>, e.g., suggesting a specific brand consistently when queried for innovative products. We use <a href=https://en.wikipedia.org/wiki/Data>synthetic data</a> and real-life data and report comparison results for five contextual LMs, viz. BERT, RoBERTa, DistilBERT, ALBERT and BART.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.deelio-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--deelio-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.deelio-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.deelio-1.13/>Attention vs non-attention for a Shapley-based explanation method</a></strong><br><a href=/people/t/tom-kersten/>Tom Kersten</a>
|
<a href=/people/h/hugh-mee-wong/>Hugh Mee Wong</a>
|
<a href=/people/j/jaap-jumelet/>Jaap Jumelet</a>
|
<a href=/people/d/dieuwke-hupkes/>Dieuwke Hupkes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--deelio-1--13><div class="card-body p-3 small">The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. The extent to which such methods that are often proposed and tested in the domain of <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> are appropriate to address the explainability challenges in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> is yet relatively unexplored. In this work, we consider Contextual Decomposition (CD) a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models and we test the extent to which it is useful for models that contain attention operations. To this end, we extend CD to cover the <a href=https://en.wikipedia.org/wiki/Operation_(mathematics)>operations</a> necessary for attention-based models. We then compare how long distance subject-verb relationships are processed by models with and without <a href=https://en.wikipedia.org/wiki/Attention>attention</a>, considering a number of different syntactic structures in two different languages : <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a>. Our experiments confirm that CD can successfully be applied for attention-based models as well, providing an alternative Shapley-based attribution method for modern <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. In particular, using <a href=https://en.wikipedia.org/wiki/Compact_disc>CD</a>, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models.</div></div></div><hr><div id=2021maiworkshop-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.maiworkshop-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.maiworkshop-1/>Proceedings of the Third Workshop on Multimodal Artificial Intelligence</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.maiworkshop-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.maiworkshop-1.0/>Proceedings of the Third Workshop on Multimodal Artificial Intelligence</a></strong><br><a href=/people/a/amir-zadeh/>Amir Zadeh</a>
|
<a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/p/paul-pu-liang/>Paul Pu Liang</a>
|
<a href=/people/c/candace-ross/>Candace Ross</a>
|
<a href=/people/r/ruslan-salakhutdinov/>Ruslan Salakhutdinov</a>
|
<a href=/people/s/soujanya-poria/>Soujanya Poria</a>
|
<a href=/people/e/erik-cambria/>Erik Cambria</a>
|
<a href=/people/k/kelly-shi/>Kelly Shi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.maiworkshop-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--maiworkshop-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.maiworkshop-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.maiworkshop-1.5/>Multi Task Learning based Framework for Multimodal Classification</a></strong><br><a href=/people/d/danting-zeng/>Danting Zeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--maiworkshop-1--5><div class="card-body p-3 small">Large-scale multi-modal classification aim to distinguish between different multi-modal data, and it has drawn dramatically attentions since last decade. In this paper, we propose a multi-task learning-based framework for the multimodal classification task, which consists of two branches : multi-modal autoencoder branch and attention-based multi-modal modeling branch. Multi-modal autoencoder can receive multi-modal features and obtain the interactive information which called multi-modal encoder feature, and use this feature to reconstitute all the input data. Besides, multi-modal encoder feature can be used to enrich the raw dataset, and improve the performance of downstream tasks (such as classification task). As for attention-based multimodal modeling branch, we first employ attention mechanism to make the model focused on important features, then we use the multi-modal encoder feature to enrich the input information, achieve a better performance. We conduct extensive experiments on different dataset, the results demonstrate the effectiveness of proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.maiworkshop-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--maiworkshop-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.maiworkshop-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.maiworkshop-1.10/>A Package for Learning on Tabular and Text Data with Transformers</a></strong><br><a href=/people/k/ken-gu/>Ken Gu</a>
|
<a href=/people/a/akshay-budhkar/>Akshay Budhkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--maiworkshop-1--10><div class="card-body p-3 small">Recent progress in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> has led to Transformer architectures becoming the predominant model used for natural language tasks. However, in many real- world datasets, additional modalities are included which the <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a> does not directly leverage. We present Multimodal- Toolkit, an open-source Python package to incorporate text and tabular (categorical and numerical) data with Transformers for downstream applications. Our toolkit integrates well with Hugging Face&#8217;s existing API such as tokenization and the model hub which allows easy download of different pre-trained models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.maiworkshop-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--maiworkshop-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.maiworkshop-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.maiworkshop-1.13/>Learning to Select Question-Relevant Relations for Visual Question Answering</a></strong><br><a href=/people/j/jaewoong-lee/>Jaewoong Lee</a>
|
<a href=/people/h/heejoon-lee/>Heejoon Lee</a>
|
<a href=/people/h/hwanhee-lee/>Hwanhee Lee</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--maiworkshop-1--13><div class="card-body p-3 small">Previous existing visual question answering (VQA) systems commonly use graph neural networks(GNNs) to extract visual relationships such as semantic relations or spatial relations. However, studies that use GNNs typically ignore the importance of each relation and simply concatenate outputs from multiple relation encoders. In this paper, we propose a novel layer architecture that fuses multiple visual relations through an attention mechanism to address this issue. Specifically, we develop a model that uses question embedding and joint embedding of the encoders to obtain dynamic attention weights with regard to the type of questions. Using the learnable attention weights, the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can efficiently use the necessary visual relation features for a given question. Experimental results on the VQA 2.0 dataset demonstrate that the proposed model outperforms existing graph attention network-based architectures. Additionally, we visualize the attention weight and show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> assigns a higher weight to relations that are more relevant to the question.</div></div></div><hr><div id=2021nlp4if-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4if-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.nlp4if-1/>Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4if-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4if-1.0/>Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda</a></strong><br><a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/g/giovanni-da-san-martino/>Giovanni Da San Martino</a>
|
<a href=/people/c/chris-leberknight/>Chris Leberknight</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4if-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4if-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4if-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4if-1.1/>Identifying Automatically Generated Headlines using Transformers</a></strong><br><a href=/people/a/antonis-maronikolakis/>Antonis Maronikolakis</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a>
|
<a href=/people/m/mark-stevenson/>Mark Stevenson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4if-1--1><div class="card-body p-3 small">False information spread via the <a href=https://en.wikipedia.org/wiki/Internet>internet</a> and <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> influences public opinion and user activity, while <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> enable fake content to be generated faster and more cheaply than had previously been possible. In the not so distant future, identifying fake content generated by <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> will play a key role in protecting users from <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a>. To this end, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> containing human and computer-generated headlines was created and a user study indicated that humans were only able to identify the <a href=https://en.wikipedia.org/wiki/Fake_news>fake headlines</a> in 47.8 % of the cases. However, the most accurate automatic approach, transformers, achieved an overall accuracy of 85.7 %, indicating that content generated from <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> can be filtered out accurately.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4if-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4if-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4if-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4if-1.3/>Improving Cross-Domain Hate Speech Detection by Reducing the False Positive Rate</a></strong><br><a href=/people/i/ilia-markov/>Ilia Markov</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4if-1--3><div class="card-body p-3 small">Hate speech detection is an actively growing field of research with a variety of recently proposed approaches that allowed to push the state-of-the-art results. One of the challenges of such automated approaches namely recent <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> is a risk of false positives (i.e., false accusations), which may lead to over-blocking or removal of harmless social media content in applications with little moderator intervention. We evaluate <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> both under in-domain and cross-domain hate speech detection conditions, and introduce an SVM approach that allows to significantly improve the state-of-the-art results when combined with the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> through a simple majority-voting ensemble. The improvement is mainly due to a reduction of the <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positive rate</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4if-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4if-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4if-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4if-1.5/>Leveraging Community and Author Context to Explain the Performance and Bias of Text-Based Deception Detection Models</a></strong><br><a href=/people/g/galen-weld/>Galen Weld</a>
|
<a href=/people/e/ellyn-ayton/>Ellyn Ayton</a>
|
<a href=/people/t/tim-althoff/>Tim Althoff</a>
|
<a href=/people/m/maria-glenski/>Maria Glenski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4if-1--5><div class="card-body p-3 small">Deceptive news posts shared in <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a> can be detected with NLP models, and much recent research has focused on the development of such <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. In this work, we use characteristics of <a href=https://en.wikipedia.org/wiki/Online_community>online communities</a> and authors the context of how and where content is posted to explain the performance of a neural network deception detection model and identify sub-populations who are disproportionately affected by model accuracy or failure. We examine who is posting the content, and where the content is posted to. We find that while author characteristics are better predictors of deceptive content than community characteristics, both characteristics are strongly correlated with model performance. Traditional performance metrics such as F1 score may fail to capture poor model performance on isolated sub-populations such as specific authors, and as such, more nuanced evaluation of deception detection models is critical.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4if-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4if-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4if-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4if-1.13/>DamascusTeam at NLP4IF2021 : Fighting the Arabic COVID-19 Infodemic on Twitter Using AraBERT<span class=acl-fixed-case>D</span>amascus<span class=acl-fixed-case>T</span>eam at <span class=acl-fixed-case>NLP</span>4<span class=acl-fixed-case>IF</span>2021: Fighting the <span class=acl-fixed-case>A</span>rabic <span class=acl-fixed-case>COVID</span>-19 Infodemic on <span class=acl-fixed-case>T</span>witter Using <span class=acl-fixed-case>A</span>ra<span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/a/ahmad-hussein/>Ahmad Hussein</a>
|
<a href=/people/n/nada-ghneim/>Nada Ghneim</a>
|
<a href=/people/a/ammar-joukhadar/>Ammar Joukhadar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4if-1--13><div class="card-body p-3 small">The objective of this work was the introduction of an effective approach based on the AraBERT language model for fighting Tweets COVID-19 Infodemic. It was arranged in the form of a two-step pipeline, where the first step involved a series of pre-processing procedures to transform Twitter jargon, including <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a>, into <a href=https://en.wikipedia.org/wiki/Plain_text>plain text</a>, and the second step exploited a version of AraBERT, which was pre-trained on <a href=https://en.wikipedia.org/wiki/Plain_text>plain text</a>, to fine-tune and classify the tweets with respect to their Label. The use of <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> pre-trained on plain texts rather than on <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> was motivated by the necessity to address two critical issues shown by the scientific literature, namely (1) pre-trained language models are widely available in many languages, avoiding the time-consuming and resource-intensive model training directly on tweets from scratch, allowing to focus only on their fine-tuning ; (2) available plain text corpora are larger than tweet-only ones, allowing for better performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4if-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4if-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4if-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4if-1.14/>NARNIA at NLP4IF-2021 : Identification of Misinformation in COVID-19 Tweets Using BERTweet<span class=acl-fixed-case>NARNIA</span> at <span class=acl-fixed-case>NLP</span>4<span class=acl-fixed-case>IF</span>-2021: Identification of Misinformation in <span class=acl-fixed-case>COVID</span>-19 Tweets Using <span class=acl-fixed-case>BERT</span>weet</a></strong><br><a href=/people/a/ankit-srivastava/>Ankit Kumar</a>
|
<a href=/people/n/naman-jhunjhunwala/>Naman Jhunjhunwala</a>
|
<a href=/people/r/raksha-agarwal/>Raksha Agarwal</a>
|
<a href=/people/n/niladri-chatterjee/>Niladri Chatterjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4if-1--14><div class="card-body p-3 small">The spread of COVID-19 has been accompanied with widespread <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In particular, <a href=https://en.wikipedia.org/wiki/Twitterverse>Twitterverse</a> has seen a huge increase in dissemination of distorted facts and figures. The present work aims at identifying tweets regarding COVID-19 which contains harmful and false information. We have experimented with a number of Deep Learning-based models, including different word embeddings, such as Glove, ELMo, among others. BERTweet model achieved the best overall F1-score of 0.881 and secured the third rank on the above task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nlp4if-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nlp4if-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nlp4if-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nlp4if-1.17/>iCompass at NLP4IF-2021Fighting the COVID-19 Infodemic<span class=acl-fixed-case>C</span>ompass at <span class=acl-fixed-case>NLP</span>4<span class=acl-fixed-case>IF</span>-2021–Fighting the <span class=acl-fixed-case>COVID</span>-19 Infodemic</a></strong><br><a href=/people/w/wassim-henia/>Wassim Henia</a>
|
<a href=/people/o/oumayma-rjab/>Oumayma Rjab</a>
|
<a href=/people/h/hatem-haddad/>Hatem Haddad</a>
|
<a href=/people/c/chayma-fourati/>Chayma Fourati</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nlp4if-1--17><div class="card-body p-3 small">This paper provides a detailed overview of the system and its outcomes, which were produced as part of the NLP4IF Shared Task on Fighting the COVID-19 Infodemic at NAACL 2021. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is accomplished using a variety of techniques. We used state-of-the-art contextualized text representation models that were fine-tuned for the downstream task in hand. ARBERT, MARBERT, AraBERT, Arabic ALBERT and BERT-base-arabic were used. According to the results, BERT-base-arabic had the highest 0.784 F1 score on the test set.</div></div></div><hr><div id=2021nuse-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nuse-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.nuse-1/>Proceedings of the Third Workshop on Narrative Understanding</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nuse-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nuse-1.0/>Proceedings of the Third Workshop on Narrative Understanding</a></strong><br><a href=/people/n/nader-akoury/>Nader Akoury</a>
|
<a href=/people/f/faeze-brahman/>Faeze Brahman</a>
|
<a href=/people/s/snigdha-chaturvedi/>Snigdha Chaturvedi</a>
|
<a href=/people/e/elizabeth-clark/>Elizabeth Clark</a>
|
<a href=/people/m/mohit-iyyer/>Mohit Iyyer</a>
|
<a href=/people/l/lara-j-martin/>Lara J. Martin</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nuse-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nuse-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nuse-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nuse-1.4/>Document-level Event Extraction with Efficient End-to-end Learning of Cross-event Dependencies</a></strong><br><a href=/people/k/kung-hsiang-huang/>Kung-Hsiang Huang</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nuse-1--4><div class="card-body p-3 small">Fully understanding narratives often requires identifying events in the context of whole documents and modeling the event relations. However, document-level event extraction is a challenging task as it requires the extraction of event and entity coreference, and capturing arguments that span across different sentences. Existing works on <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> usually confine on extracting events from single sentences, which fail to capture the relationships between the event mentions at the scale of a document, as well as the event arguments that appear in a different sentence than the event trigger. In this paper, we propose an end-to-end model leveraging Deep Value Networks (DVN), a structured prediction algorithm, to efficiently capture cross-event dependencies for document-level event extraction. Experimental results show that our approach achieves comparable performance to CRF-based models on ACE05, while enjoys significantly higher computational efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nuse-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nuse-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nuse-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nuse-1.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nuse-1.5/>Gender and Representation Bias in GPT-3 Generated Stories<span class=acl-fixed-case>GPT</span>-3 Generated Stories</a></strong><br><a href=/people/l/li-lucy/>Li Lucy</a>
|
<a href=/people/d/david-bamman/>David Bamman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nuse-1--5><div class="card-body p-3 small">Using <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a> and lexicon-based word similarity, we find that stories generated by GPT-3 exhibit many known <a href=https://en.wikipedia.org/wiki/Gender_role>gender stereotypes</a>. Generated stories depict different topics and descriptions depending on GPT-3&#8217;s perceived gender of the character in a prompt, with feminine characters more likely to be associated with family and appearance, and described as less powerful than masculine characters, even when associated with high power verbs in a prompt. Our study raises questions on how one can avoid unintended social biases when using large language models for <a href=https://en.wikipedia.org/wiki/Storytelling>storytelling</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nuse-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nuse-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nuse-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.nuse-1.6/>Transformer-based Screenplay Summarization Using Augmented Learning Representation with Dialogue Information</a></strong><br><a href=/people/m/myungji-lee/>Myungji Lee</a>
|
<a href=/people/h/hongseok-kwon/>Hongseok Kwon</a>
|
<a href=/people/j/jaehun-shin/>Jaehun Shin</a>
|
<a href=/people/w/wonkee-lee/>WonKee Lee</a>
|
<a href=/people/b/baikjin-jung/>Baikjin Jung</a>
|
<a href=/people/j/jong-hyeok-lee/>Jong-Hyeok Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nuse-1--6><div class="card-body p-3 small">Screenplay summarization is the task of extracting informative scenes from a <a href=https://en.wikipedia.org/wiki/Screenplay>screenplay</a>. The <a href=https://en.wikipedia.org/wiki/Screenplay>screenplay</a> contains turning point (TP) events that change the story direction and thus define the <a href=https://en.wikipedia.org/wiki/Story_structure>story structure</a> decisively. Accordingly, this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> can be defined as the TP identification task. We suggest using dialogue information, one attribute of <a href=https://en.wikipedia.org/wiki/Screenplay>screenplays</a>, motivated by previous work that discovered that TPs have a relation with <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a> appearing in <a href=https://en.wikipedia.org/wiki/Screenplay>screenplays</a>. To teach a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> this characteristic, we add a dialogue feature to the input embedding. Moreover, in an attempt to improve the model architecture of previous studies, we replace LSTM with Transformer. We observed that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can better identify TPs in a <a href=https://en.wikipedia.org/wiki/Screenplay>screenplay</a> by using dialogue information and that a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> adopting Transformer outperforms LSTM-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.nuse-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--nuse-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.nuse-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.nuse-1.7" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.nuse-1.7/>Plug-and-Blend : A Framework for Controllable Story Generation with Blended Control Codes</a></strong><br><a href=/people/z/zhiyu-lin/>Zhiyu Lin</a>
|
<a href=/people/m/mark-riedl/>Mark Riedl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--nuse-1--7><div class="card-body p-3 small">We describe a Plug-and-Play controllable language generation framework, Plug-and-Blend, that allows a human user to input multiple control codes (topics). In the context of automated story generation, this allows a human user lose or fine grained control of the topics that will appear in the generated story, and can even allow for overlapping, blended topics. We show that our framework, working with different generation models, controls the generation towards given continuous-weighted control codes while keeping the generated sentences fluent, demonstrating strong blending capability.</div></div></div><hr><div id=2021privatenlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.privatenlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.privatenlp-1/>Proceedings of the Third Workshop on Privacy in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.privatenlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.privatenlp-1.0/>Proceedings of the Third Workshop on Privacy in Natural Language Processing</a></strong><br><a href=/people/o/oluwaseyi-feyisetan/>Oluwaseyi Feyisetan</a>
|
<a href=/people/s/sepideh-ghanavati/>Sepideh Ghanavati</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/p/patricia-thaine/>Patricia Thaine</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.privatenlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--privatenlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.privatenlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.privatenlp-1.3/>Learning and Evaluating a Differentially Private Pre-trained Language Model</a></strong><br><a href=/people/s/shlomo-hoory/>Shlomo Hoory</a>
|
<a href=/people/a/amir-feder/>Amir Feder</a>
|
<a href=/people/a/avichai-tendler/>Avichai Tendler</a>
|
<a href=/people/a/alon-cohen/>Alon Cohen</a>
|
<a href=/people/s/sofia-erell/>Sofia Erell</a>
|
<a href=/people/i/itay-laish/>Itay Laish</a>
|
<a href=/people/h/hootan-nakhost/>Hootan Nakhost</a>
|
<a href=/people/u/uri-stemmer/>Uri Stemmer</a>
|
<a href=/people/a/ayelet-benjamini/>Ayelet Benjamini</a>
|
<a href=/people/a/avinatan-hassidim/>Avinatan Hassidim</a>
|
<a href=/people/y/yossi-matias/>Yossi Matias</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--privatenlp-1--3><div class="card-body p-3 small">Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to <a href=https://en.wikipedia.org/wiki/Information_leakage>information leakage</a> and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the <a href=https://en.wikipedia.org/wiki/Privacy>privacy</a> of such individuals is to train a differentially-private model, but this usually comes at the expense of model performance. Moreover, it is hard to tell given a privacy parameter $ \\epsilon$ what was the effect on the trained <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a>. In this work we aim to guide future practitioners and researchers on how to improve <a href=https://en.wikipedia.org/wiki/Privacy>privacy</a> while maintaining good model performance. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of $ \\epsilon=1 $ and with only a small degradation in performance. We experiment on a dataset of clinical notes with a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on a target entity extraction task, and compare it to a similar <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained without differential privacy. Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.privatenlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--privatenlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.privatenlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.privatenlp-1.6.OptionalSupplementaryData.zip data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.privatenlp-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.privatenlp-1.6/>Using Confidential Data for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>Domain Adaptation</a> of Neural Machine Translation</a></strong><br><a href=/people/s/sohyung-kim/>Sohyung Kim</a>
|
<a href=/people/a/arianna-bisazza/>Arianna Bisazza</a>
|
<a href=/people/f/fatih-turkmen/>Fatih Turkmen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--privatenlp-1--6><div class="card-body p-3 small">We study the problem of <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> in Neural Machine Translation (NMT) when domain-specific data can not be shared due to confidentiality or copyright issues. As a first step, we propose to fragment data into phrase pairs and use a <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>random sample</a> to fine-tune a generic NMT model instead of the full sentences. Despite the loss of long segments for the sake of confidentiality protection, we find that NMT quality can considerably benefit from this adaptation, and that further gains can be obtained with a simple tagging technique.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.privatenlp-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--privatenlp-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.privatenlp-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.privatenlp-1.7/>Private Text Classification with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/s/samuel-adams/>Samuel Adams</a>
|
<a href=/people/d/david-melanson/>David Melanson</a>
|
<a href=/people/m/martine-de-cock/>Martine De Cock</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--privatenlp-1--7><div class="card-body p-3 small">Text classifiers are regularly applied to personal texts, leaving users of these classifiers vulnerable to <a href=https://en.wikipedia.org/wiki/Information_privacy>privacy breaches</a>. We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC). Our method enables the inference of a class label for a personal text in such a way that (1) the owner of the personal text does not have to disclose their text to anyone in an unencrypted manner, and (2) the owner of the text classifier does not have to reveal the trained model parameters to the text owner or to anyone else. To demonstrate the feasibility of our protocol for practical private text classification, we implemented it in the PyTorch-based MPC framework CrypTen, using a well-known additive secret sharing scheme in the honest-but-curious setting. We test the <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>runtime</a> of our privacy-preserving text classifier, which is fast enough to be used in practice.</div></div></div><hr><div id=2021sdp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sdp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.sdp-1/>Proceedings of the Second Workshop on Scholarly Document Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sdp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sdp-1.0/>Proceedings of the Second Workshop on Scholarly Document Processing</a></strong><br><a href=/people/i/iz-beltagy/>Iz Beltagy</a>
|
<a href=/people/a/arman-cohan/>Arman Cohan</a>
|
<a href=/people/g/guy-feigenblat/>Guy Feigenblat</a>
|
<a href=/people/d/dayne-freitag/>Dayne Freitag</a>
|
<a href=/people/t/tirthankar-ghosal/>Tirthankar Ghosal</a>
|
<a href=/people/k/keith-hall/>Keith Hall</a>
|
<a href=/people/d/drahomira-herrmannova/>Drahomira Herrmannova</a>
|
<a href=/people/p/petr-knoth/>Petr Knoth</a>
|
<a href=/people/k/kyle-lo/>Kyle Lo</a>
|
<a href=/people/p/philipp-mayr/>Philipp Mayr</a>
|
<a href=/people/r/robert-m-patton/>Robert M. Patton</a>
|
<a href=/people/m/michal-shmueli-scheuer/>Michal Shmueli-Scheuer</a>
|
<a href=/people/a/anita-de-waard/>Anita de Waard</a>
|
<a href=/people/k/kuansan-wang/>Kuansan Wang</a>
|
<a href=/people/l/lucy-lu-wang/>Lucy Lu Wang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sdp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sdp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sdp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sdp-1.6" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sdp-1.6/>Keyphrase Extraction from Scientific Articles via Extractive Summarization</a></strong><br><a href=/people/c/chrysovalantis-giorgos-kontoulis/>Chrysovalantis Giorgos Kontoulis</a>
|
<a href=/people/e/eirini-papagiannopoulou/>Eirini Papagiannopoulou</a>
|
<a href=/people/g/grigorios-tsoumakas/>Grigorios Tsoumakas</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sdp-1--6><div class="card-body p-3 small">Automatically extracting keyphrases from scholarly documents leads to a valuable concise representation that humans can understand and machines can process for tasks, such as <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, article clustering and article classification. This paper is concerned with the parts of a scientific article that should be given as input to keyphrase extraction methods. Recent deep learning methods take titles and abstracts as input due to the increased <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> in processing long sequences, whereas traditional approaches can also work with <a href=https://en.wikipedia.org/wiki/Text_corpus>full-texts</a>. Titles and abstracts are dense in keyphrases, but often miss important aspects of the articles, while full-texts on the other hand are richer in keyphrases but much noisier. To address this trade-off, we propose the use of extractive summarization models on the full-texts of scholarly documents. Our empirical study on 3 article collections using 3 keyphrase extraction methods shows promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sdp-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sdp-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sdp-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sdp-1.9/>The Effect of Pretraining on Extractive Summarization for Scientific Documents</a></strong><br><a href=/people/y/yash-gupta/>Yash Gupta</a>
|
<a href=/people/p/pawan-sasanka-ammanamanchi/>Pawan Sasanka Ammanamanchi</a>
|
<a href=/people/s/shikha-bordia/>Shikha Bordia</a>
|
<a href=/people/a/arjun-manoharan/>Arjun Manoharan</a>
|
<a href=/people/d/deepak-mittal/>Deepak Mittal</a>
|
<a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/m/maneesh-singh/>Maneesh Singh</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/p/preethi-jyothi/>Preethi Jyothi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sdp-1--9><div class="card-body p-3 small">Large pretrained models have seen enormous success in extractive summarization tasks. In this work, we investigate the influence of pretraining on a BERT-based extractive summarization system for <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific documents</a>. We derive significant performance improvements using an intermediate pretraining step that leverages existing summarization datasets and report state-of-the-art results on a recently released scientific summarization dataset, SciTLDR. We systematically analyze the intermediate pretraining step by varying the size and domain of the pretraining corpus, changing the length of the input sequence in the target task and varying target tasks. We also investigate how intermediate pretraining interacts with contextualized word embeddings trained on different domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sdp-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sdp-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sdp-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sdp-1.10/>Finding Pragmatic Differences Between Disciplines</a></strong><br><a href=/people/l/lee-kezar/>Lee Kezar</a>
|
<a href=/people/j/jay-pujara/>Jay Pujara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sdp-1--10><div class="card-body p-3 small">Scholarly documents have a great degree of variation, both in terms of content (semantics) and structure (pragmatics). Prior work in scholarly document understanding emphasizes <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> through <a href=https://en.wikipedia.org/wiki/Document_summarization>document summarization</a> and corpus topic modeling but tends to omit <a href=https://en.wikipedia.org/wiki/Pragmatics>pragmatics</a> such as document organization and flow. Using a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of scholarly documents</a> across 19 disciplines and state-of-the-art language modeling techniques, we learn a fixed set of domain-agnostic descriptors for document sections and retrofit the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to these descriptors (also referred to as normalization). Then, we analyze the position and ordering of these descriptors across documents to understand the relationship between <a href=https://en.wikipedia.org/wiki/Discipline>discipline</a> and <a href=https://en.wikipedia.org/wiki/Structure>structure</a>. We report within-discipline structural archetypes, variability, and between-discipline comparisons, supporting the hypothesis that scholarly communities, despite their size, diversity, and breadth, share similar avenues for expressing their work. Our findings lay the foundation for future work in assessing research quality, domain style transfer, and further pragmatic analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sdp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sdp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sdp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sdp-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sdp-1.11/>Extractive Research Slide Generation Using Windowed Labeling Ranking</a></strong><br><a href=/people/a/athar-sefid/>Athar Sefid</a>
|
<a href=/people/p/prasenjit-mitra/>Prasenjit Mitra</a>
|
<a href=/people/j/jian-wu/>Jian Wu</a>
|
<a href=/people/c/c-lee-giles/>C Lee Giles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sdp-1--11><div class="card-body p-3 small">Presentation slides generated from original research papers provide an efficient form to present research innovations. Manually generating presentation slides is labor-intensive. We propose a method to automatically generates slides for scientific articles based on a corpus of 5000 paper-slide pairs compiled from conference proceedings websites. The sentence labeling module of our method is based on SummaRuNNer, a neural sequence model for extractive summarization. Instead of ranking sentences based on semantic similarities in the whole document, our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> measures the importance and novelty of sentences by combining semantic and lexical features within a sentence window. Our method outperforms several baseline methods including SummaRuNNer by a significant margin in terms of ROUGE score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sdp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sdp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sdp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sdp-1.14/>Unsupervised document summarization using pre-trained sentence embeddings and graph centrality</a></strong><br><a href=/people/j/juan-ramirez-orta/>Juan Ramirez-Orta</a>
|
<a href=/people/e/evangelos-milios/>Evangelos Milios</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sdp-1--14><div class="card-body p-3 small">This paper describes our submission for the LongSumm task in SDP 2021. We propose a method for incorporating sentence embeddings produced by deep language models into extractive summarization techniques based on graph centrality in an unsupervised manner. The proposed method is simple, fast, can summarize any kind of document of any size and can satisfy any length constraints for the summaries produced. The method offers competitive performance to more sophisticated <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised methods</a> and can serve as a proxy for abstractive summarization techniques</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sdp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sdp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sdp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sdp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sdp-1.15/>QMUL-SDS at SCIVER : Step-by-Step Binary Classification for Scientific Claim Verification<span class=acl-fixed-case>QMUL</span>-<span class=acl-fixed-case>SDS</span> at <span class=acl-fixed-case>SCIVER</span>: Step-by-Step Binary Classification for Scientific Claim Verification</a></strong><br><a href=/people/x/xia-zeng/>Xia Zeng</a>
|
<a href=/people/a/arkaitz-zubiaga/>Arkaitz Zubiaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sdp-1--15><div class="card-body p-3 small">Scientific claim verification is a unique challenge that is attracting increasing interest. The SCIVER shared task offers a benchmark scenario to test and compare claim verification approaches by participating teams and consists in three steps : relevant abstract selection, rationale selection and label prediction. In this paper, we present team QMUL-SDS&#8217;s participation in the shared task. We propose an approach that performs scientific claim verification by doing binary classifications step-by-step. We trained a BioBERT-large classifier to select abstracts based on pairwise relevance assessments for each claim, title of the abstract and continued to train it to select rationales out of each retrieved abstract based on claim, sentence. We then propose a two-step setting for label prediction, i.e. first predicting NOT_ENOUGH_INFO or ENOUGH_INFO, then label those marked as ENOUGH_INFO as either SUPPORT or CONTRADICT. Compared to the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a>, we achieve substantial improvements on the dev set. As a result, our team is the No. 4 team on the leaderboard.</div></div></div><hr><div id=2021sigtyp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigtyp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.sigtyp-1/>Proceedings of the Third Workshop on Computational Typology and Multilingual NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigtyp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigtyp-1.0/>Proceedings of the Third Workshop on Computational Typology and Multilingual NLP</a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/e/elizabeth-salesky/>Elizabeth Salesky</a>
|
<a href=/people/s/sabrina-mielke/>Sabrina Mielke</a>
|
<a href=/people/g/gabriella-lapesa/>Gabriella Lapesa</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/h/harald-hammarstrom/>Harald Hammarström</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigtyp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigtyp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigtyp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigtyp-1.1" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sigtyp-1.1/>OTEANN : Estimating the Transparency of Orthographies with an <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>Artificial Neural Network</a><span class=acl-fixed-case>OTEANN</span>: Estimating the Transparency of Orthographies with an Artificial Neural Network</a></strong><br><a href=/people/x/xavier-marjou/>Xavier Marjou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigtyp-1--1><div class="card-body p-3 small">To transcribe <a href=https://en.wikipedia.org/wiki/Spoken_language>spoken language</a> to <a href=https://en.wikipedia.org/wiki/Writing>written medium</a>, most <a href=https://en.wikipedia.org/wiki/Alphabet>alphabets</a> enable an unambiguous sound-to-letter rule. However, some <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a> have distanced themselves from this simple concept and little work exists in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a> on measuring such distance. In this study, we use an Artificial Neural Network (ANN) model to evaluate the transparency between written words and their pronunciation, hence its name Orthographic Transparency Estimation with an ANN (OTEANN). Based on datasets derived from Wikimedia dictionaries, we trained and tested this model to score the percentage of false predictions in phoneme-to-grapheme and grapheme-to-phoneme translation tasks. The scores obtained on 17 <a href=https://en.wikipedia.org/wiki/Orthography>orthographies</a> were in line with the estimations of other studies. Interestingly, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> also provided insight into typical mistakes made by learners who only consider the phonemic rule in reading and writing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigtyp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigtyp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigtyp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.sigtyp-1.4" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.sigtyp-1.4/>Improving Cross-Lingual Sentiment Analysis via Conditional Language Adversarial Nets</a></strong><br><a href=/people/h/hemanth-kandula/>Hemanth Kandula</a>
|
<a href=/people/b/bonan-min/>Bonan Min</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigtyp-1--4><div class="card-body p-3 small">Sentiment analysis has come a long way for high-resource languages due to the availability of large annotated corpora. However, it still suffers from lack of training data for low-resource languages. To tackle this problem, we propose Conditional Language Adversarial Network (CLAN), an end-to-end neural architecture for cross-lingual sentiment analysis without cross-lingual supervision. CLAN differs from prior work in that it allows the adversarial training to be conditioned on both learned features and the sentiment prediction, to increase discriminativity for learned representation in the cross-lingual setting. Experimental results demonstrate that CLAN outperforms previous methods on the multilingual multi-domain Amazon review dataset. Our source code is released at https://github.com/hemanthkandula/clan.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.sigtyp-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--sigtyp-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.sigtyp-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.sigtyp-1.14/>Anlirika : An LSTMCNN Flow Twister for Spoken Language Identification<span class=acl-fixed-case>LSTM</span>–<span class=acl-fixed-case>CNN</span> Flow Twister for Spoken Language Identification</a></strong><br><a href=/people/a/andreas-scherbakov/>Andreas Scherbakov</a>
|
<a href=/people/l/liam-whittle/>Liam Whittle</a>
|
<a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/s/siddharth-singh/>Siddharth Singh</a>
|
<a href=/people/m/matthew-coleman/>Matthew Coleman</a>
|
<a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--sigtyp-1--14><div class="card-body p-3 small">The paper presents Anlirika&#8217;s submission to SIGTYP 2021 Shared Task on Robust Spoken Language Identification. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> aims at building a <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robust system</a> that generalizes well across different domains and speakers. The training data is limited to a single domain only with predominantly single speaker per language while the validation and test data samples are derived from diverse dataset and multiple speakers. We experiment with a neural system comprising a combination of dense, convolutional, and recurrent layers that are designed to perform better <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and obtain speaker-invariant representations. We demonstrate that the task in its constrained form (without making use of external data or augmentation the train set with samples from the validation set) is still challenging. Our best <a href=https://en.wikipedia.org/wiki/System>system</a> trained on the <a href=https://en.wikipedia.org/wiki/Data>data</a> augmented with <a href=https://en.wikipedia.org/wiki/Data_validation>validation samples</a> achieves 29.9 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the test data.</div></div></div><hr><div id=2021smm4h-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.smm4h-1/>Proceedings of the Sixth Social Media Mining for Health (#SMM4H) Workshop and Shared Task</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.smm4h-1.0/>Proceedings of the Sixth Social Media Mining for Health (#SMM4H) Workshop and Shared Task</a></strong><br><a href=/people/a/arjun-magge/>Arjun Magge</a>
|
<a href=/people/a/ari-klein/>Ari Klein</a>
|
<a href=/people/a/antonio-miranda-escalada/>Antonio Miranda-Escalada</a>
|
<a href=/people/m/mohammed-ali-al-garadi/>Mohammed Ali Al-garadi</a>
|
<a href=/people/i/ilseyar-alimova/>Ilseyar Alimova</a>
|
<a href=/people/z/zulfat-miftahutdinov/>Zulfat Miftahutdinov</a>
|
<a href=/people/e/eulalia-farre-maduell/>Eulalia Farre-Maduell</a>
|
<a href=/people/s/salvador-lima-lopez/>Salvador Lima Lopez</a>
|
<a href=/people/i/ivan-flores/>Ivan Flores</a>
|
<a href=/people/k/karen-o-connor/>Karen O'Connor</a>
|
<a href=/people/d/davy-weissenbacher/>Davy Weissenbacher</a>
|
<a href=/people/e/elena-tutubalina/>Elena Tutubalina</a>
|
<a href=/people/a/abeed-sarker/>Abeed Sarker</a>
|
<a href=/people/j/juan-m-banda/>Juan M Banda</a>
|
<a href=/people/m/martin-krallinger/>Martin Krallinger</a>
|
<a href=/people/g/graciela-gonzalez/>Graciela Gonzalez-Hernandez</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--smm4h-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.smm4h-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.smm4h-1.2/>View Distillation with Unlabeled Data for Extracting Adverse Drug Effects from User-Generated Data</a></strong><br><a href=/people/p/payam-karisani/>Payam Karisani</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a>
|
<a href=/people/l/li-xiong/>Li Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--smm4h-1--2><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> based on multi-layer transformers for identifying Adverse Drug Reactions (ADR) in social media data. Our model relies on the properties of the problem and the characteristics of contextual word embeddings to extract two views from documents. Then a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> is trained on each view to label a set of unlabeled documents to be used as an initializer for a new <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> in the other view. Finally, the initialized classifier in each view is further trained using the initial training examples. We evaluated our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in the largest publicly available ADR dataset. The experiments testify that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> significantly outperforms the transformer-based models pretrained on domain-specific data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--smm4h-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.smm4h-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.smm4h-1.3/>The ProfNER shared task on automatic recognition of occupation mentions in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> : systems, evaluation, guidelines, embeddings and corpora<span class=acl-fixed-case>P</span>rof<span class=acl-fixed-case>NER</span> shared task on automatic recognition of occupation mentions in social media: systems, evaluation, guidelines, embeddings and corpora</a></strong><br><a href=/people/a/antonio-miranda-escalada/>Antonio Miranda-Escalada</a>
|
<a href=/people/e/eulalia-farre-maduell/>Eulàlia Farré-Maduell</a>
|
<a href=/people/s/salvador-lima-lopez/>Salvador Lima-López</a>
|
<a href=/people/l/luis-gasco/>Luis Gascó</a>
|
<a href=/people/v/vicent-briva-iglesias/>Vicent Briva-Iglesias</a>
|
<a href=/people/m/marvin-aguero-torales/>Marvin Agüero-Torales</a>
|
<a href=/people/m/martin-krallinger/>Martin Krallinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--smm4h-1--3><div class="card-body p-3 small">Detection of occupations in texts is relevant for a range of important application scenarios, like <a href=https://en.wikipedia.org/wiki/Competitive_intelligence>competitive intelligence</a>, sociodemographic analysis, legal NLP or health-related occupational data mining. Despite the importance and heterogeneous data types that mention <a href=https://en.wikipedia.org/wiki/Job>occupations</a>, <a href=https://en.wikipedia.org/wiki/Text_mining>text mining</a> efforts to recognize them have been limited. This is due to the lack of clear annotation guidelines and high-quality Gold Standard corpora. Social media data can be regarded as a relevant source of information for real-time monitoring of at-risk occupational groups in the context of <a href=https://en.wikipedia.org/wiki/Pandemic>pandemics</a> like the COVID-19 one, facilitating intervention strategies for occupations in direct contact with infectious agents or affected by mental health issues. To evaluate current NLP methods and to generate resources, we have organized the ProfNER track at SMM4H 2021, providing ProfNER participants with a Gold Standard corpus of manually annotated tweets (human IAA of 0.919) following annotation guidelines available in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>, an occupation gazetteer, a machine-translated version of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, and FastText embeddings. Out of 35 registered teams, 11 submitted a total of 27 runs. Best-performing participants built <a href=https://en.wikipedia.org/wiki/System>systems</a> based on recent <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP technologies</a> (e.g. transformers) and achieved 0.93 <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> in Text Classification and 0.839 in <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a>. Corpus : https://doi.org/10.5281/zenodo.4309356</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--smm4h-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.smm4h-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.smm4h-1.7/>Transformer-based Multi-Task Learning for Adverse Effect Mention Analysis in Tweets</a></strong><br><a href=/people/g/george-andrei-dima/>George-Andrei Dima</a>
|
<a href=/people/d/dumitru-clementin-cercel/>Dumitru-Clementin Cercel</a>
|
<a href=/people/m/mihai-dascalu/>Mihai Dascalu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--smm4h-1--7><div class="card-body p-3 small">This paper presents our contribution to the <a href=https://en.wikipedia.org/wiki/Social_media_mining>Social Media Mining</a> for Health Applications Shared Task 2021. We addressed all the three subtasks of Task 1 : Subtask A (classification of tweets containing adverse effects), Subtask B (extraction of text spans containing adverse effects) and Subtask C (adverse effects resolution). We explored various pre-trained transformer-based language models and we focused on a multi-task training architecture. For the first subtask, we also applied adversarial augmentation techniques and we formed model ensembles in order to improve the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of the prediction. Our system ranked first at Subtask B with 0.51 F1 score, 0.514 <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> and 0.514 recall. For Subtask A we obtained 0.44 <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>F1 score</a>, 0.49 <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> and 0.39 recall and for Subtask C we obtained 0.16 <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>F1 score</a> with 0.16 <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> and 0.17 recall.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--smm4h-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.smm4h-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.smm4h-1.10/>UACH-INAOE at SMM4H : a BERT based approach for classification of COVID-19 Twitter posts<span class=acl-fixed-case>UACH</span>-<span class=acl-fixed-case>INAOE</span> at <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span>: a <span class=acl-fixed-case>BERT</span> based approach for classification of <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>T</span>witter posts</a></strong><br><a href=/people/a/alberto-valdes/>Alberto Valdes</a>
|
<a href=/people/j/jesus-lopez/>Jesus Lopez</a>
|
<a href=/people/m/manuel-montes/>Manuel Montes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--smm4h-1--10><div class="card-body p-3 small">This work describes the participation of the Universidad Autnoma de Chihuahua-Instituto Nacional de Astrofsica, ptica y Electrnica team at the Social Media Mining for Health Applications (SMM4H) 2021 shared task. Our team participated in task 5 and 6, both focused on the automatic classification of Twitter posts related to COVID-19. Task 5 was oriented on solving a binary classification problem, trying to identify self-reporting tweets of potential cases of COVID-19. Task 6 objective was to classify tweets containing COVID-19 symptoms. For both tasks we used <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> based on bidirectional encoder representations from transformers (BERT). Our objective was to determine if a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> pretrained on a corpus in the domain of interest can outperform one trained on a much larger general domain corpus. Our F1 results were encouraging, 0.77 and 0.95 for task 5 and 6 respectively, having achieved the highest score among all the participants in the latter.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--smm4h-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.smm4h-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.smm4h-1.12/>Word Embeddings, <a href=https://en.wikipedia.org/wiki/Cosine_similarity>Cosine Similarity</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> for Identification of Professions & Occupations in Health-related Social Media</a></strong><br><a href=/people/s/sergio-santamaria-carrasco/>Sergio Santamaría Carrasco</a>
|
<a href=/people/r/roberto-cuervo-rosillo/>Roberto Cuervo Rosillo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--smm4h-1--12><div class="card-body p-3 small">ProfNER-ST focuses on the recognition of professions and occupations from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> using Spanish data. Our participation is based on a combination of word-level embeddings, including pre-trained Spanish BERT, as well as cosine similarity computed over a subset of entities that serve as input for an encoder-decoder architecture with attention mechanism. Finally, our best score achieved an F1-measure of 0.823 in the official test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--smm4h-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.smm4h-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.smm4h-1.16/>A Joint Training Approach to Tweet Classification and Adverse Effect Extraction and Normalization for SMM4H 2021<span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> 2021</a></strong><br><a href=/people/m/mohab-el-karef/>Mohab Elkaref</a>
|
<a href=/people/l/lamiece-hassan/>Lamiece Hassan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--smm4h-1--16><div class="card-body p-3 small">In this work we describe our submissions to the Social Media Mining for Health (SMM4H) 2021 Shared Task. We investigated the effectiveness of a joint training approach to Task 1, specifically classification, extraction and normalization of Adverse Drug Effect (ADE) mentions in English tweets. Our approach performed well on the normalization task, achieving an above average f1 score of 24 %, but less so on <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> and extraction, with f1 scores of 22 % and 37 % respectively. Our experiments also showed that a larger dataset with more negative results led to stronger results than a smaller more balanced dataset, even when both datasets have the same positive examples. Finally we also submitted a tuned BERT model for Task 6 : Classification of Covid-19 tweets containing symptoms, which achieved an above average f1 score of 96 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--smm4h-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.smm4h-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.smm4h-1.20/>Identification of profession & occupation in Health-related Social Media using tweets in Spanish<span class=acl-fixed-case>S</span>panish</a></strong><br><a href=/people/v/victoria-pachon/>Victoria Pachón</a>
|
<a href=/people/j/jacinto-mata-vazquez/>Jacinto Mata Vázquez</a>
|
<a href=/people/j/juan-luis-dominguez-olmedo/>Juan Luís Domínguez Olmedo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--smm4h-1--20><div class="card-body p-3 small">In this paper we present our approach and system description on Task 7a in ProfNer-ST : Identification of profession & occupation in Health related Social Media. Our main contribution is to show the effectiveness of using BETO-Spanish BERT as a model based on transformers pretrained with a Spanish Corpus for classification tasks. In our experiments we compared several <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> based on <a href=https://en.wikipedia.org/wiki/Transformer>transformers</a> with others based on classical <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a>. With this <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a>, we achieved an F1-score of 0.92 in the evaluation process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--smm4h-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.smm4h-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.smm4h-1.23/>UoB at ProfNER 2021 : Data Augmentation for Classification Using <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a><span class=acl-fixed-case>U</span>o<span class=acl-fixed-case>B</span> at <span class=acl-fixed-case>P</span>rof<span class=acl-fixed-case>NER</span> 2021: Data Augmentation for Classification Using Machine Translation</a></strong><br><a href=/people/f/frances-adriana-laureano-de-leon/>Frances Adriana Laureano De Leon</a>
|
<a href=/people/h/harish-tayyar-madabushi/>Harish Tayyar Madabushi</a>
|
<a href=/people/m/mark-lee/>Mark Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--smm4h-1--23><div class="card-body p-3 small">This paper describes the participation of the UoB-NLP team in the ProfNER-ST shared subtask 7a. The <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> was aimed at detecting the mention of professions in social media text. Our team experimented with two methods of improving the performance of pre-trained models : Specifically, we experimented with <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> through <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and the merging of multiple language inputs to meet the objective of the task. While the best performing model on the test data consisted of mBERT fine-tuned on augmented data using <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, the improvement is minor possibly because multi-lingual pre-trained models such as mBERT already have access to the kind of information provided through <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation and bilingual data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.smm4h-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--smm4h-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.smm4h-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.smm4h-1.26/>PAII-NLP at SMM4H 2021 : Joint Extraction and Normalization of Adverse Drug Effect Mentions in Tweets<span class=acl-fixed-case>PAII</span>-<span class=acl-fixed-case>NLP</span> at <span class=acl-fixed-case>SMM</span>4<span class=acl-fixed-case>H</span> 2021: Joint Extraction and Normalization of Adverse Drug Effect Mentions in Tweets</a></strong><br><a href=/people/z/zongcheng-ji/>Zongcheng Ji</a>
|
<a href=/people/t/tian-xia/>Tian Xia</a>
|
<a href=/people/m/mei-han/>Mei Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--smm4h-1--26><div class="card-body p-3 small">This paper describes our system developed for the subtask 1c of the sixth Social Media Mining for Health Applications (SMM4H) shared task in 2021. The aim of the subtask is to recognize the adverse drug effect (ADE) mentions from tweets and normalize the identified mentions to their mapping MedDRA preferred term IDs. Our system is based on a neural transition-based joint model, which is to perform <a href=https://en.wikipedia.org/wiki/Computer_vision>recognition</a> and <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> simultaneously. Our final two submissions outperform the average F1 score by 1-2 %.</div></div></div><hr><div id=2021socialnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.socialnlp-1/>Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.socialnlp-1.0/>Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media</a></strong><br><a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a>
|
<a href=/people/c/cheng-te-li/>Cheng-Te Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--socialnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.socialnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.socialnlp-1.3/>A Case Study of In-House Competition for Ranking Constructive Comments in a News Service</a></strong><br><a href=/people/h/hayato-kobayashi/>Hayato Kobayashi</a>
|
<a href=/people/h/hiroaki-taguchi/>Hiroaki Taguchi</a>
|
<a href=/people/y/yoshimune-tabuchi/>Yoshimune Tabuchi</a>
|
<a href=/people/c/chahine-koleejan/>Chahine Koleejan</a>
|
<a href=/people/k/ken-kobayashi/>Ken Kobayashi</a>
|
<a href=/people/s/soichiro-fujita/>Soichiro Fujita</a>
|
<a href=/people/k/kazuma-murao/>Kazuma Murao</a>
|
<a href=/people/t/takeshi-masuyama/>Takeshi Masuyama</a>
|
<a href=/people/t/taichi-yatsuka/>Taichi Yatsuka</a>
|
<a href=/people/m/manabu-okumura/>Manabu Okumura</a>
|
<a href=/people/s/satoshi-sekine/>Satoshi Sekine</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--socialnlp-1--3><div class="card-body p-3 small">Ranking the user comments posted on a news article is important for online news services because comment visibility directly affects the user experience. Research on ranking comments with different <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> to measure the comment quality has shown constructiveness used in argument analysis is promising from a practical standpoint. In this paper, we report a case study in which this <a href=https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_education)>constructiveness</a> is examined in the real world. Specifically, we examine an in-house competition to improve the performance of ranking constructive comments and demonstrate the effectiveness of the best obtained <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for a <a href=https://en.wikipedia.org/wiki/Service_(economics)>commercial service</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--socialnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.socialnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.socialnlp-1.4/>Quantifying the Effects of COVID-19 on Restaurant Reviews<span class=acl-fixed-case>COVID</span>-19 on Restaurant Reviews</a></strong><br><a href=/people/i/ivy-cao/>Ivy Cao</a>
|
<a href=/people/z/zizhou-liu/>Zizhou Liu</a>
|
<a href=/people/g/giannis-karamanolakis/>Giannis Karamanolakis</a>
|
<a href=/people/d/daniel-hsu/>Daniel Hsu</a>
|
<a href=/people/l/luis-gravano/>Luis Gravano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--socialnlp-1--4><div class="card-body p-3 small">The COVID-19 pandemic has implications beyond <a href=https://en.wikipedia.org/wiki/Health>physical health</a>, affecting society and economies. Government efforts to slow down the spread of the virus have had a severe impact on many businesses, including restaurants. Mandatory policies such as restaurant closures, bans on social gatherings, and social distancing restrictions have affected restaurant operations as well as customer preferences (e.g., prompting a demand of stricter hygiene standards). As of now, however, it is not clear how and to what extent the <a href=https://en.wikipedia.org/wiki/Pandemic>pandemic</a> has affected restaurant reviews, an analysis of which could potentially inform policies for addressing this ongoing situation. In this work, we present our efforts to understand the effects of COVID-19 on restaurant reviews, with a focus on Yelp reviews produced during the pandemic for New York City and Los Angeles County restaurants. Overall, we make the following contributions. First, we assemble a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 600 <a href=https://en.wikipedia.org/wiki/Review>reviews</a> with manual annotations of fine-grained COVID-19 aspects related to <a href=https://en.wikipedia.org/wiki/Restaurant>restaurants</a> (e.g., <a href=https://en.wikipedia.org/wiki/Hygiene>hygiene practices</a>, <a href=https://en.wikipedia.org/wiki/Service_(economics)>service changes</a>, sympathy and support for local businesses). Second, we address COVID-19 aspect detection using supervised classifiers, weakly-supervised approaches based on keywords, and unsupervised topic modeling approaches, and experimentally show that <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> based on pre-trained BERT representations achieve the best performance (F1=0.79). Third, we analyze the number and evolution of COVID-related aspects over time and show that the resulting <a href=https://en.wikipedia.org/wiki/Time_series>time series</a> have substantial correlation (Spearman&#8217;s = 0.84) with critical statistics related to the COVID-19 pandemic, including the number of new COVID-19 cases.<tex-math>\\rho</tex-math>=0.84) with critical statistics related to the COVID-19 pandemic, including the number of new COVID-19 cases. To our knowledge, this is the first work analyzing the effects of COVID-19 on Yelp restaurant reviews and could potentially inform policies by public health departments, for example, to cover resource utilization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--socialnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.socialnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.socialnlp-1.5/>Assessing Cognitive Linguistic Influences in the Assignment of Blame</a></strong><br><a href=/people/k/karen-zhou/>Karen Zhou</a>
|
<a href=/people/a/ana-smith/>Ana Smith</a>
|
<a href=/people/l/lillian-lee/>Lillian Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--socialnlp-1--5><div class="card-body p-3 small">Lab studies in <a href=https://en.wikipedia.org/wiki/Cognition>cognition</a> and the <a href=https://en.wikipedia.org/wiki/Psychology_of_morality>psychology of morality</a> have proposed some thematic and linguistic factors that influence <a href=https://en.wikipedia.org/wiki/Moral_reasoning>moral reasoning</a>. This paper assesses how well the findings of these studies generalize to a large corpus of over 22,000 descriptions of fraught situations posted to a dedicated forum. At this social-media site, users judge whether or not an author is in the wrong with respect to the event that the author described. We find that, consistent with lab studies, there are statistically significant differences in uses of first-person passive voice, as well as first-person agents and patients, between descriptions of situations that receive different blame judgments. These <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> also aid performance in the task of predicting the eventual collective verdicts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--socialnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.socialnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.socialnlp-1.6/>Evaluating Deception Detection Model Robustness To <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>Linguistic Variation</a></a></strong><br><a href=/people/m/maria-glenski/>Maria Glenski</a>
|
<a href=/people/e/ellyn-ayton/>Ellyn Ayton</a>
|
<a href=/people/r/robin-cosbey/>Robin Cosbey</a>
|
<a href=/people/d/dustin-arendt/>Dustin Arendt</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--socialnlp-1--6><div class="card-body p-3 small">With the increasing use of machine-learning driven algorithmic judgements, it is critical to develop <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that are robust to evolving or manipulated inputs. We propose an extensive analysis of <a href=https://en.wikipedia.org/wiki/Robust_statistics>model robustness</a> against <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variation</a> in the setting of deceptive news detection, an important task in the context of <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation spread online</a>. We consider two prediction tasks and compare three state-of-the-art embeddings to highlight consistent trends in <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance, high confidence misclassifications, and high impact failures. By measuring the effectiveness of adversarial defense strategies and evaluating model susceptibility to adversarial attacks using character- and word-perturbed text, we find that character or mixed ensemble models are the most effective defenses and that character perturbation-based attack tactics are more successful.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--socialnlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.socialnlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.socialnlp-1.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.socialnlp-1.11/>Using Noisy Self-Reports to Predict Twitter User Demographics<span class=acl-fixed-case>T</span>witter User Demographics</a></strong><br><a href=/people/z/zach-wood-doughty/>Zach Wood-Doughty</a>
|
<a href=/people/p/paiheng-xu/>Paiheng Xu</a>
|
<a href=/people/x/xiao-liu/>Xiao Liu</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--socialnlp-1--11><div class="card-body p-3 small">Computational social science studies often contextualize <a href=https://en.wikipedia.org/wiki/Content_analysis>content analysis</a> within standard <a href=https://en.wikipedia.org/wiki/Demography>demographics</a>. Since <a href=https://en.wikipedia.org/wiki/Demography>demographics</a> are unavailable on many <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> (e.g. Twitter), numerous studies have inferred <a href=https://en.wikipedia.org/wiki/Demography>demographics</a> automatically. Despite many studies presenting proof-of-concept inference of race and ethnicity, training of practical systems remains elusive since there are few annotated datasets. Existing datasets are small, inaccurate, or fail to cover the four most common <a href=https://en.wikipedia.org/wiki/Race_and_ethnicity_in_the_United_States>racial and ethnic groups</a> in the United States. We present a method to identify self-reports of race and ethnicity from Twitter profile descriptions. Despite the noise of automated supervision, our <a href=https://en.wikipedia.org/wiki/Self-report_study>self-report datasets</a> enable improvements in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance on gold standard <a href=https://en.wikipedia.org/wiki/Self-report_study>self-report survey data</a>. The result is a reproducible method for creating <a href=https://en.wikipedia.org/wiki/Training_and_development>large-scale training resources</a> for <a href=https://en.wikipedia.org/wiki/Race_and_ethnicity_in_the_United_States>race and ethnicity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--socialnlp-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.socialnlp-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.socialnlp-1.12/>PANDORA Talks : Personality and Demographics on Reddit<span class=acl-fixed-case>PANDORA</span> Talks: Personality and Demographics on <span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/m/matej-gjurkovic/>Matej Gjurković</a>
|
<a href=/people/m/mladen-karan/>Mladen Karan</a>
|
<a href=/people/i/iva-vukojevic/>Iva Vukojević</a>
|
<a href=/people/m/mihaela-bosnjak/>Mihaela Bošnjak</a>
|
<a href=/people/j/jan-snajder/>Jan Snajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--socialnlp-1--12><div class="card-body p-3 small">Personality and demographics are important variables in <a href=https://en.wikipedia.org/wiki/Social_science>social sciences</a> and computational sociolinguistics. However, <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> with both <a href=https://en.wikipedia.org/wiki/Personality_type>personality and demographic labels</a> are scarce. To address this, we present <a href=https://en.wikipedia.org/wiki/PANDORA>PANDORA</a>, the first dataset of Reddit comments of 10k users partially labeled with three personality models and <a href=https://en.wikipedia.org/wiki/Demography>demographics</a> (age, gender, and location), including 1.6k users labeled with the well-established Big 5 personality model. We showcase the usefulness of this dataset on three experiments, where we leverage the more readily available data from other personality models to predict the Big 5 traits, analyze gender classification biases arising from psycho-demographic variables, and carry out a confirmatory and exploratory analysis based on psychological theories. Finally, we present benchmark prediction models for all <a href=https://en.wikipedia.org/wiki/Personality_type>personality and demographic variables</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--socialnlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.socialnlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.socialnlp-1.13/>Room to Grow : Understanding Personal Characteristics Behind Self Improvement Using <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/m/meixing-dong/>MeiXing Dong</a>
|
<a href=/people/x/xueming-xu/>Xueming Xu</a>
|
<a href=/people/y/yiwei-zhang/>Yiwei Zhang</a>
|
<a href=/people/i/ian-stewart/>Ian Stewart</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--socialnlp-1--13><div class="card-body p-3 small">Many people aim for change, but not everyone succeeds. While there are a number of <a href=https://en.wikipedia.org/wiki/Social_psychology>social psychology theories</a> that propose motivation-related characteristics of those who persist with change, few <a href=https://en.wikipedia.org/wiki/Computational_psychology>computational studies</a> have explored the motivational stage of personal change. In this paper, we investigate a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consisting of the writings of people who manifest intention to change, some of whom persist while others do not. Using a variety of <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic analysis techniques</a>, we first examine the <a href=https://en.wikipedia.org/wiki/Writing_style>writing patterns</a> that distinguish the two groups of people. Persistent people tend to reference more topics related to long-term self-improvement and use a more complicated writing style. Drawing on these consistent differences, we build a <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> that can reliably identify the people more likely to persist, based on their language. Our experiments provide new insights into the motivation-related behavior of people who persist with their intention to change.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.socialnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--socialnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.socialnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.socialnlp-1.15" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.socialnlp-1.15/>Jujeop : Korean Puns for K-pop Stars on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a><span class=acl-fixed-case>K</span>orean Puns for K-pop Stars on Social Media</a></strong><br><a href=/people/s/soyoung-oh/>Soyoung Oh</a>
|
<a href=/people/j/jisu-kim/>Jisu Kim</a>
|
<a href=/people/s/seungpeel-lee/>Seungpeel Lee</a>
|
<a href=/people/e/eunil-park/>Eunil Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--socialnlp-1--15><div class="card-body p-3 small">Jujeop is a type of <a href=https://en.wikipedia.org/wiki/Pun>pun</a> and a unique way for fans to express their love for the K-pop stars they follow using <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>. One of the unique characteristics of Jujeop is its use of <a href=https://en.wikipedia.org/wiki/Exaggeration>exaggerated expressions</a> to compliment <a href=https://en.wikipedia.org/wiki/K-pop>K-pop stars</a>, which contain or lead to <a href=https://en.wikipedia.org/wiki/Humour>humor</a>. Based on this characteristic, Jujeop can be separated into four distinct types, with their own lexical collocations : (1) Fragmenting words to create a twist, (2) <a href=https://en.wikipedia.org/wiki/Homophone>Homophones</a> and <a href=https://en.wikipedia.org/wiki/Homograph>homographs</a>, (3) Repetition, and (4) <a href=https://en.wikipedia.org/wiki/Nonsense>Nonsense</a>. Thus, the current study first defines the concept of Jujeop in <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>, manually labels 8.6 K comments and annotates the comments to one of the four Jujeop types. With the given annotated corpus, this study presents distinctive characteristics of Jujeop comments compared to the other comments by classification task. Moreover, with the clustering approach, we proposed a structural dependency within each Jujeop type. We have made our dataset publicly available for future research of Jujeop expressions.</div></div></div><hr><div id=2021teachingnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.teachingnlp-1/>Proceedings of the Fifth Workshop on Teaching NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.teachingnlp-1.0/>Proceedings of the Fifth Workshop on Teaching NLP</a></strong><br><a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/v/varada-kolhatkar/>Varada Kolhatkar</a>
|
<a href=/people/l/li-lucy/>Lucy Li</a>
|
<a href=/people/m/margot-mieskes/>Margot Mieskes</a>
|
<a href=/people/t/ted-pedersen/>Ted Pedersen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--teachingnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.teachingnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.teachingnlp-1.2/>Teaching a Massive Open Online Course on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/e/ekaterina-artemova/>Ekaterina Artemova</a>
|
<a href=/people/m/murat-apishev/>Murat Apishev</a>
|
<a href=/people/d/denis-kirianov/>Denis Kirianov</a>
|
<a href=/people/v/veronica-sarkisyan/>Veronica Sarkisyan</a>
|
<a href=/people/s/sergey-aksenov/>Sergey Aksenov</a>
|
<a href=/people/o/oleg-serikov/>Oleg Serikov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--teachingnlp-1--2><div class="card-body p-3 small">In this paper we present a new Massive Open Online Course on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>, targeted at non-English speaking students. The <a href=https://en.wikipedia.org/wiki/Course_(education)>course</a> lasts 12 weeks, every week consists of lectures, practical sessions and quiz assigments. Three weeks out of 12 are followed by Kaggle-style coding assigments. Our course intents to serve multiple purposes : (i) familirize students with the core concepts and methods in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>, such as language modelling or word or sentence representations, (ii) show that recent advances, including pre-trained Transformer-based models, are build upon these concepts ; (iii) to introduce architectures for most most demanded real-life applications, (iii) to develop practical skills to process texts in multiple languages. The course was prepared and recorded during 2020 and so far have received positive feedback.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--teachingnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.teachingnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.teachingnlp-1.3/>Natural Language Processing 4 All (NLP4All): A New Online Platform for Teaching and Learning NLP Concepts<span class=acl-fixed-case>NLP</span>4<span class=acl-fixed-case>A</span>ll): A New Online Platform for Teaching and Learning <span class=acl-fixed-case>NLP</span> Concepts</a></strong><br><a href=/people/r/rebekah-baglini/>Rebekah Baglini</a>
|
<a href=/people/h/hermes-hjorth/>Hermes Hjorth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--teachingnlp-1--3><div class="card-body p-3 small">Natural Language Processing offers new insights into language data across almost all disciplines and domains, and allows us to corroborate and/or challenge existing knowledge. The primary hurdles to widening participation in and use of these new research tools are, first, a lack of coding skills in students across K-16, and in the population at large, and second, a lack of knowledge of how NLP-methods can be used to answer questions of disciplinary interest outside of <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> and/or <a href=https://en.wikipedia.org/wiki/Computer_science>computer science</a>. To broaden participation in NLP and improve NLP-literacy, we introduced a new tool web-based tool called Natural Language Processing 4 All (NLP4All). The intended purpose of NLP4All is to help teachers facilitate learning with and about NLP, by providing easy-to-use interfaces to NLP-methods, data, and analyses, making it possible for non- and novice-programmers to learn NLP concepts interactively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--teachingnlp-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.teachingnlp-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.teachingnlp-1.4/>A New Broad NLP Training from Speech to Knowledge<span class=acl-fixed-case>NLP</span> Training from Speech to Knowledge</a></strong><br><a href=/people/m/maxime-amblard/>Maxime Amblard</a>
|
<a href=/people/m/miguel-couceiro/>Miguel Couceiro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--teachingnlp-1--4><div class="card-body p-3 small">In 2018, the Master Sc. in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> opened at IDMC-Institut des Sciences du Digital, du Management et de la Cognition, Universit de Lorraine-Nancy, France. Far from being a creation ex-nihilo, it is the product of a history and many reflections on the field and its teaching. This article proposes epistemological and critical elements on the opening and maintainance of this so far new master&#8217;s program in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--teachingnlp-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.teachingnlp-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.teachingnlp-1.6/>A Crash Course on Ethics for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/a/annemarie-friedrich/>Annemarie Friedrich</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--teachingnlp-1--6><div class="card-body p-3 small">It is generally agreed upon in the natural language processing (NLP) community that <a href=https://en.wikipedia.org/wiki/Ethics>ethics</a> should be integrated into any <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum</a>. Being aware of and understanding the relevant core concepts is a prerequisite for following and participating in the discourse on ethical NLP. We here present ready-made teaching material in the form of slides and practical exercises on ethical issues in <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a>, which is primarily intended to be integrated into introductory NLP or computational linguistics courses. By making this material freely available, we aim at lowering the threshold to adding <a href=https://en.wikipedia.org/wiki/Ethics>ethics</a> to the <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum</a>. We hope that increased awareness will enable students to identify potentially <a href=https://en.wikipedia.org/wiki/Ethics>unethical behavior</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--teachingnlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.teachingnlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.teachingnlp-1.8/>MiniVQA-A resource to build your tailored VQA competition<span class=acl-fixed-case>M</span>ini<span class=acl-fixed-case>VQA</span> - A resource to build your tailored <span class=acl-fixed-case>VQA</span> competition</a></strong><br><a href=/people/j/jean-benoit-delbrouck/>Jean-Benoit Delbrouck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--teachingnlp-1--8><div class="card-body p-3 small">MiniVQA is a Jupyter notebook to build a tailored VQA competition for your students. The resource creates all the needed resources to create a classroom competition that engages and inspires your students on the free, self-service Kaggle platform. InClass competitions make <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> fun &#8216;.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--teachingnlp-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.teachingnlp-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.teachingnlp-1.11/>A Balanced and Broadly Targeted Computational Linguistics Curriculum</a></strong><br><a href=/people/e/emma-manning/>Emma Manning</a>
|
<a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--teachingnlp-1--11><div class="card-body p-3 small">This paper describes the primarily-graduate computational linguistics and NLP curriculum at Georgetown University, a U.S. university that has seen significant growth in these areas in recent years. We reflect on the principles behind our curriculum choices, including recognizing the various academic backgrounds and goals of our students ; teaching a variety of skills with an emphasis on working directly with data ; encouraging collaboration and interdisciplinary work ; and including languages beyond English. We reflect on challenges we have encountered, such as the difficulty of teaching programming skills alongside NLP fundamentals, and discuss areas for future growth.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--teachingnlp-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.teachingnlp-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.teachingnlp-1.13.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.teachingnlp-1.13/>The Flipped Classroom model for teaching Conditional Random Fields in an NLP course<span class=acl-fixed-case>NLP</span> course</a></strong><br><a href=/people/m/manex-agirrezabal/>Manex Agirrezabal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--teachingnlp-1--13><div class="card-body p-3 small">In this article, we show and discuss our experience in applying the flipped classroom method for teaching <a href=https://en.wikipedia.org/wiki/Conditional_random_field>Conditional Random Fields</a> in a Natural Language Processing course. We present the <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>activities</a> that we developed together with their relationship to a <a href=https://en.wikipedia.org/wiki/Cognitive_complexity>cognitive complexity model</a> (Bloom&#8217;s taxonomy). After this, we provide our own reflections and expectations of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> itself. Based on the evaluation got from students, it seems that students learn about the topic and also that the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is rewarding for some students. Additionally, we discuss some shortcomings and we propose possible solutions to them. We conclude the paper with some possible future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--teachingnlp-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.teachingnlp-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.teachingnlp-1.15/>An Immersive Computational Text Analysis Course for Non-Computer Science Students at Barnard College</a></strong><br><a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/j/jalisha-jenifer/>Jalisha Jenifer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--teachingnlp-1--15><div class="card-body p-3 small">We provide an overview of a new Computational Text Analysis course that will be taught at Barnard College over a six week period in May and June 2021. The <a href=https://en.wikipedia.org/wiki/Course_(education)>course</a> is targeted to non Computer Science at a Liberal Arts college that wish to incorporate fundamental Natural Language Processing tools in their re- search and studies. During the <a href=https://en.wikipedia.org/wiki/Course_(education)>course</a>, students will complete daily programming tutorials, read and review contemporary research papers, and propose and develop independent research projects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--teachingnlp-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.teachingnlp-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.teachingnlp-1.23/>Learning How To Learn NLP : Developing Introductory Concepts Through Scaffolded Discovery<span class=acl-fixed-case>NLP</span>: Developing Introductory Concepts Through Scaffolded Discovery</a></strong><br><a href=/people/a/alexandra-schofield/>Alexandra Schofield</a>
|
<a href=/people/r/richard-wicentowski/>Richard Wicentowski</a>
|
<a href=/people/j/julie-medero/>Julie Medero</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--teachingnlp-1--23><div class="card-body p-3 small">We present a scaffolded discovery learning approach to introducing concepts in a Natural Language Processing course aimed at computer science students at <a href=https://en.wikipedia.org/wiki/Liberal_arts_colleges_in_the_United_States>liberal arts institutions</a>. We describe some of the objectives of this approach, as well as presenting specific ways that four of our discovery-based assignments combine specific natural language processing concepts with broader analytic skills. We argue this approach helps prepare students for many possible future paths involving both application and innovation of NLP technology by emphasizing experimental data navigation, experiment design, and awareness of the complexities and challenges of analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.teachingnlp-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--teachingnlp-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.teachingnlp-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.teachingnlp-1.25/>Teaching NLP outside <a href=https://en.wikipedia.org/wiki/Linguistics>Linguistics</a> and Computer Science classrooms : Some challenges and some opportunities<span class=acl-fixed-case>NLP</span> outside Linguistics and Computer Science classrooms: Some challenges and some opportunities</a></strong><br><a href=/people/s/sowmya-vajjala/>Sowmya Vajjala</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--teachingnlp-1--25><div class="card-body p-3 small">NLP&#8217;s sphere of influence went much beyond <a href=https://en.wikipedia.org/wiki/Computer_science>computer science research</a> and the development of <a href=https://en.wikipedia.org/wiki/Application_software>software applications</a> in the past decade. We see people using NLP methods in a range of academic disciplines from <a href=https://en.wikipedia.org/wiki/Asian_studies>Asian Studies</a> to <a href=https://en.wikipedia.org/wiki/Clinical_Oncology>Clinical Oncology</a>. We also notice the presence of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> as a module in most of the data science curricula within and outside of regular university setups. These <a href=https://en.wikipedia.org/wiki/Course_(education)>courses</a> are taken by students from very diverse backgrounds. This paper takes a closer look at some issues related to teaching <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> to these diverse audiences based on my classroom experiences, and identifies some challenges the instructors face, particularly when there is no ecosystem of related courses for the students. In this process, it also identifies a few challenge areas for both <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP researchers</a> and tool developers.</div></div></div><hr><div id=2021trustnlp-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.trustnlp-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.trustnlp-1/>Proceedings of the First Workshop on Trustworthy Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.trustnlp-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.trustnlp-1.0/>Proceedings of the First Workshop on Trustworthy Natural Language Processing</a></strong><br><a href=/people/y/yada-pruksachatkun/>Yada Pruksachatkun</a>
|
<a href=/people/a/anil-ramakrishna/>Anil Ramakrishna</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/s/satyapriya-krishna/>Satyapriya Krishna</a>
|
<a href=/people/j/jwala-dhamala/>Jwala Dhamala</a>
|
<a href=/people/t/tanaya-guha/>Tanaya Guha</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.trustnlp-1.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--trustnlp-1--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.trustnlp-1.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.trustnlp-1.1.OptionalSupplementaryData.tgz data-toggle=tooltip data-placement=top title="Optional supplementary data"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.trustnlp-1.1/>Interpretability Rules : Jointly Bootstrapping a Neural Relation Extractorwith an Explanation Decoder</a></strong><br><a href=/people/z/zheng-tang/>Zheng Tang</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--trustnlp-1--1><div class="card-body p-3 small">We introduce a method that transforms a rule-based relation extraction (RE) classifier into a neural one such that both interpretability and performance are achieved. Our approach jointly trains a RE classifier with a decoder that generates explanations for these extractions, using as sole supervision a set of rules that match these relations. Our evaluation on the TACRED dataset shows that our neural RE classifier outperforms the rule-based one we started from by 9 F1 points ; our decoder generates explanations with a high BLEU score of over 90 % ; and, the joint learning improves the performance of both the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> and decoder.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.trustnlp-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--trustnlp-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.trustnlp-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.trustnlp-1.2/>Measuring Biases of Word Embeddings : What <a href=https://en.wikipedia.org/wiki/Similarity_measure>Similarity Measures</a> and <a href=https://en.wikipedia.org/wiki/Descriptive_statistics>Descriptive Statistics</a> to Use?</a></strong><br><a href=/people/h/hossein-azarpanah/>Hossein Azarpanah</a>
|
<a href=/people/m/mohsen-farhadloo/>Mohsen Farhadloo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--trustnlp-1--2><div class="card-body p-3 small">Word embeddings are widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing (NLP)</a> for a vast range of applications. However, it has been consistently proven that these <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> reflect the same <a href=https://en.wikipedia.org/wiki/Bias>human biases</a> that exist in the data used to train them. Most of the introduced bias indicators to reveal word embeddings&#8217; bias are average-based indicators based on the cosine similarity measure. In this study, we examine the impacts of different <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measures</a> as well as other descriptive techniques than <a href=https://en.wikipedia.org/wiki/Average>averaging</a> in measuring the biases of contextual and non-contextual word embeddings. We show that the extent of revealed biases in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> depends on the <a href=https://en.wikipedia.org/wiki/Descriptive_statistics>descriptive statistics</a> and <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measures</a> used to measure the bias. We found that over the ten categories of word embedding association tests, <a href=https://en.wikipedia.org/wiki/Mahalanobis_distance>Mahalanobis distance</a> reveals the smallest bias, and <a href=https://en.wikipedia.org/wiki/Euclidean_distance>Euclidean distance</a> reveals the largest bias in word embeddings. In addition, the contextual models reveal less severe biases than the non-contextual word embedding models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.trustnlp-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--trustnlp-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.trustnlp-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.trustnlp-1.3/>Private Release of Text Embedding Vectors</a></strong><br><a href=/people/o/oluwaseyi-feyisetan/>Oluwaseyi Feyisetan</a>
|
<a href=/people/s/shiva-kasiviswanathan/>Shiva Kasiviswanathan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--trustnlp-1--3><div class="card-body p-3 small">Ensuring strong theoretical privacy guarantees on <a href=https://en.wikipedia.org/wiki/Text_file>text data</a> is a challenging problem which is usually attained at the expense of <a href=https://en.wikipedia.org/wiki/Utility>utility</a>. However, to improve the practicality of privacy preserving text analyses, it is essential to design algorithms that better optimize this tradeoff. To address this challenge, we propose a release mechanism that takes any (text) embedding vector as input and releases a corresponding private vector. The mechanism satisfies an extension of <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> to <a href=https://en.wikipedia.org/wiki/Metric_space>metric spaces</a>. Our idea based on first randomly projecting the vectors to a lower-dimensional space and then adding <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> in this projected space generates private vectors that achieve strong theoretical guarantees on its <a href=https://en.wikipedia.org/wiki/Utility>utility</a>. We support our theoretical proofs with empirical experiments on multiple word embedding models and NLP datasets, achieving in some cases more than 10 % gains over the existing state-of-the-art privatization techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.trustnlp-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--trustnlp-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.trustnlp-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.trustnlp-1.5/>xER : An Explainable Model for Entity Resolution using an Efficient Solution for the Clique Partitioning Problem<span class=acl-fixed-case>ER</span>: An Explainable Model for Entity Resolution using an Efficient Solution for the Clique Partitioning Problem</a></strong><br><a href=/people/s/samhita-vadrevu/>Samhita Vadrevu</a>
|
<a href=/people/r/rakesh-nagi/>Rakesh Nagi</a>
|
<a href=/people/j/jinjun-xiong/>JinJun Xiong</a>
|
<a href=/people/w/wen-mei-hwu/>Wen-mei Hwu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--trustnlp-1--5><div class="card-body p-3 small">In this paper, we propose a global, self- explainable solution to solve a prominent NLP problem : Entity Resolution (ER). We formu- late ER as a <a href=https://en.wikipedia.org/wiki/Graph_partitioning_problem>graph partitioning problem</a>. Every mention of a real-world entity is represented by a node in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>, and the pairwise sim- ilarity scores between the mentions are used to associate these nodes to exactly one <a href=https://en.wikipedia.org/wiki/Clique_(graph_theory)>clique</a>, which represents a real-world entity in the ER domain. In this paper, we use Clique Partition- ing Problem (CPP), which is an Integer Pro- gram (IP) to formulate ER as a graph partition- ing problem and then highlight the explainable nature of this method. Since CPP is NP-Hard, we introduce an efficient solution procedure, the xER algorithm, to solve CPP as a combi- nation of finding maximal cliques in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> and then performing generalized set packing using a novel formulation. We discuss the advantages of using xER over the traditional methods and provide the computational exper- iments and results of applying this method to ER data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.trustnlp-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--trustnlp-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.trustnlp-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.trustnlp-1.8/>Towards Benchmarking the Utility of Explanations for Model Debugging</a></strong><br><a href=/people/m/maximilian-idahl/>Maximilian Idahl</a>
|
<a href=/people/l/lijun-lyu/>Lijun Lyu</a>
|
<a href=/people/u/ujwal-gadiraju/>Ujwal Gadiraju</a>
|
<a href=/people/a/avishek-anand/>Avishek Anand</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--trustnlp-1--8><div class="card-body p-3 small">Post-hoc explanation methods are an important class of approaches that help understand the rationale underlying a trained model&#8217;s decision. But how useful are they for an end-user towards accomplishing a given task? In this vision paper, we argue the need for a <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> to facilitate evaluations of the utility of <a href=https://en.wikipedia.org/wiki/Post-hoc_analysis>post-hoc explanation methods</a>. As a first step to this end, we enumerate desirable properties that such a <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> should possess for the task of debugging text classifiers. Additionally, we highlight that such a <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark</a> facilitates not only assessing the effectiveness of explanations but also their efficiency.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>