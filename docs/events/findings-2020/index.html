<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Findings of the Association for Computational Linguistics (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Findings of the Association for Computational Linguistics (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020findings-emnlp>Findings of the Association for Computational Linguistics: EMNLP 2020</a>
<span class="badge badge-info align-middle ml-1">149&nbsp;papers</span></li></ul></div></div><div id=2020findings-emnlp><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.findings-emnlp/>Findings of the Association for Computational Linguistics: EMNLP 2020</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.0/>Findings of the Association for Computational Linguistics: EMNLP 2020</a></strong><br><a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/y/yulan-he/>Yulan He</a>
|
<a href=/people/y/yang-liu-icsi/>Yang Liu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.1/>Fully Quantized Transformer for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/g/gabriele-prato/>Gabriele Prato</a>
|
<a href=/people/e/ella-charlaix/>Ella Charlaix</a>
|
<a href=/people/m/mehdi-rezagholizadeh/>Mehdi Rezagholizadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--1><div class="card-body p-3 small">State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing <a href=https://en.wikipedia.org/wiki/Computational_cost>computational costs</a> of such <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT : an all-inclusive quantization strategy for the Transformer. To the best of our knowledge, we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer. Indeed, compared to <a href=https://en.wikipedia.org/wiki/Significant_figures>full-precision</a>, our 8-bit models score greater or equal <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> on most tasks. Comparing ourselves to all previously proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>, we achieve state-of-the-art <a href=https://en.wikipedia.org/wiki/Quantization_(signal_processing)>quantization</a> results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.2/>Summarizing Chinese Medical Answer with Graph Convolution Networks and Question-focused Dual Attention<span class=acl-fixed-case>C</span>hinese Medical Answer with Graph Convolution Networks and Question-focused Dual Attention</a></strong><br><a href=/people/n/ningyu-zhang/>Ningyu Zhang</a>
|
<a href=/people/s/shumin-deng/>Shumin Deng</a>
|
<a href=/people/j/juan-li/>Juan Li</a>
|
<a href=/people/x/xi-chen/>Xi Chen</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a>
|
<a href=/people/h/huajun-chen/>Huajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--2><div class="card-body p-3 small">Online search engines are a popular source of medical information for users, where users can enter questions and obtain relevant answers. It is desirable to generate answer summaries for <a href=https://en.wikipedia.org/wiki/Web_search_engine>online search engines</a>, particularly summaries that can reveal direct answers to questions. Moreover, answer summaries are expected to reveal the most relevant information in response to questions ; hence, the summaries should be generated with a focus on the question, which is a challenging topic-focused summarization task. In this paper, we propose an approach that utilizes graph convolution networks and question-focused dual attention for Chinese medical answer summarization. We first organize the original long answer text into a medical concept graph with graph convolution networks to better understand the internal structure of the text and the correlation between medical concepts. Then, we introduce a question-focused dual attention mechanism to generate summaries relevant to questions. Experimental results demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate more coherent and informative summaries compared with baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.7/>Reducing Sentiment Bias in <a href=https://en.wikipedia.org/wiki/Language_model>Language Models</a> via Counterfactual Evaluation</a></strong><br><a href=/people/p/po-sen-huang/>Po-Sen Huang</a>
|
<a href=/people/h/huan-zhang/>Huan Zhang</a>
|
<a href=/people/r/ray-jiang/>Ray Jiang</a>
|
<a href=/people/r/robert-stanforth/>Robert Stanforth</a>
|
<a href=/people/j/johannes-welbl/>Johannes Welbl</a>
|
<a href=/people/j/jack-rae/>Jack Rae</a>
|
<a href=/people/v/vishal-maini/>Vishal Maini</a>
|
<a href=/people/d/dani-yogatama/>Dani Yogatama</a>
|
<a href=/people/p/pushmeet-kohli/>Pushmeet Kohli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--7><div class="card-body p-3 small">Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> capable of generating coherent texts, it also prompts <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of <a href=https://en.wikipedia.org/wiki/Bias>bias</a> exhibited by <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> : bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model&#8217;s latent representations. The <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizations</a> improve fairness metrics while retaining comparable levels of <a href=https://en.wikipedia.org/wiki/Perplexity>perplexity</a> and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.11" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.11/>Difference-aware Knowledge Selection for Knowledge-grounded Conversation Generation</a></strong><br><a href=/people/c/chujie-zheng/>Chujie Zheng</a>
|
<a href=/people/y/yunbo-cao/>Yunbo Cao</a>
|
<a href=/people/d/daxin-jiang/>Daxin Jiang</a>
|
<a href=/people/m/minlie-huang/>Minlie Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--11><div class="card-body p-3 small">In a multi-turn knowledge-grounded dialog, the difference between the knowledge selected at different turns usually provides potential clues to knowledge selection, which has been largely neglected in previous research. In this paper, we propose a difference-aware knowledge selection method. It first computes the difference between the candidate knowledge sentences provided at the current turn and those chosen in the previous turns. Then, the differential information is fused with or disentangled from the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> to facilitate final knowledge selection. Automatic, human observational, and interactive evaluation shows that our method is able to select knowledge more accurately and generate more informative responses, significantly outperforming the state-of-the-art baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.21.OptionalSupplementaryMaterial.bbl data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.21/>Semantic Matching for Sequence-to-Sequence Learning</a></strong><br><a href=/people/r/ruiyi-zhang/>Ruiyi Zhang</a>
|
<a href=/people/c/changyou-chen/>Changyou Chen</a>
|
<a href=/people/x/xinyuan-zhang/>Xinyuan Zhang</a>
|
<a href=/people/k/ke-bai/>Ke Bai</a>
|
<a href=/people/l/lawrence-carin/>Lawrence Carin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--21><div class="card-body p-3 small">In sequence-to-sequence models, classical optimal transport (OT) can be applied to semantically match generated sentences with target sentences. However, in non-parallel settings, target sentences are usually unavailable. To tackle this issue without losing the benefits of classical OT, we present a semantic matching scheme based on the Optimal Partial Transport (OPT). Specifically, our approach partially matches semantically meaningful words between source and partial target sequences. To overcome the difficulty of detecting active regions in OPT (corresponding to the words needed to be matched), we further exploit prior knowledge to perform partial matching. Extensive experiments are conducted to evaluate the proposed approach, showing consistent improvements over sequence-to-sequence tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.24.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.24/>Gradient-based Analysis of NLP Models is Manipulable<span class=acl-fixed-case>NLP</span> Models is Manipulable</a></strong><br><a href=/people/j/junlin-wang/>Junlin Wang</a>
|
<a href=/people/j/jens-tuyls/>Jens Tuyls</a>
|
<a href=/people/e/eric-wallace/>Eric Wallace</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--24><div class="card-body p-3 small">Gradient-based analysis methods, such as saliency map visualizations and adversarial input perturbations, have found widespread use in interpreting neural NLP models due to their simplicity, flexibility, and most importantly, the fact that they directly reflect the model internals. In this paper, however, we demonstrate that the <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a> of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> are easily manipulable, and thus bring into question the reliability of gradient-based analyses. In particular, we merge the layers of a target <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with a Facade Model that overwhelms the <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a> without affecting the predictions. This Facade Model can be trained to have <a href=https://en.wikipedia.org/wiki/Gradient>gradients</a> that are misleading and irrelevant to the task, such as focusing only on the <a href=https://en.wikipedia.org/wiki/Stop_words>stop words</a> in the input. On a variety of NLP tasks (sentiment analysis, NLI, and QA), we show that the merged model effectively fools different analysis tools : saliency maps differ significantly from the original model&#8217;s, input reduction keeps more irrelevant input tokens, and adversarial perturbations identify unimportant tokens as being highly important.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.25/>Pretrain-KGE : Learning <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>Knowledge Representation</a> from Pretrained Language Models<span class=acl-fixed-case>KGE</span>: Learning Knowledge Representation from Pretrained Language Models</a></strong><br><a href=/people/z/zhiyuan-zhang/>Zhiyuan Zhang</a>
|
<a href=/people/x/xiaoqian-liu/>Xiaoqian Liu</a>
|
<a href=/people/y/yi-zhang/>Yi Zhang</a>
|
<a href=/people/q/qi-su/>Qi Su</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/b/bin-he/>Bin He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--25><div class="card-body p-3 small">Conventional knowledge graph embedding (KGE) often suffers from limited knowledge representation, leading to performance degradation especially on the low-resource problem. To remedy this, we propose to enrich <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>knowledge representation</a> via pretrained language models by leveraging <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a> from pretrained models. Specifically, we present a universal training framework named Pretrain-KGE consisting of three phases : semantic-based fine-tuning phase, knowledge extracting phase and KGE training phase. Extensive experiments show that our proposed Pretrain-KGE can improve results over KGE models, especially on solving the low-resource problem.<i>Pretrain-KGE</i> consisting of three phases: semantic-based fine-tuning phase, knowledge extracting phase and KGE training phase. Extensive experiments show that our proposed Pretrain-KGE can improve results over KGE models, especially on solving the low-resource problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.33/>Control, Generate, Augment : A Scalable Framework for Multi-Attribute Text Generation</a></strong><br><a href=/people/g/giuseppe-russo/>Giuseppe Russo</a>
|
<a href=/people/n/nora-hollenstein/>Nora Hollenstein</a>
|
<a href=/people/c/claudiu-cristian-musat/>Claudiu Cristian Musat</a>
|
<a href=/people/c/ce-zhang/>Ce Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--33><div class="card-body p-3 small">We introduce CGA, a conditional VAE architecture, to control, generate, and augment text. CGA is able to generate natural English sentences controlling multiple semantic and syntactic attributes by combining <a href=https://en.wikipedia.org/wiki/Adversarial_learning>adversarial learning</a> with a context-aware loss and a cyclical word dropout routine. We demonstrate the value of the individual model components in an ablation study. The <a href=https://en.wikipedia.org/wiki/Scalability>scalability</a> of our approach is ensured through a single <a href=https://en.wikipedia.org/wiki/Discriminator>discriminator</a>, independently of the number of attributes. We show high quality, <a href=https://en.wikipedia.org/wiki/Diversity_(politics)>diversity</a> and attribute control in the generated sentences through a series of automatic and human assessments. As the main application of our work, we test the potential of this new NLG model in a data augmentation scenario. In a downstream NLP task, the sentences generated by our CGA model show significant improvements over a strong baseline, and a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance often comparable to adding same amount of additional real data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.35.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.35/>Dual Low-Rank Multimodal Fusion</a></strong><br><a href=/people/t/tao-jin/>Tao Jin</a>
|
<a href=/people/s/siyu-huang/>Siyu Huang</a>
|
<a href=/people/y/yingming-li/>Yingming Li</a>
|
<a href=/people/z/zhongfei-zhang/>Zhongfei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--35><div class="card-body p-3 small">Tensor-based fusion methods have been proven effective in multimodal fusion tasks. However, existing tensor-based methods make a poor use of the fine-grained temporal dynamics of multimodal sequential features. Motivated by this observation, this paper proposes a novel multimodal fusion method called Fine-Grained Temporal Low-Rank Multimodal Fusion (FT-LMF). FT-LMF correlates the features of individual time steps between multiple modalities, while it involves multiplications of high-order tensors in its calculation. This paper further proposes Dual Low-Rank Multimodal Fusion (Dual-LMF) to reduce the computational complexity of FT-LMF through low-rank tensor approximation along dual dimensions of input features. Dual-LMF is conceptually simple and practically effective and efficient. Empirical studies on benchmark multimodal analysis tasks show that our proposed methods outperform the state-of-the-art tensor-based fusion methods with a similar <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.38.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.38" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.38/>A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels</a></strong><br><a href=/people/y/youxuan-jiang/>Youxuan Jiang</a>
|
<a href=/people/h/huaiyu-zhu/>Huaiyu Zhu</a>
|
<a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a>
|
<a href=/people/y/yunyao-li/>Yunyao Li</a>
|
<a href=/people/w/walter-lasecki/>Walter Lasecki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--38><div class="card-body p-3 small">Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing methods</a> have either had low <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> or required substantial <a href=https://en.wikipedia.org/wiki/Annotation>expert annotation</a>. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for predicate labels and 93 % for argument labels, which is comparable to expert agreement. Compared to prior work on <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> for SRL, we decrease expert effort by 4x, from 56 % to 14 % of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.39" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.39/>KorNLI and KorSTS : New Benchmark Datasets for Korean Natural Language Understanding<span class=acl-fixed-case>K</span>or<span class=acl-fixed-case>NLI</span> and <span class=acl-fixed-case>K</span>or<span class=acl-fixed-case>STS</span>: New Benchmark Datasets for <span class=acl-fixed-case>K</span>orean Natural Language Understanding</a></strong><br><a href=/people/j/jiyeon-ham/>Jiyeon Ham</a>
|
<a href=/people/y/yo-joong-choe/>Yo Joong Choe</a>
|
<a href=/people/k/kyubyong-park/>Kyubyong Park</a>
|
<a href=/people/i/ilji-choi/>Ilji Choi</a>
|
<a href=/people/h/hyungjoon-soh/>Hyungjoon Soh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--39><div class="card-body p-3 small">Natural language inference (NLI) and semantic textual similarity (STS) are key tasks in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding (NLU)</a>. Although several <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> for those tasks have been released in English and a few other languages, there are no publicly available NLI or STS datasets in the <a href=https://en.wikipedia.org/wiki/Korean_language>Korean language</a>. Motivated by this, we construct and release new datasets for Korean NLI and STS, dubbed KorNLI and KorSTS, respectively. Following previous approaches, we machine-translate existing English training sets and manually translate development and test sets into <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>. To accelerate research on Korean NLU, we also establish <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> on KorNLI and KorSTS. Our datasets are publicly available at https://github.com/kakaobrain/KorNLUDatasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.43/>Claim Check-Worthiness Detection as Positive Unlabelled Learning</a></strong><br><a href=/people/d/dustin-wright/>Dustin Wright</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--43><div class="card-body p-3 small">As the first step of <a href=https://en.wikipedia.org/wiki/Fact-checking>automatic fact checking</a>, claim check-worthiness detection is a critical component of fact checking systems. There are multiple lines of research which study this problem : check-worthiness ranking from political speeches and debates, rumour detection on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, and citation needed detection from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. To date, there has been no structured comparison of these various <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> to understand their relatedness, and no investigation into whether or not a unified approach to all of them is achievable. In this work, we illuminate a central challenge in claim check-worthiness detection underlying all of these tasks, being that they hinge upon detecting both how factual a sentence is, as well as how likely a sentence is to be believed without verification. As such, annotators only mark those instances they judge to be clear-cut check-worthy. Our best performing method is a unified approach which automatically corrects for this using a variant of positive unlabelled learning that finds instances which were incorrectly labelled as not check-worthy. In applying this, we out-perform the state of the art in two of the three <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> studied for claim check-worthiness detection in <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.44" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.44/>ConceptBert : Concept-Aware Representation for Visual Question Answering<span class=acl-fixed-case>C</span>oncept<span class=acl-fixed-case>B</span>ert: Concept-Aware Representation for Visual Question Answering</a></strong><br><a href=/people/f/francois-garderes/>François Gardères</a>
|
<a href=/people/m/maryam-ziaeefard/>Maryam Ziaeefard</a>
|
<a href=/people/b/baptiste-abeloos/>Baptiste Abeloos</a>
|
<a href=/people/f/freddy-lecue/>Freddy Lecue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--44><div class="card-body p-3 small">Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the <a href=https://en.wikipedia.org/wiki/Computer_vision>computer vision</a> and the natural language processing communities. A VQA model combines visual and textual features in order to answer questions grounded in an <a href=https://en.wikipedia.org/wiki/Image>image</a>. Current works in VQA focus on questions which are answerable by direct analysis of the question and image alone. We present a concept-aware algorithm, ConceptBert, for questions which require <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a>, or basic factual knowledge from external structured content. Given an <a href=https://en.wikipedia.org/wiki/Image>image</a> and a question in natural language, ConceptBert requires visual elements of the <a href=https://en.wikipedia.org/wiki/Image>image</a> and a Knowledge Graph (KG) to infer the correct answer. We introduce a multi-modal representation which learns a joint Concept-Vision-Language embedding inspired by the popular BERT architecture. We exploit ConceptNet KG for encoding the common sense knowledge and evaluate our methodology on the Outside Knowledge-VQA (OK-VQA) and VQA datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.53.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--53 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.53 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940115 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.53" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.53/>PBoS : Probabilistic Bag-of-Subwords for Generalizing Word Embedding<span class=acl-fixed-case>PB</span>o<span class=acl-fixed-case>S</span>: Probabilistic Bag-of-Subwords for Generalizing Word Embedding</a></strong><br><a href=/people/z/zhao-jinman/>Zhao Jinman</a>
|
<a href=/people/s/shawn-zhong/>Shawn Zhong</a>
|
<a href=/people/x/xiaomin-zhang/>Xiaomin Zhang</a>
|
<a href=/people/y/yingyu-liang/>Yingyu Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--53><div class="card-body p-3 small">We look into the task of generalizing word embeddings : given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the spellings of words and propose a model, along with an efficient <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, that simultaneously models subword segmentation and computes subword-based compositional word embedding. We call the model probabilistic bag-of-subwords (PBoS), as it applies bag-of-subwords for all possible segmentations based on their likelihood. Inspections and affix prediction experiment show that PBoS is able to produce meaningful subword segmentations and subword rankings without any source of explicit morphological knowledge. Word similarity and POS tagging experiments show clear advantages of PBoS over previous subword-level models in the quality of generated word embeddings across languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.54/>Interpretable Entity Representations through Large-Scale Typing</a></strong><br><a href=/people/y/yasumasa-onoe/>Yasumasa Onoe</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--54><div class="card-body p-3 small">In standard methodology for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, entities in text are typically embedded in dense vector spaces with pre-trained models. The <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> produced this way are effective when fed into downstream models, but they require end-task fine-tuning and are fundamentally difficult to interpret. In this paper, we present an approach to creating entity representations that are human readable and achieve high performance on entity-related tasks out of the box. Our representations are vectors whose values correspond to <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior probabilities</a> over fine-grained entity types, indicating the confidence of a typing model&#8217;s decision that the entity belongs to the corresponding type. We obtain these representations using a fine-grained entity typing model, trained either on supervised ultra-fine entity typing data (Choi et al. 2018) or distantly-supervised examples from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. On entity probing tasks involving recognizing entity identity, our <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> used in parameter-free downstream models achieve competitive performance with ELMo- and BERT-based embeddings in trained models. We also show that it is possible to reduce the size of our type set in a learning-based way for particular domains. Finally, we show that these embeddings can be post-hoc modified through a small number of rules to incorporate <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> and improve performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.55/>Empirical Studies of Institutional Federated Learning For <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/x/xinghua-zhu/>Xinghua Zhu</a>
|
<a href=/people/j/jianzong-wang/>Jianzong Wang</a>
|
<a href=/people/z/zhenhou-hong/>Zhenhou Hong</a>
|
<a href=/people/j/jing-xiao/>Jing Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--55><div class="card-body p-3 small">Federated learning has sparkled new interests in the deep learning society to make use of isolated data sources from independent institutes. With the development of novel training tools, we have successfully deployed federated natural language processing networks on GPU-enabled server clusters. This paper demonstrates federated training of a popular NLP model, TextCNN, with applications in sentence intent classification. Furthermore, <a href=https://en.wikipedia.org/wiki/Differential_privacy>differential privacy</a> is introduced to protect participants in the training process, in a manageable manner. Distinguished from previous client-level privacy protection schemes, the proposed differentially private federated learning procedure is defined in the dataset sample level, inherent with the applications among institutions instead of individual users. Optimal settings of hyper-parameters for the federated TextCNN model are studied through comprehensive experiments. We also evaluated the performance of federated TextCNN model under imbalanced data load configuration. Experiments show that, the sampling ratio has a large impact on the performance of the <a href=https://en.wikipedia.org/wiki/Statistical_model>FL models</a>, causing up to 38.4 % decrease in the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>test accuracy</a>, while they are robust to different noise multiplier levels, with less than 3 % variance in the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>test accuracy</a>. It is also found that the FL models are sensitive to <a href=https://en.wikipedia.org/wiki/Load_balancing_(computing)>data load balancedness</a> among client datasets. When the <a href=https://en.wikipedia.org/wiki/Load_(computing)>data load</a> is imbalanced, model performance dropped by up to 10 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.56/>NeuReduce : Reducing Mixed Boolean-Arithmetic Expressions by <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Network</a><span class=acl-fixed-case>N</span>eu<span class=acl-fixed-case>R</span>educe: Reducing Mixed <span class=acl-fixed-case>B</span>oolean-Arithmetic Expressions by Recurrent Neural Network</a></strong><br><a href=/people/w/weijie-feng/>Weijie Feng</a>
|
<a href=/people/b/binbin-liu/>Binbin Liu</a>
|
<a href=/people/d/dongpeng-xu/>Dongpeng Xu</a>
|
<a href=/people/q/qilong-zheng/>Qilong Zheng</a>
|
<a href=/people/y/yun-xu/>Yun Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--56><div class="card-body p-3 small">Mixed Boolean-Arithmetic (MBA) expressions involve both <a href=https://en.wikipedia.org/wiki/Arithmetic>arithmetic calculation</a> (e.g.,plus, minus, multiply) and <a href=https://en.wikipedia.org/wiki/Bitwise_operation>bitwise computation</a> (e.g., and, or, negate, xor). MBA expressions have been widely applied in <a href=https://en.wikipedia.org/wiki/Obfuscation_(software)>software obfuscation</a>, transforming <a href=https://en.wikipedia.org/wiki/Computer_program>programs</a> from a simple form to a complex form. MBA expressions are challenging to be simplified, because the interleaving bitwise and arithmetic operations causing mathematical reduction laws to be ineffective. Our goal is to recover the original, simple form from an obfuscated MBA expression. In this paper, we first propose NeuReduce, a string to string method based on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to automatically learn and reduce complex MBA expressions. We develop a comprehensive MBA dataset, including one million diversified MBA expression samples and corresponding simplified forms. After training on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, NeuReduce can reduce MBA rules to homelier but mathematically equivalent forms. By comparing with three state-of-the-art MBA reduction methods, our evaluation result shows that NeuReduce outperforms all other tools in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, <a href=https://en.wikipedia.org/wiki/Time_complexity>solving time</a>, and <a href=https://en.wikipedia.org/wiki/Overhead_(computing)>performance overhead</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.57.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--57 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.57 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.57/>From Language to Language-ish : How Brain-Like is an LSTM’s Representation of Nonsensical Language Stimuli?<span class=acl-fixed-case>LSTM</span>’s Representation of Nonsensical Language Stimuli?</a></strong><br><a href=/people/m/maryam-hashemzadeh/>Maryam Hashemzadeh</a>
|
<a href=/people/g/greta-kaufeld/>Greta Kaufeld</a>
|
<a href=/people/m/martha-white/>Martha White</a>
|
<a href=/people/a/andrea-e-martin/>Andrea E. Martin</a>
|
<a href=/people/a/alona-fyshe/>Alona Fyshe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--57><div class="card-body p-3 small">The representations generated by many models of language (word embeddings, <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain&#8217;s reaction to syntactically and semantically sound language stimuli. In this study, we asked : how does an LSTM (long short term memory) language model, trained (by and large) on semantically and syntactically intact language, represent a language sample with degraded semantic or syntactic information? Does the LSTM representation still resemble the brain&#8217;s reaction? We found that, even for some kinds of nonsensical language, there is a statistically significant relationship between the brain&#8217;s activity and the representations of an LSTM. This indicates that, at least in some instances, LSTMs and the human brain handle nonsensical data similarly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.59/>Cascaded Semantic and Positional Self-Attention Network for Document Classification</a></strong><br><a href=/people/j/juyong-jiang/>Juyong Jiang</a>
|
<a href=/people/j/jie-zhang/>Jie Zhang</a>
|
<a href=/people/k/kai-zhang/>Kai Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--59><div class="card-body p-3 small">Transformers have shown great success in learning representations for <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a>. However, an open challenge still remains on how to systematically aggregate semantic information (word embedding) with positional (or temporal) information (word orders). In this work, we propose a new architecture to aggregate the two sources of information using cascaded semantic and positional self-attention network (CSPAN) in the context of <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. The <a href=https://en.wikipedia.org/wiki/CSPAN>CSPAN</a> uses a semantic self-attention layer cascaded with Bi-LSTM to process the semantic and positional information in a sequential manner, and then adaptively combine them together through a residue connection. Compared with commonly used positional encoding schemes, CSPAN can exploit the interaction between <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and word positions in a more interpretable and adaptive manner, and the classification performance can be notably improved while simultaneously preserving a compact model size and high <a href=https://en.wikipedia.org/wiki/Convergence_of_random_variables>convergence rate</a>. We evaluate the CSPAN model on several benchmark data sets for <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a> with careful ablation studies, and demonstrate the encouraging results compared with state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.60/>Toward Recognizing More Entity Types in NER : An Efficient Implementation using Only Entity Lexicons<span class=acl-fixed-case>NER</span>: An Efficient Implementation using Only Entity Lexicons</a></strong><br><a href=/people/m/minlong-peng/>Minlong Peng</a>
|
<a href=/people/r/ruotian-ma/>Ruotian Ma</a>
|
<a href=/people/q/qi-zhang/>Qi Zhang</a>
|
<a href=/people/l/lujun-zhao/>Lujun Zhao</a>
|
<a href=/people/m/mengxi-wei/>Mengxi Wei</a>
|
<a href=/people/c/changlong-sun/>Changlong Sun</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--60><div class="card-body p-3 small">In this work, we explore the way to quickly adjust an existing named entity recognition (NER) system to make it capable of recognizing entity types not defined in the <a href=https://en.wikipedia.org/wiki/System>system</a>. As an illustrative example, consider the case that a NER system has been built to recognize person and organization names, and now it requires to additionally recognize job titles. Such a situation is common in the industrial areas, where the entity types required to recognize vary a lot in different products and keep changing. To avoid laborious data labeling and achieve fast adaptation, we propose to adjust the existing NER system using the previously labeled data and entity lexicons of the newly introduced entity types. We formulate such a <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> as a partially supervised learning problem and accordingly propose an effective <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> to solve the <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. Comprehensive experimental studies on several public NER datasets validate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.64.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--64 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.64 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940112 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.64" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.64/>Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior</a></strong><br><a href=/people/z/zi-lin/>Zi Lin</a>
|
<a href=/people/j/jeremiah-liu/>Jeremiah Liu</a>
|
<a href=/people/z/zi-yang/>Zi Yang</a>
|
<a href=/people/n/nan-hua/>Nan Hua</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--64><div class="card-body p-3 small">Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which penalizes an entire residual module in a Transformer model toward an identity mapping. Our method identifies and discards unimportant non-linear mappings in the residual connections by applying a thresholding operator on the function norm, and is applicable to any structured module including a single attention head, an entire attention blocks, or a feed-forward subnetwork. Furthermore, we introduce spectral normalization to stabilize the distribution of the post-activation values of the Transformer layers, further improving the pruning effectiveness of the proposed methodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to demonstrate that SNIP achieves effective pruning results while maintaining comparable performance. Specifically, we improve the performance over the state-of-the-art by 0.5 to 1.0 % on average at 50 % compression ratio.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.65" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.65/>Rethinking Self-Attention : Towards Interpretability in Neural Parsing</a></strong><br><a href=/people/k/khalil-mrini/>Khalil Mrini</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/q/quan-hung-tran/>Quan Hung Tran</a>
|
<a href=/people/t/trung-bui/>Trung Bui</a>
|
<a href=/people/w/walter-chang/>Walter Chang</a>
|
<a href=/people/n/ndapandula-nakashole/>Ndapa Nakashole</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--65><div class="card-body p-3 small">Attention mechanisms have improved the performance of NLP tasks while allowing <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to remain explainable. Self-attention is currently widely used, however interpretability is difficult due to the numerous attention distributions. Recent work has shown that model representations can benefit from label-specific information, while facilitating <a href=https://en.wikipedia.org/wiki/Prediction>interpretation of predictions</a>. We introduce the Label Attention Layer : a new form of self-attention where attention heads represent labels. We test our novel layer by running constituency and dependency parsing experiments and show our new model obtains new state-of-the-art results for both tasks on both the Penn Treebank (PTB) and Chinese Treebank. Additionally, our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> requires fewer self-attention layers compared to existing work. Finally, we find that the Label Attention heads learn relations between <a href=https://en.wikipedia.org/wiki/Syntactic_category>syntactic categories</a> and show pathways to analyze errors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.69.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--69 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.69 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.69.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.69" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.69/>Syntactic and Semantic-driven Learning for Open Information Extraction</a></strong><br><a href=/people/j/jialong-tang/>Jialong Tang</a>
|
<a href=/people/y/yaojie-lu/>Yaojie Lu</a>
|
<a href=/people/h/hongyu-lin/>Hongyu Lin</a>
|
<a href=/people/x/xianpei-han/>Xianpei Han</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/x/xinyan-xiao/>Xinyan Xiao</a>
|
<a href=/people/h/hua-wu/>Hua Wu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--69><div class="card-body p-3 small">One of the biggest bottlenecks in building accurate, high coverage neural open IE systems is the need for large labelled corpora. The diversity of <a href=https://en.wikipedia.org/wiki/Open_domain>open domain corpora</a> and the variety of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language expressions</a> further exacerbate this problem. In this paper, we propose a syntactic and semantic-driven learning approach, which can learn neural open IE models without any human-labelled data by leveraging syntactic and semantic knowledge as noisier, higher-level supervision. Specifically, we first employ syntactic patterns as data labelling functions and pretrain a base model using the generated labels. Then we propose a syntactic and semantic-driven reinforcement learning algorithm, which can effectively generalize the base model to open situations with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. Experimental results show that our approach significantly outperforms the supervised counterparts, and can even achieve competitive performance to supervised state-of-the-art (SoA) model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.75/>Actor-Double-Critic : Incorporating Model-Based Critic for Task-Oriented Dialogue Systems</a></strong><br><a href=/people/y/yen-chen-wu/>Yen-chen Wu</a>
|
<a href=/people/b/bo-hsiang-tseng/>Bo-Hsiang Tseng</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--75><div class="card-body p-3 small">In order to improve the sample-efficiency of deep reinforcement learning (DRL), we implemented imagination augmented agent (I2A) in spoken dialogue systems (SDS). Although I2A achieves a higher success rate than <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> by augmenting predicted future into a policy network, its complicated architecture introduces unwanted instability. In this work, we propose actor-double-critic (ADC) to improve the stability and overall performance of I2A. ADC simplifies the architecture of I2A to reduce excessive parameters and hyper-parameters. More importantly, a separate model-based critic shares parameters between actions and makes <a href=https://en.wikipedia.org/wiki/Backpropagation>back-propagation</a> explicit. In our experiments on Cambridge Restaurant Booking task, ADC enhances success rates considerably and shows robustness to imperfect environment models. In addition, ADC exhibits the stability and sample-efficiency as significantly reducing the baseline standard deviation of success rates and reaching the 80 % success rate with half training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.77.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.77/>Sequential Span Classification with Neural Semi-Markov CRFs for Biomedical Abstracts<span class=acl-fixed-case>M</span>arkov <span class=acl-fixed-case>CRF</span>s for Biomedical Abstracts</a></strong><br><a href=/people/k/kosuke-yamada/>Kosuke Yamada</a>
|
<a href=/people/t/tsutomu-hirao/>Tsutomu Hirao</a>
|
<a href=/people/r/ryohei-sasano/>Ryohei Sasano</a>
|
<a href=/people/k/koichi-takeda/>Koichi Takeda</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--77><div class="card-body p-3 small">Dividing biomedical abstracts into several segments with rhetorical roles is essential for supporting researchers&#8217; information access in the biomedical domain. Conventional methods have regarded the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> as a sequence labeling task based on sequential sentence classification, i.e., they assign a rhetorical label to each sentence by considering the context in the abstract. However, these methods have a critical problem : they are prone to mislabel longer continuous sentences with the same rhetorical label. To tackle the problem, we propose sequential span classification that assigns a rhetorical label, not to a single sentence but to a span that consists of continuous sentences. Accordingly, we introduce Neural Semi-Markov Conditional Random Fields to assign the labels to such spans by considering all possible spans of various lengths. Experimental results obtained from PubMed 20k RCT and NICTA-PIBOSO datasets demonstrate that our proposed method achieved the best micro sentence-F1 score as well as the best micro span-F1 score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.79.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.79" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.79/>AirConcierge : Generating Task-Oriented Dialogue via Efficient Large-Scale Knowledge Retrieval<span class=acl-fixed-case>A</span>ir<span class=acl-fixed-case>C</span>oncierge: Generating Task-Oriented Dialogue via Efficient Large-Scale Knowledge Retrieval</a></strong><br><a href=/people/c/chieh-yang-chen/>Chieh-Yang Chen</a>
|
<a href=/people/p/pei-hsin-wang/>Pei-Hsin Wang</a>
|
<a href=/people/s/shih-chieh-chang/>Shih-Chieh Chang</a>
|
<a href=/people/d/da-cheng-juan/>Da-Cheng Juan</a>
|
<a href=/people/w/wei-wei/>Wei Wei</a>
|
<a href=/people/j/jia-yu-pan/>Jia-Yu Pan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--79><div class="card-body p-3 small">Despite recent success in neural task-oriented dialogue systems, developing such a real-world system involves accessing large-scale knowledge bases (KBs), which can not be simply encoded by neural approaches, such as memory network mechanisms. To alleviate the above problem, we propose, an end-to-end trainable text-to-SQL guided framework to learn a <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>neural agent</a> that interacts with KBs using the generated SQL queries. Specifically, the neural agent first learns to ask and confirm the customer&#8217;s intent during the multi-turn interactions, then dynamically determining when to ground the user constraints into executable SQL queries so as to fetch relevant information from KBs. With the help of our method, the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> can use less but more accurate fetched results to generate useful responses efficiently, instead of incorporating the entire KBs. We evaluate the proposed method on the AirDialogue dataset, a large corpus released by Google, containing the conversations of customers booking flight tickets from the agent. The experimental results show that significantly improves over previous work in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and the BLEU score, which demonstrates not only the ability to achieve the given <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> but also the good quality of the generated dialogues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940629 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.83/>Cross-lingual Alignment Methods for Multilingual BERT : A Comparative Study<span class=acl-fixed-case>BERT</span>: A Comparative Study</a></strong><br><a href=/people/s/saurabh-kulshreshtha/>Saurabh Kulshreshtha</a>
|
<a href=/people/j/jose-luis-redondo-garcia/>Jose Luis Redondo Garcia</a>
|
<a href=/people/c/ching-yun-chang/>Ching-Yun Chang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--83><div class="card-body p-3 small">Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work propose several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs dictionary-based supervision and rotational vs fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks : Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3 % <a href=https://en.wikipedia.org/wiki/F-number>F1</a> improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment methods</a> to the type of task and proximity to the transfer language. We also find that <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> from <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> is generally superior to dictionary alignments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.86" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.86/>Persian Ezafe Recognition Using Transformers and Its Role in Part-Of-Speech Tagging<span class=acl-fixed-case>P</span>ersian Ezafe Recognition Using Transformers and Its Role in Part-Of-Speech Tagging</a></strong><br><a href=/people/e/ehsan-doostmohammadi/>Ehsan Doostmohammadi</a>
|
<a href=/people/m/minoo-nassajian/>Minoo Nassajian</a>
|
<a href=/people/a/adel-rahimi/>Adel Rahimi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--86><div class="card-body p-3 small">Ezafe is a <a href=https://en.wikipedia.org/wiki/Grammatical_particle>grammatical particle</a> in some <a href=https://en.wikipedia.org/wiki/Iranian_languages>Iranian languages</a> that links two words together. Regardless of the important information it conveys, it is almost always not indicated in <a href=https://en.wikipedia.org/wiki/Persian_alphabet>Persian script</a>, resulting in mistakes in reading complex sentences and errors in natural language processing tasks. In this paper, we experiment with different machine learning methods to achieve state-of-the-art results in the task of ezafe recognition. Transformer-based methods, BERT and XLMRoBERTa, achieve the best results, the latter achieving 2.68 % F1-score more than the previous <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>. We, moreover, use ezafe information to improve Persian part-of-speech tagging results and show that such information will not be useful to transformer-based methods and explain why that might be the case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.90.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--90 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.90 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.90.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940138 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.90" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.90/>Generative Data Augmentation for Commonsense Reasoning</a></strong><br><a href=/people/y/yiben-yang/>Yiben Yang</a>
|
<a href=/people/c/chaitanya-malaviya/>Chaitanya Malaviya</a>
|
<a href=/people/j/jared-fernandez/>Jared Fernandez</a>
|
<a href=/people/s/swabha-swayamdipta/>Swabha Swayamdipta</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/j/ji-ping-wang/>Ji-Ping Wang</a>
|
<a href=/people/c/chandra-bhagavatula/>Chandra Bhagavatula</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/d/doug-downey/>Doug Downey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--90><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a> depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a novel generative data augmentation technique, G-DAUGC, that aims to achieve more accurate and robust learning in a low-resource setting. Our approach generates synthetic examples using pretrained language models and selects the most informative and diverse set of examples for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>. On experiments with multiple commonsense reasoning benchmarks, G-DAUGC consistently outperforms existing data augmentation methods based on back-translation, establishing a new state-of-the-art on WinoGrande, <a href=https://en.wikipedia.org/wiki/CODAH>CODAH</a>, and CommonsenseQA, as well as enhances out-of-distribution generalization, proving to be robust against adversaries or perturbations. Our analysis demonstrates that G-DAUGC produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.98/>Semi-Supervised Learning for Video Captioning</a></strong><br><a href=/people/k/ke-lin/>Ke Lin</a>
|
<a href=/people/z/zhuoxin-gan/>Zhuoxin Gan</a>
|
<a href=/people/l/liwei-wang/>Liwei Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--98><div class="card-body p-3 small">Deep neural networks have made great success on video captioning in supervised learning setting. However, annotating videos with descriptions is very expensive and time-consuming. If the video captioning algorithm can benefit from a large number of unlabeled videos, the cost of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> can be reduced. In the proposed study, we make the first attempt to train the video captioning model on labeled data and unlabeled data jointly, in a semi-supervised learning manner. For labeled data, we train them with the traditional cross-entropy loss. For unlabeled data, we leverage a self-critical policy gradient method with the difference between the scores obtained by <a href=https://en.wikipedia.org/wiki/Monte_Carlo_method>Monte-Carlo sampling</a> and greedy decoding as the reward function, while the scores are the negative K-L divergence between output distributions of original video data and augmented video data. The final <a href=https://en.wikipedia.org/wiki/Loss_function>loss</a> is the <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted sum of losses</a> obtained by labeled data and unlabeled data. Experiments conducted on VATEX, MSR-VTT and MSVD dataset demonstrate that the introduction of unlabeled data can improve the performance of the video captioning model. The proposed semi-supervised learning algorithm also outperforms several state-of-the-art semi-supervised learning approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.99.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940647 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.99" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.99/>Multi2OIE : Multilingual Open Information Extraction Based on Multi-Head Attention with BERT<span class=acl-fixed-case>M</span>ultiˆ2<span class=acl-fixed-case>OIE</span>: Multilingual Open Information Extraction Based on Multi-Head Attention with <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/y/youngbin-ro/>Youngbin Ro</a>
|
<a href=/people/y/yukyung-lee/>Yukyung Lee</a>
|
<a href=/people/p/pilsung-kang/>Pilsung Kang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--99><div class="card-body p-3 small">In this paper, we propose Multi^2OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is a <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence-labeling system</a> with an efficient and effective argument extraction method. We use a query, key, and value setting inspired by the Multimodal Transformer to replace the previously used bidirectional long short-term memory architecture with multi-head attention. Multi^2OIE outperforms existing sequence-labeling systems with high computational efficiency on two benchmark evaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed method to multilingual open IE using multilingual BERT. Experimental results on new benchmark datasets introduced for two languages (Spanish and Portuguese) demonstrate that our model outperforms other multilingual systems without training data for the target languages.<tex-math>^2</tex-math>OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query, key, and value setting inspired by the Multimodal Transformer to replace the previously used bidirectional long short-term memory architecture with multi-head attention. Multi<tex-math>^2</tex-math>OIE outperforms existing sequence-labeling systems with high computational efficiency on two benchmark evaluation datasets, Re-OIE2016 and CaRB. Additionally, we apply the proposed method to multilingual open IE using multilingual BERT. Experimental results on new benchmark datasets introduced for two languages (Spanish and Portuguese) demonstrate that our model outperforms other multilingual systems without training data for the target languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.101/>Using the Past Knowledge to Improve Sentiment Classification</a></strong><br><a href=/people/q/qi-qin/>Qi Qin</a>
|
<a href=/people/w/wenpeng-hu/>Wenpeng Hu</a>
|
<a href=/people/b/bing-liu/>Bing Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--101><div class="card-body p-3 small">This paper studies sentiment classification in the lifelong learning setting that incrementally learns a sequence of sentiment classification tasks. It proposes a new lifelong learning model (called L2PG) that can retain and selectively transfer the knowledge learned in the past to help learn the new task. A key innovation of this proposed model is a novel parameter-gate (p-gate) mechanism that regulates the flow or transfer of the previously learned knowledge to the new task. Specifically, it can selectively use the network parameters (which represent the retained knowledge gained from the previous tasks) to assist the learning of the new task t. Knowledge distillation is also employed in the process to preserve the past knowledge by approximating the network output at the state when task t-1 was learned. Experimental results show that L2PG outperforms strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>, including even multiple task learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.102.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.102/>High-order Semantic Role Labeling</a></strong><br><a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/k/kevin-parnow/>Kevin Parnow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--102><div class="card-body p-3 small">Semantic role labeling is primarily used to identify <a href=https://en.wikipedia.org/wiki/Predicate_(grammar)>predicates</a>, <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>arguments</a>, and their <a href=https://en.wikipedia.org/wiki/Semantic_relation>semantic relationships</a>. Due to the limitations of modeling methods and the conditions of pre-identified predicates, previous work has focused on the relationships between predicates and arguments and the correlations between arguments at most, while the correlations between predicates have been neglected for a long time. High-order features and structure learning were very common in modeling such <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlations</a> before the neural network era. In this paper, we introduce a high-order graph structure for the neural semantic role labeling model, which enables the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to explicitly consider not only the isolated predicate-argument pairs but also the interaction between the predicate-argument pairs. Experimental results on 7 languages of the CoNLL-2009 benchmark show that the high-order structural learning techniques are beneficial to the strong performing SRL models and further boost our baseline to achieve new state-of-the-art results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--108 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.108.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940168 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.108" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.108/>Dynamic Semantic Matching and Aggregation Network for Few-shot Intent Detection</a></strong><br><a href=/people/h/hoang-nguyen/>Hoang Nguyen</a>
|
<a href=/people/c/chenwei-zhang/>Chenwei Zhang</a>
|
<a href=/people/c/congying-xia/>Congying Xia</a>
|
<a href=/people/p/philip-s-yu/>Philip Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--108><div class="card-body p-3 small">Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity measure and overly fine-grained matching components. These limitations inhibit generalizing capability towards Generalized Few-shot Learning settings where both seen and novel classes are co-existent. In this paper, we propose a novel Semantic Matching and Aggregation Network where semantic components are distilled from utterances via multi-head self-attention with additional dynamic regularization constraints. These semantic components capture <a href=https://en.wikipedia.org/wiki/High-_and_low-level>high-level information</a>, resulting in more effective matching between instances. Our multi-perspective matching method provides a comprehensive matching measure to enhance representations of both labeled and unlabeled instances. We also propose a more challenging evaluation setting that considers <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification</a> on the joint all-class label space. Extensive experimental results demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. Our code and data are publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--112 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.112/>What Can We Do to Improve <a href=https://en.wikipedia.org/wiki/Peer_review>Peer Review</a> in NLP?<span class=acl-fixed-case>NLP</span>?</a></strong><br><a href=/people/a/anna-rogers/>Anna Rogers</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--112><div class="card-body p-3 small">Peer review is our best tool for judging the quality of conference submissions, but <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is becoming increasingly spurious. We argue that a part of the problem is that the reviewers and area chairs face a poorly defined task forcing apples-to-oranges comparisons. There are several potential ways forward, but the key difficulty is creating the incentives and mechanisms for their consistent implementation in the NLP community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--116 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.116" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.116/>Examining the Ordering of Rhetorical Strategies in Persuasive Requests</a></strong><br><a href=/people/o/omar-shaikh/>Omar Shaikh</a>
|
<a href=/people/j/jiaao-chen/>Jiaao Chen</a>
|
<a href=/people/j/jon-saad-falcon/>Jon Saad-Falcon</a>
|
<a href=/people/p/polo-chau/>Polo Chau</a>
|
<a href=/people/d/diyi-yang/>Diyi Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--116><div class="card-body p-3 small">Interpreting how persuasive language influences audiences has implications across many domains like <a href=https://en.wikipedia.org/wiki/Advertising>advertising</a>, <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a>, and <a href=https://en.wikipedia.org/wiki/Propaganda>propaganda</a>. Persuasion relies on more than a message&#8217;s content. Arranging the order of the message itself (i.e., ordering specific rhetorical strategies) also plays an important role. To examine how strategy orderings contribute to persuasiveness, we first utilize a Variational Autoencoder model to disentangle content and rhetorical strategies in textual requests from a large-scale loan request corpus. We then visualize interplay between content and strategy through an attentional LSTM that predicts the success of textual requests. We find that specific (orderings of) strategies interact uniquely with a request&#8217;s content to impact success rate, and thus the persuasiveness of a request.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--122 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.122/>A Compare Aggregate Transformer for Understanding Document-grounded Dialogue</a></strong><br><a href=/people/l/longxuan-ma/>Longxuan Ma</a>
|
<a href=/people/w/weinan-zhang/>Wei-Nan Zhang</a>
|
<a href=/people/r/runxin-sun/>Runxin Sun</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--122><div class="card-body p-3 small">Unstructured documents serving as external knowledge of the dialogues help to generate more informative responses. Previous research focused on knowledge selection (KS) in the document with dialogue. However, dialogue history that is not related to the current dialogue may introduce <a href=https://en.wikipedia.org/wiki/Noise_(electronics)>noise</a> in the KS processing. In this paper, we propose a Compare Aggregate Transformer (CAT) to jointly denoise the dialogue context and aggregate the document information for response generation. We designed two different comparison mechanisms to reduce <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> (before and during decoding). In addition, we propose two <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for evaluating document utilization efficiency based on word overlap. Experimental results on the CMU_DoG dataset show that the proposed CAT model outperforms the state-of-the-art approach and strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--124 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.124/>Modeling Intra and Inter-modality Incongruity for Multi-Modal Sarcasm Detection</a></strong><br><a href=/people/h/hongliang-pan/>Hongliang Pan</a>
|
<a href=/people/z/zheng-lin/>Zheng Lin</a>
|
<a href=/people/p/peng-fu/>Peng Fu</a>
|
<a href=/people/y/yatao-qi/>Yatao Qi</a>
|
<a href=/people/w/weiping-wang/>Weiping Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--124><div class="card-body p-3 small">Sarcasm is a pervasive phenomenon in today&#8217;s <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. These platforms allow users to create multi-modal messages, including <a href=https://en.wikipedia.org/wiki/Text_messaging>texts</a>, <a href=https://en.wikipedia.org/wiki/Image_sharing>images</a>, and <a href=https://en.wikipedia.org/wiki/Online_video_platform>videos</a>. Existing multi-modal sarcasm detection methods either simply concatenate the features from multi modalities or fuse the multi modalities information in a designed manner. However, they ignore the incongruity character in sarcastic utterance, which is often manifested between modalities or within modalities. Inspired by this, we propose a BERT architecture-based model, which concentrates on both intra and inter-modality incongruity for multi-modal sarcasm detection. To be specific, we are inspired by the idea of self-attention mechanism and design inter-modality attention to capturing inter-modality incongruity. In addition, the co-attention mechanism is applied to model the contradiction within the text. The incongruity information is then used for <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. The experimental results demonstrate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on a public multi-modal sarcasm detection dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.131.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--131 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.131 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.131.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.131/>Conditional Neural Generation using Sub-Aspect Functions for Extractive News Summarization</a></strong><br><a href=/people/z/zhengyuan-liu/>Zhengyuan Liu</a>
|
<a href=/people/k/ke-shi/>Ke Shi</a>
|
<a href=/people/n/nancy-chen/>Nancy Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--131><div class="card-body p-3 small">Much progress has been made in <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a>, fueled by neural architectures using large-scale training corpora. However, in the news domain, neural models easily overfit by leveraging position-related features due to the prevalence of the inverted pyramid writing style. In addition, there is an unmet need to generate a variety of summaries for different users. In this paper, we propose a neural framework that can flexibly control summary generation by introducing a set of sub-aspect functions (i.e. importance, <a href=https://en.wikipedia.org/wiki/Diversity_(politics)>diversity</a>, position). These sub-aspect functions are regulated by a set of <a href=https://en.wikipedia.org/wiki/Control_code>control codes</a> to decide which sub-aspect to focus on during summary generation. We demonstrate that extracted summaries with minimal position bias is comparable with those generated by standard models that take advantage of position preference. We also show that news summaries generated with a focus on <a href=https://en.wikipedia.org/wiki/Multiculturalism>diversity</a> can be more preferred by human raters. These results suggest that a more flexible neural summarization framework providing more control options could be desirable in tailoring to different user preferences, which is useful since it is often impractical to articulate such preferences for different applications a priori.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.134.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940121 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.134" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.134/>Inexpensive Domain Adaptation of Pretrained Language Models : Case Studies on Biomedical NER and Covid-19 QA<span class=acl-fixed-case>NER</span> and Covid-19 <span class=acl-fixed-case>QA</span></a></strong><br><a href=/people/n/nina-poerner/>Nina Poerner</a>
|
<a href=/people/u/ulli-waltinger/>Ulli Waltinger</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--134><div class="card-body p-3 small">Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of <a href=https://en.wikipedia.org/wiki/Computer_hardware>hardware</a>, <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>runtime</a> and <a href=https://en.wikipedia.org/wiki/Carbon_dioxide_in_Earth&#8217;s_atmosphere>CO 2 emissions</a>. Here, we propose a cheaper alternative : We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of a general-domain PTLM. We evaluate on eight English biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60 % of the BioBERT-BERT F1 delta, at 5 % of BioBERT&#8217;s CO 2 footprint and 2 % of its cloud compute cost. We also show how to quickly adapt an existing general-domain Question Answering (QA) model to an emerging domain : the Covid-19 pandemic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.135.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--135 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.135 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940063 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.135" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.135/>Semantically Driven Sentence Fusion : Modeling and Evaluation</a></strong><br><a href=/people/e/eyal-ben-david/>Eyal Ben-David</a>
|
<a href=/people/o/orgad-keller/>Orgad Keller</a>
|
<a href=/people/e/eric-malmi/>Eric Malmi</a>
|
<a href=/people/i/idan-szpektor/>Idan Szpektor</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--135><div class="card-body p-3 small">Sentence fusion is the task of joining related sentences into coherent text. Current training and evaluation schemes for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> are based on single reference ground-truths and do not account for valid fusion variants. We show that this hinders <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> from robustly capturing the semantic relationship between input sentences. To alleviate this, we present an approach in which ground-truth solutions are automatically expanded into multiple references via curated equivalence classes of connective phrases. We apply this method to a large-scale dataset and use the augmented dataset for both model training and evaluation. To improve the learning of semantic representation using multiple references, we enrich the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> with auxiliary discourse classification tasks under a multi-tasking framework. Our experiments highlight the improvements of our approach over <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.140.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--140 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.140 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.140" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.140/>StyleDGPT : Stylized Response Generation with Pre-trained Language Models<span class=acl-fixed-case>S</span>tyle<span class=acl-fixed-case>DGPT</span>: Stylized Response Generation with Pre-trained Language Models</a></strong><br><a href=/people/z/ze-yang/>Ze Yang</a>
|
<a href=/people/w/wei-wu/>Wei Wu</a>
|
<a href=/people/c/can-xu/>Can Xu</a>
|
<a href=/people/x/xinnian-liang/>Xinnian Liang</a>
|
<a href=/people/j/jiaqi-bai/>Jiaqi Bai</a>
|
<a href=/people/l/liran-wang/>Liran Wang</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--140><div class="card-body p-3 small">Generating responses following a desired style has great potentials to extend applications of open-domain dialogue systems, yet is refrained by lacking of parallel data for training. In this work, we explore the challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> with pre-trained <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> that have brought breakthrough to various <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language tasks</a>. To this end, we introduce a KL loss and a style classifier to the fine-tuning step in order to steer response generation towards the target style in both a word-level and a sentence-level. Comprehensive empirical studies with two public datasets indicate that our model can significantly outperform state-of-the-art methods in terms of both style consistency and contextual coherence.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.141.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--141 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.141 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.141/>Enhancing Automated Essay Scoring Performance via Fine-tuning Pre-trained Language Models with Combination of <a href=https://en.wikipedia.org/wiki/Regression_analysis>Regression</a> and Ranking</a></strong><br><a href=/people/r/ruosong-yang/>Ruosong Yang</a>
|
<a href=/people/j/jiannong-cao/>Jiannong Cao</a>
|
<a href=/people/z/zhiyuan-wen/>Zhiyuan Wen</a>
|
<a href=/people/y/youzheng-wu/>Youzheng Wu</a>
|
<a href=/people/x/xiaodong-he/>Xiaodong He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--141><div class="card-body p-3 small">Automated Essay Scoring (AES) is a critical text regression task that automatically assigns scores to essays based on their writing quality. Recently, the performance of sentence prediction tasks has been largely improved by using Pre-trained Language Models via fusing representations from different layers, constructing an auxiliary sentence, using <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, etc. However, to solve the AES task, previous works utilize shallow neural networks to learn essay representations and constrain calculated scores with regression loss or ranking loss, respectively. Since shallow neural networks trained on limited samples show poor performance to capture deep semantic of texts. And without an accurate scoring function, ranking loss and regression loss measures two different aspects of the calculated scores. To improve <a href=https://en.wikipedia.org/wiki/Advanced_Encryption_Standard>AES</a>&#8217;s performance, we find a new way to fine-tune pre-trained language models with multiple losses of the same task. In this paper, we propose to utilize a pre-trained language model to learn text representations first. With scores calculated from the representations, <a href=https://en.wikipedia.org/wiki/Mean_square_error>mean square error loss</a> and the batch-wise ListNet loss with dynamic weights constrain the scores simultaneously. We utilize Quadratic Weighted Kappa to evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the Automated Student Assessment Prize dataset. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms not only state-of-the-art neural models near 3 percent but also the latest statistic model. Especially on the two narrative prompts, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performs much better than all other state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.143.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--143 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.143 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.143/>Inferring about fraudulent collusion risk on Brazilian public works contracts in official texts using a Bi-LSTM approach<span class=acl-fixed-case>B</span>razilian public works contracts in official texts using a <span class=acl-fixed-case>B</span>i-<span class=acl-fixed-case>LSTM</span> approach</a></strong><br><a href=/people/m/marcos-lima/>Marcos Lima</a>
|
<a href=/people/r/roberta-silva/>Roberta Silva</a>
|
<a href=/people/f/felipe-lopes-de-souza-mendes/>Felipe Lopes de Souza Mendes</a>
|
<a href=/people/l/leonardo-r-de-carvalho/>Leonardo R. de Carvalho</a>
|
<a href=/people/a/aleteia-araujo/>Aleteia Araujo</a>
|
<a href=/people/f/flavio-de-barros-vidal/>Flavio de Barros Vidal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--143><div class="card-body p-3 small">Public works procurements move US$ 10 billion yearly in Brazil and are a preferred field for <a href=https://en.wikipedia.org/wiki/Collusion>collusion</a> and <a href=https://en.wikipedia.org/wiki/Fraud>fraud</a>. Federal Police and audit agencies investigate collusion (bid-rigging), over-pricing, and delivery fraud in this field and efforts have been employed to early detect fraud and <a href=https://en.wikipedia.org/wiki/Collusion>collusion</a> on public works procurements. The current automatic methods of <a href=https://en.wikipedia.org/wiki/Fraud_detection>fraud detection</a> use <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> to <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> and usually do not involve annotated data. The use of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> for this kind of <a href=https://en.wikipedia.org/wiki/Application_software>application</a> is rare. Our work introduces a new dataset formed by public procurement calls available on Brazilian official journal (Dirio Oficial da Unio), using by 15,132,968 textual entries of which 1,907 are annotated risky entries. Both bottleneck deep neural network and BiLSTM shown competitive compared with classical <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> and achieved better precision (93.0 % and 92.4 %, respectively), which signs improvements in a <a href=https://en.wikipedia.org/wiki/Criminal_investigation>criminal fraud investigation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.144.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--144 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.144 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.144" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.144/>Data-to-Text Generation with Style Imitation</a></strong><br><a href=/people/s/shuai-lin/>Shuai Lin</a>
|
<a href=/people/w/wentao-wang/>Wentao Wang</a>
|
<a href=/people/z/zichao-yang/>Zichao Yang</a>
|
<a href=/people/x/xiaodan-liang/>Xiaodan Liang</a>
|
<a href=/people/f/frank-f-xu/>Frank F. Xu</a>
|
<a href=/people/e/eric-xing/>Eric Xing</a>
|
<a href=/people/z/zhiting-hu/>Zhiting Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--144><div class="card-body p-3 small">Recent neural approaches to data-to-text generation have mostly focused on improving content fidelity while lacking explicit control over writing styles (e.g., sentence structures, word choices). More traditional <a href=https://en.wikipedia.org/wiki/System>systems</a> use <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>templates</a> to determine the realization of text. Yet manual or automatic construction of high-quality templates is difficult, and a <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>template</a> acting as hard constraints could harm content fidelity when it does not match the record perfectly. We study a new way of <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>stylistic control</a> by using existing sentences as soft templates. That is, a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> learns to imitate the <a href=https://en.wikipedia.org/wiki/Writing_style>writing style</a> of any given exemplar sentence, with automatic adaptions to faithfully describe the record. The <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is challenging due to the lack of parallel data. We develop a neural approach that includes a hybrid attention-copy mechanism, learns with weak supervisions, and is enhanced with a new content coverage constraint. We conduct experiments in <a href=https://en.wikipedia.org/wiki/Restaurant>restaurants</a> and <a href=https://en.wikipedia.org/wiki/Sport>sports domains</a>. Results show our approach achieves stronger performance than a range of comparison methods. Our approach balances well between content fidelity and style control given exemplars that match the records to varying degrees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.145.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--145 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.145 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.145" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.145/>Teaching Machine Comprehension with Compositional Explanations</a></strong><br><a href=/people/q/qinyuan-ye/>Qinyuan Ye</a>
|
<a href=/people/x/xiao-huang/>Xiao Huang</a>
|
<a href=/people/e/elizabeth-boschee/>Elizabeth Boschee</a>
|
<a href=/people/x/xiang-ren/>Xiang Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--145><div class="card-body p-3 small">Advances in machine reading comprehension (MRC) rely heavily on the collection of large scale human-annotated examples in the form of (question, paragraph, answer) triples. In contrast, humans are typically able to generalize with only a few examples, relying on deeper underlying <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>, linguistic sophistication, and/or simply superior <a href=https://en.wikipedia.org/wiki/Deductive_reasoning>deductive powers</a>. In this paper, we focus on teaching machines reading comprehension, using a small number of semi-structured explanations that explicitly inform machines why answer spans are correct. We extract structured variables and rules from explanations and compose neural module teachers that annotate instances for training downstream MRC models. We use learnable neural modules and soft logic to handle linguistic variation and overcome sparse coverage ; the modules are jointly optimized with the MRC model to improve final performance. On the SQuAD dataset, our proposed method achieves 70.14 % <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a> with supervision from 26 explanations, comparable to plain <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> using 1,100 labeled instances, yielding a 12x speed up.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.147.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--147 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.147 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940631 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.147" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.147/>SimAlign : High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings<span class=acl-fixed-case>S</span>im<span class=acl-fixed-case>A</span>lign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings</a></strong><br><a href=/people/m/masoud-jalili-sabet/>Masoud Jalili Sabet</a>
|
<a href=/people/p/philipp-dufter/>Philipp Dufter</a>
|
<a href=/people/f/francois-yvon/>François Yvon</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--147><div class="card-body p-3 small">Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data and <a href=https://en.wikipedia.org/wiki/Data_quality>quality</a> decreases as less <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is available. We propose word alignment methods that require no <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a>. The key idea is to leverage multilingual word embeddings both static and contextualized for <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a>. Our multilingual embeddings are created from <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a> only without relying on any parallel data or <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a>. We find that alignments created from <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are superior for four and comparable for two language pairs compared to those produced by traditional statistical aligners even with abundant parallel data ; e.g., contextualized embeddings achieve a word alignment F1 for English-German that is 5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.148.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--148 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.148 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.148" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.148/>TweetEval : Unified Benchmark and Comparative Evaluation for Tweet Classification<span class=acl-fixed-case>T</span>weet<span class=acl-fixed-case>E</span>val: Unified Benchmark and Comparative Evaluation for Tweet Classification</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/l/leonardo-neves/>Leonardo Neves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--148><div class="card-body p-3 small">The experimental landscape in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> for <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> trained on such domain-specific data. In this paper, we propose a new evaluation framework (TweetEval) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.151.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--151 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.151 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.151/>Cost-effective Selection of Pretraining Data : A Case Study of Pretraining BERT on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a><span class=acl-fixed-case>BERT</span> on Social Media</a></strong><br><a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/b/ben-hachey/>Ben Hachey</a>
|
<a href=/people/c/cecile-paris/>Cecile Paris</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--151><div class="card-body p-3 small">Recent studies on domain-specific BERT models show that effectiveness on downstream tasks can be improved when <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are pretrained on in-domain data. Often, the pretraining data used in these <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are selected based on their subject matter, e.g., <a href=https://en.wikipedia.org/wiki/Biology>biology</a> or <a href=https://en.wikipedia.org/wiki/Computer_science>computer science</a>. Given the range of applications using social media text, and its unique language variety, we pretrain two models on <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> and <a href=https://en.wikipedia.org/wiki/Internet_forum>forum text</a> respectively, and empirically demonstrate the effectiveness of these two resources. In addition, we investigate how <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity measures</a> can be used to nominate in-domain pretraining data. We publicly release our pretrained models at https://bit.ly/35RpTf0.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.152.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--152 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.152 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940122 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.152" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.152/>TopicBERT for Energy Efficient Document Classification<span class=acl-fixed-case>T</span>opic<span class=acl-fixed-case>BERT</span> for Energy Efficient Document Classification</a></strong><br><a href=/people/y/yatin-chaudhary/>Yatin Chaudhary</a>
|
<a href=/people/p/pankaj-gupta/>Pankaj Gupta</a>
|
<a href=/people/k/khushbu-saxena/>Khushbu Saxena</a>
|
<a href=/people/v/vivek-kulkarni/>Vivek Kulkarni</a>
|
<a href=/people/t/thomas-runkler/>Thomas Runkler</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--152><div class="card-body p-3 small">Prior research notes that BERT&#8217;s computational cost grows quadratically with sequence length thus leading to longer <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training times</a>, higher GPU memory constraints and <a href=https://en.wikipedia.org/wiki/Greenhouse_gas>carbon emissions</a>. While recent work seeks to address these scalability issues at pre-training, these issues are also prominent in <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> especially for long sequence tasks like <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. Our work thus focuses on optimizing the <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> for <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. We achieve this by complementary learning of both topic and language models in a unified framework, named TopicBERT. This significantly reduces the number of self-attention operations a main performance bottleneck. Consequently, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a 1.4x (40 %) <a href=https://en.wikipedia.org/wiki/Speedup>speedup</a> with 40 % reduction in <a href=https://en.wikipedia.org/wiki/Carbon_dioxide_in_Earth&#8217;s_atmosphere>CO2 emission</a> while retaining 99.9 % performance over 5 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.162.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--162 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.162 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.162.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.162/>Active Learning Approaches to Enhancing Neural Machine Translation</a></strong><br><a href=/people/y/yuekai-zhao/>Yuekai Zhao</a>
|
<a href=/people/h/haoran-zhang/>Haoran Zhang</a>
|
<a href=/people/s/shuchang-zhou/>Shuchang Zhou</a>
|
<a href=/people/z/zhihua-zhang/>Zhihua Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--162><div class="card-body p-3 small">Active learning is an efficient approach for mitigating <a href=https://en.wikipedia.org/wiki/Data_dependency>data dependency</a> when training neural machine translation (NMT) models. In this paper, we explore new training frameworks by incorporating <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> into various techniques such as <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and iterative back-translation (IBT) under a limited human translation budget. We design a word frequency based acquisition function and combine it with a strong uncertainty based method. The combined <a href=https://en.wikipedia.org/wiki/Methodology>method</a> steadily outperforms all other acquisition functions in various scenarios. As far as we know, we are the first to do a large-scale study on actively training Transformer for <a href=https://en.wikipedia.org/wiki/Neurotransmission>NMT</a>. Specifically, with a human translation budget of only 20 % of the original <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>, we manage to surpass Transformer trained on the entire <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> in three language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.164.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--164 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.164 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.164.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940170 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.164/>Continual Learning Long Short Term Memory</a></strong><br><a href=/people/x/xin-guo/>Xin Guo</a>
|
<a href=/people/y/yu-tian/>Yu Tian</a>
|
<a href=/people/q/qinghan-xue/>Qinghan Xue</a>
|
<a href=/people/p/panos-lampropoulos/>Panos Lampropoulos</a>
|
<a href=/people/s/steven-eliuk/>Steven Eliuk</a>
|
<a href=/people/k/kenneth-barner/>Kenneth Barner</a>
|
<a href=/people/x/xiaolong-wang/>Xiaolong Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--164><div class="card-body p-3 small">Catastrophic forgetting in <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> indicates the performance decreasing of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell in Recurrent Neural Network (RNN) in this paper. CL-LSTM considers not only the state of each individual task&#8217;s output gates but also the correlation of the states between tasks, so that the deep learning models can incrementally learn new tasks without catastrophically forgetting previously tasks. Experimental results demonstrate significant improvements of CL-LSTM over state-of-the-art approaches on spoken language understanding (SLU) tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.166.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--166 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.166 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.166.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940105 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.166" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.166/>Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers</a></strong><br><a href=/people/b/brian-lester/>Brian Lester</a>
|
<a href=/people/d/daniel-pressel/>Daniel Pressel</a>
|
<a href=/people/a/amy-hemmeter/>Amy Hemmeter</a>
|
<a href=/people/s/sagnik-ray-choudhury/>Sagnik Ray Choudhury</a>
|
<a href=/people/s/srinivas-bangalore/>Srinivas Bangalore</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--166><div class="card-body p-3 small">Current state-of-the-art models for named entity recognition (NER) are neural models with a conditional random field (CRF) as the final layer. Entities are represented as per-token labels with a special structure in order to decode them into spans. Current work eschews prior knowledge of how the span encoding scheme works and relies on the CRF learning which transitions are illegal and which are not to facilitate global coherence. We find that by constraining the output to suppress illegal transitions we can train a tagger with a cross-entropy loss twice as fast as a CRF with differences in <a href=https://en.wikipedia.org/wiki/F-number>F1</a> that are statistically insignificant, effectively eliminating the need for a CRF. We analyze the dynamics of tag co-occurrence to explain when these constraints are most effective and provide open source implementations of our tagger in both <a href=https://en.wikipedia.org/wiki/PyTorch>PyTorch</a> and <a href=https://en.wikipedia.org/wiki/TensorFlow>TensorFlow</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.168.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--168 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.168 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.168.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.168/>TED : A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising<span class=acl-fixed-case>TED</span>: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising</a></strong><br><a href=/people/z/ziyi-yang/>Ziyi Yang</a>
|
<a href=/people/c/chenguang-zhu/>Chenguang Zhu</a>
|
<a href=/people/r/robert-gmyr/>Robert Gmyr</a>
|
<a href=/people/m/michael-zeng/>Michael Zeng</a>
|
<a href=/people/x/xuedong-huang/>Xuedong Huang</a>
|
<a href=/people/e/eric-darve/>Eric Darve</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--168><div class="card-body p-3 small">Text summarization aims to extract essential information from a piece of text and transform the text into a concise version. Existing unsupervised abstractive summarization models leverage recurrent neural networks framework while the recently proposed transformer exhibits much more capability. Moreover, most of previous summarization models ignore abundant unlabeled corpora resources available for pretraining. In order to address these issues, we propose TED, a transformer-based unsupervised abstractive summarization system with pretraining on large-scale data. We first leverage the lead bias in <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> to pretrain the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on millions of unlabeled corpora. Next, we finetune TED on target domains through theme modeling and a denoising autoencoder to enhance the quality of generated summaries. Notably, TED outperforms all unsupervised abstractive baselines on NYT, CNN / DM and English Gigaword datasets with various document styles. Further analysis shows that the summaries generated by <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED</a> are highly abstractive, and each component in the <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> of <a href=https://en.wikipedia.org/wiki/TED_(conference)>TED</a> is highly effective.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.169.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--169 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.169 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.169/>Improving End-to-End Bangla Speech Recognition with <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>Semi-supervised Training</a><span class=acl-fixed-case>B</span>angla Speech Recognition with Semi-supervised Training</a></strong><br><a href=/people/n/nafis-sadeq/>Nafis Sadeq</a>
|
<a href=/people/n/nafis-tahmid-chowdhury/>Nafis Tahmid Chowdhury</a>
|
<a href=/people/f/farhan-tanvir-utshaw/>Farhan Tanvir Utshaw</a>
|
<a href=/people/s/shafayat-ahmed/>Shafayat Ahmed</a>
|
<a href=/people/m/muhammad-abdullah-adnan/>Muhammad Abdullah Adnan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--169><div class="card-body p-3 small">Automatic speech recognition systems usually require large annotated speech corpus for training. The manual annotation of a large corpus is very difficult. It can be very helpful to use unsupervised and semi-supervised learning methods in addition to <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>. In this work, we focus on using a semi-supervised training approach for Bangla Speech Recognition that can exploit large unpaired audio and text data. We encode speech and text data in an intermediate domain and propose a novel <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> based on the global encoding distance between encoded data to guide the <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised training</a>. Our proposed method reduces the Word Error Rate (WER) of the <a href=https://en.wikipedia.org/wiki/System>system</a> from 37 % to 31.9 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.171.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--171 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.171 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.171.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.171" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.171/>UNIFIEDQA : Crossing Format Boundaries with a Single QA System<span class=acl-fixed-case>UNIFIEDQA</span>: Crossing Format Boundaries with a Single <span class=acl-fixed-case>QA</span> System</a></strong><br><a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/s/sewon-min/>Sewon Min</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a>
|
<a href=/people/o/oyvind-tafjord/>Oyvind Tafjord</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--171><div class="card-body p-3 small">Question answering (QA) tasks have been posed using a variety of <a href=https://en.wikipedia.org/wiki/File_format>formats</a>, such as extractive span selection, <a href=https://en.wikipedia.org/wiki/Multiple_choice>multiple choice</a>, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.173.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--173 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.173 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.173" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.173/>Pragmatic Issue-Sensitive Image Captioning</a></strong><br><a href=/people/a/allen-nie/>Allen Nie</a>
|
<a href=/people/r/reuben-cohn-gordon/>Reuben Cohn-Gordon</a>
|
<a href=/people/c/christopher-potts/>Christopher Potts</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--173><div class="card-body p-3 small">Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a <a href=https://en.wikipedia.org/wiki/Article_(publishing)>newspaper article</a> about a <a href=https://en.wikipedia.org/wiki/Sport>sports event</a>, a caption that not only identifies the player in a picture but also comments on their ethnicity could create unwanted reader reactions. To address this, we propose Issue-Sensitive Image Captioning (ISIC). In ISIC, the captioner is given a target image and an issue, which is a set of images partitioned in a way that specifies what information is relevant. For the sports article, we could construct a <a href=https://en.wikipedia.org/wiki/Partition_of_a_set>partition</a> that places images into <a href=https://en.wikipedia.org/wiki/Equivalence_class>equivalence classes</a> based on player position. To model this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we use an extension of the Rational Speech Acts model. Our extension is built on top of state-of-the-art pretrained neural image captioners and explicitly uses image partitions to control caption generation. In both automatic and human evaluations, we show that these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> generate captions that are descriptive and issue-sensitive. Finally, we show how ISIC can complement and enrich the related task of Visual Question Answering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.175.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--175 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.175 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.175" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.175/>Adversarial Subword Regularization for Robust Neural Machine Translation</a></strong><br><a href=/people/j/jungsoo-park/>Jungsoo Park</a>
|
<a href=/people/m/mujeen-sung/>Mujeen Sung</a>
|
<a href=/people/j/jinhyuk-lee/>Jinhyuk Lee</a>
|
<a href=/people/j/jaewoo-kang/>Jaewoo Kang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--175><div class="card-body p-3 small">Exposing diverse subword segmentations to neural machine translation (NMT) models often improves the robustness of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> as NMT models can experience various subword candidates. However, the diversification of subword segmentations mostly relies on the pre-trained subword language models from which erroneous segmentations of unseen words are less likely to be sampled. In this paper, we present adversarial subword regularization (ADVSR) to study whether gradient signals during training can be a substitute criterion for exposing diverse subword segmentations. We experimentally show that our model-based adversarial samples effectively encourage NMT models to be less sensitive to segmentation errors and improve the performance of NMT models in low-resource and out-domain datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.176.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--176 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.176 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940178 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.176/>Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays<span class=acl-fixed-case>X</span>-rays</a></strong><br><a href=/people/j/jianmo-ni/>Jianmo Ni</a>
|
<a href=/people/c/chun-nan-hsu/>Chun-Nan Hsu</a>
|
<a href=/people/a/amilcare-gentili/>Amilcare Gentili</a>
|
<a href=/people/j/julian-mcauley/>Julian McAuley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--176><div class="card-body p-3 small">Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists&#8217; workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are affected by <a href=https://en.wikipedia.org/wiki/Bias_of_an_estimator>data bias</a> (e.g. label imbalance) and face common issues inherent in text generation models (e.g. repetition). In this work, we focus on reporting abnormal findings on radiology images ; instead of training on complete radiology reports, we propose a method to identify abnormal findings from the reports in addition to grouping them with unsupervised clustering and minimal rules. We formulate the task as cross-modal retrieval and propose Conditional Visual-Semantic Embeddings to align images and fine-grained abnormal findings in a joint embedding space. We demonstrate that our method is able to retrieve abnormal findings and outperforms existing generation models on both clinical correctness and text generation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.177.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--177 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.177 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.177/>SynET : Synonym Expansion using Transitivity<span class=acl-fixed-case>S</span>yn<span class=acl-fixed-case>ET</span>: Synonym Expansion using Transitivity</a></strong><br><a href=/people/j/jiale-yu/>Jiale Yu</a>
|
<a href=/people/y/yongliang-shen/>Yongliang Shen</a>
|
<a href=/people/x/xinyin-ma/>Xinyin Ma</a>
|
<a href=/people/c/chenghao-jia/>Chenghao Jia</a>
|
<a href=/people/c/chen-chen/>Chen Chen</a>
|
<a href=/people/w/weiming-lu/>Weiming Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--177><div class="card-body p-3 small">In this paper, we study a new task of synonym expansion using transitivity, and propose a novel approach named SynET, which considers both the contexts of two given synonym pairs. It introduces an auxiliary task to reduce the impact of noisy sentences, and proposes a Multi-Perspective Entity Matching Network to match entities from multiple perspectives. Extensive experiments on a real-world dataset show the effectiveness of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.178.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--178 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.178 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.178/>Scheduled DropHead : A Regularization Method for Transformer Models<span class=acl-fixed-case>D</span>rop<span class=acl-fixed-case>H</span>ead: A Regularization Method for Transformer Models</a></strong><br><a href=/people/w/wangchunshu-zhou/>Wangchunshu Zhou</a>
|
<a href=/people/t/tao-ge/>Tao Ge</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a>
|
<a href=/people/k/ke-xu/>Ke Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--178><div class="card-body p-3 small">We introduce DropHead, a structured dropout method specifically designed for regularizing the multi-head attention mechanism which is a key component of <a href=https://en.wikipedia.org/wiki/Transformer>transformer</a>. In contrast to the conventional dropout mechanism which randomly drops units or connections, DropHead drops entire <a href=https://en.wikipedia.org/wiki/Attentional_control>attention heads</a> during training to prevent the multi-head attention model from being dominated by a small portion of <a href=https://en.wikipedia.org/wiki/Attentional_control>attention heads</a>. It can help reduce the risk of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and allow the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to better benefit from the multi-head attention. Given the interaction between <a href=https://en.wikipedia.org/wiki/Multi-headedness>multi-headedness</a> and training dynamics, we further propose a novel dropout rate scheduler to adjust the dropout rate of DropHead throughout training, which results in a better regularization effect. Experimental results demonstrate that our proposed approach can improve transformer models by 0.9 BLEU score on WMT14 En-De translation task and around 1.0 <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for various text classification tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.180.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--180 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.180 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940033 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.180/>Automatically Identifying Gender Issues in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> using Perturbations</a></strong><br><a href=/people/h/hila-gonen/>Hila Gonen</a>
|
<a href=/people/k/kellie-webster/>Kellie Webster</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--180><div class="card-body p-3 small">The successful application of neural methods to <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using synthetic examples, we develop a novel technique to mine examples from real world data to explore challenges for deployed systems. We use our method to compile an <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>evaluation benchmark</a> spanning examples for four languages from three language families, which we publicly release to facilitate research. The examples in our <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> expose where model representations are gendered, and the unintended consequences these gendered representations can have in downstream application.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.181.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--181 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.181 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.181/>Ruler : Data Programming by Demonstration for Document Labeling</a></strong><br><a href=/people/s/sara-evensen/>Sara Evensen</a>
|
<a href=/people/c/chang-ge/>Chang Ge</a>
|
<a href=/people/c/cagatay-demiralp/>Cagatay Demiralp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--181><div class="card-body p-3 small">Data programming aims to reduce the cost of curating training data by encoding <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> as labeling functions over source data. As such it not only requires <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain expertise</a> but also <a href=https://en.wikipedia.org/wiki/Computer_programming>programming experience</a>, a skill that many subject matter experts lack. Additionally, generating functions by enumerating rules is not only time consuming but also inherently difficult, even for people with programming experience. In this paper we introduce Ruler, an interactive system that synthesizes labeling rules using span-level interactive demonstrations over document examples. Ruler is a first-of-a-kind implementation of data programming by demonstration (DPBD). This new framework aims to relieve users from the burden of writing labeling functions, enabling them to focus on higher-level semantic analysis, such as identifying relevant signals for the labeling task. We compare Ruler with conventional data programming through a user study conducted with 10 data scientists who were asked to create labeling functions for sentiment and spam classification tasks. Results show <a href=https://en.wikipedia.org/wiki/Ruler>Ruler</a> is easier to learn and to use, and that it offers higher overall user-satisfaction while providing model performances comparable to those achieved by conventional data programming.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.182.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--182 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.182 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.182/>Dual Reconstruction : a Unifying Objective for Semi-Supervised Neural Machine Translation</a></strong><br><a href=/people/w/weijia-xu/>Weijia Xu</a>
|
<a href=/people/x/xing-niu/>Xing Niu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--182><div class="card-body p-3 small">While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction objective that provides a unified view of Iterative Back-Translation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative Back-Translation is more effective than Dual Learning despite its relative simplicity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.185.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--185 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.185 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.185/>Multi-pretraining for Large-scale Text Classification</a></strong><br><a href=/people/k/kang-min-kim/>Kang-Min Kim</a>
|
<a href=/people/b/bumsu-hyeon/>Bumsu Hyeon</a>
|
<a href=/people/y/yeachan-kim/>Yeachan Kim</a>
|
<a href=/people/j/jun-hyung-park/>Jun-Hyung Park</a>
|
<a href=/people/s/sangkeun-lee/>SangKeun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--185><div class="card-body p-3 small">Deep neural network-based pretraining methods have achieved impressive results in many natural language processing tasks including <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. However, their applicability to large-scale text classification with numerous categories (e.g., several thousands) is yet to be well-studied, where the training data is insufficient and skewed in terms of categories. In addition, existing pretraining methods usually involve excessive computation and <a href=https://en.wikipedia.org/wiki/Memory_management>memory overheads</a>. In this paper, we develop a novel multi-pretraining framework for large-scale text classification. This multi-pretraining framework includes both a self-supervised pretraining and a weakly supervised pretraining. We newly introduce an out-of-context words detection task on the unlabeled data as the self-supervised pretraining. It captures the topic-consistency of words used in sentences, which is proven to be useful for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. In addition, we propose a weakly supervised pretraining, where labels for text classification are obtained automatically from an existing approach. Experimental results clearly show that both pretraining approaches are effective for large-scale text classification task. The proposed scheme exhibits significant improvements as much as 3.8 % in terms of macro-averaging F1-score over strong pretraining methods, while being computationally efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.189.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--189 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.189 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.189.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.189/>Extracting Chemical-Protein Interactions via Calibrated Deep Neural Network and Self-training</a></strong><br><a href=/people/d/dongha-choi/>Dongha Choi</a>
|
<a href=/people/h/hyunju-lee/>Hyunju Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--189><div class="card-body p-3 small">The extraction of interactions between chemicals and proteins from several biomedical articles is important in many fields of <a href=https://en.wikipedia.org/wiki/Medical_research>biomedical research</a> such as <a href=https://en.wikipedia.org/wiki/Drug_development>drug development</a> and prediction of drug side effects. Several <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing methods</a>, including deep neural network (DNN) models, have been applied to address this problem. However, these methods were trained with hard-labeled data, which tend to become over-confident, leading to degradation of the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>model reliability</a>. To estimate the <a href=https://en.wikipedia.org/wiki/Uncertainty>data uncertainty</a> and improve the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a>, <a href=https://en.wikipedia.org/wiki/Calibration>calibration techniques</a> have been applied to <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>. In this study, to extract chemicalprotein interactions, we propose a DNN-based approach incorporating uncertainty information and calibration techniques. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> first encodes the input sequence using a pre-trained language-understanding model, following which it is trained using two calibration methods : mixup training and addition of a <a href=https://en.wikipedia.org/wiki/Confidence_interval>confidence penalty loss</a>. Finally, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is re-trained with <a href=https://en.wikipedia.org/wiki/Augmented_reality>augmented data</a> that are extracted using the estimated uncertainties. Our approach has achieved state-of-the-art performance with regard to the Biocreative VI ChemProt task, while preserving higher calibration abilities than those of previous approaches. Furthermore, our approach also presents the possibilities of using uncertainty estimation for performance improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.191.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--191 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.191 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.191.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940723 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.191" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.191/>MedICaT : A Dataset of Medical Images, Captions, and Textual References<span class=acl-fixed-case>M</span>ed<span class=acl-fixed-case>IC</span>a<span class=acl-fixed-case>T</span>: A Dataset of Medical Images, Captions, and Textual References</a></strong><br><a href=/people/s/sanjay-subramanian/>Sanjay Subramanian</a>
|
<a href=/people/l/lucy-lu-wang/>Lucy Lu Wang</a>
|
<a href=/people/b/ben-bogin/>Ben Bogin</a>
|
<a href=/people/s/sachin-mehta/>Sachin Mehta</a>
|
<a href=/people/m/madeleine-van-zuylen/>Madeleine van Zuylen</a>
|
<a href=/people/s/sravanthi-parasa/>Sravanthi Parasa</a>
|
<a href=/people/s/sameer-singh/>Sameer Singh</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/h/hannaneh-hajishirzi/>Hannaneh Hajishirzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--191><div class="card-body p-3 small">Understanding the relationship between figures and text is key to <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific document understanding</a>. Medical figures in particular are quite complex, often consisting of several subfigures (75 % of figures in our dataset), with detailed text describing their content. Previous work studying figures in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific papers</a> focused on classifying figure content rather than understanding how images relate to the text. To address challenges in figure retrieval and figure-to-text alignment, we introduce MedICaT, a dataset of <a href=https://en.wikipedia.org/wiki/Medical_imaging>medical images</a> in context. MedICaT consists of 217 K images from 131 K open access biomedical papers, and includes captions, inline references for 74 % of figures, and manually annotated subfigures and subcaptions for a subset of figures. Using MedICaT, we introduce the task of subfigure to subcaption alignment in compound figures and demonstrate the utility of inline references in image-text matching. Our data and code can be accessed at https://github.com/allenai/medicat.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.198.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--198 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.198 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.198" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.198/>Zero-Shot Rationalization by Multi-Task Transfer Learning from Question Answering</a></strong><br><a href=/people/p/po-nien-kung/>Po-Nien Kung</a>
|
<a href=/people/t/tse-hsuan-yang/>Tse-Hsuan Yang</a>
|
<a href=/people/y/yi-cheng-chen/>Yi-Cheng Chen</a>
|
<a href=/people/s/sheng-siang-yin/>Sheng-Siang Yin</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--198><div class="card-body p-3 small">Extracting rationales can help human understand which information the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> utilizes and how it makes the prediction towards better <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>. However, annotating rationales requires much effort and only few datasets contain such labeled rationales, making <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> for <a href=https://en.wikipedia.org/wiki/Rationalization_(sociology)>rationalization</a> difficult. In this paper, we propose a novel approach that leverages the benefits of both <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> and <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> for generating rationales through <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a> in a zero-shot fashion. For two benchmark rationalization datasets, the proposed method achieves comparable or even better performance of rationalization without any supervised signal, demonstrating the great potential of zero-shot rationalization for better <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--201 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.201.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.201/>Service-oriented Text-to-SQL Parsing<span class=acl-fixed-case>SQL</span> Parsing</a></strong><br><a href=/people/w/wangsu-hu/>Wangsu Hu</a>
|
<a href=/people/j/jilei-tian/>Jilei Tian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--201><div class="card-body p-3 small">The <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a> from <a href=https://en.wikipedia.org/wiki/Relational_database>relational database</a> requires professionals who has an understanding of structural query language such as <a href=https://en.wikipedia.org/wiki/SQL>SQL</a>. TEXT2SQL models apply <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a> to enable user interacting the database via <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language utterance</a>. Current TEXT2SQL models normally focus on generating complex SQL query in a precise and complete fashion while certain features of real-world application in the production environment is not fully addressed. This paper is aimed to develop a service-oriented Text-to-SQL parser that translates natural language utterance to structural and executable SQL query. We introduce a algorithmic framework named Semantic-Enriched SQL generator (SE-SQL) that enables flexibly access database than rigid API in the application while keeping the performance quality for the most commonly used cases. The qualitative result shows that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 88.3 % execution accuracy on WikiSQL task, outperforming <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by 13 % error reduction. Moreover, the framework considers several service-oriented needs including low-complexity inference, out-of-table rejection, and <a href=https://en.wikipedia.org/wiki/Text_normalization>text normalization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--203 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.203/>Reducing Quantity Hallucinations in Abstractive Summarization</a></strong><br><a href=/people/z/zheng-zhao/>Zheng Zhao</a>
|
<a href=/people/s/shay-b-cohen/>Shay B. Cohen</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--203><div class="card-body p-3 small">It is well-known that abstractive summaries are subject to hallucinationincluding material that is not supported by the original text. While summaries can be made hallucination-free by limiting them to general phrases, such summaries would fail to be very informative. Alternatively, one can try to avoid <a href=https://en.wikipedia.org/wiki/Hallucination>hallucinations</a> by verifying that any specific entities in the summary appear in the original text in a similar context. This is the approach taken by our <a href=https://en.wikipedia.org/wiki/System>system</a>, Herman. The <a href=https://en.wikipedia.org/wiki/System>system</a> learns to recognize and verify quantity entities (dates, numbers, sums of money, etc.) in a beam-worth of abstractive summaries produced by state-of-the-art models, in order to up-rank those summaries whose quantity terms are supported by the original text. Experimental results demonstrate that the ROUGE scores of such up-ranked summaries have a higher <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>Precision</a> than summaries that have not been up-ranked, without a comparable loss in <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a>, resulting in higher F1. Preliminary human evaluation of up-ranked vs. original summaries shows people&#8217;s preference for the former.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--205 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.205/>Sparse and Decorrelated Representations for Stable Zero-shot NMT<span class=acl-fixed-case>NMT</span></a></strong><br><a href=/people/b/bokyung-son/>Bokyung Son</a>
|
<a href=/people/s/sungwon-lyu/>Sungwon Lyu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--205><div class="card-body p-3 small">Using a single encoder and decoder for all directions and training with English-centric data is a popular scheme for multilingual NMT. However, zero-shot translation under this scheme is vulnerable to changes in training conditions, as the model degenerates by decoding non-English texts into English regardless of the target specifier token. We present that enforcing both sparsity and decorrelation on encoder intermediate representations with the SLNI regularizer (Aljundi et al., 2019) efficiently mitigates this problem, without performance loss in supervised directions. Notably, effects of SLNI turns out to be irrelevant to promoting language-invariance in encoder representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--207 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.207/>BERT-MK : Integrating Graph Contextualized Knowledge into Pre-trained Language Models<span class=acl-fixed-case>BERT</span>-<span class=acl-fixed-case>MK</span>: Integrating Graph Contextualized Knowledge into Pre-trained Language Models</a></strong><br><a href=/people/b/bin-he/>Bin He</a>
|
<a href=/people/d/di-zhou/>Di Zhou</a>
|
<a href=/people/j/jinghui-xiao/>Jinghui Xiao</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a>
|
<a href=/people/n/nicholas-jing-yuan/>Nicholas Jing Yuan</a>
|
<a href=/people/t/tong-xu/>Tong Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--207><div class="card-body p-3 small">Complex node interactions are common in knowledge graphs (KGs), and these interactions can be considered as contextualized knowledge exists in the topological structure of KGs. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, neglecting the usage of graph contextualized knowledge. To utilize these unexploited graph-level knowledge, we propose an approach to model <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>subgraphs</a> in a medical KG. Then, the learned <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge</a> is integrated with a pre-trained language model to do the <a href=https://en.wikipedia.org/wiki/Knowledge>knowledge generalization</a>. Experimental results demonstrate that our model achieves the state-of-the-art performance on several medical NLP tasks, and the improvement above MedERNIE indicates that graph contextualized knowledge is beneficial.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--208 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.208.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.208/>Recursive Top-Down Production for Sentence Generation with Latent Trees</a></strong><br><a href=/people/s/shawn-tan/>Shawn Tan</a>
|
<a href=/people/y/yikang-shen/>Yikang Shen</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/a/aaron-courville/>Aaron Courville</a>
|
<a href=/people/t/timothy-odonnell/>Timothy J. O’Donnell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--208><div class="card-body p-3 small">We model the recursive production property of <a href=https://en.wikipedia.org/wiki/Context-free_grammar>context-free grammars</a> for natural and synthetic languages. To this end, we present a <a href=https://en.wikipedia.org/wiki/Dynamic_programming>dynamic programming algorithm</a> that marginalises over latent binary tree structures with N leaves, allowing us to compute the likelihood of a sequence of N tokens under a latent tree model, which we maximise to train a recursive neural function. We demonstrate performance on two synthetic tasks : <a href=https://en.wikipedia.org/wiki/Boolean_satisfiability_problem>SCAN</a>, where it outperforms previous models on the <a href=https://en.wikipedia.org/wiki/Boolean_satisfiability_problem>LENGTH split</a>, and English question formation, where it performs comparably to decoders with the ground-truth tree structure. We also present experimental results on German-English translation on the Multi30k dataset, and qualitatively analyse the induced tree structures our model learns for the SCAN tasks and the German-English translation task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--213 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.213" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.213/>Differentially Private Representation for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> : Formal Guarantee and An Empirical Study on Privacy and Fairness<span class=acl-fixed-case>NLP</span>: Formal Guarantee and An Empirical Study on Privacy and Fairness</a></strong><br><a href=/people/l/lingjuan-lyu/>Lingjuan Lyu</a>
|
<a href=/people/x/xuanli-he/>Xuanli He</a>
|
<a href=/people/y/yitong-li/>Yitong Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--213><div class="card-body p-3 small">It has been demonstrated that hidden representation learned by <a href=https://en.wikipedia.org/wiki/Deep_learning>deep model</a> can encode private information of the input, hence can be exploited to recover such <a href=https://en.wikipedia.org/wiki/Information>information</a> with reasonable accuracy. To address this issue, we propose a novel approach called Differentially Private Neural Representation (DPNR) to preserve privacy of the extracted representation from text. DPNR utilises Differential Privacy (DP) to provide formal privacy guarantee. Further, we show that masking words via dropout can further enhance <a href=https://en.wikipedia.org/wiki/Privacy>privacy</a>. To maintain utility of the learned representation, we integrate DP-noisy representation into a robust training process to derive a robust target model, which also helps for model fairness over various demographic variables. Experimental results on <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a> under various parameter settings demonstrate that DPNR largely reduces privacy leakage without significantly sacrificing the main task performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--217 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.217/>ProphetNet : Predicting Future N-gram for Sequence-to-SequencePre-training<span class=acl-fixed-case>P</span>rophet<span class=acl-fixed-case>N</span>et: Predicting Future N-gram for Sequence-to-<span class=acl-fixed-case>S</span>equence<span class=acl-fixed-case>P</span>re-training</a></strong><br><a href=/people/w/weizhen-qi/>Weizhen Qi</a>
|
<a href=/people/y/yu-yan/>Yu Yan</a>
|
<a href=/people/y/yeyun-gong/>Yeyun Gong</a>
|
<a href=/people/d/dayiheng-liu/>Dayiheng Liu</a>
|
<a href=/people/n/nan-duan/>Nan Duan</a>
|
<a href=/people/j/jiusheng-chen/>Jiusheng Chen</a>
|
<a href=/people/r/ruofei-zhang/>Ruofei Zhang</a>
|
<a href=/people/m/ming-zhou/>Ming Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--217><div class="card-body p-3 small">This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to plan for the future tokens and prevent <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> on strong local correlations. We pre-train ProphetNet using a base scale dataset (16 GB) and a large-scale dataset (160 GB), respectively. Then we conduct experiments on CNN / DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> compared to the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> using the same scale pre-training corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--218 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.218/>DivGAN : Towards Diverse Paraphrase Generation via Diversified Generative Adversarial Network<span class=acl-fixed-case>D</span>iv<span class=acl-fixed-case>GAN</span>: Towards Diverse Paraphrase Generation via Diversified Generative Adversarial Network</a></strong><br><a href=/people/y/yue-cao/>Yue Cao</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--218><div class="card-body p-3 small">Paraphrases refer to texts that convey the same meaning with different expression forms. Traditional seq2seq-based models on <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> mainly focus on the <a href=https://en.wikipedia.org/wiki/Fidelity>fidelity</a> while ignoring the diversity of outputs. In this paper, we propose a deep generative model to generate diverse paraphrases. We build our model based on the conditional generative adversarial network, and propose to incorporate a simple yet effective diversity loss term into the model in order to improve the diversity of outputs. The proposed diversity loss maximizes the ratio of pairwise distance between the generated texts and their corresponding <a href=https://en.wikipedia.org/wiki/Latent_variable>latent codes</a>, forcing the generator to focus more on the <a href=https://en.wikipedia.org/wiki/Latent_variable>latent codes</a> and produce diverse samples. Experimental results on benchmarks of <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> show that our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can generate more diverse paraphrases compared with baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.225.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--225 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.225 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.225" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.225/>Improving Compositional Generalization in Semantic Parsing</a></strong><br><a href=/people/i/inbar-oren/>Inbar Oren</a>
|
<a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/n/nitish-gupta/>Nitish Gupta</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a>
|
<a href=/people/j/jonathan-berant/>Jonathan Berant</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--225><div class="card-body p-3 small">Generalization of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to out-of-distribution (OOD) data has captured tremendous attention recently. Specifically, compositional generalization, i.e., whether a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> generalizes to new structures built of components observed during training, has sparked substantial interest. In this work, we investigate compositional generalization in <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, a natural test-bed for compositional generalization, as output programs are constructed from sub-components. We analyze a wide variety of models and propose multiple extensions to the <a href=https://en.wikipedia.org/wiki/Attention>attention module</a> of the <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a>, aiming to improve compositional generalization. We find that the following factors improve compositional generalization : (a) using contextual representations, such as ELMo and BERT, (b) informing the decoder what input tokens have previously been attended to, (c) training the decoder attention to agree with pre-computed token alignments, and (d) downsampling examples corresponding to frequent program templates. While we substantially reduce the gap between in-distribution and OOD generalization, performance on OOD compositions is still substantially lower.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.226.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--226 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.226 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.226/>Answer Span Correction in Machine Reading Comprehension</a></strong><br><a href=/people/r/revanth-gangi-reddy/>Revanth Gangi Reddy</a>
|
<a href=/people/m/md-arafat-sultan/>Md Arafat Sultan</a>
|
<a href=/people/e/efsun-sarioglu-kayi/>Efsun Sarioglu Kayi</a>
|
<a href=/people/r/rong-zhang/>Rong Zhang</a>
|
<a href=/people/v/vittorio-castelli/>Vittorio Castelli</a>
|
<a href=/people/a/avirup-sil/>Avi Sil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--226><div class="card-body p-3 small">Answer validation in machine reading comprehension (MRC) consists of verifying an extracted answer against an input context and question pair. Previous work has looked at re-assessing the answerability of the question given the extracted answer. Here we address a different problem : the tendency of existing MRC systems to produce partially correct answers when presented with answerable questions. We explore the nature of such errors and propose a post-processing correction method that yields statistically significant performance improvements over state-of-the-art MRC systems in both monolingual and multilingual evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.229.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--229 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.229 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.229/>How Does <a href=https://en.wikipedia.org/wiki/Context_(computing)>Context</a> Matter? On the Robustness of Event Detection with Context-Selective Mask Generalization</a></strong><br><a href=/people/j/jian-liu/>Jian Liu</a>
|
<a href=/people/y/yubo-chen/>Yubo Chen</a>
|
<a href=/people/k/kang-liu/>Kang Liu</a>
|
<a href=/people/y/yantao-jia/>Yantao Jia</a>
|
<a href=/people/z/zhicheng-sheng/>Zhicheng Sheng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--229><div class="card-body p-3 small">Event detection (ED) aims to identify and classify event triggers in texts, which is a crucial subtask of event extraction (EE). Despite many advances in ED, the existing studies are typically centered on improving the overall performance of an ED model, which rarely consider the robustness of an ED model. This paper aims to fill this research gap by stressing the importance of robustness modeling in ED models. We first pinpoint three stark cases demonstrating the brittleness of the existing ED models. After analyzing the underlying reason, we propose a new training mechanism, called context-selective mask generalization for ED, which can effectively mine context-specific patterns for learning and robustify an ED model. The experimental results have confirmed the effectiveness of our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> regarding defending against adversarial attacks, exploring unseen predicates, and tackling ambiguity cases. Moreover, a deeper analysis suggests that our approach can learn a complementary predictive bias with most ED models that use <a href=https://en.wikipedia.org/wiki/Context_(language_use)>full context</a> for <a href=https://en.wikipedia.org/wiki/Feature_learning>feature learning</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--230 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.230" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.230/>Adaptive Feature Selection for End-to-End Speech Translation</a></strong><br><a href=/people/b/biao-zhang/>Biao Zhang</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/r/rico-sennrich/>Rico Sennrich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--230><div class="card-body p-3 small">Information in speech signals is not evenly distributed, making it an additional challenge for end-to-end (E2E) speech translation (ST) to learn to focus on informative features. In this paper, we propose adaptive feature selection (AFS) for encoder-decoder based E2E ST. We first pre-train an <a href=https://en.wikipedia.org/wiki/Speech_recognition>ASR encoder</a> and apply AFS to dynamically estimate the importance of each encoded speech feature to <a href=https://en.wikipedia.org/wiki/Speech_recognition>ASR</a>. A ST encoder, stacked on top of the ASR encoder, then receives the filtered features from the (frozen) ASR encoder. We take L0DROP (Zhang et al., 2020) as the backbone for AFS, and adapt it to sparsify speech features with respect to both temporal and feature dimensions. Results on LibriSpeech EnFr and MuST-C benchmarks show that AFS facilitates learning of ST by pruning out ~84 % temporal features, yielding an average translation gain of ~1.3-1.6 BLEU and a decoding speedup of ~1.4x. In particular, AFS reduces the performance gap compared to the cascade baseline, and outperforms it on LibriSpeech En-Fr with a <a href=https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms>BLEU score</a> of 18.56 (without data augmentation).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--231 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.231" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.231/>Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization</a></strong><br><a href=/people/h/hanqi-jin/>Hanqi Jin</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--231><div class="card-body p-3 small">Single-document and multi-document summarizations are very closely related in both task definition and solution method. In this work, we propose to improve neural abstractive multi-document summarization by jointly learning an abstractive single-document summarizer. We build a unified model for single-document and multi-document summarizations by fully sharing the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and decoder and utilizing a decoding controller to aggregate the <a href=https://en.wikipedia.org/wiki/Code>decoder</a>&#8217;s outputs for multiple input documents. We evaluate our model on two multi-document summarization datasets : Multi-News and DUC-04. Experimental results show the efficacy of our approach, and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can substantially outperform several strong <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a>. We also verify the helpfulness of single-document summarization to abstractive multi-document summarization task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--234 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.234" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.234/>Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction</a></strong><br><a href=/people/z/zhen-wu/>Zhen Wu</a>
|
<a href=/people/c/chengcan-ying/>Chengcan Ying</a>
|
<a href=/people/f/fei-zhao/>Fei Zhao</a>
|
<a href=/people/z/zhifang-fan/>Zhifang Fan</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/r/rui-xia/>Rui Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--234><div class="card-body p-3 small">Aspect-oriented Fine-grained Opinion Extraction (AFOE) aims at extracting aspect terms and opinion terms from review in the form of opinion pairs or additionally extracting sentiment polarity of aspect term to form opinion triplet. Because of containing several opinion factors, the complete AFOE task is usually divided into multiple subtasks and achieved in the <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a>. However, pipeline approaches easily suffer from <a href=https://en.wikipedia.org/wiki/Error_propagation>error propagation</a> and inconvenience in real-world scenarios. To this end, we propose a novel tagging scheme, Grid Tagging Scheme (GTS), to address the AFOE task in an end-to-end fashion only with one unified grid tagging task. Additionally, we design an effective inference strategy on GTS to exploit mutual indication between different opinion factors for more accurate extractions. To validate the feasibility and compatibility of GTS, we implement three different GTS models respectively based on CNN, BiLSTM, and BERT, and conduct experiments on the aspect-oriented opinion pair extraction and opinion triplet extraction datasets. Extensive experimental results indicate that GTS models outperform strong baselines significantly and achieve state-of-the-art performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--235 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.235" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.235/>Learning Numeral Embedding</a></strong><br><a href=/people/c/chengyue-jiang/>Chengyue Jiang</a>
|
<a href=/people/z/zhonglin-nian/>Zhonglin Nian</a>
|
<a href=/people/k/kaihao-guo/>Kaihao Guo</a>
|
<a href=/people/s/shanbo-chu/>Shanbo Chu</a>
|
<a href=/people/y/yinggong-zhao/>Yinggong Zhao</a>
|
<a href=/people/l/libin-shen/>Libin Shen</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--235><div class="card-body p-3 small">Word embedding is an essential building block for <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Although <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> has been extensively studied over the years, the problem of how to effectively embed <a href=https://en.wikipedia.org/wiki/Numeral_system>numerals</a>, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce. In this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for <a href=https://en.wikipedia.org/wiki/Numerical_digit>numerals</a>. We first induce a finite set of prototype numerals using either a <a href=https://en.wikipedia.org/wiki/Self-organizing_map>self-organizing map</a> or a <a href=https://en.wikipedia.org/wiki/Mixture_model>Gaussian mixture model</a>. We then represent the embedding of a <a href=https://en.wikipedia.org/wiki/Numeral_system>numeral</a> as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as <a href=https://en.wikipedia.org/wiki/Skip-gram>skip-gram</a> for training. We evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks : word similarity, embedding numeracy, numeral prediction, and <a href=https://en.wikipedia.org/wiki/Sequence_labeling>sequence labeling</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--237 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.237.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.237/>Fast End-to-end Coreference Resolution for Korean<span class=acl-fixed-case>K</span>orean</a></strong><br><a href=/people/c/cheoneum-park/>Cheoneum Park</a>
|
<a href=/people/j/jamin-shin/>Jamin Shin</a>
|
<a href=/people/s/sungjoon-park/>Sungjoon Park</a>
|
<a href=/people/j/joonho-lim/>Joonho Lim</a>
|
<a href=/people/c/changki-lee/>Changki Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--237><div class="card-body p-3 small">Recently, end-to-end neural network-based approaches have shown significant improvements over traditional pipeline-based models in English coreference resolution. However, such advancements came at a cost of <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> and recent works have not focused on tackling this problem. Hence, in this paper, to cope with this issue, we propose BERT-SRU-based Pointer Networks that leverages the linguistic property of head-final languages. Applying this model to the Korean coreference resolution, we significantly reduce the coreference linking search space. Combining this with Ensemble Knowledge Distillation, we maintain state-of-the-art performance 66.9 % of CoNLL F1 on ETRI test set while achieving 2x speedup (30 doc / sec) in document processing time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.238.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--238 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.238 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940065 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.238/>Toward Stance-based Personas for Opinionated Dialogues</a></strong><br><a href=/people/t/thomas-scialom/>Thomas Scialom</a>
|
<a href=/people/s/serra-sinem-tekiroglu/>Serra Sinem Tekiroğlu</a>
|
<a href=/people/j/jacopo-staiano/>Jacopo Staiano</a>
|
<a href=/people/m/marco-guerini/>Marco Guerini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--238><div class="card-body p-3 small">In the context of chit-chat dialogues it has been shown that endowing <a href=https://en.wikipedia.org/wiki/System>systems</a> with a persona profile is important to produce more coherent and meaningful conversations. Still, the representation of such <a href=https://en.wikipedia.org/wiki/Persona>personas</a> has thus far been limited to a fact-based representation (e.g. I have two cats.). We argue that these <a href=https://en.wikipedia.org/wiki/Representation_(arts)>representations</a> remain superficial w.r.t. the complexity of <a href=https://en.wikipedia.org/wiki/Personality_psychology>human personality</a>. In this work, we propose to make a step forward and investigate stance-based persona, trying to grasp more profound characteristics, such as opinions, <a href=https://en.wikipedia.org/wiki/Value_(ethics)>values</a>, and beliefs to drive <a href=https://en.wikipedia.org/wiki/Language_generation>language generation</a>. To this end, we introduce a novel dataset allowing to explore different stance-based persona representations and their impact on claim generation, showing that they are able to grasp abstract and profound aspects of the author persona.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.239.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--239 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.239 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.239.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.239/>Hierarchical Pre-training for Sequence Labelling in Spoken Dialog</a></strong><br><a href=/people/e/emile-chapuis/>Emile Chapuis</a>
|
<a href=/people/p/pierre-colombo/>Pierre Colombo</a>
|
<a href=/people/m/matteo-manica/>Matteo Manica</a>
|
<a href=/people/m/matthieu-labeau/>Matthieu Labeau</a>
|
<a href=/people/c/chloe-clavel/>Chloé Clavel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--239><div class="card-body p-3 small">Sequence labelling tasks like Dialog Act and Emotion / Sentiment identification are a key component of spoken dialog systems. In this work, we propose a new approach to learn generic representations adapted to spoken dialog, which we evaluate on a new <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> we call Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE benchmark (SILICONE). SILICONE is model-agnostic and contains 10 different datasets of various sizes. We obtain our representations with a hierarchical encoder based on transformer architectures, for which we extend two well-known pre-training objectives. Pre-training is performed on OpenSubtitles : a large corpus of <a href=https://en.wikipedia.org/wiki/Dialogue>spoken dialog</a> containing over 2.3 billion of tokens. We demonstrate how hierarchical encoders achieve competitive results with consistently fewer parameters compared to state-of-the-art models and we show their importance for both pre-training and fine-tuning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.241.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--241 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.241 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940171 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.241/>Out-of-Sample Representation Learning for Knowledge Graphs</a></strong><br><a href=/people/m/marjan-albooyeh/>Marjan Albooyeh</a>
|
<a href=/people/r/rishab-goel/>Rishab Goel</a>
|
<a href=/people/s/seyed-mehran-kazemi/>Seyed Mehran Kazemi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--241><div class="card-body p-3 small">Many important problems can be formulated as reasoning in <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a>. Representation learning has proved extremely effective for <a href=https://en.wikipedia.org/wiki/Transductive_reasoning>transductive reasoning</a>, in which one needs to make new predictions for already observed entities. This is true for both attributed graphs(where each entity has an initial feature vector) and non-attributed graphs (where the only initial information derives from known relations with other entities). For out-of-sample reasoning, where one needs to make predictions for entities that were unseen at training time, much prior work considers <a href=https://en.wikipedia.org/wiki/Attributed_graph>attributed graph</a>. However, this <a href=https://en.wikipedia.org/wiki/Graph_theory>problem</a> is surprisingly under-explored for <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>non-attributed graphs</a>. In this paper, we study the out-of-sample representation learning problem for non-attributed knowledge graphs, create benchmark datasets for this task, develop several <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and baselines, and provide empirical analyses and comparisons of the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.242.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--242 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.242 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.242" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.242/>Fine-Grained Grounding for Multimodal Speech Recognition</a></strong><br><a href=/people/t/tejas-srinivasan/>Tejas Srinivasan</a>
|
<a href=/people/r/ramon-sanabria/>Ramon Sanabria</a>
|
<a href=/people/f/florian-metze/>Florian Metze</a>
|
<a href=/people/d/desmond-elliott/>Desmond Elliott</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--242><div class="card-body p-3 small">Multimodal automatic speech recognition systems integrate information from images to improve <a href=https://en.wikipedia.org/wiki/Speech>speech recognition quality</a>, by grounding the <a href=https://en.wikipedia.org/wiki/Speech>speech</a> in the visual context. While visual signals have been shown to be useful for recovering entities that have been masked in the <a href=https://en.wikipedia.org/wiki/Sound>audio</a>, these models should be capable of recovering a broader range of word types. Existing systems rely on global visual features that represent the entire image, but localizing the relevant regions of the image will make it possible to recover a larger set of words, such as <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a> and <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that uses finer-grained visual information from different parts of the <a href=https://en.wikipedia.org/wiki/Image>image</a>, using automatic object proposals. In experiments on the Flickr8 K Audio Captions Corpus, we find that our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> improves over approaches that use global visual features, that the proposals enable the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to recover entities and other related words, such as adjectives, and that improvements are due to the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s ability to localize the correct proposals.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.243.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--243 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.243 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.243.OptionalSupplementaryMaterial.txt data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.243/>Unsupervised Expressive Rules Provide Explainability and Assist Human Experts Grasping New Domains</a></strong><br><a href=/people/e/eyal-shnarch/>Eyal Shnarch</a>
|
<a href=/people/l/leshem-choshen/>Leshem Choshen</a>
|
<a href=/people/g/guy-moshkowich/>Guy Moshkowich</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--243><div class="card-body p-3 small">Approaching new data can be quite deterrent ; you do not know how your categories of interest are realized in it, commonly, there is no labeled data at hand, and the performance of domain adaptation methods is unsatisfactory. Aiming to assist domain experts in their first steps into a new task over a new corpus, we present an unsupervised approach to reveal complex rules which cluster the unexplored corpus by its prominent categories (or facets). These <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> are human-readable, thus providing an important ingredient which has become in short supply lately-explainability. Each <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rule</a> provides an explanation for the commonality of all the texts it clusters together. The experts can then identify which <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> best capture texts of their categories of interest, and utilize them to deepen their understanding of these categories. These rules can also bootstrap the process of data labeling by pointing at a subset of the corpus which is enriched with texts demonstrating the target categories. We present an extensive evaluation of the usefulness of these rules in identifying target categories, as well as a user study which assesses their interpretability.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.254.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--254 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.254 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.254.OptionalSupplementaryMaterial.txt data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.254" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.254/>Corpora Evaluation and System Bias Detection in Multi-document Summarization</a></strong><br><a href=/people/a/alvin-dey/>Alvin Dey</a>
|
<a href=/people/t/tanya-chowdhury/>Tanya Chowdhury</a>
|
<a href=/people/y/yash-kumar/>Yash Kumar</a>
|
<a href=/people/t/tanmoy-chakraborty/>Tanmoy Chakraborty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--254><div class="card-body p-3 small">Multi-document summarization (MDS) is the task of reflecting key points from any set of documents into a concise text paragraph. In the past, it has been used to aggregate <a href=https://en.wikipedia.org/wiki/News>news</a>, <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, <a href=https://en.wikipedia.org/wiki/Review>product reviews</a>, etc. from various sources. Owing to no standard definition of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, we encounter a plethora of datasets with varying levels of overlap and conflict between participating documents. There is also no standard regarding what constitutes <a href=https://en.wikipedia.org/wiki/Summary_(law)>summary information</a> in <a href=https://en.wikipedia.org/wiki/Summary_(law)>MDS</a>. Adding to the challenge is the fact that new systems report results on a set of chosen datasets, which might not correlate with their performance on the other <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. In this paper, we study this heterogeneous task with the help of a few widely used MDS corpora and a suite of state-of-theart models. We make an attempt to quantify the quality of summarization corpus and prescribe a list of points to consider while proposing a new MDS corpus. Next, we analyze the reason behind the absence of an MDS system which achieves superior performance across all corpora. We then observe the extent to which <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>system metrics</a> are influenced, and bias is propagated due to corpus properties. The scripts to reproduce the experiments in this work are available at https://github.com/LCS2-IIITD/summarization_bias.git</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.255.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--255 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.255 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.255.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.255" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.255/>Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem</a></strong><br><a href=/people/s/shucheng-li/>Shucheng Li</a>
|
<a href=/people/l/lingfei-wu/>Lingfei Wu</a>
|
<a href=/people/s/shiwei-feng/>Shiwei Feng</a>
|
<a href=/people/f/fangli-xu/>Fangli Xu</a>
|
<a href=/people/f/fengyuan-xu/>Fengyuan Xu</a>
|
<a href=/people/s/sheng-zhong/>Sheng Zhong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--255><div class="card-body p-3 small">The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>, <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>, and math word problem solving. However, these models either only consider input objects as <a href=https://en.wikipedia.org/wiki/Sequence>sequences</a> while ignoring the important structural information for <a href=https://en.wikipedia.org/wiki/Code>encoding</a>, or they simply treat output objects as sequence outputs instead of structural objects for decoding. In this paper, we present a novel Graph-to-Tree Neural Networks, namely Graph2Tree consisting of a graph encoder and a hierarchical tree decoder, that encodes an augmented graph-structured input and decodes a tree-structured output. In particular, we investigated our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for solving two problems, neural semantic parsing and math word problem. Our extensive experiments demonstrate that our Graph2Tree model outperforms or matches the performance of other state-of-the-art models on these tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.256.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--256 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.256 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.256/>Target Conditioning for One-to-Many Generation</a></strong><br><a href=/people/m/marie-anne-lachaux/>Marie-Anne Lachaux</a>
|
<a href=/people/a/armand-joulin/>Armand Joulin</a>
|
<a href=/people/g/guillaume-lample/>Guillaume Lample</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--256><div class="card-body p-3 small">Neural Machine Translation (NMT) models often lack diversity in their generated translations, even when paired with <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithm</a>, like <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>. A challenge is that the diversity in translations are caused by the variability in the target language, and can not be inferred from the source sentence alone. In this paper, we propose to explicitly model this one-to-many mapping by conditioning the decoder of a NMT model on a <a href=https://en.wikipedia.org/wiki/Latent_variable>latent variable</a> that represents the domain of target sentences. The domain is a <a href=https://en.wikipedia.org/wiki/Discrete_and_continuous_variables>discrete variable</a> generated by a target encoder that is jointly trained with the NMT model. The predicted domain of target sentences are given as input to the decoder during training. At <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, we can generate diverse translations by decoding with different domains. Unlike our strongest baseline (Shen et al., 2019), our method can scale to any number of domains without affecting the performance or the training time. We assess the quality and diversity of translations generated by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with several <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a>, on three different datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.258.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--258 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.258 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.258/>FENAS : Flexible and Expressive Neural Architecture Search<span class=acl-fixed-case>FENAS</span>: Flexible and Expressive Neural Architecture Search</a></strong><br><a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--258><div class="card-body p-3 small">Architecture search is the automatic process of designing the model or cell structure that is optimal for the given dataset or task. Recently, this approach has shown good improvements in terms of performance (tested on language modeling and image classification) with reasonable training speed using a weight sharing-based approach called Efficient Neural Architecture Search (ENAS). In this work, we propose a novel architecture search algorithm called Flexible and Expressible Neural Architecture Search (FENAS), with more flexible and expressible search space than ENAS, in terms of more activation functions, input edges, and atomic operations. Also, our FENAS approach is able to reproduce the well-known LSTM and GRU architectures (unlike ENAS), and is also able to initialize with them for finding <a href=https://en.wikipedia.org/wiki/Computer_architecture>architectures</a> more efficiently. We explore this extended search space via evolutionary search and show that FENAS performs significantly better on several popular text classification tasks and performs similar to <a href=https://en.wikipedia.org/wiki/ENAS>ENAS</a> on standard language model benchmark. Further, we present ablations and analyses on our FENAS approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.259.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--259 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.259 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.259" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.259/>Inferring symmetry in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a></a></strong><br><a href=/people/c/chelsea-tanchip/>Chelsea Tanchip</a>
|
<a href=/people/l/lei-yu/>Lei Yu</a>
|
<a href=/people/a/aotao-xu/>Aotao Xu</a>
|
<a href=/people/y/yang-xu/>Yang Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--259><div class="card-body p-3 small">We present a methodological framework for inferring symmetry of verb predicates in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Empirical work on predicate symmetry has taken two main approaches. The feature-based approach focuses on <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> pertaining to <a href=https://en.wikipedia.org/wiki/Symmetry>symmetry</a>. The context-based approach denies the existence of absolute symmetry but instead argues that such <a href=https://en.wikipedia.org/wiki/Inference>inference</a> is context dependent. We develop methods that formalize these approaches and evaluate them against a novel symmetry inference sentence (SIS) dataset comprised of 400 naturalistic usages of literature-informed verbs spanning the spectrum of symmetry-asymmetry. Our results show that a hybrid transfer learning model that integrates <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> with contextualized language models most faithfully predicts the <a href=https://en.wikipedia.org/wiki/Empirical_evidence>empirical data</a>. Our work integrates existing approaches to symmetry in natural language and suggests how symmetry inference can improve systematicity in state-of-the-art <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.260.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--260 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.260 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.260" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.260/>A Concise Model for Multi-Criteria Chinese Word Segmentation with Transformer Encoder<span class=acl-fixed-case>C</span>hinese Word Segmentation with Transformer Encoder</a></strong><br><a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/h/hengzhi-pei/>Hengzhi Pei</a>
|
<a href=/people/h/hang-yan/>Hang Yan</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--260><div class="card-body p-3 small">Multi-criteria Chinese word segmentation (MCCWS) aims to exploit the relations among the multiple heterogeneous segmentation criteria and further improve the performance of each single criterion. Previous work usually regards MCCWS as different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, which are learned together under the multi-task learning framework. In this paper, we propose a concise but effective unified model for MCCWS, which is fully-shared for all the criteria. By leveraging the powerful ability of the Transformer encoder, the proposed unified model can segment <a href=https://en.wikipedia.org/wiki/Written_Chinese>Chinese text</a> according to a unique criterion-token indicating the output criterion. Besides, the proposed <a href=https://en.wikipedia.org/wiki/Unified_Model>unified model</a> can segment both <a href=https://en.wikipedia.org/wiki/Simplified_Chinese_characters>simplified and traditional Chinese</a> and has an excellent transfer capability. Experiments on eight datasets with different criteria show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.262.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--262 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.262 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.262.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.262/>Enhancing Content Planning for Table-to-Text Generation with Data Understanding and Verification</a></strong><br><a href=/people/h/heng-gong/>Heng Gong</a>
|
<a href=/people/w/wei-bi/>Wei Bi</a>
|
<a href=/people/x/xiaocheng-feng/>Xiaocheng Feng</a>
|
<a href=/people/b/bing-qin/>Bing Qin</a>
|
<a href=/people/x/xiaojiang-liu/>Xiaojiang Liu</a>
|
<a href=/people/t/ting-liu/>Ting Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--262><div class="card-body p-3 small">Neural table-to-text models, which select and order salient data, as well as verbalizing them fluently via surface realization, have achieved promising progress. Based on results from previous work, the performance bottleneck of current models lies in the stage of content planing (selecting and ordering salient content from the input). That is, performance drops drastically when an <a href=https://en.wikipedia.org/wiki/Oracle_machine>oracle content plan</a> is replaced by a model-inferred one during surface realization. In this paper, we propose to enhance neural content planning by (1) understanding data values with contextual numerical value representations that bring the sense of value comparison into content planning ; (2) verifying the importance and ordering of the selected sequence of records with policy gradient. We evaluated our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on ROTOWIRE and MLB, two datasets on this task, and results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms existing systems with respect to content planning metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.264.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--264 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.264 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940104 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.264/>DiPair : Fast and Accurate Distillation for Trillion-Scale Text Matching and Pair Modeling<span class=acl-fixed-case>D</span>i<span class=acl-fixed-case>P</span>air: Fast and Accurate Distillation for Trillion-Scale Text Matching and Pair Modeling</a></strong><br><a href=/people/j/jiecao-chen/>Jiecao Chen</a>
|
<a href=/people/l/liu-yang/>Liu Yang</a>
|
<a href=/people/k/karthik-raman/>Karthik Raman</a>
|
<a href=/people/m/michael-bendersky/>Michael Bendersky</a>
|
<a href=/people/j/jung-jung-yeh/>Jung-Jung Yeh</a>
|
<a href=/people/y/yun-zhou/>Yun Zhou</a>
|
<a href=/people/m/marc-najork/>Marc Najork</a>
|
<a href=/people/d/danyang-cai/>Danyang Cai</a>
|
<a href=/people/e/ehsan-emadzadeh/>Ehsan Emadzadeh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--264><div class="card-body p-3 small">Pre-trained models like BERT ((Devlin et al., 2018) have dominated NLP / IR applications such as single sentence classification, text pair classification, and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. However, deploying these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> in real systems is highly non-trivial due to their exorbitant computational costs. A common remedy to this is knowledge distillation (Hinton et al., 2015), leading to faster inference. However as we show here existing works are not optimized for dealing with pairs (or tuples) of texts. Consequently, <a href=https://en.wikipedia.org/wiki/IEEE_802.11a-1999>they</a> are either not scalable or demonstrate subpar performance. In this work, we propose DiPair a novel framework for distilling fast and accurate models on text pair tasks. Coupled with an end-to-end training strategy, DiPair is both highly scalable and offers improved quality-speed tradeoffs. Empirical studies conducted on both academic and real-world e-commerce benchmarks demonstrate the efficacy of the proposed approach with speedups of over 350x and minimal quality drop relative to the cross-attention teacher BERT model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.270.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--270 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.270 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940651 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.270/>An Instance Level Approach for Shallow Semantic Parsing in Scientific Procedural Text<span class=acl-fixed-case>A</span>n <span class=acl-fixed-case>I</span>nstance <span class=acl-fixed-case>L</span>evel <span class=acl-fixed-case>A</span>pproach for <span class=acl-fixed-case>S</span>hallow <span class=acl-fixed-case>S</span>emantic <span class=acl-fixed-case>P</span>arsing in <span class=acl-fixed-case>S</span>cientific <span class=acl-fixed-case>P</span>rocedural <span class=acl-fixed-case>T</span>ext</a></strong><br><a href=/people/d/daivik-swarup/>Daivik Swarup</a>
|
<a href=/people/a/ahsaas-bajaj/>Ahsaas Bajaj</a>
|
<a href=/people/s/sheshera-mysore/>Sheshera Mysore</a>
|
<a href=/people/t/tim-ogorman/>Tim O’Gorman</a>
|
<a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--270><div class="card-body p-3 small">In specific domains, such as procedural scientific text, human labeled data for <a href=https://en.wikipedia.org/wiki/Shallow_semantic_parsing>shallow semantic parsing</a> is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different ways of expressing relations in a small number of grammatically similar labeled sentences may provide high coverage of semantic structures in the corpus, through an appropriately rich similarity metric. In light of this opportunity, this paper explores an instance-based approach to the relation prediction sub-task within shallow semantic parsing, in which semantic labels from structurally similar sentences in the training set are copied to test sentences. Candidate similar sentences are retrieved using SciBERT embeddings. For labels where it is possible to copy from a similar sentence we employ an instance level copy network, when this is not possible, a globally shared parametric model is employed. Experiments show our approach outperforms both baseline and prior methods by 0.75 to 3 F1 absolute in the Wet Lab Protocol Corpus and 1 F1 absolute in the Materials Science Procedural Text Corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.271.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--271 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.271 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940109 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.271/>General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference</a></strong><br><a href=/people/j/jingfei-du/>Jingfei Du</a>
|
<a href=/people/m/myle-ott/>Myle Ott</a>
|
<a href=/people/h/haoran-li/>Haoran Li</a>
|
<a href=/people/x/xing-zhou/>Xing Zhou</a>
|
<a href=/people/v/veselin-stoyanov/>Veselin Stoyanov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--271><div class="card-body p-3 small">The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a single piece of text. In that case, <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> during <a href=https://en.wikipedia.org/wiki/Inference>inference</a> can be amortized over the different predictions (tasks) using a shared text encoder. We compare approaches for training such an <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and show that encoders pre-trained over multiple tasks generalize well to unseen tasks. We also compare ways of extracting fixed- and limited-size representations from this <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a>, including pooling features extracted from multiple layers or positions. Our best approach compares favorably to knowledge distillation, achieving higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and lower <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> once the <a href=https://en.wikipedia.org/wiki/System>system</a> is handling around 7 tasks. Further, we show that through binary quantization, we can reduce the size of the extracted representations by a factor of 16 to store them for later use. The resulting method offers a compelling solution for using large-scale pre-trained models at a fraction of the <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> when multiple tasks are performed on the same text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.277.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--277 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.277 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.277/>Knowing What You Know : Calibrating Dialogue Belief State Distributions via Ensembles</a></strong><br><a href=/people/c/carel-van-niekerk/>Carel van Niekerk</a>
|
<a href=/people/m/michael-heck/>Michael Heck</a>
|
<a href=/people/c/christian-geishauser/>Christian Geishauser</a>
|
<a href=/people/h/hsien-chin-lin/>Hsien-chin Lin</a>
|
<a href=/people/n/nurul-lubis/>Nurul Lubis</a>
|
<a href=/people/m/marco-moresi/>Marco Moresi</a>
|
<a href=/people/m/milica-gasic/>Milica Gasic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--277><div class="card-body p-3 small">The ability to accurately track what happens during a conversation is essential for the performance of a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a>. Current state-of-the-art multi-domain dialogue state trackers achieve just over 55 % accuracy on the current go-to benchmark, which means that in almost every second dialogue turn they place full confidence in an incorrect dialogue state. Belief trackers, on the other hand, maintain a distribution over possible dialogue states. However, they lack in performance compared to dialogue state trackers, and do not produce well calibrated distributions. In this work we present state-of-the-art performance in <a href=https://en.wikipedia.org/wiki/Calibration>calibration</a> for multi-domain dialogue belief trackers using a calibrated ensemble of models. Our resulting dialogue belief tracker also outperforms previous dialogue belief tracking models in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.281.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--281 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.281 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.281/>Towards Domain-Independent Text Structuring Trainable on Large Discourse Treebanks</a></strong><br><a href=/people/g/grigorii-guz/>Grigorii Guz</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--281><div class="card-body p-3 small">Text structuring is a fundamental step in NLG, especially when generating multi-sentential text. With the goal of fostering more general and data-driven approaches to text structuring, we propose the new and domain-independent NLG task of structuring and ordering a (possibly large) set of EDUs. We then present a solution for this task that combines neural dependency tree induction with pointer networks, and can be trained on large discourse treebanks that have only recently become available. Further, we propose a new evaluation metric that is arguably more suitable for our new <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> compared to existing content ordering metrics. Finally, we empirically show that our approach outperforms competitive alternatives on the proposed measure and is equivalent in performance with respect to previously established measures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.283.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--283 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.283 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.283/>A Multilingual View of Unsupervised Machine Translation</a></strong><br><a href=/people/x/xavier-garcia/>Xavier Garcia</a>
|
<a href=/people/p/pierre-foret/>Pierre Foret</a>
|
<a href=/people/t/thibault-sellam/>Thibault Sellam</a>
|
<a href=/people/a/ankur-parikh/>Ankur Parikh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--283><div class="card-body p-3 small">We present a probabilistic framework for multilingual neural machine translation that encompasses supervised and unsupervised setups, focusing on unsupervised translation. In addition to studying the vanilla case where there is only monolingual data available, we propose a novel setup where one language in the (source, target) pair is not associated with any parallel data, but there may exist auxiliary parallel data that contains the other. This auxiliary data can naturally be utilized in our probabilistic framework via a novel cross-translation loss term. Empirically, we show that our approach results in higher BLEU scores over state-of-the-art <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised models</a> on the WMT&#8217;14 English-French, WMT&#8217;16 English-German, and WMT&#8217;16 English-Romanian datasets in most directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.287.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--287 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.287 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940035 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.287" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.287/>KoBE : Knowledge-Based Machine Translation Evaluation<span class=acl-fixed-case>K</span>o<span class=acl-fixed-case>BE</span>: Knowledge-Based Machine Translation Evaluation</a></strong><br><a href=/people/z/zorik-gekhman/>Zorik Gekhman</a>
|
<a href=/people/r/roee-aharoni/>Roee Aharoni</a>
|
<a href=/people/g/genady-beryozkin/>Genady Beryozkin</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/w/wolfgang-macherey/>Wolfgang Macherey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--287><div class="card-body p-3 small">We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a large-scale multilingual knowledge base, and (2) measuring the <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> of the grounded entities found in the candidate vs. those found in the source. Our approach achieves the highest correlation with human judgements on 9 out of the 18 language pairs from the WMT19 benchmark for evaluation without references, which is the largest number of wins for a single evaluation method on this task. On 4 language pairs, we also achieve higher correlation with human judgements than <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. To foster further research, we release a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> containing 1.8 million grounded entity mentions across 18 language pairs from the WMT19 metrics track data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.290.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--290 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.290 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.290" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.290/>Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer</a></strong><br><a href=/people/x/xuelu-chen/>Xuelu Chen</a>
|
<a href=/people/m/muhao-chen/>Muhao Chen</a>
|
<a href=/people/c/changjun-fan/>Changjun Fan</a>
|
<a href=/people/a/ankith-uppunda/>Ankith Uppunda</a>
|
<a href=/people/y/yizhou-sun/>Yizhou Sun</a>
|
<a href=/people/c/carlo-zaniolo/>Carlo Zaniolo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--290><div class="card-body p-3 small">Predicting missing facts in a knowledge graph(KG) is a crucial task in knowledge base construction and reasoning, and it has been the subject of much research in recent works us-ing KG embeddings. While existing KG embedding approaches mainly learn and predict facts within a single KG, a more plausible solution would benefit from the knowledge in multiple language-specific KGs, considering that different KGs have their own strengths and limitations on <a href=https://en.wikipedia.org/wiki/Data_quality>data quality</a> and coverage. This is quite challenging since the transfer of knowledge among multiple independently maintained KGs is often hindered by the insufficiency of alignment information and inconsistency of described facts. In this paper, we propose kens, a novel framework for embedding learning and ensemble knowledge transfer across a number of language-specific KGs. KEnS embeds all KGs in a shared embedding space, where the association of entities is captured based on self-learning. Then, KEnS performs ensemble inference to com-bine prediction results from multiple language-specific embeddings, for which multiple en-semble techniques are investigated. Experiments on the basis of five real-world language-specific KGs show that, by effectively identifying and leveraging complementary knowledge, KEnS consistently improves state-of-the-art methods on KG completion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.291.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--291 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.291 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.291" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.291/>Towards Controllable Biases in Language Generation<span class=acl-fixed-case>C</span>ontrollable <span class=acl-fixed-case>B</span>iases in <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>G</span>eneration</a></strong><br><a href=/people/e/emily-sheng/>Emily Sheng</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/p/prem-natarajan/>Prem Natarajan</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--291><div class="card-body p-3 small">We present a general approach towards controllable societal biases in natural language generation (NLG). Building upon the idea of adversarial triggers, we develop a method to induce societal biases in generated text when input prompts contain mentions of specific demographic groups. We then analyze two scenarios : 1) inducing negative biases for one demographic and positive biases for another demographic, and 2) equalizing biases between demographics. The former <a href=https://en.wikipedia.org/wiki/Scenario>scenario</a> enables us to detect the types of <a href=https://en.wikipedia.org/wiki/Bias>biases</a> present in the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Specifically, we show the effectiveness of our approach at facilitating bias analysis by finding topics that correspond to demographic inequalities in generated text and comparing the relative effectiveness of inducing biases for different demographics. The second <a href=https://en.wikipedia.org/wiki/Scenario>scenario</a> is useful for mitigating biases in downstream applications such as dialogue generation. In our experiments, the mitigation technique proves to be effective at equalizing the amount of biases across demographics while simultaneously generating less negatively biased text overall.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.294.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--294 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.294 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.294.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940653 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.294" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.294/>Graph-to-Graph Transformer for Transition-based Dependency Parsing</a></strong><br><a href=/people/a/alireza-mohammadshahi/>Alireza Mohammadshahi</a>
|
<a href=/people/j/james-henderson/>James Henderson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--294><div class="card-body p-3 small">We propose the Graph2Graph Transformer architecture for conditioning on and predicting arbitrary graphs, and apply it to the challenging task of transition-based dependency parsing. After proposing two novel Transformer models of transition-based dependency parsing as strong baselines, we show that adding the proposed mechanisms for conditioning on and predicting graphs of Graph2Graph Transformer results in significant improvements, both with and without BERT pre-training. The novel baselines and their integration with Graph2Graph Transformer significantly outperform the state-of-the-art in traditional transition-based dependency parsing on both English Penn Treebank, and 13 languages of Universal Dependencies Treebanks. Graph2Graph Transformer can be integrated with many previous structured prediction methods, making it easy to apply to a wide range of NLP tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.297.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--297 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.297 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.297/>A Novel Challenge Set for Hebrew Morphological Disambiguation and Diacritics Restoration<span class=acl-fixed-case>H</span>ebrew Morphological Disambiguation and Diacritics Restoration</a></strong><br><a href=/people/a/avi-shmidman/>Avi Shmidman</a>
|
<a href=/people/j/joshua-guedalia/>Joshua Guedalia</a>
|
<a href=/people/s/shaltiel-shmidman/>Shaltiel Shmidman</a>
|
<a href=/people/m/moshe-koppel/>Moshe Koppel</a>
|
<a href=/people/r/reut-tsarfaty/>Reut Tsarfaty</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--297><div class="card-body p-3 small">One of the primary tasks of <a href=https://en.wikipedia.org/wiki/Parsing>morphological parsers</a> is the disambiguation of homographs. Particularly difficult are cases of unbalanced ambiguity, where one of the possible analyses is far more frequent than the others. In such cases, there may not exist sufficient examples of the minority analyses in order to properly evaluate performance, nor to train effective <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. In this paper we address the issue of unbalanced morphological ambiguities in <a href=https://en.wikipedia.org/wiki/Hebrew_language>Hebrew</a>. We offer a challenge set for Hebrew homographs the first of its kind containing substantial attestation of each analysis of 21 Hebrew homographs. We show that the current SOTA of Hebrew disambiguation performs poorly on cases of unbalanced ambiguity. Leveraging our new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we achieve a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> for all 21 words, improving the overall average F1 score from 0.67 to 0.95. Our resulting annotated datasets are made publicly available for further research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.299.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--299 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.299 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.299/>A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge Graph</a></strong><br><a href=/people/l/lin-qiao/>Lin Qiao</a>
|
<a href=/people/j/jianhao-yan/>Jianhao Yan</a>
|
<a href=/people/f/fandong-meng/>Fandong Meng</a>
|
<a href=/people/z/zhendong-yang/>Zhendong Yang</a>
|
<a href=/people/j/jie-zhou/>Jie Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--299><div class="card-body p-3 small">Generating a vivid, novel, and diverse essay with only several given topic words is a promising task of <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a>. Previous work in this task exists two challenging problems : neglect of sentiment beneath the text and insufficient utilization of topic-related knowledge. Therefore, we propose a novel Sentiment Controllable topic-to- essay generator with a Topic Knowledge Graph enhanced decoder, named SCTKG, which is based on the conditional variational auto-encoder (CVAE) framework. We firstly inject the sentiment information into the generator for controlling sentiment for each sentence, which leads to various generated essays. Then we design a Topic Knowledge Graph enhanced decoder. Unlike existing models that use knowledge entities separately, our model treats knowledge graph as a whole and encodes more structured, connected semantic information in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> to generate a more relevant essay. Experimental results show that our SCTKG can generate sentiment controllable essays and outperform the state-of-the-art approach in terms of topic relevance, fluency, and diversity on both automatic and human evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--301 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.301" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.301/>RealToxicityPrompts : Evaluating Neural Toxic Degeneration in Language Models<span class=acl-fixed-case>R</span>eal<span class=acl-fixed-case>T</span>oxicity<span class=acl-fixed-case>P</span>rompts: Evaluating Neural Toxic Degeneration in Language Models</a></strong><br><a href=/people/s/samuel-gehman/>Samuel Gehman</a>
|
<a href=/people/s/suchin-gururangan/>Suchin Gururangan</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--301><div class="card-body p-3 small">Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100 K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from <a href=https://en.wikipedia.org/wiki/Toxicity_(disambiguation)>toxicity</a> than simpler solutions (e.g., banning bad words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2 ; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--305 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.305" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.305/>On the Importance of Adaptive Data Collection for Extremely Imbalanced Pairwise Tasks<span class=acl-fixed-case>O</span>n the <span class=acl-fixed-case>I</span>mportance of <span class=acl-fixed-case>A</span>daptive <span class=acl-fixed-case>D</span>ata <span class=acl-fixed-case>C</span>ollection for <span class=acl-fixed-case>E</span>xtremely <span class=acl-fixed-case>I</span>mbalanced <span class=acl-fixed-case>P</span>airwise <span class=acl-fixed-case>T</span>asks</a></strong><br><a href=/people/s/stephen-mussmann/>Stephen Mussmann</a>
|
<a href=/people/r/robin-jia/>Robin Jia</a>
|
<a href=/people/p/percy-liang/>Percy Liang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--305><div class="card-body p-3 small">Many pairwise classification tasks, such as <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a> and <a href=https://en.wikipedia.org/wiki/Open-domain_question_answering>open-domain question answering</a>, naturally have extreme label imbalance (e.g., 99.99 % of examples are negatives). In contrast, many recent <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> heuristically choose examples to ensure label balance. We show that these heuristics lead to trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that generalize poorly : State-of-the art <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> trained on <a href=https://en.wikipedia.org/wiki/QQP>QQP</a> and WikiQA each have only 2.4 % average precision when evaluated on realistically imbalanced test data. We instead collect training data with <a href=https://en.wikipedia.org/wiki/Active_learning_(machine_learning)>active learning</a>, using a BERT-based embedding model to efficiently retrieve uncertain points from a very large pool of unlabeled utterance pairs. By creating balanced training data with more informative negative examples, <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> greatly improves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>average precision</a> to 32.5 % on <a href=https://en.wikipedia.org/wiki/QQP>QQP</a> and 20.1 % on WikiQA.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--312 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.312.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.312/>A Semantics-based Approach to Disclosure Classification in <a href=https://en.wikipedia.org/wiki/User-generated_content>User-Generated Online Content</a></a></strong><br><a href=/people/c/chandan-akiti/>Chandan Akiti</a>
|
<a href=/people/a/anna-squicciarini/>Anna Squicciarini</a>
|
<a href=/people/s/sarah-rajtmajer/>Sarah Rajtmajer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--312><div class="card-body p-3 small">As users engage in <a href=https://en.wikipedia.org/wiki/Public_sphere>public discourse</a>, the rate of voluntarily disclosed personal information has seen a steep increase. So-called <a href=https://en.wikipedia.org/wiki/Self-disclosure>self-disclosure</a> can result in a number of privacy concerns. Users are often unaware of the sheer amount of personal information they share across online forums, commentaries, and <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>, as well as the power of modern AI to synthesize and gain insights from this <a href=https://en.wikipedia.org/wiki/Data>data</a>. This paper presents an approach to detect emotional and informational self-disclosure in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. We hypothesize that identifying <a href=https://en.wikipedia.org/wiki/Frame_semantics_(linguistics)>frame semantics</a> can meaningfully support this task. Specifically, we use <a href=https://en.wikipedia.org/wiki/Semantic_Role_Labeling>Semantic Role Labeling</a> to identify the lexical units and their semantic roles that signal <a href=https://en.wikipedia.org/wiki/Self-disclosure>self-disclosure</a>. Experimental results on Reddit data show the performance gain of our method when compared to standard text classification methods based on BiLSTM, and BERT. In addition to improved performance, our approach provides insights into the drivers of disclosure behaviors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--313 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.313.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.313" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.313/>Mining Knowledge for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a> from Wikipedia Categories<span class=acl-fixed-case>W</span>ikipedia Categories</a></strong><br><a href=/people/m/mingda-chen/>Mingda Chen</a>
|
<a href=/people/z/zewei-chu/>Zewei Chu</a>
|
<a href=/people/k/karl-stratos/>Karl Stratos</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--313><div class="card-body p-3 small">Accurate lexical entailment (LE) and natural language inference (NLI) often require large quantities of costly annotations. To alleviate the need for labeled data, we introduce WikiNLI : a resource for improving model performance on NLI and LE tasks. It contains 428,899 pairs of phrases constructed from naturally annotated category hierarchies in <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. We show that we can improve strong baselines such as BERT and RoBERTa by pretraining them on WikiNLI and transferring the models on downstream tasks. We conduct systematic comparisons with phrases extracted from other <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a> such as <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a> and <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a> to find that pretraining on WikiNLI gives the best performance. In addition, we construct WikiNLI in other languages, and show that pretraining on them improves performance on NLI tasks of corresponding languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--314 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.314/>OCNLI : Original Chinese Natural Language Inference<span class=acl-fixed-case>OCNLI</span>: <span class=acl-fixed-case>O</span>riginal <span class=acl-fixed-case>C</span>hinese <span class=acl-fixed-case>N</span>atural <span class=acl-fixed-case>L</span>anguage <span class=acl-fixed-case>I</span>nference</a></strong><br><a href=/people/h/hai-hu/>Hai Hu</a>
|
<a href=/people/k/kyle-richardson/>Kyle Richardson</a>
|
<a href=/people/l/liang-xu/>Liang Xu</a>
|
<a href=/people/l/lu-li/>Lu Li</a>
|
<a href=/people/s/sandra-kubler/>Sandra Kübler</a>
|
<a href=/people/l/lawrence-s-moss/>Lawrence Moss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--314><div class="card-body p-3 small">Despite the tremendous recent progress on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference (NLI)</a>, driven largely by large-scale investment in new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> (e.g.,SNLI, MNLI) and advances in <a href=https://en.wikipedia.org/wiki/Mathematical_model>modeling</a>, most progress has been limited to <a href=https://en.wikipedia.org/wiki/English_language>English</a> due to a lack of reliable datasets for most of the world&#8217;s languages. In this paper, we present the first large-scale NLI dataset (consisting of ~56,000 annotated sentence pairs) for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> called the Original <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese Natural Language Inference dataset (OCNLI)</a>. Unlike recent attempts at extending NLI to other languages, our dataset does not rely on any <a href=https://en.wikipedia.org/wiki/Automatic_translation>automatic translation</a> or non-expert annotation. Instead, we elicit <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> from native speakers specializing in <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>. We follow closely the annotation protocol used for MNLI, but create new strategies for eliciting diverse hypotheses. We establish several baseline results on our dataset using state-of-the-art pre-trained models for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, and find even the best performing models to be far outpaced by human performance (~12 % absolute performance gap), making it a challenging new resource that we hope will help to accelerate progress in <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese NLU</a>. To the best of our knowledge, this is the first human-elicited MNLI-style corpus for a non-English language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--315 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.315/>Margin-aware Unsupervised Domain Adaptation for Cross-lingual Text Labeling</a></strong><br><a href=/people/d/dejiao-zhang/>Dejiao Zhang</a>
|
<a href=/people/r/ramesh-nallapati/>Ramesh Nallapati</a>
|
<a href=/people/h/henghui-zhu/>Henghui Zhu</a>
|
<a href=/people/f/feng-nan/>Feng Nan</a>
|
<a href=/people/c/cicero-dos-santos/>Cicero Nogueira dos Santos</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a>
|
<a href=/people/b/bing-xiang/>Bing Xiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--315><div class="card-body p-3 small">Unsupervised domain adaptation addresses the problem of leveraging labeled data in a source domain to learn a well-performing <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> in a target domain where labels are unavailable. In this paper, we improve upon a recent theoretical work (Zhang et al., 2019b) and adopt the Margin Disparity Discrepancy (MDD) unsupervised domain adaptation algorithm to solve the cross-lingual text labeling problems. Experiments on cross-lingual document classification and NER demonstrate the proposed domain adaptation approach advances the state-of-the-art results by a large margin. Specifically, we improve MDD by efficiently optimizing the margin loss on the source domain via Virtual Adversarial Training (VAT). This bridges the gap between theory and the <a href=https://en.wikipedia.org/wiki/Loss_function>loss function</a> used in the original work Zhang et al. (2019b), and thereby significantly boosts the performance. Our numerical results also indicate that VAT can remarkably improve the <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> performance of both domains for various domain adaptation approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--323 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940174 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.323" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.323/>Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher</a></strong><br><a href=/people/g/giannis-karamanolakis/>Giannis Karamanolakis</a>
|
<a href=/people/d/daniel-hsu/>Daniel Hsu</a>
|
<a href=/people/l/luis-gravano/>Luis Gravano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--323><div class="card-body p-3 small">Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-lingual resources, such as parallel corpora, while less expensive cross-lingual representation learning approaches train <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> without target labeled documents. In this work, we propose a cross-lingual teacher-student method, CLTS, that generates weak supervision in the target language using minimal cross-lingual resources, in the form of a small number of word translations. Given a limited translation budget, CLTS extracts and transfers only the most important task-specific seed words across languages and initializes a teacher classifier based on the translated seed words. Then, CLTS iteratively trains a more powerful student that also exploits the context of the seed words in unlabeled target documents and outperforms the teacher. CLTS is simple and surprisingly effective in 18 diverse languages : by transferring just 20 seed words, even a bag-of-words logistic regression student outperforms state-of-the-art cross-lingual methods (e.g., based on multilingual BERT). Moreover, CLTS can accommodate any type of student classifier : leveraging a monolingual BERT student leads to further improvements and outperforms even more expensive approaches by up to 12 % in accuracy. Finally, CLTS addresses emerging tasks in low-resource languages using just a small number of <a href=https://en.wikipedia.org/wiki/Translations>word translations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--328 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.328" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.328/>Using Visual Feature Space as a Pivot Across Languages</a></strong><br><a href=/people/z/ziyan-yang/>Ziyan Yang</a>
|
<a href=/people/l/leticia-pinto-alva/>Leticia Pinto-Alva</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/v/vicente-ordonez/>Vicente Ordonez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--328><div class="card-body p-3 small">Our work aims to leverage visual feature space to pass information across languages. We show that models trained to generate textual captions in more than one language conditioned on an input image can leverage their jointly trained <a href=https://en.wikipedia.org/wiki/Feature_space>feature space</a> during <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> to pivot across languages. We particularly demonstrate improved quality on a <a href=https://en.wikipedia.org/wiki/Closed_captioning>caption</a> generated from an input <a href=https://en.wikipedia.org/wiki/Image>image</a>, by leveraging a caption in a second language. More importantly, we demonstrate that even without conditioning on any <a href=https://en.wikipedia.org/wiki/Visual_system>visual input</a>, the model demonstrates to have learned implicitly to perform to some extent <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> from one language to another in their shared visual feature space. We show results in German-English, and Japanese-English language pairs that pave the way for using the visual world to learn a common representation for language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--332 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.332" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.332/>Document Classification for COVID-19 Literature<span class=acl-fixed-case>COVID</span>-19 Literature</a></strong><br><a href=/people/b/bernal-jimenez-gutierrez/>Bernal Jimenez Gutierrez</a>
|
<a href=/people/j/jucheng-zeng/>Jucheng Zeng</a>
|
<a href=/people/d/dongdong-zhang/>Dongdong Zhang</a>
|
<a href=/people/p/ping-zhang/>Ping Zhang</a>
|
<a href=/people/y/yu-su/>Yu Su</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--332><div class="card-body p-3 small">The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset, a growing collection of 23,000 research papers regarding the novel 2019 coronavirus. We find that pre-trained language models fine-tuned on this dataset outperform all other baselines and that BioBERT surpasses the others by a small margin with micro-F1 and accuracy scores of around 86 % and 75 % respectively on the test set. We evaluate the <a href=https://en.wikipedia.org/wiki/Data_efficiency>data efficiency</a> and generalizability of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> as essential features of any system prepared to deal with an urgent situation like the current <a href=https://en.wikipedia.org/wiki/Health_crisis>health crisis</a>. We perform a data ablation study to determine how important article titles are for achieving reasonable performance on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Finally, we explore 50 errors made by the best performing models on LitCovid documents and find that they often (1) correlate certain labels too closely together and (2) fail to focus on discriminative sections of the articles ; both of which are important issues to address in future work. Both data and code are available on GitHub.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--333 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940111 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.333/>Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension</a></strong><br><a href=/people/a/adyasha-maharana/>Adyasha Maharana</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--333><div class="card-body p-3 small">Reading comprehension models often overfit to nuances of training datasets and fail at <a href=https://en.wikipedia.org/wiki/Adversarial_system>adversarial evaluation</a>. Training with adversarially augmented dataset improves <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> against those adversarial attacks but hurts <a href=https://en.wikipedia.org/wiki/Generalization>generalization of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a></a>. In this work, we present several effective adversaries and automated data augmentation policy search methods with the goal of making reading comprehension models more robust to adversarial evaluation, but also improving generalization to the source domain as well as new domains and languages. We first propose three new methods for generating QA adversaries, that introduce multiple points of confusion within the context, show dependence on insertion location of the distractor, and reveal the compounding effect of mixing adversarial strategies with syntactic and semantic paraphrasing methods. Next, we find that augmenting the training datasets with uniformly sampled adversaries improves <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to the adversarial attacks but leads to decline in performance on the original unaugmented dataset. We address this issue via RL and more efficient Bayesian policy search methods for automatically learning the best augmentation policy combinations of the transformation probability for each adversary in a large search space. Using these learned policies, we show that adversarial training can lead to significant improvements in in-domain, out-of-domain, and cross-lingual (German, Russian, Turkish) generalization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.335.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--335 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.335 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.335.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940182 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.335/>Dr. Summarize : Global Summarization of Medical Dialogue by Exploiting Local Structures.</a></strong><br><a href=/people/a/anirudh-joshi/>Anirudh Joshi</a>
|
<a href=/people/n/namit-katariya/>Namit Katariya</a>
|
<a href=/people/x/xavier-amatriain/>Xavier Amatriain</a>
|
<a href=/people/a/anitha-kannan/>Anitha Kannan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--335><div class="card-body p-3 small">Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open-ended conversation with very domain-specific elements that require expertise and medical knowledge. Summarization of medical conversations is a particularly important aspect of medical conversation understanding since it addresses a very real need in medical practice : capturing the most important aspects of a medical encounter so that they can be used for medical decision making and subsequent follow ups. In this paper we present a novel approach to medical conversation summarization that leverages the unique and independent local structures created when gathering a patient&#8217;s medical history. Our approach is a variation of the pointer generator network where we introduce a penalty on the generator distribution, and we explicitly model negations. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> also captures important properties of medical conversations such as medical knowledge coming from standardized medical ontologies better than when those concepts are introduced explicitly. Through evaluation by doctors, we show that our approach is preferred on twice the number of summaries to the baseline pointer generator model and captures most or all of the information in 80 % of the conversations making it a realistic alternative to costly manual summarization by medical experts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.347.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--347 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.347 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940709 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.347/>Joint Turn and Dialogue level User Satisfaction Estimation on Multi-Domain Conversations</a></strong><br><a href=/people/p/praveen-kumar-bodigutla/>Praveen Kumar Bodigutla</a>
|
<a href=/people/a/aditya-tiwari/>Aditya Tiwari</a>
|
<a href=/people/s/spyros-matsoukas/>Spyros Matsoukas</a>
|
<a href=/people/j/josep-valls-vargas/>Josep Valls-Vargas</a>
|
<a href=/people/l/lazaros-polymenakos/>Lazaros Polymenakos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--347><div class="card-body p-3 small">Dialogue level quality estimation is vital for optimizing data driven dialogue management. Current automated methods to estimate turn and dialogue level user satisfaction employ hand-crafted features and rely on complex annotation schemes, which reduce the generalizability of the trained models. We propose a novel user satisfaction estimation approach which minimizes an adaptive multi-task loss function in order to jointly predict turn-level Response Quality labels provided by experts and explicit dialogue-level ratings provided by end users. The proposed BiLSTM based deep neural net model automatically weighs each turn&#8217;s contribution towards the estimated dialogue-level rating, implicitly encodes temporal dependencies, and removes the need to hand-craft features. On dialogues sampled from 28 <a href=https://en.wikipedia.org/wiki/Alexa_Internet>Alexa domains</a>, two dialogue systems and three user groups, the joint dialogue-level satisfaction estimation model achieved up to an absolute 27 % (0.43-0.70) and 7 % (0.63-0.70) improvement in <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>linear correlation</a> performance over baseline deep neural net and benchmark Gradient boosting regression models, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.349.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--349 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.349 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.349/>Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training</a></strong><br><a href=/people/r/renjie-zheng/>Renjie Zheng</a>
|
<a href=/people/m/mingbo-ma/>Mingbo Ma</a>
|
<a href=/people/b/baigong-zheng/>Baigong Zheng</a>
|
<a href=/people/k/kaibo-liu/>Kaibo Liu</a>
|
<a href=/people/j/jiahong-yuan/>Jiahong Yuan</a>
|
<a href=/people/k/kenneth-church/>Kenneth Church</a>
|
<a href=/people/l/liang-huang/>Liang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--349><div class="card-body p-3 small">Simultaneous speech-to-speech translation is an extremely challenging but widely useful scenario that aims to generate target-language speech only a few seconds behind the source-language speech. In addition, we have to continuously translate a speech of multiple sentences, but all recent solutions merely focus on the single-sentence scenario. As a result, current approaches will accumulate more and more latencies in later sentences when the speaker talks faster and introduce unnatural pauses into translated speech when the speaker talks slower. To overcome these issues, we propose Self-Adaptive Translation which flexibly adjusts the length of translations to accommodate different source speech rates. At similar levels of translation quality (as measured by BLEU), our method generates more fluent target speech latency than the baseline, in both Zh-En directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.351.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--351 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.351 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.351.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.351/>MCMH : Learning Multi-Chain Multi-Hop Rules for Knowledge Graph Reasoning<span class=acl-fixed-case>MCMH</span>: Learning Multi-Chain Multi-Hop Rules for Knowledge Graph Reasoning</a></strong><br><a href=/people/l/lu-zhang/>Lu Zhang</a>
|
<a href=/people/m/mo-yu/>Mo Yu</a>
|
<a href=/people/t/tian-gao/>Tian Gao</a>
|
<a href=/people/y/yue-yu/>Yue Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--351><div class="card-body p-3 small">Multi-hop reasoning approaches over <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graphs</a> infer a missing relationship between entities with a multi-hop rule, which corresponds to a chain of relationships. We extend existing works to consider a generalized form of multi-hop rules, where each rule is a set of <a href=https://en.wikipedia.org/wiki/Relation_(database)>relation chains</a>. To learn such generalized rules efficiently, we propose a two-step approach that first selects a small set of relation chains as a rule and then evaluates the confidence of the target relationship by jointly scoring the selected chains. A game-theoretical framework is proposed to this end to simultaneously optimize the rule selection and prediction steps. Empirical results show that our multi-chain multi-hop (MCMH) rules result in superior results compared to the standard single-chain approaches, justifying both our formulation of generalized rules and the effectiveness of the proposed learning framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.352.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--352 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.352 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.352.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.352" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.352/>Finding the Optimal Vocabulary Size for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/t/thamme-gowda/>Thamme Gowda</a>
|
<a href=/people/j/jonathan-may/>Jonathan May</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--352><div class="card-body p-3 small">We cast neural machine translation (NMT) as a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification task</a> in an autoregressive setting and analyze the limitations of both <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> and autoregression components. Classifiers are known to perform better with balanced class distributions during training. Since the Zipfian nature of languages causes imbalanced classes, we explore its effect on NMT. We analyze the effect of various <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary sizes</a> on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> performance on multiple languages with many data sizes, and reveal an explanation for why certain <a href=https://en.wikipedia.org/wiki/Vocabulary_size>vocabulary sizes</a> are better than others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.355.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--355 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.355 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.355/>Generalizable and Explainable Dialogue Generation via Explicit Action Learning</a></strong><br><a href=/people/x/xinting-huang/>Xinting Huang</a>
|
<a href=/people/j/jianzhong-qi/>Jianzhong Qi</a>
|
<a href=/people/y/yu-sun/>Yu Sun</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--355><div class="card-body p-3 small">Response generation for task-oriented dialogues implicitly optimizes two objectives at the same time : task completion and language quality. Conditioned response generation serves as an effective approach to separately and better optimize these two <a href=https://en.wikipedia.org/wiki/Goal>objectives</a>. Such an <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> relies on system action annotations which are expensive to obtain. To alleviate the need of action annotations, latent action learning is introduced to map each utterance to a latent representation. However, this approach is prone to over-dependence on the training data, and the generalization capability is thus restricted. To address this issue, we propose to learn <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language actions</a> that represent utterances as a span of words. This explicit action representation promotes <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> via the compositional structure of language. It also enables an explainable generation process. Our proposed <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised approach</a> learns a memory component to summarize system utterances into a short span of words. To further promote a compact action representation, we propose an auxiliary task that restores state annotations as the summarized dialogue context using the memory component. Our proposed approach outperforms latent action baselines on MultiWOZ, a benchmark multi-domain dataset.<i>natural language actions</i> that represent utterances as a span of words. This explicit action representation promotes generalization via the compositional structure of language. It also enables an explainable generation process. Our proposed unsupervised approach learns a memory component to summarize system utterances into a short span of words. To further promote a compact action representation, we propose an auxiliary task that restores state annotations as the summarized dialogue context using the memory component. Our proposed approach outperforms latent action baselines on MultiWOZ, a benchmark multi-domain dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.356.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--356 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.356 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.356/>More <a href=https://en.wikipedia.org/wiki/Embedding>Embeddings</a>, Better Sequence Labelers?</a></strong><br><a href=/people/x/xinyu-wang/>Xinyu Wang</a>
|
<a href=/people/y/yong-jiang/>Yong Jiang</a>
|
<a href=/people/n/nguyen-bach/>Nguyen Bach</a>
|
<a href=/people/t/tao-wang/>Tao Wang</a>
|
<a href=/people/z/zhongqiang-huang/>Zhongqiang Huang</a>
|
<a href=/people/f/fei-huang/>Fei Huang</a>
|
<a href=/people/k/kewei-tu/>Kewei Tu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--356><div class="card-body p-3 small">Recent work proposes a family of contextual embeddings that significantly improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of sequence labelers over non-contextual embeddings. However, there is no definite conclusion on whether we can build better sequence labelers by combining different kinds of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> in various settings. In this paper, we conduct extensive experiments on 3 tasks over 18 datasets and 8 languages to study the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of sequence labeling with various embedding concatenations and make three observations : (1) concatenating more embedding variants leads to better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in rich-resource and cross-domain settings and some conditions of low-resource settings ; (2) concatenating contextual sub-word embeddings with contextual character embeddings hurts the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> in extremely low-resource settings ; (3) based on the conclusion of (1), concatenating additional similar contextual embeddings can not lead to further improvements. We hope these conclusions can help people build stronger sequence labelers in various settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.363.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--363 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.363 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.363" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.363/>Temporal Reasoning in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a></a></strong><br><a href=/people/s/siddharth-vashishtha/>Siddharth Vashishtha</a>
|
<a href=/people/a/adam-poliak/>Adam Poliak</a>
|
<a href=/people/y/yash-kumar-lal/>Yash Kumar Lal</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a>
|
<a href=/people/a/aaron-steven-white/>Aaron Steven White</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--363><div class="card-body p-3 small">We introduce five new natural language inference (NLI) datasets focused on temporal reasoning. We recast four existing datasets annotated for event durationhow long an event lastsand event orderinghow events are temporally arrangedinto more than one million NLI examples. We use these datasets to investigate how well neural models trained on a popular NLI corpus capture these forms of temporal reasoning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.366.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--366 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.366 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.366" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.366/>An Empirical Methodology for Detecting and Prioritizing Needs during Crisis Events</a></strong><br><a href=/people/m/m-janina-sarol/>M. Janina Sarol</a>
|
<a href=/people/l/ly-dinh/>Ly Dinh</a>
|
<a href=/people/r/rezvaneh-rezapour/>Rezvaneh Rezapour</a>
|
<a href=/people/c/chieh-li-chin/>Chieh-Li Chin</a>
|
<a href=/people/p/pingjing-yang/>Pingjing Yang</a>
|
<a href=/people/j/jana-diesner/>Jana Diesner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--366><div class="card-body p-3 small">In times of crisis, identifying essential needs is crucial to providing appropriate resources and services to affected entities. Social media platforms such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> contain a vast amount of information about the general public&#8217;s needs. However, the sparsity of information and the amount of noisy content present a challenge for practitioners to effectively identify relevant information on these <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a>. This study proposes two novel methods for two needs detection tasks : 1) extracting a list of needed resources, such as <a href=https://en.wikipedia.org/wiki/Mask>masks</a> and <a href=https://en.wikipedia.org/wiki/Ventilation_(architecture)>ventilators</a>, and 2) detecting sentences that specify who-needs-what resources (e.g., we need testing). We evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on a set of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> about the COVID-19 crisis. For extracting a list of needs, we compare our results against two official lists of resources, achieving 0.64 <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>. For detecting who-needs-what sentences, we compared our results against a set of 1,000 <a href=https://en.wikipedia.org/wiki/Annotation>annotated tweets</a> and achieved a 0.68 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.367.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--367 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.367 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940131 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.367" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.367/>SupMMD : A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy<span class=acl-fixed-case>S</span>up<span class=acl-fixed-case>MMD</span>: A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy</a></strong><br><a href=/people/u/umanga-bista/>Umanga Bista</a>
|
<a href=/people/a/alexander-mathews/>Alexander Mathews</a>
|
<a href=/people/a/aditya-menon/>Aditya Menon</a>
|
<a href=/people/l/lexing-xie/>Lexing Xie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--367><div class="card-body p-3 small">Most work on <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a> has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information present in each set, is of equal practical interest (e.g., presenting readers with updates on an evolving news topic). In this work, we present SupMMD, a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two-sample testing. SupMMD combines both <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> for <a href=https://en.wikipedia.org/wiki/Salience_(neuroscience)>salience</a> and <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised learning</a> for coverage and diversity. Further, we adapt <a href=https://en.wikipedia.org/wiki/Multiple_kernel_learning>multiple kernel learning</a> to make use of <a href=https://en.wikipedia.org/wiki/Similarity_measure>similarity</a> across multiple information sources (e.g., <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>text features</a> and knowledge based concepts). We show the efficacy of SupMMD in both generic and update summarization tasks by meeting or exceeding the current state-of-the-art on the DUC-2004 and TAC-2009 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.372.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--372 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.372 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.372.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.372" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.372/>TinyBERT : Distilling BERT for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Understanding</a><span class=acl-fixed-case>T</span>iny<span class=acl-fixed-case>BERT</span>: Distilling <span class=acl-fixed-case>BERT</span> for Natural Language Understanding</a></strong><br><a href=/people/x/xiaoqi-jiao/>Xiaoqi Jiao</a>
|
<a href=/people/y/yichun-yin/>Yichun Yin</a>
|
<a href=/people/l/lifeng-shang/>Lifeng Shang</a>
|
<a href=/people/x/xin-jiang/>Xin Jiang</a>
|
<a href=/people/x/xiao-chen/>Xiao Chen</a>
|
<a href=/people/l/linlin-li/>Linlin Li</a>
|
<a href=/people/f/fang-wang/>Fang Wang</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--372><div class="card-body p-3 small">Language model pre-training, such as <a href=https://en.wikipedia.org/wiki/BERT>BERT</a>, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate <a href=https://en.wikipedia.org/wiki/Inference>inference</a> and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8 % the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28 % parameters and ~31 % inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.373.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--373 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.373 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940808 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.373" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.373/>Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder</a></strong><br><a href=/people/a/alvin-chan/>Alvin Chan</a>
|
<a href=/people/y/yi-tay/>Yi Tay</a>
|
<a href=/people/y/yew-soon-ong/>Yew-Soon Ong</a>
|
<a href=/people/a/aston-zhang/>Aston Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--373><div class="card-body p-3 small">This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a &#8216;backdoor poisoning&#8217; attack on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a>. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1 % poisoned data, our experiments show that a victim BERT finetuned classifier&#8217;s predictions can be steered to the poison target class with success rates of > 80 % when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.<tex-math>>80\\%</tex-math> when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.384.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--384 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.384 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.384.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.384/>How Can Self-Attention Networks Recognize Dyck-n Languages?<span class=acl-fixed-case>D</span>yck-n Languages?</a></strong><br><a href=/people/j/javid-ebrahimi/>Javid Ebrahimi</a>
|
<a href=/people/d/dhruv-gelda/>Dhruv Gelda</a>
|
<a href=/people/w/wei-zhang/>Wei Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--384><div class="card-body p-3 small">We focus on the recognition of Dyck-n (Dn) languages with self-attention (SA) networks, which has been deemed to be a difficult task for these <a href=https://en.wikipedia.org/wiki/Neural_network>networks</a>. We compare the performance of two variants of SA, one with a starting symbol (SA+) and one without (SA-). Our results show that SA+ is able to generalize to longer sequences and deeper dependencies. For D2, we find that <a href=https://en.wikipedia.org/wiki/S-number>SA-</a> completely breaks down on long sequences whereas the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/S-number>SA+</a> is 58.82 %. We find attention maps learned by SA+ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of SA networks is at par with LSTMs, which provides evidence on the ability of SA to learn <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchies</a> without <a href=https://en.wikipedia.org/wiki/Recursion>recursion</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.385.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--385 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.385 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.385.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.385/>Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation</a></strong><br><a href=/people/q/qiang-wang/>Qiang Wang</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a>
|
<a href=/people/j/jingbo-zhu/>Jingbo Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--385><div class="card-body p-3 small">The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices (e.g., mobile phones) may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> to train a flexible depth model that can adapt to different depth configurations during <a href=https://en.wikipedia.org/wiki/Inference>inference</a>. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training methodLayerDrop.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.389.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--389 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.389 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.389" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.389/>What’s so special about BERT’s layers? A closer look at the NLP pipeline in monolingual and multilingual models<span class=acl-fixed-case>BERT</span>’s layers? A closer look at the <span class=acl-fixed-case>NLP</span> pipeline in monolingual and multilingual models</a></strong><br><a href=/people/w/wietse-de-vries/>Wietse de Vries</a>
|
<a href=/people/a/andreas-van-cranenburgh/>Andreas van Cranenburgh</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--389><div class="card-body p-3 small">Peeking into the inner workings of BERT has shown that its <a href=https://en.wikipedia.org/wiki/Abstraction_layer>layers</a> resemble the classical NLP pipeline, with progressively more complex tasks being concentrated in later layers. To investigate to what extent these results also hold for a language other than <a href=https://en.wikipedia.org/wiki/English_language>English</a>, we probe a Dutch BERT-based model and the multilingual BERT model for Dutch NLP tasks. In addition, through a deeper analysis of <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, we show that also within a given task, information is spread over different parts of the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> and the <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> might not be as neat as it seems. Each layer has different specialisations, so that it may be more useful to combine information from different layers, instead of selecting a single one based on the best overall performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.390.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--390 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.390 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.390" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.390/>Leakage-Adjusted Simulatability : Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?</a></strong><br><a href=/people/p/peter-hase/>Peter Hase</a>
|
<a href=/people/s/shiyue-zhang/>Shiyue Zhang</a>
|
<a href=/people/h/harry-xie/>Harry Xie</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--390><div class="card-body p-3 small">Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of <a href=https://en.wikipedia.org/wiki/Interpretation_(logic)>evaluating explanations</a> from the the model simulatability perspective. Our contributions are as follows : (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model&#8217;s output, while controlling for how <a href=https://en.wikipedia.org/wiki/Explanation>explanations</a> can directly leak the output. We use a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> as a proxy for a <a href=https://en.wikipedia.org/wiki/Human_subject_research>human observer</a>, and validate this choice with two <a href=https://en.wikipedia.org/wiki/Human_subject_research>human subject experiments</a>. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches ; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.398.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--398 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.398 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.398.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.398" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.398/>Parsing All : Syntax and Semantics, Dependencies and Spans</a></strong><br><a href=/people/j/junru-zhou/>Junru Zhou</a>
|
<a href=/people/z/zuchao-li/>Zuchao Li</a>
|
<a href=/people/h/hai-zhao/>Hai Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--398><div class="card-body p-3 small">Both syntactic and semantic structures are key linguistic contextual clues, in which parsing the latter has been well shown beneficial from parsing the former. However, few works ever made an attempt to let <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> help syntactic parsing. As linguistic representation formalisms, both <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and semantics may be represented in either span (constituent / phrase) or <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency</a>, on both of which joint learning was also seldom explored. In this paper, we propose a novel joint model of syntactic and semantic parsing on both span and dependency representations, which incorporates syntactic information effectively in the encoder of <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> and benefits from two representation formalisms in a uniform way. The experiments show that <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> can benefit each other by optimizing joint objectives. Our single model achieves new state-of-the-art or competitive results on both span and dependency semantic parsing on Propbank benchmarks and both dependency and constituent syntactic parsing on Penn Treebank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--400 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.400 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.400.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.400/>Improving Limited Labeled Dialogue State Tracking with Self-Supervision</a></strong><br><a href=/people/c/chien-sheng-wu/>Chien-Sheng Wu</a>
|
<a href=/people/s/steven-c-h-hoi/>Steven C.H. Hoi</a>
|
<a href=/people/c/caiming-xiong/>Caiming Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--400><div class="card-body p-3 small">Existing dialogue state tracking (DST) models require plenty of labeled data. However, collecting high-quality labels is costly, especially when the number of domains increases. In this paper, we address a practical <a href=https://en.wikipedia.org/wiki/Discrete-time_stochastic_process>DST problem</a> that is rarely discussed, i.e., learning efficiently with limited labeled data. We present and investigate two self-supervised objectives : preserving latent consistency and modeling conversational behavior. We encourage a DST model to have consistent latent distributions given a perturbed input, making it more robust to an unseen scenario. We also add an auxiliary utterance generation task, modeling a potential correlation between conversational behavior and dialogue states. The experimental results show that our proposed self-supervised signals can improve <a href=https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)>joint goal accuracy</a> by 8.95 % when only 1 % labeled data is used on the MultiWOZ dataset. We can achieve an additional 1.76 % improvement if some unlabeled data is jointly trained as <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a>. We analyze and visualize how our proposed self-supervised signals help the DST task and hope to stimulate future data-efficient DST research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--408 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.408.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940092 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.408" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.408/>Decoding Language Spatial Relations to 2D Spatial Arrangements<span class=acl-fixed-case>D</span>ecoding Language Spatial Relations to 2<span class=acl-fixed-case>D</span> Spatial Arrangements</a></strong><br><a href=/people/g/gorjan-radevski/>Gorjan Radevski</a>
|
<a href=/people/g/guillem-collell/>Guillem Collell</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>
|
<a href=/people/t/tinne-tuytelaars/>Tinne Tuytelaars</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--408><div class="card-body p-3 small">We address the problem of multimodal spatial understanding by decoding a set of language-expressed spatial relations to a set of 2D spatial arrangements in a multi-object and multi-relationship setting. We frame the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> as arranging a scene of clip-arts given a textual description. We propose a simple and effective model architecture Spatial-Reasoning Bert (SR-Bert), trained to decode text to 2D spatial arrangements in a non-autoregressive manner. SR-Bert can decode both explicit and implicit language to 2D spatial arrangements, generalizes to out-of-sample data to a reasonable extent and can generate complete abstract scenes if paired with a clip-arts predictor. Finally, we qualitatively evaluate our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> with a <a href=https://en.wikipedia.org/wiki/User_study>user study</a>, validating that our generated spatial arrangements align with <a href=https://en.wikipedia.org/wiki/Expectation_(epistemic)>human expectation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--409 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.409/>The Dots Have Their Values : Exploiting the Node-Edge Connections in Graph-based Neural Models for Document-level Relation Extraction</a></strong><br><a href=/people/h/hieu-minh-tran/>Hieu Minh Tran</a>
|
<a href=/people/m/minh-trung-nguyen/>Minh Trung Nguyen</a>
|
<a href=/people/t/thien-huu-nguyen/>Thien Huu Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--409><div class="card-body p-3 small">The goal of Document-level Relation Extraction (DRE) is to recognize the relations between entity mentions that can span beyond sentence boundary. The current state-of-the-art method for this problem has involved the graph-based edge-oriented model where the entity mentions, entities, and sentences in the documents are used as the nodes of the document graphs for <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a>. However, this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> does not capture the representations for the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> in the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a>, thus preventing it from effectively encoding the specific and relevant information of the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> for DRE. To address this issue, we propose to explicitly compute the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> for the <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> in the graph-based edge-oriented model for <a href=https://en.wikipedia.org/wiki/Directed_acyclic_graph>DRE</a>. These node representations allow us to introduce two novel representation regularization mechanisms to improve the <a href=https://en.wikipedia.org/wiki/Representation_theory>representation vectors</a> for <a href=https://en.wikipedia.org/wiki/Directed_acyclic_graph>DRE</a>. The experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on two <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--412 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.412" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.412/>Long Document Ranking with Query-Directed Sparse Transformer</a></strong><br><a href=/people/j/jyun-yu-jiang/>Jyun-Yu Jiang</a>
|
<a href=/people/c/chenyan-xiong/>Chenyan Xiong</a>
|
<a href=/people/c/chia-jung-lee/>Chia-Jung Lee</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--412><div class="card-body p-3 small">The computing cost of transformer self-attention often necessitates breaking long documents to fit in pretrained models in document ranking tasks. In this paper, we design Query-Directed Sparse attention that induces IR-axiomatic structures in transformer self-attention. Our model, QDS-Transformer, enforces the principle properties desired in ranking : local contextualization, hierarchical representation, and query-oriented proximity matching, while it also enjoys efficiency from sparsity. Experiments on four fully supervised and few-shot TREC document ranking benchmarks demonstrate the consistent and robust advantage of QDS-Transformer over previous approaches, as they either retrofit long documents into BERT or use sparse attention without emphasizing IR principles. We further quantify the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computing complexity</a> and demonstrates that our sparse attention with TVM implementation is twice more efficient that the fully-connected self-attention. All source codes, trained model, and predictions of this work are available at https://github.com/hallogameboy/QDS-Transformer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--413 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.413.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.413/>Visuo-Linguistic Question Answering (VLQA) Challenge<span class=acl-fixed-case>VLQA</span>) Challenge</a></strong><br><a href=/people/s/shailaja-keyur-sampat/>Shailaja Keyur Sampat</a>
|
<a href=/people/y/yezhou-yang/>Yezhou Yang</a>
|
<a href=/people/c/chitta-baral/>Chitta Baral</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--413><div class="card-body p-3 small">Understanding <a href=https://en.wikipedia.org/wiki/Image>images</a> and text together is an important aspect of <a href=https://en.wikipedia.org/wiki/Cognition>cognition</a> and building advanced Artificial Intelligence (AI) systems. As a community, we have achieved good benchmarks over language and vision domains separately, however joint reasoning is still a challenge for state-of-the-art computer vision and natural language processing (NLP) systems. We propose a novel task to derive joint inference about a given image-text modality and compile the Visuo-Linguistic Question Answering (VLQA) challenge corpus in a question answering setting. Each dataset item consists of an <a href=https://en.wikipedia.org/wiki/Image>image</a> and a reading passage, where questions are designed to combine both <a href=https://en.wikipedia.org/wiki/Visual_system>visual and textual information</a> i.e., ignoring either modality would make the question unanswerable. We first explore the best existing vision-language architectures to solve VLQA subsets and show that they are unable to reason well. We then develop a <a href=https://en.wikipedia.org/wiki/Modular_programming>modular method</a> with slightly better baseline performance, but it is still far behind human performance. We believe that VLQA will be a good benchmark for reasoning over a visuo-linguistic context. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, code and leaderboard is available at https://shailaja183.github.io/vlqa/.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--415 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.415" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.415/>Exploring BERT’s Sensitivity to Lexical Cues using Tests from Semantic Priming<span class=acl-fixed-case>BERT</span>’s Sensitivity to Lexical Cues using Tests from Semantic Priming</a></strong><br><a href=/people/k/kanishka-misra/>Kanishka Misra</a>
|
<a href=/people/a/allyson-ettinger/>Allyson Ettinger</a>
|
<a href=/people/j/julia-rayz/>Julia Rayz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--415><div class="card-body p-3 small">Models trained to estimate word probabilities in context have become ubiquitous in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. How do these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> use <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical cues</a> in context to inform their <a href=https://en.wikipedia.org/wiki/Lexical_analysis>word probabilities</a>? To answer this question, we present a case study analyzing the pre-trained BERT model with tests informed by <a href=https://en.wikipedia.org/wiki/Semantic_priming>semantic priming</a>. Using English lexical stimuli that show <a href=https://en.wikipedia.org/wiki/Priming_(psychology)>priming</a> in humans, we find that BERT too shows <a href=https://en.wikipedia.org/wiki/Priming_(psychology)>priming</a>, predicting a word with greater probability when the context includes a related word versus an unrelated one. This effect decreases as the amount of information provided by the context increases. Follow-up analysis shows BERT to be increasingly distracted by related prime words as context becomes more informative, assigning lower probabilities to related words. Our findings highlight the importance of considering contextual constraint effects when studying <a href=https://en.wikipedia.org/wiki/Word_prediction>word prediction</a> in these models, and highlight possible parallels with human processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--416 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940120 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.416" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.416/>Multi-hop Question Generation with Graph Convolutional Network</a></strong><br><a href=/people/d/dan-su/>Dan Su</a>
|
<a href=/people/y/yan-xu/>Yan Xu</a>
|
<a href=/people/w/wenliang-dai/>Wenliang Dai</a>
|
<a href=/people/z/ziwei-ji/>Ziwei Ji</a>
|
<a href=/people/t/tiezheng-yu/>Tiezheng Yu</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--416><div class="card-body p-3 small">Multi-hop Question Generation (QG) aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop QG, where the questions are generated from the sentence containing the answer or nearby sentences in the same paragraph without complex reasoning. To address the additional challenges in multi-hop QG, we propose Multi-Hop Encoding Fusion Network for Question Generation (MulQG), which does context encoding in multiple hops with Graph Convolutional Network and encoding fusion via an Encoder Reasoning Gate. To the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence-level information</a>. Empirical results on HotpotQA dataset demonstrate the effectiveness of our method, in comparison with baselines on automatic evaluation metrics. Moreover, from the human evaluation, our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to generate fluent questions with high completeness and outperforms the strongest <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> by 20.8 % in the multi-hop evaluation. on. The code is publicly availableat https://github.com/HLTCHKU</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--418 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940700 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.418/>Thinking Like a Skeptic : Defeasible Inference in Natural Language</a></strong><br><a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/j/jena-d-hwang/>Jena D. Hwang</a>
|
<a href=/people/c/chandra-bhagavatula/>Chandra Bhagavatula</a>
|
<a href=/people/m/maxwell-forbes/>Maxwell Forbes</a>
|
<a href=/people/r/ronan-le-bras/>Ronan Le Bras</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--418><div class="card-body p-3 small">Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of contemporary data-driven research on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a> and <a href=https://en.wikipedia.org/wiki/Commonsense_reasoning>commonsense reasoning</a>. We introduce Defeasible NLI (abbreviated -NLI), a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for defeasible inference in <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. Defeasible NLI contains extensions to three existing inference datasets covering diverse modes of reasoning : <a href=https://en.wikipedia.org/wiki/Common_sense>common sense</a>, natural language inference, and <a href=https://en.wikipedia.org/wiki/Social_norm>social norms</a>. From Defeasible NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, <a href=https://en.wikipedia.org/wiki/Generative_model>generative models</a> trained on this <a href=https://en.wikipedia.org/wiki/Data>data</a> are capable of writing sentences that weaken or strengthen a specified <a href=https://en.wikipedia.org/wiki/Inference>inference</a> up to 68 % of the time.<tex-math>\\delta</tex-math>-NLI), a dataset for defeasible inference in natural language. Defeasible NLI contains extensions to three existing inference datasets covering diverse modes of reasoning: common sense, natural language inference, and social norms. From Defeasible NLI, we develop both a classification and generation task for defeasible inference, and demonstrate that the generation task is much more challenging. Despite lagging human performance, however, generative models trained on this data are capable of writing sentences that weaken or strengthen a specified inference up to 68% of the time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--420 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940091 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.findings-emnlp.420" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.420/>Language-Conditioned Feature Pyramids for Visual Selection Tasks<span class=acl-fixed-case>C</span>onditioned <span class=acl-fixed-case>F</span>eature <span class=acl-fixed-case>P</span>yramids for <span class=acl-fixed-case>V</span>isual <span class=acl-fixed-case>S</span>election <span class=acl-fixed-case>T</span>asks</a></strong><br><a href=/people/t/taichi-iki/>Taichi Iki</a>
|
<a href=/people/a/akiko-aizawa/>Akiko Aizawa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--420><div class="card-body p-3 small">Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that fuse <a href=https://en.wikipedia.org/wiki/Visual_system>visual and linguistic features</a> have been proposed. However, few models consider the fusion of linguistic features with multiple <a href=https://en.wikipedia.org/wiki/Visual_system>visual features</a> with different sizes of <a href=https://en.wikipedia.org/wiki/Receptive_field>receptive fields</a>, though the proper size of the receptive field of <a href=https://en.wikipedia.org/wiki/Visual_system>visual features</a> intuitively varies depending on expressions. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network architecture</a> that modulates visual features with varying sizes of <a href=https://en.wikipedia.org/wiki/Receptive_field>receptive field</a> by <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> on tasks related to referring expression comprehension in two visual dialogue games. The results show the advantages and broad applicability of our <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>. Source code is available at https://github.com/Alab-NII/lcfp.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--421 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.findings-emnlp.421.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.421/>Learning to Classify Events from Human Needs Category Descriptions</a></strong><br><a href=/people/h/haibo-ding/>Haibo Ding</a>
|
<a href=/people/z/zhe-feng/>Zhe Feng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--421><div class="card-body p-3 small">We study the problem of learning an event classifier from human needs category descriptions, which is challenging due to : (1) the use of highly abstract concepts in natural language descriptions, (2) the difficulty of choosing key concepts. To tackle these two challenges, we propose LeaPI, a zero-shot learning method that first automatically generate weak labels by instantiating high-level concepts with prototypical instances and then trains a human needs classifier with the weakly labeled data. To filter noisy concepts, we design a reinforced selection algorithm to choose high-quality concepts for instantiation. Experimental results on the human needs categorization task show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperforms baseline methods, producing substantially better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.431.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--431 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.431 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.431/>Understanding User Resistance Strategies in Persuasive Conversations</a></strong><br><a href=/people/y/youzhi-tian/>Youzhi Tian</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/c/chen-li/>Chen Li</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--431><div class="card-body p-3 small">Persuasive dialog systems have various usages, such as donation persuasion and physical exercise persuasion. Previous persuasive dialog systems research mostly focused on analyzing the persuader&#8217;s strategies and paid little attention to the persuadee (user). However, understanding and addressing users&#8217; resistance strategies is an essential job of a persuasive dialog system. So, we adopt a preliminary framework on persuasion resistance in <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a> and design a fine-grained resistance strategy annotation scheme. We annotate the PersuasionForGood dataset with the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a>. With the enriched annotations, we build a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> to predict the resistance strategies. Furthermore, we analyze the relationships between <a href=https://en.wikipedia.org/wiki/Persuasion>persuasion strategies</a> and persuasion resistance strategies. Our work lays the ground for developing a persuasive dialogue system that can understand and address user resistance strategy appropriately. The code and data will be released.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.433.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--433 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.433 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38940118 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.433/>Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation</a></strong><br><a href=/people/i/insoo-chung/>Insoo Chung</a>
|
<a href=/people/b/byeongwook-kim/>Byeongwook Kim</a>
|
<a href=/people/y/yoonjung-choi/>Yoonjung Choi</a>
|
<a href=/people/s/se-jung-kwon/>Se Jung Kwon</a>
|
<a href=/people/y/yongkweon-jeon/>Yongkweon Jeon</a>
|
<a href=/people/b/baeseong-park/>Baeseong Park</a>
|
<a href=/people/s/sangha-kim/>Sangha Kim</a>
|
<a href=/people/d/dongsoo-lee/>Dongsoo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--433><div class="card-body p-3 small">The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during <a href=https://en.wikipedia.org/wiki/Inference>inference</a>, especially when the target device is limited in <a href=https://en.wikipedia.org/wiki/Computational_resource>computational resources</a> such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of <a href=https://en.wikipedia.org/wiki/Quantization_(signal_processing)>quantization bits</a>, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a <a href=https://en.wikipedia.org/wiki/Quantization_(signal_processing)>mixed precision quantization strategy</a> to represent Transformer weights by an extremely low number of bits (e.g., under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8 smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3 reduction in run-time memory footprints and 3.5 speed up (Galaxy N10 +) such that our proposed compression strategy enables efficient implementation for on-device NMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.findings-emnlp.445.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--findings-emnlp--445 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.findings-emnlp.445 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.findings-emnlp.445/>IndicNLPSuite : Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian Languages</a><span class=acl-fixed-case>I</span>ndic<span class=acl-fixed-case>NLPS</span>uite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for <span class=acl-fixed-case>I</span>ndian Languages</a></strong><br><a href=/people/d/divyanshu-kakwani/>Divyanshu Kakwani</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/s/satish-golla/>Satish Golla</a>
|
<a href=/people/g/gokul-n-c/>Gokul N.C.</a>
|
<a href=/people/a/avik-bhattacharyya/>Avik Bhattacharyya</a>
|
<a href=/people/m/mitesh-m-khapra/>Mitesh M. Khapra</a>
|
<a href=/people/p/pratyush-kumar/>Pratyush Kumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--findings-emnlp--445><div class="card-body p-3 small">In this paper, we introduce NLP resources for 11 major <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian languages</a> from two major language families. These resources include : (a) large-scale sentence-level monolingual corpora, (b) pre-trained word embeddings, (c) pre-trained language models, and (d) multiple NLU evaluation datasets (IndicGLUE benchmark). The monolingual corpora contains a total of 8.8 billion tokens across all 11 languages and <a href=https://en.wikipedia.org/wiki/Indian_English>Indian English</a>, primarily sourced from <a href=https://en.wikipedia.org/wiki/Web_crawler>news crawls</a>. The <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> are based on <a href=https://en.wikipedia.org/wiki/FastText>FastText</a>, hence suitable for handling morphological complexity of <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian languages</a>. The pre-trained language models are based on the compact ALBERT model. Lastly, we compile the (IndicGLUE benchmark for Indian language NLU. To this end, we create datasets for the following tasks : Article Genre Classification, Headline Prediction, Wikipedia Section-Title Prediction, Cloze-style Multiple choice QA, Winograd NLI and COPA. We also include publicly available datasets for some Indic languages for tasks like <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a>, Cross-lingual Sentence Retrieval, <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>Paraphrase detection</a>, etc. Our <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are competitive or better than existing pre-trained embeddings on multiple tasks. We hope that the availability of the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> will accelerate Indic NLP research which has the potential to impact more than a billion people. It can also help the community in evaluating advances in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> over a more diverse pool of languages. The data and models are available at.<i>IndicGLUE</i> benchmark). The monolingual corpora contains a total of 8.8 billion tokens across all 11 languages and Indian English, primarily sourced from news crawls. The word embeddings are based on <i>FastText</i>, hence suitable for handling morphological complexity of Indian languages. The pre-trained language models are based on the compact ALBERT model. Lastly, we compile the (<i>IndicGLUE</i> benchmark for Indian language NLU. To this end, we create datasets for the following tasks: Article Genre Classification, Headline Prediction, Wikipedia Section-Title Prediction, Cloze-style Multiple choice QA, Winograd NLI and COPA. We also include publicly available datasets for some Indic languages for tasks like Named Entity Recognition, Cross-lingual Sentence Retrieval, Paraphrase detection, <i>etc.</i> Our embeddings are competitive or better than existing pre-trained embeddings on multiple tasks. We hope that the availability of the dataset will accelerate Indic NLP research which has the potential to impact more than a billion people. It can also help the community in evaluating advances in NLP over a more diverse pool of languages. The data and models are available at <url>https://indicnlp.ai4bharat.org</url>.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>