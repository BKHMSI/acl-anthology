<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Noisy User-generated Text (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Noisy User-generated Text (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020wnut-1>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</a>
<span class="badge badge-info align-middle ml-1">30&nbsp;papers</span></li></ul></div></div><div id=2020wnut-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.wnut-1/>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.0/>Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</a></strong><br><a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/t/timothy-baldwin/>Tim Baldwin</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.5/>Combining BERT with Static Word Embeddings for Categorizing Social Media<span class=acl-fixed-case>BERT</span> with Static Word Embeddings for Categorizing Social Media</a></strong><br><a href=/people/i/israa-alghanmi/>Israa Alghanmi</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa Anke</a>
|
<a href=/people/s/steven-schockaert/>Steven Schockaert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--5><div class="card-body p-3 small">Pre-trained neural language models (LMs) have achieved impressive results in various natural language processing tasks, across different languages. Surprisingly, this extends to the <a href=https://en.wikipedia.org/wiki/Social_media>social media genre</a>, despite the fact that <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> often has very different characteristics from the language that LMs have seen during training. A particularly striking example is the performance of AraBERT, an LM for the <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a>, which is successful in categorizing social media posts in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic dialects</a>, despite only having been trained on Modern Standard <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. Our hypothesis in this paper is that the performance of LMs for <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> can nonetheless be improved by incorporating static word vectors that have been specifically trained on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We show that a simple method for incorporating such word vectors is indeed successful in several Arabic and English benchmarks. Curiously, however, we also find that similar improvements are possible with word vectors that have been trained on traditional text sources (e.g. Wikipedia).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.7/>PHINC : A Parallel Hinglish Social Media Code-Mixed Corpus for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a><span class=acl-fixed-case>PHINC</span>: A Parallel <span class=acl-fixed-case>H</span>inglish Social Media Code-Mixed Corpus for Machine Translation</a></strong><br><a href=/people/v/vivek-srivastava/>Vivek Srivastava</a>
|
<a href=/people/m/mayank-singh/>Mayank Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--7><div class="card-body p-3 small">Code-mixing is the phenomenon of using more than one language in a sentence. In the <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual communities</a>, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> is a very frequently observed pattern of communication on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. Flexibility to use multiple languages in one text message might help to communicate efficiently with the target audience. But, the noisy user-generated code-mixed text adds to the challenge of processing and understanding <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> to a much larger extent. Machine translation from monolingual source to the target language is a well-studied research problem. Here, we demonstrate that widely popular and sophisticated translation systems such as <a href=https://en.wikipedia.org/wiki/Google_Translate>Google Translate</a> fail at times to translate code-mixed text effectively. To address this challenge, we present a <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a> of the 13,738 code-mixed Hindi-English sentences and their corresponding human translation in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. In addition, we also propose a translation pipeline build on top of <a href=https://en.wikipedia.org/wiki/Google_Translate>Google Translate</a>. The evaluation of the proposed <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> on PHINC demonstrates an increase in the performance of the underlying <a href=https://en.wikipedia.org/wiki/System>system</a>. With minimal effort, we can extend the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and the proposed approach to other code-mixing language pairs.<tex-math>PHINC</tex-math> demonstrates an increase in the performance of the underlying system. With minimal effort, we can extend the dataset and the proposed approach to other code-mixing language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.11/>Non-ingredient Detection in User-generated Recipes using the Sequence Tagging Approach</a></strong><br><a href=/people/y/yasuhiro-yamaguchi/>Yasuhiro Yamaguchi</a>
|
<a href=/people/s/shintaro-inuzuka/>Shintaro Inuzuka</a>
|
<a href=/people/m/makoto-hiramatsu/>Makoto Hiramatsu</a>
|
<a href=/people/j/jun-harashima/>Jun Harashima</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--11><div class="card-body p-3 small">Recently, the number of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated recipes</a> on the Internet has increased. In such <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a>, users are generally supposed to write a title, an ingredient list, and steps to create a dish. However, some items in an ingredient list in a <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated recipe</a> are not actually edible ingredients. For example, headings, comments, and <a href=https://en.wikipedia.org/wiki/Kitchenware>kitchenware</a> sometimes appear in an ingredient list because users can freely write the list in their recipes. Such noise makes it difficult for computers to use <a href=https://en.wikipedia.org/wiki/Recipe>recipes</a> for a variety of <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>, such as calorie estimation. To address this issue, we propose a non-ingredient detection method inspired by a neural sequence tagging model. In our experiment, we annotated 6,675 ingredients in 600 user-generated recipes and showed that our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieved a 93.3 <a href=https://en.wikipedia.org/wiki/F-number>F1 score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.14/>An Empirical Analysis of Human-Bot Interaction on Reddit<span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/m/ming-cheng-ma/>Ming-Cheng Ma</a>
|
<a href=/people/j/john-p-lalor/>John P. Lalor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--14><div class="card-body p-3 small">Automated agents (bots) have emerged as an ubiquitous and influential presence on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Bots engage on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> by posting content and replying to other users on the platform. In this work we conduct an empirical analysis of the activity of a single <a href=https://en.wikipedia.org/wiki/Internet_bot>bot</a> on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. Our goal is to determine whether bot activity (in the form of posted comments on the website) has an effect on how humans engage on <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. We find that (1) the sentiment of a bot comment has a significant, positive effect on the subsequent human reply, and (2) human Reddit users modify their comment behaviors to overlap with the text of the bot, similar to how humans modify their text to mimic other humans in conversation. Understanding human-bot interactions on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> with relatively simple <a href=https://en.wikipedia.org/wiki/Internet_bot>bots</a> is important for preparing for more advanced <a href=https://en.wikipedia.org/wiki/Internet_bot>bots</a> in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.15/>Detecting Trending Terms in Cybersecurity Forum Discussions</a></strong><br><a href=/people/j/jack-hughes/>Jack Hughes</a>
|
<a href=/people/s/seth-aycock/>Seth Aycock</a>
|
<a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a>
|
<a href=/people/a/alice-hutchings/>Alice Hutchings</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--15><div class="card-body p-3 small">We present a lightweight method for identifying currently trending terms in relation to a known prior of terms, using a weighted log-odds ratio with an informative prior. We apply this method to a dataset of posts from an English-language underground hacking forum, spanning over ten years of activity, with posts containing misspellings, <a href=https://en.wikipedia.org/wiki/Orthography>orthographic variation</a>, <a href=https://en.wikipedia.org/wiki/Acronym>acronyms</a>, and <a href=https://en.wikipedia.org/wiki/Slang>slang</a>. Our statistical approach supports analysis of linguistic change and discussion topics over time, without a requirement to train a <a href=https://en.wikipedia.org/wiki/Topic_model>topic model</a> for each time interval for analysis. We evaluate the approach by comparing the results to TF-IDF using the discounted cumulative gain metric with human annotations, finding our method outperforms TF-IDF on <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.18/>Punctuation Restoration using Transformer Models for High-and Low-Resource Languages</a></strong><br><a href=/people/t/tanvirul-alam/>Tanvirul Alam</a>
|
<a href=/people/a/akib-khan/>Akib Khan</a>
|
<a href=/people/f/firoj-alam/>Firoj Alam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--18><div class="card-body p-3 small">Punctuation restoration is a common post-processing problem for Automatic Speech Recognition (ASR) systems. It is important to improve the readability of the transcribed text for the human reader and facilitate NLP tasks. Current state-of-art address this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> using different <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>. Recently, transformer models have proven their success in downstream NLP tasks, and these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have been explored very little for the punctuation restoration problem. In this work, we explore different transformer based models and propose an augmentation strategy for this task, focusing on high-resource (English) and low-resource (Bangla) languages. For <a href=https://en.wikipedia.org/wiki/English_language>English</a>, we obtain comparable state-of-the-art results, while for <a href=https://en.wikipedia.org/wiki/Bengali_language>Bangla</a>, it is the first reported work, which can serve as a strong baseline for future work. We have made our developed Bangla dataset publicly available for the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.20" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.20/>Fine-Tuning MT systems for Robustness to Second-Language Speaker Variations<span class=acl-fixed-case>MT</span> systems for Robustness to Second-Language Speaker Variations</a></strong><br><a href=/people/m/md-mahfuz-ibn-alam/>Md Mahfuz Ibn Alam</a>
|
<a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--20><div class="card-body p-3 small">The performance of neural machine translation (NMT) systems only trained on a single language variant degrades when confronted with even slightly different language variations. With this work, we build upon previous work to explore how to mitigate this issue. We show that <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> using naturally occurring noise along with pseudo-references (i.e. corrected non-native inputs translated using the baseline NMT system) is a promising solution towards systems robust to such type of input variations. We focus on four translation pairs, from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, with our system achieving improvements of up to 3.1 BLEU points compared to the baselines, establishing a new state-of-the-art on the JFLEG-ES dataset. All datasets and code are publicly available here : https://github.com/mahfuzibnalam/finetuning_for_robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wnut-1.21.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wnut-1.21/>Impact of ASR on Alzheimer’s Disease Detection : All Errors are Equal, but Deletions are More Equal than Others<span class=acl-fixed-case>ASR</span> on <span class=acl-fixed-case>A</span>lzheimer’s Disease Detection: All Errors are Equal, but Deletions are More Equal than Others</a></strong><br><a href=/people/a/aparna-balagopalan/>Aparna Balagopalan</a>
|
<a href=/people/k/ksenia-shkaruta/>Ksenia Shkaruta</a>
|
<a href=/people/j/jekaterina-novikova/>Jekaterina Novikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--21><div class="card-body p-3 small">Automatic Speech Recognition (ASR) is a critical component of any fully-automated speech-based dementia detection model. However, despite years of speech recognition research, little is known about the impact of ASR accuracy on dementia detection. In this paper, we experiment with controlled amounts of artificially generated ASR errors and investigate their influence on dementia detection. We find that deletion errors affect <a href=https://en.wikipedia.org/wiki/Detection_theory>detection</a> performance the most, due to their impact on the features of syntactic complexity and discourse representation in speech. We show the trend to be generalisable across two different datasets for cognitive impairment detection. As a conclusion, we propose optimising the ASR to reflect a higher penalty for <a href=https://en.wikipedia.org/wiki/Deletion_(genetics)>deletion errors</a> in order to improve dementia detection performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.22/>Detecting Entailment in Code-Mixed Hindi-English Conversations<span class=acl-fixed-case>H</span>indi-<span class=acl-fixed-case>E</span>nglish Conversations</a></strong><br><a href=/people/s/sharanya-chakravarthy/>Sharanya Chakravarthy</a>
|
<a href=/people/a/anjana-umapathy/>Anjana Umapathy</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--22><div class="card-body p-3 small">The presence of large-scale corpora for Natural Language Inference (NLI) has spurred deep learning research in this area, though much of this research has focused solely on monolingual data. Code-mixing is the intertwined usage of multiple languages, and is commonly seen in informal conversations among <a href=https://en.wikipedia.org/wiki/Multilingualism>polyglots</a>. Given the rising importance of dialogue agents, it is imperative that they understand <a href=https://en.wikipedia.org/wiki/Code-mixing>code-mixing</a>, but the scarcity of code-mixed Natural Language Understanding (NLU) datasets has precluded research in this area. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by Khanuja et. al. for detecting conversational entailment in code-mixed Hindi-English text is the first of its kind. We investigate the effectiveness of <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>, <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>, <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, and architectural approaches to address the code-mixed, conversational, and low-resource aspects of this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. We obtain an 8.09 % increase in test set accuracy over the current state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.24/>Annotation Efficient <a href=https://en.wikipedia.org/wiki/Language_identification>Language Identification</a> from Weak Labels</a></strong><br><a href=/people/s/shriphani-palakodety/>Shriphani Palakodety</a>
|
<a href=/people/a/ashiqur-khudabukhsh/>Ashiqur KhudaBukhsh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--24><div class="card-body p-3 small">India is home to several languages with more than 30 m speakers. These <a href=https://en.wikipedia.org/wiki/Language>languages</a> exhibit significant presence on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a>. However, several of these widely-used languages are under-addressed by current Natural Language Processing (NLP) models and resources. User generated social media content in these <a href=https://en.wikipedia.org/wiki/Language>languages</a> is also typically authored in the <a href=https://en.wikipedia.org/wiki/Latin_script>Roman script</a> as opposed to the traditional native script further contributing to resource scarcity. In this paper, we leverage a minimally supervised NLP technique to obtain weak language labels from a large-scale Indian social media corpus leading to a robust and annotation-efficient language-identification technique spanning nine Romanized Indian languages. In fast-spreading pandemic situations such as the current COVID-19 situation, information processing objectives might be heavily tilted towards under-served languages in densely populated regions. We release our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to facilitate downstream analyses in these low-resource languages. Experiments across multiple <a href=https://en.wikipedia.org/wiki/Social_media>social media corpora</a> demonstrate the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>&#8217;s robustness and provide several interesting insights on Indian language usage patterns on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. We release an annotated data set of 1,000 comments in ten <a href=https://en.wikipedia.org/wiki/Romanization_(cultural)>Romanized languages</a> as a social media evaluation benchmark.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wnut-1.25.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wnut-1.25/>Fantastic Features and Where to Find Them : Detecting Cognitive Impairment with a Subsequence Classification Guided Approach</a></strong><br><a href=/people/b/ben-eyre/>Ben Eyre</a>
|
<a href=/people/a/aparna-balagopalan/>Aparna Balagopalan</a>
|
<a href=/people/j/jekaterina-novikova/>Jekaterina Novikova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--25><div class="card-body p-3 small">Despite the widely reported success of embedding-based machine learning methods on natural language processing tasks, the use of more easily interpreted engineered features remains common in fields such as cognitive impairment (CI) detection. Manually engineering features from <a href=https://en.wikipedia.org/wiki/Noisy_text>noisy text</a> is time and resource consuming, and can potentially result in <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a> that do not enhance <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> performance. To combat this, we describe a new approach to <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a> that leverages sequential machine learning models and <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> to predict which <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> help enhance performance. We provide a concrete example of this <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on a standard data set of CI speech and demonstrate that CI classification accuracy improves by 2.3 % over a strong baseline when using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> produced by this <a href=https://en.wikipedia.org/wiki/Methodology>method</a>. This demonstration provides an example of how this method can be used to assist <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> in fields where <a href=https://en.wikipedia.org/wiki/Interpretability>interpretability</a> is important, such as <a href=https://en.wikipedia.org/wiki/Health_care>health care</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wnut-1.28.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.28" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.28/>Civil Unrest on Twitter (CUT): A Dataset of Tweets to Support Research on Civil Unrest<span class=acl-fixed-case>T</span>witter (<span class=acl-fixed-case>CUT</span>): A Dataset of Tweets to Support Research on Civil Unrest</a></strong><br><a href=/people/j/justin-sech/>Justin Sech</a>
|
<a href=/people/a/alexandra-delucia/>Alexandra DeLucia</a>
|
<a href=/people/a/anna-l-buczak/>Anna L. Buczak</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--28><div class="card-body p-3 small">We present CUT, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for studying <a href=https://en.wikipedia.org/wiki/Civil_disorder>Civil Unrest</a> on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Our dataset includes 4,381 tweets related to <a href=https://en.wikipedia.org/wiki/Civil_disorder>civil unrest</a>, hand-annotated with information related to the study of civil unrest discussion and events. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is drawn from 42 countries from 2014 to 2019. We present <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline systems</a> trained on this <a href=https://en.wikipedia.org/wiki/Data>data</a> for the identification of tweets related to <a href=https://en.wikipedia.org/wiki/Civil_disorder>civil unrest</a>. We include a discussion of ethical issues related to research on this topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.30.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--30 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.30 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.30" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.30/>Representation learning of writing style</a></strong><br><a href=/people/j/julien-hay/>Julien Hay</a>
|
<a href=/people/b/bich-lien-doan/>Bich-Lien Doan</a>
|
<a href=/people/f/fabrice-popineau/>Fabrice Popineau</a>
|
<a href=/people/o/ouassim-ait-elhara/>Ouassim Ait Elhara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--30><div class="card-body p-3 small">In this paper, we introduce a new method of <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> that aims to embed documents in a stylometric space. Previous studies in the field of <a href=https://en.wikipedia.org/wiki/Authorship_analysis>authorship analysis</a> focused on feature engineering techniques in order to represent document styles and to enhance <a href=https://en.wikipedia.org/wiki/Computer_simulation>model</a> performance in specific tasks. Instead, we directly embed documents in a stylometric space by relying on a reference set of authors and the intra-author consistency property which is one of two components in our definition of <a href=https://en.wikipedia.org/wiki/Writing_style>writing style</a>. The main intuition of this paper is that we can define a general stylometric space from a set of reference authors such that, in this space, the coordinates of different documents will be close when the documents are by the same author, and spread away when they are by different authors, even for documents by authors who are not in the set of reference authors. The method we propose allows for the clustering of documents based on stylistic clues reflecting the authorship of documents. For the empirical validation of the method, we train a deep neural network model to predict authors of a large reference dataset consisting of news and blog articles. Albeit the learning process is supervised, it does not require a dedicated labeling of the data but <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> relies only on the metadata of the articles which are available in huge amounts. We evaluate the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on multiple <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, on both the authorship clustering and the authorship attribution tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.wnut-1.31.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.wnut-1.31/>A Little Birdie Told Me... -Inductive Biases for Rumour Stance Detection on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/k/karthik-radhakrishnan/>Karthik Radhakrishnan</a>
|
<a href=/people/t/tushar-kanakagiri/>Tushar Kanakagiri</a>
|
<a href=/people/s/sharanya-chakravarthy/>Sharanya Chakravarthy</a>
|
<a href=/people/v/vidhisha-balachandran/>Vidhisha Balachandran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--31><div class="card-body p-3 small">The rise in the usage of <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has placed <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> in a central position for <a href=https://en.wikipedia.org/wiki/Dissemination>news dissemination</a> and consumption. This greatly increases the potential for proliferation of <a href=https://en.wikipedia.org/wiki/Rumor>rumours</a> and <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a>. In an effort to mitigate the spread of rumours, we tackle the related task of identifying the stance (Support, Deny, Query, Comment) of a <a href=https://en.wikipedia.org/wiki/Social_media_marketing>social media post</a>. Unlike previous works, we impose <a href=https://en.wikipedia.org/wiki/Inductive_reasoning>inductive biases</a> that capture platform specific user behavior. These <a href=https://en.wikipedia.org/wiki/Bias>biases</a>, coupled with social media fine-tuning of BERT allow for better <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>, thus yielding an F1 score of 58.7 on the SemEval 2019 task on rumour stance detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.34" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.34/>IITKGP at W-NUT 2020 Shared Task-1 : Domain specific BERT representation for Named Entity Recognition of lab protocol<span class=acl-fixed-case>IITKGP</span> at <span class=acl-fixed-case>W</span>-<span class=acl-fixed-case>NUT</span> 2020 Shared Task-1: Domain specific <span class=acl-fixed-case>BERT</span> representation for Named Entity Recognition of lab protocol</a></strong><br><a href=/people/t/tejas-vaidhya/>Tejas Vaidhya</a>
|
<a href=/people/a/ayush-kaushal/>Ayush Kaushal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--34><div class="card-body p-3 small">Supervised models trained to predict properties from representations have been achieving high accuracy on a variety of tasks. For in-stance, the BERT family seems to work exceptionally well on the downstream task from NER tagging to the range of other linguistictasks. But the vocabulary used in the <a href=https://en.wikipedia.org/wiki/Medicine>medical field</a> contains a lot of different tokens used only in the <a href=https://en.wikipedia.org/wiki/Healthcare_industry>medical industry</a> such as the name of different diseases, devices, organisms, medicines, etc. that makes it difficult for traditional BERT model to create contextualized embedding. In this paper, we are going to illustrate the <a href=https://en.wikipedia.org/wiki/System>System</a> for Named Entity Tagging based on Bio-Bert. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> gives substantial improvements over the baseline and stood the fourth runner up in terms of <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a>, and first runner up in terms of <a href=https://en.wikipedia.org/wiki/Recall_(memory)>Recall</a> with just 2.21 <a href=https://en.wikipedia.org/wiki/F1_score>F1 score</a> behind the best one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.38.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--38 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.38 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.38/>mgsohrab at WNUT 2020 Shared Task-1 : Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols<span class=acl-fixed-case>WNUT</span> 2020 Shared Task-1: Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols</a></strong><br><a href=/people/m/mohammad-golam-sohrab/>Mohammad Golam Sohrab</a>
|
<a href=/people/a/anh-khoa-duong-nguyen/>Anh-Khoa Duong Nguyen</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/h/hiroya-takamura/>Hiroya Takamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--38><div class="card-body p-3 small">We present a neural exhaustive approach that addresses named entity recognition (NER) and relation recognition (RE), for the entity and re- lation recognition over the wet-lab protocols shared task. We introduce BERT-based neural exhaustive approach that enumerates all pos- sible spans as potential entity mentions and classifies them into entity types or no entity with deep neural networks to address NER. To solve relation extraction task, based on the NER predictions or given gold mentions we create all possible trigger-argument pairs and classify them into relation types or no relation. In NER task, we achieved 76.60 % in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> as third rank system among the partic- ipated systems. In relation extraction task, we achieved 80.46 % in terms of <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> as the top system in the relation extraction or recognition task. Besides we compare our model based on the wet lab protocols corpus (WLPC) with the WLPC baseline and dynamic graph-based in- formation extraction (DyGIE) systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.41/>WNUT-2020 Task 2 : Identification of Informative COVID-19 English Tweets<span class=acl-fixed-case>WNUT</span>-2020 Task 2: Identification of Informative <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/d/dat-quoc-nguyen/>Dat Quoc Nguyen</a>
|
<a href=/people/t/thanh-vu/>Thanh Vu</a>
|
<a href=/people/a/afshin-rahimi/>Afshin Rahimi</a>
|
<a href=/people/m/mai-hoang-dao/>Mai Hoang Dao</a>
|
<a href=/people/l/linh-the-nguyen/>Linh The Nguyen</a>
|
<a href=/people/l/long-doan/>Long Doan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--41><div class="card-body p-3 small">In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of 10 K Tweets</a> and organize the development and evaluation phases for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised training</a> performs well in this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.45/>Siva at WNUT-2020 Task 2 : Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets<span class=acl-fixed-case>WNUT</span>-2020 Task 2: Fine-tuning Transformer Neural Networks for Identification of Informative Covid-19 Tweets</a></strong><br><a href=/people/s/siva-sai/>Siva Sai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--45><div class="card-body p-3 small">Social media witnessed vast amounts of <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> being circulated every day during the Covid-19 pandemic so much so that the WHO Director-General termed the phenomenon as infodemic. The ill-effects of such <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> are multifarious. Thus, identifying and eliminating the sources of <a href=https://en.wikipedia.org/wiki/Misinformation>misinformation</a> becomes very crucial, especially when <a href=https://en.wikipedia.org/wiki/Mass_psychogenic_illness>mass panic</a> can be controlled only through the right information. However, manual identification is arduous, with such large amounts of data being generated every day. This shows the importance of automatic identification of misinformative posts on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. WNUT-2020 Task 2 aims at building <a href=https://en.wikipedia.org/wiki/System>systems</a> for automatic identification of informative tweets. In this paper, I discuss my approach to WNUT-2020 Task 2. I fine-tuned eleven variants of four transformer networks -BERT, RoBERTa, XLM-RoBERTa, ELECTRA, on top of two different preprocessing techniques to reap good results. My top submission achieved an F1-score of 85.3 % in the final evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.48.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--48 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.48 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.48/>CXP949 at WNUT-2020 Task 2 : Extracting Informative COVID-19 Tweets-RoBERTa Ensembles and The Continued Relevance of Handcrafted Features<span class=acl-fixed-case>CXP</span>949 at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Extracting Informative <span class=acl-fixed-case>COVID</span>-19 Tweets - <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a Ensembles and The Continued Relevance of Handcrafted Features</a></strong><br><a href=/people/c/calum-perrio/>Calum Perrio</a>
|
<a href=/people/h/harish-tayyar-madabushi/>Harish Tayyar Madabushi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--48><div class="card-body p-3 small">This paper presents our submission to Task 2 of the Workshop on Noisy User-generated Text. We explore improving the performance of a pre-trained transformer-based language model fine-tuned for text classification through an ensemble implementation that makes use of corpus level information and a handcrafted feature. We test the effectiveness of including the aforementioned <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> in accommodating the challenges of a noisy data set centred on a specific subject outside the remit of the pre-training data. We show that inclusion of additional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> can improve <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> results and achieve a score within 2 points of the top performing team.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.55/>CSECU-DSG at WNUT-2020 Task 2 : Exploiting Ensemble of Transfer Learning and Hand-crafted Features for Identification of Informative COVID-19 English Tweets<span class=acl-fixed-case>CSECU</span>-<span class=acl-fixed-case>DSG</span> at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Exploiting Ensemble of Transfer Learning and Hand-crafted Features for Identification of Informative <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/f/fareen-tasneem/>Fareen Tasneem</a>
|
<a href=/people/j/jannatun-naim/>Jannatun Naim</a>
|
<a href=/people/r/radiathun-tasnia/>Radiathun Tasnia</a>
|
<a href=/people/t/tashin-hossain/>Tashin Hossain</a>
|
<a href=/people/a/abu-nowshed-chy/>Abu Nowshed Chy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--55><div class="card-body p-3 small">COVID-19 pandemic has become the trending topic on twitter and people are interested in sharing diverse information ranging from new cases, healthcare guidelines, medicine, and vaccine news. Such information assists the people to be updated about the situation as well as beneficial for public safety personnel for decision making. However, the informal nature of <a href=https://en.wikipedia.org/wiki/Twitter>twitter</a> makes it challenging to refine the informative tweets from the huge tweet streams. To address these challenges WNUT-2020 introduced a shared task focusing on COVID-19 related informative tweet identification. In this paper, we describe our participation in this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We propose a neural model that adopts the strength of <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and hand-crafted features in a unified architecture. To extract the transfer learning features, we utilize the state-of-the-art pre-trained sentence embedding model BERT, RoBERTa, and InferSent, whereas various twitter characteristics are exploited to extract the hand-crafted features. Next, various feature combinations are utilized to train a set of multilayer perceptron (MLP) as the base-classifier. Finally, a majority voting based fusion approach is employed to determine the informative tweets. Our approach achieved competitive performance and outperformed the baseline by 7 % (approx.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.wnut-1.56" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.56/>IRLab@IITBHU at WNUT-2020 Task 2 : Identification of informative COVID-19 English Tweets using BERT<span class=acl-fixed-case>IRL</span>ab@<span class=acl-fixed-case>IITBHU</span> at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Identification of informative <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>E</span>nglish Tweets using <span class=acl-fixed-case>BERT</span></a></strong><br><a href=/people/s/supriya-chanda/>Supriya Chanda</a>
|
<a href=/people/e/eshita-nandy/>Eshita Nandy</a>
|
<a href=/people/s/sukomal-pal/>Sukomal Pal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--56><div class="card-body p-3 small">This paper reports our submission to the shared Task 2 : Identification of informative COVID-19 English tweets at W-NUT 2020. We attempted a few techniques, and we briefly explain here two models that showed promising results in tweet classification tasks : DistilBERT and <a href=https://en.wikipedia.org/wiki/FastText>FastText</a>. DistilBERT achieves a F1 score of 0.7508 on the test set, which is the best of our submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.58/>DSC-IIT ISM at WNUT-2020 Task 2 : Detection of COVID-19 informative tweets using RoBERTa<span class=acl-fixed-case>DSC</span>-<span class=acl-fixed-case>IIT</span> <span class=acl-fixed-case>ISM</span> at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Detection of <span class=acl-fixed-case>COVID</span>-19 informative tweets using <span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>BERT</span>a</a></strong><br><a href=/people/s/sirigireddy-dhana-laxmi/>Sirigireddy Dhana Laxmi</a>
|
<a href=/people/r/rohit-agarwal/>Rohit Agarwal</a>
|
<a href=/people/a/aman-sinha/>Aman Sinha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--58><div class="card-body p-3 small">Social media such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> is a hotspot of user-generated information. In this ongoing Covid-19 pandemic, there has been an abundance of data on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> which can be classified as informative and uninformative content. In this paper, we present our work to detect informative Covid-19 English tweets using RoBERTa model as a part of the W-NUT workshop 2020. We show the efficacy of our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> on a public dataset with an <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 0.89 on the validation dataset and 0.87 on the <a href=https://en.wikipedia.org/wiki/Score_(statistics)>leaderboard</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.60/>NLPRL at WNUT-2020 Task 2 : ELMo-based System for Identification of COVID-19 Tweets<span class=acl-fixed-case>NLPRL</span> at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: <span class=acl-fixed-case>ELM</span>o-based System for Identification of <span class=acl-fixed-case>COVID</span>-19 Tweets</a></strong><br><a href=/people/r/rajesh-kumar-mundotiya/>Rajesh Kumar Mundotiya</a>
|
<a href=/people/r/rupjyoti-baruah/>Rupjyoti Baruah</a>
|
<a href=/people/b/bhavana-srivastava/>Bhavana Srivastava</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--60><div class="card-body p-3 small">The Coronavirus pandemic has been a dominating news on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people and their related symptoms, as available on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, can help in <a href=https://en.wikipedia.org/wiki/Preventive_healthcare>prevention</a> and taking precautions. This is an example of using noisy text processing for <a href=https://en.wikipedia.org/wiki/Emergency_management>disaster management</a>. This paper discusses the NLPRL results in Shared Task-2 of WNUT-2020 workshop. We have considered this problem as a binary classification problem and have used a pre-trained ELMo embedding with GRU units. This approach helps classify the tweets with <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> as 80.85 % and 78.54 % as <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> on the provided test dataset. The experimental code is available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.63/>ComplexDataLab at W-NUT 2020 Task 2 : Detecting Informative COVID-19 Tweets by Attending over Linked Documents<span class=acl-fixed-case>C</span>omplex<span class=acl-fixed-case>D</span>ata<span class=acl-fixed-case>L</span>ab at <span class=acl-fixed-case>W</span>-<span class=acl-fixed-case>NUT</span> 2020 Task 2: Detecting Informative <span class=acl-fixed-case>COVID</span>-19 Tweets by Attending over Linked Documents</a></strong><br><a href=/people/k/kellin-pelrine/>Kellin Pelrine</a>
|
<a href=/people/j/jacob-danovitch/>Jacob Danovitch</a>
|
<a href=/people/a/albert-orozco-camacho/>Albert Orozco Camacho</a>
|
<a href=/people/r/reihaneh-rabbany/>Reihaneh Rabbany</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--63><div class="card-body p-3 small">Given the global scale of COVID-19 and the flood of social media content related to it, how can we find informative discussions? We present Gapformer, which effectively classifies content as informative or not. It reformulates the problem as <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph classification</a>, drawing on not only the tweet but connected webpages and entities. We leverage a pre-trained language model as well as the connections between nodes to learn a pooled representation for each document network. We show it outperforms several competitive baselines and present ablation studies supporting the benefit of the linked information. Code is available on Github.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.65.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--65 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.65 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.65/>LynyrdSkynyrd at WNUT-2020 Task 2 : Semi-Supervised Learning for Identification of Informative COVID-19 English Tweets<span class=acl-fixed-case>L</span>ynyrd<span class=acl-fixed-case>S</span>kynyrd at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Semi-Supervised Learning for Identification of Informative <span class=acl-fixed-case>COVID</span>-19 <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/a/abhilasha-sancheti/>Abhilasha Sancheti</a>
|
<a href=/people/k/kushal-chawla/>Kushal Chawla</a>
|
<a href=/people/g/gaurav-verma/>Gaurav Verma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--65><div class="card-body p-3 small">In this work, we describe our system for WNUT-2020 shared task on the identification of informative COVID-19 English tweets. Our system is an ensemble of various machine learning methods, leveraging both traditional feature-based classifiers as well as recent advances in pre-trained language models that help in capturing the syntactic, semantic, and contextual features from the tweets. We further employ pseudo-labelling to incorporate the unlabelled Twitter data released on the pandemic. Our best performing <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves an F1-score of 0.9179 on the provided validation set and 0.8805 on the blind test-set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.73/>SunBear at WNUT-2020 Task 2 : Improving BERT-Based Noisy Text Classification with Knowledge of the Data domain<span class=acl-fixed-case>S</span>un<span class=acl-fixed-case>B</span>ear at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: Improving <span class=acl-fixed-case>BERT</span>-Based Noisy Text Classification with Knowledge of the Data domain</a></strong><br><a href=/people/l/linh-doan-bao/>Linh Doan Bao</a>
|
<a href=/people/v/viet-anh-nguyen/>Viet Anh Nguyen</a>
|
<a href=/people/q/quang-pham-huu/>Quang Pham Huu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--73><div class="card-body p-3 small">This paper proposes an improved custom model for WNUT task 2 : Identification of Informative COVID-19 English Tweet. We improve experiment with the effectiveness of <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning methodologies</a> for state-of-the-art <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> RoBERTa. We make a preliminary instantiation of this formal <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> for the text classification approaches. With appropriate training techniques, our model is able to achieve 0.9218 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> on public validation set and the ensemble version settles at top 9 <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> (0.9005) and top 2 Recall (0.9301) on private test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.75.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--75 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.75 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.75/>COVCOR20 at WNUT-2020 Task 2 : An Attempt to Combine <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Expert rules<span class=acl-fixed-case>COVCOR</span>20 at <span class=acl-fixed-case>WNUT</span>-2020 Task 2: An Attempt to Combine Deep Learning and Expert rules</a></strong><br><a href=/people/a/ali-hurriyetoglu/>Ali Hürriyetoğlu</a>
|
<a href=/people/a/ali-safaya/>Ali Safaya</a>
|
<a href=/people/o/osman-mutlu/>Osman Mutlu</a>
|
<a href=/people/n/nelleke-oostdijk/>Nelleke Oostdijk</a>
|
<a href=/people/e/erdem-yoruk/>Erdem Yörük</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--75><div class="card-body p-3 small">In the scope of WNUT-2020 Task 2, we developed various text classification systems, using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a> and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically informed rules, we found that through the integration of (the output of) the three systems a better performance could be achieved than the standalone performance of each approach in a cross-validation setting. However, on the test data the performance of the <a href=https://en.wikipedia.org/wiki/Integral>integration</a> was slightly lower than our best performing deep learning model. These results hardly indicate any progress in line of integrating <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and expert rules driven systems. We expect that the release of the annotation manuals and gold labels of the test data after this workshop will shed light on these perplexing results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.76/>TEST_POSITIVE at W-NUT 2020 Shared Task-3 : Cross-task modeling<span class=acl-fixed-case>TEST</span>_<span class=acl-fixed-case>POSITIVE</span> at <span class=acl-fixed-case>W</span>-<span class=acl-fixed-case>NUT</span> 2020 Shared Task-3: Cross-task modeling</a></strong><br><a href=/people/c/chacha-chen/>Chacha Chen</a>
|
<a href=/people/c/chieh-yang-huang/>Chieh-Yang Huang</a>
|
<a href=/people/y/yaqi-hou/>Yaqi Hou</a>
|
<a href=/people/y/yang-shi/>Yang Shi</a>
|
<a href=/people/e/enyan-dai/>Enyan Dai</a>
|
<a href=/people/j/jiaqi-wang/>Jiaqi Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--76><div class="card-body p-3 small">The competition of extracting COVID-19 events from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> is to develop systems that can automatically extract related events from <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. The built <a href=https://en.wikipedia.org/wiki/System>system</a> should identify different pre-defined slots for each event, in order to answer important questions (e.g., Who is tested positive? What is the age of the person? Where is he / she?). To tackle these challenges, we propose the Joint Event Multi-task Learning (JOELIN) model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. Moreover, we implement a type-aware post-processing procedure using named entity recognition (NER) to further filter the predictions. JOELIN outperforms the BERT baseline by 17.2 % in micro F1.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.wnut-1.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--wnut-1--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.wnut-1.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.wnut-1.80/>HLTRI at W-NUT 2020 Shared Task-3 : COVID-19 Event Extraction from Twitter Using Multi-Task Hopfield Pooling<span class=acl-fixed-case>HLTRI</span> at <span class=acl-fixed-case>W</span>-<span class=acl-fixed-case>NUT</span> 2020 Shared Task-3: <span class=acl-fixed-case>COVID</span>-19 Event Extraction from <span class=acl-fixed-case>T</span>witter Using Multi-Task Hopfield Pooling</a></strong><br><a href=/people/m/maxwell-weinzierl/>Maxwell Weinzierl</a>
|
<a href=/people/s/sanda-harabagiu/>Sanda Harabagiu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--wnut-1--80><div class="card-body p-3 small">Extracting structured knowledge involving self-reported events related to the COVID-19 pandemic from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> has the potential to inform surveillance systems that play a critical role in <a href=https://en.wikipedia.org/wiki/Public_health>public health</a>. The event extraction challenge presented by the W-NUT 2020 Shared Task 3 focused on the identification of five types of events relevant to the COVID-19 pandemic and their respective set of pre-defined slots encoding demographic, epidemiological, clinical as well as spatial, temporal or subjective knowledge. Our participation in the challenge led to the design of a neural architecture for jointly identifying all Event Slots expressed in a tweet relevant to an event of interest. This <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> uses COVID-Twitter-BERT as the pre-trained language model. In addition, to learn text span embeddings for each Event Slot, we relied on a special case of <a href=https://en.wikipedia.org/wiki/Hopfield_network>Hopfield Networks</a>, namely Hopfield pooling. The results of the shared task evaluation indicate that our system performs best when it is trained on a larger dataset, while <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> remains competitive when training on smaller datasets.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>