<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>International Conference on Natural Language Processing (2019) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>International Conference on Natural Language Processing (2019)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2019icon-1>Proceedings of the 16th International Conference on Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">15&nbsp;papers</span></li></ul></div></div><div id=2019icon-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2019.icon-1/>Proceedings of the 16th International Conference on Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.0/>Proceedings of the 16th International Conference on Natural Language Processing</a></strong><br><a href=/people/d/dipti-misra-sharma/>Dipti Misra Sharma</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharya</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.2.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--2 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.2 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.2/>A Deep Ensemble Framework for Fake News Detection and Multi-Class Classification of Short Political Statements</a></strong><br><a href=/people/a/arjun-roy/>Arjun Roy</a>
|
<a href=/people/k/kingshuk-basak/>Kingshuk Basak</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--2><div class="card-body p-3 small">Fake news, <a href=https://en.wikipedia.org/wiki/Rumor>rumor</a>, incorrect information, and misinformation detection are nowadays crucial issues as these might have serious consequences for our social fabrics. Such information is increasing rapidly due to the availability of enormous web information sources including <a href=https://en.wikipedia.org/wiki/Social_media>social media feeds</a>, <a href=https://en.wikipedia.org/wiki/Blog>news blogs</a>, <a href=https://en.wikipedia.org/wiki/Online_newspaper>online newspapers</a> etc. In this paper, we develop various deep learning models for detecting <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and classifying them into the pre-defined fine-grained categories. At first, we develop individual models based on Convolutional Neural Network (CNN), and Bi-directional Long Short Term Memory (Bi-LSTM) networks. The <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> obtained from these two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are fed into a Multi-layer Perceptron Model (MLP) for the final <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Our experiments on a <a href=https://en.wikipedia.org/wiki/Benchmarking>benchmark dataset</a> show promising results with an overall <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 44.87 %, which outperforms the current state of the arts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.4/>Introducing Aspects of <a href=https://en.wikipedia.org/wiki/Creativity>Creativity</a> in Automatic Poetry Generation</a></strong><br><a href=/people/b/brendan-bena/>Brendan Bena</a>
|
<a href=/people/j/jugal-kalita/>Jugal Kalita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--4><div class="card-body p-3 small">Poetry Generation involves teaching systems to automatically generate text that resembles <a href=https://en.wikipedia.org/wiki/Poetry>poetic work</a>. A <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning system</a> can learn to generate <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a> on its own by training on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of poems</a> and modeling the particular style of language. In this paper, we propose taking an approach that fine-tunes GPT-2, a pre-trained language model, to our downstream task of poetry generation. We extend prior work on poetry generation by introducing creative elements. Specifically, we generate <a href=https://en.wikipedia.org/wiki/Poetry>poems</a> that express emotion and elicit the same in readers, and <a href=https://en.wikipedia.org/wiki/Poetry>poems</a> that use the language of dreamscalled dream poetry. We are able to produce <a href=https://en.wikipedia.org/wiki/Poetry>poems</a> that correctly elicit the emotions of sadness and joy 87.5 and 85 percent, respectively, of the time. We produce dreamlike poetry by training on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of texts</a> that describe <a href=https://en.wikipedia.org/wiki/Dream>dreams</a>. Poems from this <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> are shown to capture elements of <a href=https://en.wikipedia.org/wiki/Dream_vision>dream poetry</a> with scores of no less than 3.2 on the <a href=https://en.wikipedia.org/wiki/Likert_scale>Likert scale</a>. We perform crowdsourced human-evaluation for all our <a href=https://en.wikipedia.org/wiki/Poetry>poems</a>. We also make use of the Coh-Metrix tool, outlining metrics we use to gauge the quality of text generated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.5/>Incorporating Sub-Word Level Information in Language Invariant Neural Event Detection</a></strong><br><a href=/people/s/suhan-prabhu/>Suhan Prabhu</a>
|
<a href=/people/p/pranav-goel/>Pranav Goel</a>
|
<a href=/people/a/alok-debnath/>Alok Debnath</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--5><div class="card-body p-3 small">Detection of TimeML events in text have traditionally been done on <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> such as TimeBanks. However, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> have not been applied to these <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a>, because these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> seldom contain more than 10,000 <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>event mentions</a>. Traditional architectures revolve around highly feature engineered, language specific statistical models. In this paper, we present a Language Invariant Neural Event Detection (ALINED) architecture. ALINED uses an aggregation of both sub-word level features as well as lexical and structural information. This is achieved by combining convolution over character embeddings, with recurrent layers over contextual word embeddings. We find that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> extracts relevant <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for event span identification without relying on language specific features. We compare the performance of our language invariant model to the current state-of-the-art in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>. We outperform the F1-score of the state of the art in <a href=https://en.wikipedia.org/wiki/English_language>English</a> by 1.65 points. We achieve <a href=https://en.wikipedia.org/wiki/International_Federation_of_the_Phonographic_Industry>F1-scores</a> of 84.96, 80.87 and 74.81 on <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a> respectively which is comparable to the current states of the art for these languages. We also introduce the automatic annotation of events in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, a low resource language, with an F1-Score of 77.13.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.6/>Event Centric Entity Linking for Hindi News Articles : A Knowledge Graph Based Approach<span class=acl-fixed-case>H</span>indi News Articles: A Knowledge Graph Based Approach</a></strong><br><a href=/people/p/pranav-goel/>Pranav Goel</a>
|
<a href=/people/s/suhan-prabhu/>Suhan Prabhu</a>
|
<a href=/people/a/alok-debnath/>Alok Debnath</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--6><div class="card-body p-3 small">We describe the development of a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> from an event annotated corpus by presenting a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> that identifies and extracts the relations between entities and events from <a href=https://en.wikipedia.org/wiki/Hindi>Hindi news articles</a>. Due to the semantic implications of argument identification for events in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>, we use a combined syntactic argument and semantic role identification methodology. To the best of our knowledge, no other <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> exists for this purpose. The extracted combined role information is incorporated in a <a href=https://en.wikipedia.org/wiki/Knowledge_graph>knowledge graph</a> that can be queried via subgraph extraction for basic questions. The architectures presented in this paper can be used for participant extraction and event-entity linking in most <a href=https://en.wikipedia.org/wiki/Indo-Aryan_languages>Indo-Aryan languages</a>, due to similar syntactic and semantic properties of event arguments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.8.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--8 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.8 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2019.icon-1.8.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2019.icon-1.8/>Non-native Accent Partitioning for Speakers of Indian Regional Languages<span class=acl-fixed-case>I</span>ndian Regional Languages</a></strong><br><a href=/people/r/radha-krishna-guntur/>Radha Krishna Guntur</a>
|
<a href=/people/k/krishnan-ramakrishnan/>Krishnan Ramakrishnan</a>
|
<a href=/people/v/vinay-kumar-mittal/>Vinay Kumar Mittal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--8><div class="card-body p-3 small">Acoustic features extracted from the speech signal can help in identifying speaker related multiple information such as geographical origin, <a href=https://en.wikipedia.org/wiki/Regional_accents_of_English>regional accent</a> and <a href=https://en.wikipedia.org/wiki/Nativity_of_Jesus>nativity</a>. In this paper, classification of native speakers of South Indian languages is carried out based upon the accent of their non-native language, i.e., <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Four South Indian languages : <a href=https://en.wikipedia.org/wiki/Kannada>Kannada</a>, <a href=https://en.wikipedia.org/wiki/Malayalam>Malayalam</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a>, and <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a> are examined. A database of English speech from the native speakers of these languages, along with the native language speech data was collected, from a non-overlapping set of speakers. Segment level acoustic features F0 and Mel-frequency cepstral coefficients (MFCCs) are used. Accent partitioning of non-native English speech data is carried out using multiple classifiers : k-nearest neighbour (KNN), <a href=https://en.wikipedia.org/wiki/Linear_discriminant_analysis>linear discriminant analysis (LDA)</a> and <a href=https://en.wikipedia.org/wiki/Support_vector_machine>support vector machine (SVM)</a>, for validation and comparison of results. Classification accuracies of 86.6 % are observed using KNN, and 89.2 % or more than 90 % using SVM classifier. A study of acoustic feature F0 contour, related to L2 intonation, showed that native speakers of <a href=https://en.wikipedia.org/wiki/Kannada>Kannada language</a> are quite distinct as compared to those of Tamil or Telugu languages. It is also observed that identification of Malayalam and Kannada speakers from their English speech accent is relatively easier than Telugu or Tamil speakers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.9/>A little perturbation makes a difference : Treebank augmentation by perturbation improves transfer parsing</a></strong><br><a href=/people/a/ayan-das/>Ayan Das</a>
|
<a href=/people/s/sudeshna-sarkar/>Sudeshna Sarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--9><div class="card-body p-3 small">We present an approach for cross-lingual transfer of dependency parser so that the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> trained on a single source language can more effectively cater to diverse target languages. In this work, we show that the cross-lingual performance of the <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> can be enhanced by over-generating the source language treebank. For this, the source language treebank is augmented with its perturbed version in which controlled perturbation is introduced in the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse trees</a> by stochastically reordering the positions of the dependents with respect to their heads while keeping the structure of the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse trees</a> unchanged. This enables the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to capture diverse syntactic patterns in addition to those that are found in the source language. The resulting <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> is found to more effectively parse target languages with different <a href=https://en.wikipedia.org/wiki/Syntax_(programming_languages)>syntactic structures</a>. With <a href=https://en.wikipedia.org/wiki/English_language>English</a> as the source language, our system shows an average improvement of 6.7 % and 7.7 % in terms of <a href=https://en.wikipedia.org/wiki/Universal_asynchronous_receiver-transmitter>UAS</a> and LAS over 29 target languages compared to the baseline single source parser trained using unperturbed source language treebank. This also results in significant improvement over the transfer parser proposed by (CITATION) that involves an order-free parser algorithm.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.12/>Sanskrit Segmentation revisited<span class=acl-fixed-case>S</span>anskrit Segmentation revisited</a></strong><br><a href=/people/s/sriram-krishnan/>Sriram Krishnan</a>
|
<a href=/people/a/amba-kulkarni/>Amba Kulkarni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--12><div class="card-body p-3 small">Computationally analyzing Sanskrit texts requires proper segmentation in the initial stages. There have been various <a href=https://en.wikipedia.org/wiki/Tool>tools</a> developed for Sanskrit text segmentation. Of these, Grard Huet&#8217;s Reader in the Sanskrit Heritage Engine analyzes the input text and segments it based on the word parameters-phases like iic, ifc, Pr, Subst, etc., and sandhi (or transition) that takes place at the end of a word with the initial part of the next word. And it enlists all the possible solutions differentiating them with the help of the <a href=https://en.wikipedia.org/wiki/Phase_(matter)>phases</a>. The <a href=https://en.wikipedia.org/wiki/Phase_(matter)>phases</a> and their analyses have their use in the domain of <a href=https://en.wikipedia.org/wiki/Parsing>sentential parsers</a>. In segmentation, though, they are not used beyond deciding whether the words formed with the phases are morphologically valid. This paper tries to modify the above segmenter by ignoring the phase details (except for a few cases), and also proposes a <a href=https://en.wikipedia.org/wiki/Probability_function>probability function</a> to prioritize the list of solutions to bring up the most valid solutions at the top.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.15/>Dataset for Aspect Detection on Mobile reviews in Hindi<span class=acl-fixed-case>H</span>indi</a></strong><br><a href=/people/p/pruthwik-mishra/>Pruthwik Mishra</a>
|
<a href=/people/a/ayush-joshi/>Ayush Joshi</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--15><div class="card-body p-3 small">In recent years <a href=https://en.wikipedia.org/wiki/Opinion_mining>Opinion Mining</a> has become one of the very interesting fields of <a href=https://en.wikipedia.org/wiki/Language_processing>Language Processing</a>. To extract the gist of a sentence in a shorter and efficient manner is what <a href=https://en.wikipedia.org/wiki/Opinion_mining>opinion mining</a> provides. In this paper we focus on detecting aspects for a particular <a href=https://en.wikipedia.org/wiki/Domain_(mathematical_analysis)>domain</a>. While relevant research work has been done in aspect detection in resource rich languages like <a href=https://en.wikipedia.org/wiki/English_language>English</a>, we are trying to do the same in a relatively resource poor Hindi language. Here we present a corpus of mobile reviews which are labelled with carefully curated aspects. The motivation behind Aspect detection is to get information on a finer level about the data. In this paper we identify all aspects related to the gadget which are present on the reviews given online on various websites. We also propose <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline models</a> to detect <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspects</a> in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi text</a> after conducting various experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.18/>Towards Handling Verb Phrase Ellipsis in English-Hindi Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>H</span>indi Machine Translation</a></strong><br><a href=/people/n/niyati-bafna/>Niyati Bafna</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--18><div class="card-body p-3 small">English-Hindi machine translation systems have difficulty interpreting verb phrase ellipsis (VPE) in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, and commit errors in translating sentences with VPE. We present a solution and theoretical backing for the treatment of English VPE, with the specific scope of enabling English-Hindi MT, based on an understanding of the syntactical phenomenon of verb-stranding verb phrase ellipsis in Hindi (VVPE). We implement a rule-based system to perform the following sub-tasks : 1) Verb ellipsis identification in the English source sentence, 2) Elided verb phrase head identification 3) Identification of verb segment which needs to be induced at the site of <a href=https://en.wikipedia.org/wiki/Ellipsis_(linguistics)>ellipsis</a> 4) Modify input sentence ; i.e. resolving <a href=https://en.wikipedia.org/wiki/Verb&#8211;object&#8211;subject>VPE</a> and inducing the required <a href=https://en.wikipedia.org/wiki/Verb&#8211;object&#8211;subject>verb segment</a>. This <a href=https://en.wikipedia.org/wiki/System>system</a> obtains 94.83 percent <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> and 83.04 percent <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> on subtask (1), tested on 3900 sentences from the BNC corpus. This is competitive with state-of-the-art results. We measure <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of subtasks (2) and (3) together, and obtain a 91 percent <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on 200 sentences taken from the WSJ corpus. Finally, in order to indicate the relevance of ellipsis handling to MT, we carried out a manual analysis of the English-Hindi MT outputs of 100 sentences after passing it through our system. We set up a basic metric (1-5) for this evaluation, where 5 indicates drastic improvement, and obtained an average of 3.55. As far as we know, this is the first attempt to target ellipsis resolution in the context of improving English-Hindi machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.22/>Kunji : A Resource Management System for Higher Productivity in Computer Aided Translation Tools</a></strong><br><a href=/people/p/priyank-gupta/>Priyank Gupta</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Misra Sharma</a>
|
<a href=/people/r/rashid-ahmad/>Rashid Ahmad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--22><div class="card-body p-3 small">Complex NLP applications, such as machine translation systems, utilize various kinds of resources namely lexical, multiword, domain dictionaries, maps and rules etc. Similarly, translators working on Computer Aided Translation workbenches, also require help from various kinds of resources-glossaries, <a href=https://en.wikipedia.org/wiki/Terminology>terminologies</a>, <a href=https://en.wikipedia.org/wiki/Concordance_(publishing)>concordances</a> and <a href=https://en.wikipedia.org/wiki/Translation_memory>translation memory</a> in the workbenches in order to increase their productivity. Additionally, translators have to look away from the workbenches for linguistic resources like Named Entities, Multiwords, lexical and lexeme dictionaries in order to get help, as the available resources like concordances, terminologies and glossaries are often not enough. In this paper we present <a href=https://en.wikipedia.org/wiki/Kunji>Kunji</a>, a <a href=https://en.wikipedia.org/wiki/Resource_management_(computing)>resource management system</a> for translation workbenches and MT modules. This <a href=https://en.wikipedia.org/wiki/System>system</a> can be easily integrated in translation workbenches and can also be used as a management tool for resources for MT systems. The described <a href=https://en.wikipedia.org/wiki/Resource_management_(computing)>resource management system</a> has been integrated in a translation workbench Transzaar. We also study the impact of providing this <a href=https://en.wikipedia.org/wiki/Resource_management>resource management system</a> along with linguistic resources on the productivity of translators for English-Hindi language pair. When the linguistic resources like <a href=https://en.wikipedia.org/wiki/Lexeme>lexeme</a>, NER and MWE dictionaries were made available to translators in addition to their regular translation memories, <a href=https://en.wikipedia.org/wiki/Concordance_(publishing)>concordances</a> and <a href=https://en.wikipedia.org/wiki/Terminology>terminologies</a>, their productivity increased by 15.61 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.25/>Unsung Challenges of Building and Deploying Language Technologies for Low Resource Language Communities</a></strong><br><a href=/people/p/pratik-joshi/>Pratik Joshi</a>
|
<a href=/people/c/christain-barnes/>Christain Barnes</a>
|
<a href=/people/s/sebastin-santy/>Sebastin Santy</a>
|
<a href=/people/s/simran-khanuja/>Simran Khanuja</a>
|
<a href=/people/s/sanket-shah/>Sanket Shah</a>
|
<a href=/people/a/anirudh-srinivasan/>Anirudh Srinivasan</a>
|
<a href=/people/s/satwik-bhattamishra/>Satwik Bhattamishra</a>
|
<a href=/people/s/sunayana-sitaram/>Sunayana Sitaram</a>
|
<a href=/people/m/monojit-choudhury/>Monojit Choudhury</a>
|
<a href=/people/k/kalika-bali/>Kalika Bali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--25><div class="card-body p-3 small">In this paper, we examine and analyze the challenges associated with developing and introducing <a href=https://en.wikipedia.org/wiki/Language_technology>language technologies</a> to low-resource language communities. While doing so we bring to light the successes and failures of past work in this area, challenges being faced in doing so, and what have they achieved. Throughout this paper, we take a problem-facing approach and describe essential factors which the success of such <a href=https://en.wikipedia.org/wiki/Technology>technologies</a> hinges upon. We present the various aspects in a manner which clarify and lay out the different tasks involved, which can aid organizations looking to make an impact in this area. We take the example of <a href=https://en.wikipedia.org/wiki/Gondi_language>Gondi</a>, an extremely-low resource Indian language, to reinforce and complement our discussion.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.26/>DRCoVe : An Augmented Word Representation Approach using Distributional and Relational Context<span class=acl-fixed-case>DRC</span>o<span class=acl-fixed-case>V</span>e: An Augmented Word Representation Approach using Distributional and Relational Context</a></strong><br><a href=/people/m/md-aslam-parwez/>Md. Aslam Parwez</a>
|
<a href=/people/m/muhammad-abulaish/>Muhammad Abulaish</a>
|
<a href=/people/m/mohd-fazil/>Mohd Fazil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--26><div class="card-body p-3 small">Word representation using the distributional information of words from a sizeable corpus is considered efficacious in many natural language processing and text mining applications. However, distributional representation of a word is unable to capture distant relational knowledge, representing the relational semantics. In this paper, we propose a novel word representation approach using distributional and relational contexts, DRCoVe, which augments the distributional representation of a word using the relational semantics extracted as syntactic and semantic association among entities from the underlying corpus. Unlike existing approaches that use external knowledge bases representing the relational semantics for enhanced word representation, DRCoVe uses typed dependencies (aka syntactic dependencies) to extract relational knowledge from the underlying corpus. The proposed approach is applied over a biomedical text corpus to learn <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representation</a> and compared with <a href=https://en.wikipedia.org/wiki/GloVe_(machine_learning)>GloVe</a>, which is one of the most popular word embedding approaches. The evaluation results on various benchmark datasets for word similarity and word categorization tasks demonstrate the effectiveness of DRCoVe over the <a href=https://en.wikipedia.org/wiki/GloVe>GloVe</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2019.icon-1.27/>A Deep Learning Approach for Automatic Detection of Fake News</a></strong><br><a href=/people/t/tanik-saikh/>Tanik Saikh</a>
|
<a href=/people/a/arkadipta-de/>Arkadipta De</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--27><div class="card-body p-3 small">Fake news detection is a very prominent and essential task in the field of <a href=https://en.wikipedia.org/wiki/Journalism>journalism</a>. This challenging <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> is seen so far in the field of <a href=https://en.wikipedia.org/wiki/Politics>politics</a>, but it could be even more challenging when it is to be determined in the multi-domain platform. In this paper, we propose two effective models based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> for solving fake news detection problem in online news contents of multiple domains. We evaluate our techniques on the two recently released datasets, namely Fake News AMT and Celebrity for fake news detection. The proposed <a href=https://en.wikipedia.org/wiki/System>systems</a> yield encouraging performance, outperforming the current hand-crafted feature engineering based state-of-the-art system with a significant margin of 3.08 % and 9.3 % by the two <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, respectively. In order to exploit the datasets, available for the related tasks, we perform cross-domain analysis (model trained on FakeNews AMT and tested on <a href=https://en.wikipedia.org/wiki/Celebrity>Celebrity</a> and vice versa) to explore the applicability of our systems across the domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2019.icon-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2019--icon-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2019.icon-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2019.icon-1.28.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2019.icon-1.28/>Samajh-Boojh : A Reading Comprehension system in Hindi<span class=acl-fixed-case>H</span>indi</a></strong><br><a href=/people/s/shalaka-vaidya/>Shalaka Vaidya</a>
|
<a href=/people/h/hiranmai-sri-adibhatla/>Hiranmai Sri Adibhatla</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2019--icon-1--28><div class="card-body p-3 small">This paper presents a novel approach designed to answer questions on a reading comprehension passage. It is an end-to-end system which first focuses on comprehending the given passage wherein it converts <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured passage</a> into a <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> and later proceeds to answer the questions related to the passage using solely the aforementioned <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a>. To the best of our knowledge, the proposed <a href=https://en.wikipedia.org/wiki/Design>design</a> is first of its kind which accounts for entire process of comprehending the passage and then answering the questions associated with the passage. The comprehension stage converts the passage into a Discourse Collection that comprises of the relation shared amongst logical sentences in given passage along with the key characteristics of each sentence. This <a href=https://en.wikipedia.org/wiki/Design>design</a> has its applications in academic domain, query comprehension in speech systems among others.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>