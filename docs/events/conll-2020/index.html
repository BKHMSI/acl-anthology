<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Conference on Computational Natural Language Learning (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Conference on Computational Natural Language Learning (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020conll-1>Proceedings of the 24th Conference on Computational Natural Language Learning</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#2020conll-shared>Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li></ul></div></div><div id=2020conll-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.conll-1/>Proceedings of the 24th Conference on Computational Natural Language Learning</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.0/>Proceedings of the 24th Conference on Computational Natural Language Learning</a></strong><br><a href=/people/r/raquel-fernandez/>Raquel Fernández</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.conll-1.3" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.3/>Neural Proof Nets</a></strong><br><a href=/people/k/konstantinos-kogkalidis/>Konstantinos Kogkalidis</a>
|
<a href=/people/m/michael-moortgat/>Michael Moortgat</a>
|
<a href=/people/r/richard-moot/>Richard Moot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--3><div class="card-body p-3 small">Linear logic and the linear -calculus have a long standing tradition in the study of natural language form and <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning</a>. Among the proof calculi of linear logic, <a href=https://en.wikipedia.org/wiki/Proof_nets>proof nets</a> are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on Thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear -calculus with an accuracy of as high as 70 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.6/>On the Frailty of Universal POS Tags for Neural UD Parsers<span class=acl-fixed-case>POS</span> Tags for Neural <span class=acl-fixed-case>UD</span> Parsers</a></strong><br><a href=/people/m/mark-anderson/>Mark Anderson</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--6><div class="card-body p-3 small">We present an analysis on the effect UPOS accuracy has on <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance. Results suggest that leveraging UPOS tags as fea-tures for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionality. We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.11/>Bridging Information-Seeking Human Gaze and Machine Reading Comprehension</a></strong><br><a href=/people/j/jonathan-malmaud/>Jonathan Malmaud</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a>
|
<a href=/people/y/yevgeni-berzak/>Yevgeni Berzak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--11><div class="card-body p-3 small">In this work, we analyze how human gaze during <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> is conditioned on the given <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension question</a>, and whether this signal can be beneficial for machine reading comprehension. To this end, we collect a new eye-tracking dataset with a large number of participants engaging in a multiple choice reading comprehension task. Our analysis of this data reveals increased fixation times over parts of the text that are most relevant for answering the question. Motivated by this finding, we propose making automated reading comprehension more human-like by mimicking human information-seeking reading behavior during <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>. We demonstrate that this approach leads to performance gains on multiple choice question answering in English for a state-of-the-art reading comprehension model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.12.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--12 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.12 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.conll-1.12" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.12/>A Corpus of Very Short Scientific Summaries</a></strong><br><a href=/people/y/yifan-chen/>Yifan Chen</a>
|
<a href=/people/t/tamara-polajnar/>Tamara Polajnar</a>
|
<a href=/people/c/colin-batchelor/>Colin Batchelor</a>
|
<a href=/people/s/simone-teufel/>Simone Teufel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--12><div class="card-body p-3 small">We present a new summarisation task, taking <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific articles</a> and producing journal table-of-contents entries in the chemistry domain. These are one- or two-sentence author-written summaries that present the key findings of a paper. This is a first look at this summarisation task with an open access publication corpus consisting of titles and abstracts, as input texts, and short author-written advertising blurbs, as the ground truth. We introduce the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and evaluate <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> with state-of-the-art summarisation methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.conll-1.16" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.16/>Identifying Incorrect Labels in the CoNLL-2003 Corpus<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NLL</span>-2003 Corpus</a></strong><br><a href=/people/f/frederick-reiss/>Frederick Reiss</a>
|
<a href=/people/h/hong-xu/>Hong Xu</a>
|
<a href=/people/b/bryan-cutler/>Bryan Cutler</a>
|
<a href=/people/k/karthik-muthuraman/>Karthik Muthuraman</a>
|
<a href=/people/z/zachary-eichenberger/>Zachary Eichenberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--16><div class="card-body p-3 small">The CoNLL-2003 corpus for English-language named entity recognition (NER) is one of the most influential corpora for NER model research. A large number of publications, including many landmark works, have used this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> as a source of ground truth for NER tasks. In this paper, we examine this corpus and identify over 1300 incorrect labels (out of 35089 in the corpus). In particular, the number of incorrect labels in the test fold is comparable to the number of errors that state-of-the-art models make when running <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> over this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. We describe the process by which we identified these incorrect labels, using novel variants of techniques from <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised learning</a>. We also summarize the types of errors that we found, and we revisit several recent results in <a href=https://en.wikipedia.org/wiki/Errors_and_residuals>NER</a> in light of the corrected data. Finally, we show experimentally that our corrections to the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> have a positive impact on three state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.20/>Cross-lingual Embeddings Reveal Universal and Lineage-Specific Patterns in Grammatical Gender Assignment</a></strong><br><a href=/people/h/hartger-veeman/>Hartger Veeman</a>
|
<a href=/people/m/marc-allassonniere-tang/>Marc Allassonnière-Tang</a>
|
<a href=/people/a/aleksandrs-berdicevskis/>Aleksandrs Berdicevskis</a>
|
<a href=/people/a/ali-basirat/>Ali Basirat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--20><div class="card-body p-3 small">Grammatical gender is assigned to nouns differently in different languages. Are all factors that influence <a href=https://en.wikipedia.org/wiki/Sex_assignment>gender assignment</a> idiosyncratic to languages or are there any that are universal? Using cross-lingual aligned word embeddings, we perform two experiments to address these questions about <a href=https://en.wikipedia.org/wiki/Language_typology>language typology</a> and <a href=https://en.wikipedia.org/wiki/Cognition>human cognition</a>. In both experiments, we predict the gender of nouns in language X using a <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> trained on the nouns of language Y, and take the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier&#8217;s accuracy</a> as a measure of transferability of gender systems. First, we show that for 22 <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European languages</a> the <a href=https://en.wikipedia.org/wiki/Language_transfer>transferability</a> decreases as the <a href=https://en.wikipedia.org/wiki/Phylogenetic_tree>phylogenetic distance</a> increases. This correlation supports the claim that some gender assignment factors are idiosyncratic, and as the languages diverge, the proportion of shared inherited idiosyncrasies diminishes. Second, we show that when the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> is trained on two <a href=https://en.wikipedia.org/wiki/Afroasiatic_languages>Afro-Asiatic languages</a> and tested on the same 22 <a href=https://en.wikipedia.org/wiki/Indo-European_languages>Indo-European languages</a> (or vice versa), its performance is still significantly above the chance baseline, thus showing that universal factors exist and, moreover, can be captured by word embeddings. When the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> is tested across families and on inanimate nouns only, the performance is still above baseline, indicating that the universal factors are not limited to <a href=https://en.wikipedia.org/wiki/Sex>biological sex</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.conll-1.21.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.conll-1.21/>Modelling Lexical Ambiguity with Density Matrices</a></strong><br><a href=/people/f/francois-meyer/>Francois Meyer</a>
|
<a href=/people/m/martha-lewis/>Martha Lewis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--21><div class="card-body p-3 small">Words can have multiple senses. Compositional distributional models of meaning have been argued to deal well with finer shades of meaning variation known as <a href=https://en.wikipedia.org/wiki/Polysemy>polysemy</a>, but are not so well equipped to handle word senses that are etymologically unrelated, or <a href=https://en.wikipedia.org/wiki/Homonym>homonymy</a>. Moving from <a href=https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)>vectors</a> to <a href=https://en.wikipedia.org/wiki/Density_matrix>density matrices</a> allows us to encode a <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> over different senses of a word, and can also be accommodated within a compositional distributional model of meaning. In this paper we present three new neural models for learning <a href=https://en.wikipedia.org/wiki/Density_matrix>density matrices</a> from a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, and test their ability to discriminate between <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> on a range of compositional datasets. When paired with a particular composition method, our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms existing vector-based compositional models as well as strong sentence encoders.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.conll-1.25" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.25/>Word Representations Concentrate and This is Good News !</a></strong><br><a href=/people/r/romain-couillet/>Romain Couillet</a>
|
<a href=/people/y/yagmur-gizem-cinar/>Yagmur Gizem Cinar</a>
|
<a href=/people/e/eric-gaussier/>Eric Gaussier</a>
|
<a href=/people/m/muhammad-imran/>Muhammad Imran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--25><div class="card-body p-3 small">This article establishes that, unlike the legacy tf*idf representation, recent natural language representations (word embedding vectors) tend to exhibit a so-called concentration of measure phenomenon, in the sense that, as the representation size p and database size n are both large, their behavior is similar to that of large dimensional Gaussian random vectors. This phenomenon may have important consequences as <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language data</a> could be amenable to improvement, thereby providing new theoretical insights into the field of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>.<i>concentration of measure phenomenon</i>, in the sense that, as the representation size <tex-math>p</tex-math> and database size <tex-math>n</tex-math> are both large, their behavior is similar to that of large dimensional Gaussian random vectors. This phenomenon may have important consequences as machine learning algorithms for natural language data could be amenable to improvement, thereby providing new theoretical insights into the field of natural language processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.26.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--26 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.26 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.conll-1.26.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.conll-1.26/>LazImpa : Lazy and Impatient neural agents learn to communicate efficiently<span class=acl-fixed-case>L</span>az<span class=acl-fixed-case>I</span>mpa”: Lazy and Impatient neural agents learn to communicate efficiently</a></strong><br><a href=/people/m/mathieu-rita/>Mathieu Rita</a>
|
<a href=/people/r/rahma-chaabouni/>Rahma Chaabouni</a>
|
<a href=/people/e/emmanuel-dupoux/>Emmanuel Dupoux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--26><div class="card-body p-3 small">Previous work has shown that <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>artificial neural agents</a> naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new <a href=https://en.wikipedia.org/wiki/Communication_system>communication system</a>, LazImpa, where the speaker is made increasingly lazy, i.e., avoids long messages, and the listener impatient, i.e., seeks to guess the intended content as soon as possible.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.conll-1.36.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.conll-1.36" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.36/>Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models</a></strong><br><a href=/people/s/steven-derby/>Steven Derby</a>
|
<a href=/people/p/paul-miller/>Paul Miller</a>
|
<a href=/people/b/barry-devereux/>Barry Devereux</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--36><div class="card-body p-3 small">Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a>. Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks. In this paper, we make meaningful comparisons between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent. We also construct a new set of word embeddings using the output embeddings to create locally-optimal approximations for the intermediate representations from the <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. These locally-optimal embeddings demonstrate excellent performance across all our evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.40/>Do n’t Parse, Insert : Multilingual Semantic Parsing with Insertion Based Decoding</a></strong><br><a href=/people/q/qile-zhu/>Qile Zhu</a>
|
<a href=/people/h/haidar-khan/>Haidar Khan</a>
|
<a href=/people/s/saleh-soltan/>Saleh Soltan</a>
|
<a href=/people/s/stephen-rawls/>Stephen Rawls</a>
|
<a href=/people/w/wael-hamza/>Wael Hamza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--40><div class="card-body p-3 small">Semantic parsing is one of the key components of <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding systems</a>. A successful <a href=https://en.wikipedia.org/wiki/Parsing>parse</a> transforms an input utterance to an action that is easily understood by the <a href=https://en.wikipedia.org/wiki/System>system</a>. Many <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> have been proposed to solve this problem, from conventional rule-based or statistical slot-filling systems to shift-reduce based neural parsers. For complex parsing tasks, the state-of-the-art method is based on an autoregressive sequence to sequence model that generates the parse directly. This model is slow at inference time, generating <a href=https://en.wikipedia.org/wiki/Parsing>parses</a> in O(n) decoding steps (n is the length of the target sequence). In addition, we demonstrate that this method performs poorly in zero-shot cross-lingual transfer learning settings. In this paper, we propose a non-autoregressive parser which is based on the insertion transformer to overcome these two issues. Our approach 1) speeds up decoding by 3x while outperforming the <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive model</a> and 2) significantly improves cross-lingual transfer in the low-resource setting by 37 % compared to autoregressive baseline. We test our approach on three wellknown monolingual datasets : ATIS, SNIPS and TOP. For cross-lingual semantic parsing, we use the MultiATIS++ and the multilingual TOP datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.49.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--49 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.49 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.49/>Cloze Distillation : Improving Neural Language Models with Human Next-Word Prediction</a></strong><br><a href=/people/t/tiwalayo-eisape/>Tiwalayo Eisape</a>
|
<a href=/people/n/noga-zaslavsky/>Noga Zaslavsky</a>
|
<a href=/people/r/roger-levy/>Roger Levy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--49><div class="card-body p-3 small">Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus probabilities</a> and human next-word predictions. Here we evaluate several state-of-the-art <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> for their match to human next-word predictions and to reading time behavior from <a href=https://en.wikipedia.org/wiki/Eye_movement>eye movements</a>. We then propose a novel method for distilling the linguistic information implicit in human linguistic predictions into pre-trained LMs : Cloze Distillation. We apply this method to a baseline neural LM and show potential improvement in reading time prediction and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> to held-out human cloze data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-1.52.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-1--52 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-1.52 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.conll-1.52" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2020.conll-1.52/>From Dataset Recycling to Multi-Property Extraction and Beyond</a></strong><br><a href=/people/t/tomasz-dwojak/>Tomasz Dwojak</a>
|
<a href=/people/m/michal-pietruszka/>Michał Pietruszka</a>
|
<a href=/people/l/lukasz-borchmann/>Łukasz Borchmann</a>
|
<a href=/people/j/jakub-chledowski/>Jakub Chłędowski</a>
|
<a href=/people/f/filip-gralinski/>Filip Graliński</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-1--52><div class="card-body p-3 small">This paper investigates various Transformer architectures on the WikiReading Information Extraction and Machine Reading Comprehension dataset. The proposed dual-source model outperforms the current state-of-the-art by a large margin. Next, we introduce WikiReading Recycled-a newly developed public dataset, and the task of multiple-property extraction. It uses the same data as WikiReading but does not inherit its predecessor&#8217;s identified disadvantages. In addition, we provide a human-annotated test set with diagnostic subsets for a detailed analysis of <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance.</div></div></div><hr><div id=2020conll-shared><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/2020.conll-shared/>Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-shared.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.conll-shared.0/>Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing</a></strong><br><a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/o/omri-abend/>Omri Abend</a>
|
<a href=/people/l/lasha-abzianidze/>Lasha Abzianidze</a>
|
<a href=/people/j/johan-bos/>Johan Bos</a>
|
<a href=/people/j/jan-hajic/>Jan Hajič</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a>
|
<a href=/people/b/bin-li/>Bin Li</a>
|
<a href=/people/t/tim-o-gorman/>Tim O'Gorman</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a>
|
<a href=/people/d/daniel-zeman/>Daniel Zeman</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-shared.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-shared--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-shared.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.conll-shared.4/>Hitachi at MRP 2020 : Text-to-Graph-Notation Transducer<span class=acl-fixed-case>MRP</span> 2020: Text-to-Graph-Notation Transducer</a></strong><br><a href=/people/h/hiroaki-ozaki/>Hiroaki Ozaki</a>
|
<a href=/people/g/gaku-morio/>Gaku Morio</a>
|
<a href=/people/y/yuta-koreeda/>Yuta Koreeda</a>
|
<a href=/people/t/terufumi-morishita/>Terufumi Morishita</a>
|
<a href=/people/t/toshinori-miyoshi/>Toshinori Miyoshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-shared--4><div class="card-body p-3 small">This paper presents our proposed <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> for the shared task on Meaning Representation Parsing (MRP 2020) at CoNLL, where participant systems were required to parse five types of <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> in different languages. We propose to unify these tasks as a text-to-graph-notation transduction in which we convert an input text into a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph notation</a>. To this end, we designed a novel Plain Graph Notation (PGN) that handles various <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> universally. Then, our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> predicts a PGN-based sequence by leveraging Transformers and biaffine attentions. Notably, our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> can handle any PGN-formatted graphs with fewer framework-specific modifications. As a result, ensemble versions of the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> tied for 1st place in both cross-framework and cross-lingual tracks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.conll-shared.7.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--conll-shared--7 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.conll-shared.7 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.conll-shared.7/>HUJI-KU at MRP 2020 : Two Transition-based Neural Parsers<span class=acl-fixed-case>HUJI</span>-<span class=acl-fixed-case>KU</span> at <span class=acl-fixed-case>MRP</span> 2020: Two Transition-based Neural Parsers</a></strong><br><a href=/people/o/ofir-arviv/>Ofir Arviv</a>
|
<a href=/people/r/ruixiang-cui/>Ruixiang Cui</a>
|
<a href=/people/d/daniel-hershcovich/>Daniel Hershcovich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--conll-shared--7><div class="card-body p-3 small">This paper describes the HUJI-KU system submission to the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2020 Conference for Computational Language Learning (CoNLL), employing TUPA and the HIT-SCIR parser, which were, respectively, the baseline system and winning system in the 2019 MRP shared task. Both are transition-based parsers using BERT contextualized embeddings. We generalized TUPA to support the newly-added MRP frameworks and languages, and experimented with multitask learning with the HIT-SCIR parser. We reached 4th place in both the crossframework and cross-lingual tracks.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>