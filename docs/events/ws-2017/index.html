<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Other Workshops and Events (2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Other Workshops and Events (2017)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#w17-01>Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-02>Proceedings of the 21st Nordic Conference on Computational Linguistics</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-03>Proceedings of the joint workshop on NLP for Computer Assisted Language Learning and NLP for Language Acquisition</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-04>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-05>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-06>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-07>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2017)</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w17-08>Proceedings of the 11th Linguistic Annotation Workshop</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w17-09>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w17-10>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w17-11>Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w17-12>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</a>
<span class="badge badge-info align-middle ml-1">22&nbsp;papers</span></li><li><a class=align-middle href=#w17-13>Proceedings of the Third Arabic Natural Language Processing Workshop</a>
<span class="badge badge-info align-middle ml-1">20&nbsp;papers</span></li><li><a class=align-middle href=#w17-14>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w17-15>Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w17-16>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w17-17>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</a>
<span class="badge badge-info align-middle ml-1">23&nbsp;papers</span></li><li><a class=align-middle href=#w17-18>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w17-19>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#w17-20>Proceedings of the Sixth Workshop on Vision and Language</a>
<span class="badge badge-info align-middle ml-1">5&nbsp;papers</span></li><li><a class=align-middle href=#w17-22>Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#w17-23>BioNLP 2017</a>
<span class="badge badge-info align-middle ml-1">43&nbsp;papers</span></li><li><a class=align-middle href=#w17-24>Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w17-25>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#w17-26>Proceedings of the 2nd Workshop on Representation Learning for NLP</a>
<span class="badge badge-info align-middle ml-1">24&nbsp;papers</span></li><li><a class=align-middle href=#w17-27>Proceedings of the Events and Stories in the News Workshop</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w17-28>Proceedings of the First Workshop on Language Grounding for Robotics</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w17-29>Proceedings of the Second Workshop on NLP and Computational Social Science</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#w17-30>Proceedings of the First Workshop on Abusive Language Online</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#w17-31>Proceedings of the Fourth Workshop on Computational Linguistics and Clinical Psychology — From Linguistic Signal to Clinical Reality</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w17-32>Proceedings of the First Workshop on Neural Machine Translation</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w17-34>Proceedings of the 15th Meeting on the Mathematics of Language</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-35>Proceedings of the 10th International Conference on Natural Language Generation</a>
<span class="badge badge-info align-middle ml-1">33&nbsp;papers</span></li><li><a class=align-middle href=#w17-36>Proceedings of the 6th Workshop on Recent Advances in RST and Related Formalisms</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-37>Proceedings of the 1st Workshop on Explainable Computational Intelligence (XCI 2017)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-38>Proceedings of the Linguistic Resources for Automatic Natural Language Generation - LiRA@NLG</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-39>Proceedings of the Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2017)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-40>Proceedings of the 13th International Conference on Finite State Methods and Natural Language Processing (FSMNLP 2017)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-41>Proceedings of the First Workshop on Subword and Character Level Models in NLP</a>
<span class="badge badge-info align-middle ml-1">22&nbsp;papers</span></li><li><a class=align-middle href=#w17-42>Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism</a>
<span class="badge badge-info align-middle ml-1">19&nbsp;papers</span></li><li><a class=align-middle href=#w17-43>Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">8&nbsp;papers</span></li><li><a class=align-middle href=#w17-44>Proceedings of the 3rd Workshop on Noisy User-generated Text</a>
<span class="badge badge-info align-middle ml-1">22&nbsp;papers</span></li><li><a class=align-middle href=#w17-45>Proceedings of the Workshop on New Frontiers in Summarization</a>
<span class="badge badge-info align-middle ml-1">11&nbsp;papers</span></li><li><a class=align-middle href=#w17-46>Proceedings of the Workshop on Speech-Centric Natural Language Processing</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w17-47>Proceedings of the Second Conference on Machine Translation</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-48>Proceedings of the Third Workshop on Discourse in Machine Translation</a>
<span class="badge badge-info align-middle ml-1">13&nbsp;papers</span></li><li><a class=align-middle href=#w17-49>Proceedings of the Workshop on Stylistic Variation</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li><li><a class=align-middle href=#w17-50>Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</a>
<span class="badge badge-info align-middle ml-1">43&nbsp;papers</span></li><li><a class=align-middle href=#w17-51>Proceedings of the 4th Workshop on Argument Mining</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#w17-52>Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a>
<span class="badge badge-info align-middle ml-1">28&nbsp;papers</span></li><li><a class=align-middle href=#w17-53>Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP</a>
<span class="badge badge-info align-middle ml-1">9&nbsp;papers</span></li><li><a class=align-middle href=#w17-54>Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li><li><a class=align-middle href=#w17-55>Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</a>
<span class="badge badge-info align-middle ml-1">42&nbsp;papers</span></li><li><a class=align-middle href=#w17-56>Proceedings of the First Workshop on Curation and Applications of Parallel and Comparable Corpora</a>
<span class="badge badge-info align-middle ml-1">3&nbsp;papers</span></li><li><a class=align-middle href=#w17-57>Proceedings of the 4th Workshop on Asian Translation (WAT2017)</a>
<span class="badge badge-info align-middle ml-1">12&nbsp;papers</span></li><li><a class=align-middle href=#w17-58>Proceedings of the International Workshop on Digital Disease Detection using Social Media 2017 (DDDSM-2017)</a>
<span class="badge badge-info align-middle ml-1">7&nbsp;papers</span></li><li><a class=align-middle href=#w17-59>Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2017)</a>
<span class="badge badge-info align-middle ml-1">10&nbsp;papers</span></li><li><a class=align-middle href=#w17-60>Proceedings of the 9th SIGHAN Workshop on Chinese Language Processing</a>
<span class="badge badge-info align-middle ml-1">4&nbsp;papers</span></li><li><a class=align-middle href=#w17-62>Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-63>Proceedings of the 15th International Conference on Parsing Technologies</a>
<span class="badge badge-info align-middle ml-1">16&nbsp;papers</span></li><li><a class=align-middle href=#w17-65>Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-66>Proceedings of the 11th Brazilian Symposium in Information and Human Language Technology</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-68>IWCS 2017 - 12th International Conference on Computational Semantics - Long papers</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-69>IWCS 2017 — 12th International Conference on Computational Semantics — Short papers</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-70>Proceedings of Language, Ontology, Terminology and Knowledge Structures Workshop (LOTKS 2017)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-71>Proceedings of the IWCS workshop on Foundations of Situated and Multimodal Communication</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-72>Proceedings of the Computing Natural Language Inference Workshop</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-73>Proceedings of the 2nd Workshop on Semantic Deep Learning (SemDeep-2)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-74>Proceedings of the 13th Joint ISO-ACL Workshop on Interoperable Semantic Annotation (ISA-13)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-75>Proceedings of the 14th International Conference on Natural Language Processing (ICON-2017)</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-76>Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-77>Proceedings of the 1st Workshop on Natural Language Processing and Information Retrieval associated with RANLP 2017</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-78>Proceedings of the Workshop Knowledge Resources for the Socio-Economic Sciences and Humanities associated with RANLP 2017</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-79>Proceedings of the Workshop Human-Informed Translation and Interpreting Technology</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-80>Proceedings of the Biomedical NLP Workshop associated with RANLP 2017</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li><li><a class=align-middle href=#w17-81>Proceedings of the First Workshop on Language technology for Digital Humanities in Central and (South-)Eastern Europe</a>
<span class="badge badge-info align-middle ml-1">1&nbsp;paper</span></li></ul></div></div><div id=w17-01><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-01.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-01/>Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0100/>Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages</a></strong><br><a href=/people/a/antti-arppe/>Antti Arppe</a>
|
<a href=/people/j/jeff-good/>Jeff Good</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a>
|
<a href=/people/j/jordan-lachler/>Jordan Lachler</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/l/lane-schwartz/>Lane Schwartz</a></span></p></div><hr><div id=w17-02><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-02.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-02/>Proceedings of the 21st Nordic Conference on Computational Linguistics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0200/>Proceedings of the 21st Nordic Conference on Computational Linguistics</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/n/nina-tahmasebi/>Nina Tahmasebi</a></span></p></div><hr><div id=w17-03><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-03/>Proceedings of the joint workshop on NLP for Computer Assisted Language Learning and NLP for Language Acquisition</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0300/>Proceedings of the joint workshop on <span class=acl-fixed-case>NLP</span> for Computer Assisted Language Learning and <span class=acl-fixed-case>NLP</span> for Language Acquisition</a></strong><br><a href=/people/e/elena-volodina/>Elena Volodina</a>
|
<a href=/people/g/gintare-grigonyte/>Gintarė Grigonytė</a>
|
<a href=/people/i/ildiko-pilan/>Ildikó Pilán</a>
|
<a href=/people/k/kristina-nilsson-bjorkenstam/>Kristina Nilsson Björkenstam</a>
|
<a href=/people/l/lars-borin/>Lars Borin</a></span></p></div><hr><div id=w17-04><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-04.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-04/>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0400/>Proceedings of the <span class=acl-fixed-case>N</span>o<span class=acl-fixed-case>D</span>a<span class=acl-fixed-case>L</span>i<span class=acl-fixed-case>D</span>a 2017 Workshop on Universal Dependencies (<span class=acl-fixed-case>UDW</span> 2017)</a></strong><br><a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a>
|
<a href=/people/s/sebastian-schuster/>Sebastian Schuster</a></span></p></div><hr><div id=w17-05><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-05.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-05/>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0500/>Proceedings of the <span class=acl-fixed-case>N</span>o<span class=acl-fixed-case>D</span>a<span class=acl-fixed-case>L</span>i<span class=acl-fixed-case>D</span>a 2017 Workshop on Processing Historical Language</a></strong><br><a href=/people/g/gerlof-bouma/>Gerlof Bouma</a>
|
<a href=/people/y/yvonne-adesam/>Yvonne Adesam</a></span></p></div><hr><div id=w17-06><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-06.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-06/>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0600/>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</a></strong><br><a href=/people/f/francis-tyers/>Francis M. Tyers</a>
|
<a href=/people/m/michael-riessler/>Michael Rießler</a>
|
<a href=/people/t/tommi-a-pirinen/>Tommi A. Pirinen</a>
|
<a href=/people/t/trond-trosterud/>Trond Trosterud</a></span></p></div><hr><div id=w17-07><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-07/>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0700/>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (<span class=acl-fixed-case>CMCL</span> 2017)</a></strong><br><a href=/people/t/ted-gibson/>Ted Gibson</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a>
|
<a href=/people/a/asad-sayeed/>Asad Sayeed</a>
|
<a href=/people/m/marten-van-schijndel/>Martin van Schijndel</a>
|
<a href=/people/w/william-schuler/>William Schuler</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0701.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0701 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0701 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0701/>Entropy Reduction correlates with <a href=https://en.wikipedia.org/wiki/Temporal_lobe>temporal lobe activity</a></a></strong><br><a href=/people/m/matthew-nelson/>Matthew Nelson</a>
|
<a href=/people/s/stanislas-dehaene/>Stanislas Dehaene</a>
|
<a href=/people/c/christophe-pallier/>Christophe Pallier</a>
|
<a href=/people/j/john-hale/>John Hale</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0701><div class="card-body p-3 small">Using the Entropy Reduction incremental complexity metric, we relate high gamma power signals from the brains of epileptic patients to incremental stages of syntactic analysis in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>. We find that signals recorded intracranially from the anterior Inferior Temporal Sulcus (aITS) and the posterior Inferior Temporal Gyrus (pITG) correlate with word-by-word Entropy Reduction values derived from phrase structure grammars for those languages. In the anterior region, this correlation persists even in combination with surprisal co-predictors from PCFG and ngram models. The result confirms the idea that the brain&#8217;s temporal lobe houses a <a href=https://en.wikipedia.org/wiki/Parsing>parsing function</a>, one whose incremental processing difficulty profile reflects changes in <a href=https://en.wikipedia.org/wiki/Uncertainty_principle>grammatical uncertainty</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0703/>Grounding sound change in ideal observer models of perception</a></strong><br><a href=/people/z/zachary-burchill/>Zachary Burchill</a>
|
<a href=/people/t/t-florian-jaeger/>T. Florian Jaeger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0703><div class="card-body p-3 small">An important predictor of historical sound change, <a href=https://en.wikipedia.org/wiki/Functional_load>functional load</a>, fails to capture insights from <a href=https://en.wikipedia.org/wiki/Speech_perception>speech perception</a>. Building on ideal observer models of word recognition, we devise a new definition of <a href=https://en.wikipedia.org/wiki/Functional_load>functional load</a> that incorporates both a priori predictability and perceptual information. We explore this new <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measure</a> with a simple <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and find that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> outperforms traditional <a href=https://en.wikipedia.org/wiki/Measure_(mathematics)>measures</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0704/>Oh, I’ve Heard That Before : Modelling Own-Dialect Bias After <a href=https://en.wikipedia.org/wiki/Perceptual_learning>Perceptual Learning</a> by Weighting Training Data<span class=acl-fixed-case>I</span>’ve Heard That Before”: Modelling Own-Dialect Bias After Perceptual Learning by Weighting Training Data</a></strong><br><a href=/people/r/rachael-tatman/>Rachael Tatman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0704><div class="card-body p-3 small">Human listeners are able to quickly and robustly adapt to new accents and do so by using information about speaker&#8217;s identities. This paper will present experimental evidence that, even considering information about speaker&#8217;s identities, listeners retain a strong bias towards the acoustics of their own dialect after dialect learning. Participants&#8217; behaviour was accurately mimicked by a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> which was trained on more cases from the base dialect and fewer from the target dialect. This suggests that imbalanced training data may result in automatic speech recognition errors consistent with those of speakers from populations over-represented in the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0705.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0705 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0705 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0705/>Inherent Biases of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a> for Phonological Assimilation and Dissimilation</a></strong><br><a href=/people/a/amanda-doucette/>Amanda Doucette</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0705><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network model</a> of phonological pattern learning is proposed. The model is a relatively simple <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> with one <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent layer</a>, and displays biases in learning that mimic observed biases in human learning. Single-feature patterns are learned faster than two-feature patterns, and vowel or consonant-only patterns are learned faster than <a href=https://en.wikipedia.org/wiki/Pattern>patterns</a> involving vowels and consonants, mimicking the results of laboratory learning experiments. In non-recurrent models, capturing these biases requires the use of alpha features or some other representation of repeated features, but with a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>, these elaborations are not necessary.</div></div></div><hr><div id=w17-08><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-08.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-08/>Proceedings of the 11th Linguistic Annotation Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0800/>Proceedings of the 11th Linguistic Annotation Workshop</a></strong><br><a href=/people/n/nathan-schneider/>Nathan Schneider</a>
|
<a href=/people/n/nianwen-xue/>Nianwen Xue</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-0801" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-0801/>Readers vs. Writers vs. Texts : Coping with Different Perspectives of Text Understanding in Emotion Annotation</a></strong><br><a href=/people/s/sven-buechel/>Sven Buechel</a>
|
<a href=/people/u/udo-hahn/>Udo Hahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0801><div class="card-body p-3 small">We here examine how different <a href=https://en.wikipedia.org/wiki/Point_of_view_(philosophy)>perspectives</a> of understanding <a href=https://en.wikipedia.org/wiki/Writing>written discourse</a>, like the reader&#8217;s, the writer&#8217;s or the text&#8217;s point of view, affect the quality of emotion annotations. We conducted a series of annotation experiments on two <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, a popular movie review corpus and a genre- and domain-balanced corpus of standard English. We found statistical evidence that the writer&#8217;s perspective yields superior annotation quality overall. However, the quality one perspective yields compared to the other(s) seems to depend on the domain the utterance originates from. Our data further suggest that the popular movie review data set suffers from an atypical bimodal distribution which may decrease model performance when used as a training resource.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-0802" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-0802/>Finding Good Conversations Online : The Yahoo News Annotated Comments Corpus<span class=acl-fixed-case>Y</span>ahoo <span class=acl-fixed-case>N</span>ews Annotated Comments Corpus</a></strong><br><a href=/people/c/courtney-napoles/>Courtney Napoles</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/a/aasish-pappu/>Aasish Pappu</a>
|
<a href=/people/e/enrica-rosato/>Enrica Rosato</a>
|
<a href=/people/b/brian-provenzale/>Brian Provenzale</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0802><div class="card-body p-3 small">This work presents a dataset and annotation scheme for the new task of identifying good conversations that occur online, which we call ERICs : Engaging, Respectful, and/or Informative Conversations. We develop a <a href=https://en.wikipedia.org/wiki/Taxonomy_(general)>taxonomy</a> to reflect features of entire threads and individual comments which we believe contribute to identifying ERICs ; code a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of Yahoo News comment threads (2.4k threads and 10k comments) and 1k threads from the Internet Argument Corpus ; and analyze the features characteristic of ERICs. This is one of the largest annotated corpora of online human dialogues, with the most detailed set of annotations. It will be valuable for identifying ERICs and other aspects of <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation</a>, <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>, and <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0803/>Crowdsourcing discourse interpretations : On the influence of context and the reliability of a connective insertion task</a></strong><br><a href=/people/m/merel-scholman/>Merel Scholman</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0803><div class="card-body p-3 small">Traditional discourse annotation tasks are considered costly and time-consuming, and the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> and <a href=https://en.wikipedia.org/wiki/Validity_(statistics)>validity</a> of these tasks is in question. In this paper, we investigate whether <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> can be used to obtain reliable discourse relation annotations. We also examine the influence of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a> on the <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of the <a href=https://en.wikipedia.org/wiki/Data>data</a>. The results of a crowdsourced connective insertion task showed that the method can be used to obtain reliable annotations : The majority of the inserted connectives converged with the original label. Further, the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is sensitive to the fact that multiple senses can often be inferred for a single relation. Regarding the presence of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context</a>, the results show no significant difference in distributions of insertions between conditions overall. However, a by-item comparison revealed several characteristics of segments that determine whether the presence of context makes a difference in annotations. The findings discussed in this paper can be taken as evidence that <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> can be used as a valuable method to obtain insights into the sense(s) of relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0804/>A Code-Switching Corpus of Turkish-German Conversations<span class=acl-fixed-case>T</span>urkish-<span class=acl-fixed-case>G</span>erman Conversations</a></strong><br><a href=/people/o/ozlem-cetinoglu/>Özlem Çetinoğlu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0804><div class="card-body p-3 small">We present a code-switching corpus of <a href=https://en.wikipedia.org/wiki/Turks_in_Germany>Turkish-German</a> that is collected by recording conversations of bilinguals. The recordings are then transcribed in two layers following speech and orthography conventions, and annotated with <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence boundaries</a> and intersentential, intrasentential, and intra-word switch points. The total amount of data is 5 hours of speech which corresponds to 3614 sentences. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> aims at serving as a resource for <a href=https://en.wikipedia.org/wiki/Speech_analysis>speech or text analysis</a>, as well as a collection for <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic inquiries</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-0805" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-0805/>Annotating omission in statement pairs</a></strong><br><a href=/people/h/hector-martinez-alonso/>Héctor Martínez Alonso</a>
|
<a href=/people/a/amaury-delamaire/>Amaury Delamaire</a>
|
<a href=/people/b/benoit-sagot/>Benoît Sagot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0805><div class="card-body p-3 small">We focus on the identification of omission in statement pairs. We compare three <a href=https://en.wikipedia.org/wiki/Crowdsourcing>annotation schemes</a>, namely two different <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing schemes</a> and <a href=https://en.wikipedia.org/wiki/Crowdsourcing>manual expert annotation</a>. We show that the simplest of the two crowdsourcing approaches yields a better annotation quality than the more complex one. We use a dedicated <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> to assess whether the annotators&#8217; behavior can be explained by straightforward <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a>. The <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> benefits from a <a href=https://en.wikipedia.org/wiki/Conceptual_model>modeling</a> that uses lexical information beyond length and overlap measures. However, for our task, we argue that expert and not crowdsourcing-based annotation is the best compromise between annotation cost and <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0806/>Annotating <a href=https://en.wikipedia.org/wiki/Speech>Speech</a>, Attitude and Perception Reports</a></strong><br><a href=/people/c/corien-bary/>Corien Bary</a>
|
<a href=/people/l/leopold-hess/>Leopold Hess</a>
|
<a href=/people/k/kees-thijs/>Kees Thijs</a>
|
<a href=/people/p/peter-berck/>Peter Berck</a>
|
<a href=/people/i/iris-hendrickx/>Iris Hendrickx</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0806><div class="card-body p-3 small">We present REPORTS, an annotation scheme for the annotation of speech, attitude and perception reports. Such a <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a> makes it possible to annotate the various <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text elements</a> involved in such reports (e.g. embedding entity, <a href=https://en.wikipedia.org/wiki/Complement_(set_theory)>complement</a>, <a href=https://en.wikipedia.org/wiki/Complement_(set_theory)>complement head</a>) and their relations in a uniform way, which in turn facilitates the automatic extraction of information on, for example, <a href=https://en.wikipedia.org/wiki/Complement_(set_theory)>complementation</a> and vocabulary distribution. We also present the Ancient Greek corpus RAG (Thucydides&#8217; History of the Peloponnesian War), to which we have applied this scheme using the annotation tool BRAT. We discuss some of the issues, both theoretical and practical, that we encountered, show how the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> helps in answering specific questions, and conclude that REPORTS fitted in well with our needs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0808 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0808/>Representation and Interchange of Linguistic Annotation. An In-Depth, Side-by-Side Comparison of Three Designs</a></strong><br><a href=/people/r/richard-eckart-de-castilho/>Richard Eckart de Castilho</a>
|
<a href=/people/n/nancy-ide/>Nancy Ide</a>
|
<a href=/people/e/emanuele-lapponi/>Emanuele Lapponi</a>
|
<a href=/people/s/stephan-oepen/>Stephan Oepen</a>
|
<a href=/people/k/keith-suderman/>Keith Suderman</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a>
|
<a href=/people/m/marc-verhagen/>Marc Verhagen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0808><div class="card-body p-3 small">For decades, most self-respecting linguistic engineering initiatives have designed and implemented custom representations for various layers of, for example, morphological, syntactic, and semantic analysis. Despite occasional efforts at harmonization or even standardization, our field today is blessed with a multitude of ways of encoding and exchanging linguistic annotations of these types, both at the levels of &#8216;abstract syntax&#8217;, naming choices, and of course file formats. To a large degree, it is possible to work within and across design plurality by conversion, and often there may be good reasons for divergent design reflecting differences in use. However, it is likely that some abstract commonalities across choices of representation are obscured by more superficial differences, and conversely there is no obvious procedure to tease apart what actually constitute contentful vs. mere technical divergences. In this study, we seek to conceptually align three representations for common types of morpho-syntactic analysis, pinpoint what in our view constitute contentful differences, and reflect on the underlying principles and specific requirements that led to individual choices. We expect that a more in-depth understanding of these choices across designs may led to increased harmonization, or at least to more informed design of future representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0809 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0809/>TDB 1.1 : Extensions on Turkish Discourse Bank<span class=acl-fixed-case>TDB</span> 1.1: Extensions on <span class=acl-fixed-case>T</span>urkish Discourse Bank</a></strong><br><a href=/people/d/deniz-zeyrek/>Deniz Zeyrek</a>
|
<a href=/people/m/murathan-kurfali/>Murathan Kurfalı</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0809><div class="card-body p-3 small">This paper presents the recent developments on Turkish Discourse Bank (TDB). First, the resource is summarized and an evaluation is presented. Then, TDB 1.1, i.e. enrichments on 10 % of the corpus are described (namely, senses for explicit discourse connectives, and new annotations for three discourse relation types-implicit relations, entity relations and alternative lexicalizations). The method of <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> is explained and the data are evaluated.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0810.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0810 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0810 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0810/>Two Layers of Annotation for Representing Event Mentions in News Stories</a></strong><br><a href=/people/m/maria-pia-di-buono/>Maria Pia di Buono</a>
|
<a href=/people/m/martin-tutek/>Martin Tutek</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/b/bojana-dalbelo-basic/>Bojana Dalbelo Bašić</a>
|
<a href=/people/n/natasa-milic-frayling/>Nataša Milić-Frayling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0810><div class="card-body p-3 small">In this paper, we describe our preliminary study on annotating event mention as a part of our research on high-precision news event extraction models. To this end, we propose a two-layer annotation scheme, designed to separately capture the functional and conceptual aspects of event mentions. We hypothesize that the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> can be improved by modeling and extracting separately the different aspects of news events, and then combining the extracted information by leveraging the complementarities of the <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a>. In addition, we carry out a preliminary <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> using the proposed scheme and analyze the <a href=https://en.wikipedia.org/wiki/Annotation>annotation quality</a> in terms of <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0811/>Word Similarity Datasets for <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian Languages</a> : Annotation and Baseline Systems<span class=acl-fixed-case>I</span>ndian Languages: Annotation and Baseline Systems</a></strong><br><a href=/people/s/syed-sarfaraz-akhtar/>Syed Sarfaraz Akhtar</a>
|
<a href=/people/a/arihant-gupta/>Arihant Gupta</a>
|
<a href=/people/a/avijit-vajpayee/>Avijit Vajpayee</a>
|
<a href=/people/a/arjit-srivastava/>Arjit Srivastava</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0811><div class="card-body p-3 small">With the advent of <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>word representations</a>, word similarity tasks are becoming increasing popular as an evaluation metric for the quality of the <a href=https://en.wikipedia.org/wiki/Word_processor_(electronic_device)>representations</a>. In this paper, we present manually annotated monolingual word similarity datasets of six Indian languages-Urdu, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a>, <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a>, <a href=https://en.wikipedia.org/wiki/Punjabi_language>Punjabi</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil</a> and <a href=https://en.wikipedia.org/wiki/Gujarati_language>Gujarati</a>. These <a href=https://en.wikipedia.org/wiki/Language>languages</a> are most spoken Indian languages worldwide after <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/Bengali_language>Bengali</a>. For the construction of these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, our approach relies on translation and re-annotation of word similarity datasets of English. We also present baseline scores for word representation models using state-of-the-art techniques for <a href=https://en.wikipedia.org/wiki/Urdu>Urdu</a>, <a href=https://en.wikipedia.org/wiki/Telugu_language>Telugu</a> and <a href=https://en.wikipedia.org/wiki/Marathi_language>Marathi</a> by evaluating them on newly created word similarity datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0813.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0813 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0813 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0813/>Catching the Common Cause : Extraction and Annotation of Causal Relations and their Participants</a></strong><br><a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0813><div class="card-body p-3 small">In this paper, we present a simple, yet effective method for the automatic identification and extraction of causal relations from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, based on a large English-German parallel corpus. The goal of this effort is to create a <a href=https://en.wikipedia.org/wiki/Lexical_resource>lexical resource</a> for German causal relations. The resource will consist of a lexicon that describes constructions that trigger causality as well as the participants of the causal event, and will be augmented by a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> with annotated instances for each entry, that can be used as training data to develop a system for automatic classification of causal relations. Focusing on verbs, our method harvested a set of 100 different lexical triggers of causality, including support verb constructions. At the moment, our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> includes over 1,000 annotated instances. The <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> and the annotated data will be made available to the research community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0814 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0814/>Assessing SRL Frameworks with Automatic Training Data Expansion<span class=acl-fixed-case>SRL</span> Frameworks with Automatic Training Data Expansion</a></strong><br><a href=/people/s/silvana-hartmann/>Silvana Hartmann</a>
|
<a href=/people/e/eva-mujdricza-maydt/>Éva Mújdricza-Maydt</a>
|
<a href=/people/i/ilia-kuznetsov/>Ilia Kuznetsov</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0814><div class="card-body p-3 small">We present the first experiment-based study that explicitly contrasts the three major semantic role labeling frameworks. As a prerequisite, we create a dataset labeled with parallel FrameNet-, PropBank-, and VerbNet-style labels for <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We train a state-of-the-art SRL tool for <a href=https://en.wikipedia.org/wiki/German_language>German</a> for the different annotation styles and provide a comparative analysis across frameworks. We further explore the behavior of the <a href=https://en.wikipedia.org/wiki/Software_framework>frameworks</a> with automatic training data generation. VerbNet provides larger semantic expressivity than <a href=https://en.wikipedia.org/wiki/PropBank>PropBank</a>, and we find that its generalization capacity approaches <a href=https://en.wikipedia.org/wiki/PropBank>PropBank</a> in SRL training, but it benefits less from training data expansion than the sparse-data affected FrameNet.</div></div></div><hr><div id=w17-09><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-09.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-09/>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0900/>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</a></strong><br><a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/n/nasrin-mostafazadeh/>Nasrin Mostafazadeh</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/a/annie-louis/>Annie Louis</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0901.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0901 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0901/>Inducing Script Structure from Crowdsourced Event Descriptions via Semi-Supervised Clustering</a></strong><br><a href=/people/l/lilian-wanzare/>Lilian Wanzare</a>
|
<a href=/people/a/alessandra-zarcone/>Alessandra Zarcone</a>
|
<a href=/people/s/stefan-thater/>Stefan Thater</a>
|
<a href=/people/m/manfred-pinkal/>Manfred Pinkal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0901><div class="card-body p-3 small">We present a semi-supervised clustering approach to induce script structure from crowdsourced descriptions of event sequences by grouping event descriptions into paraphrase sets (representing event types) and inducing their temporal order. Our approach exploits semantic and positional similarity and allows for flexible event order, thus overcoming the rigidity of previous approaches. We incorporate crowdsourced alignments as prior knowledge and show that exploiting a small number of <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignments</a> results in a substantial improvement in cluster quality over state-of-the-art models and provides an appropriate basis for the induction of temporal order. We also show a coverage study to demonstrate the scalability of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-0902" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-0902/>A Consolidated Open Knowledge Representation for Multiple Texts</a></strong><br><a href=/people/r/rachel-wities/>Rachel Wities</a>
|
<a href=/people/v/vered-shwartz/>Vered Shwartz</a>
|
<a href=/people/g/gabriel-stanovsky/>Gabriel Stanovsky</a>
|
<a href=/people/m/meni-adler/>Meni Adler</a>
|
<a href=/people/o/ori-shapira/>Ori Shapira</a>
|
<a href=/people/s/shyam-upadhyay/>Shyam Upadhyay</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a>
|
<a href=/people/e/eugenio-martinez-camara/>Eugenio Martinez Camara</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/i/ido-dagan/>Ido Dagan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0902><div class="card-body p-3 small">We propose to move from Open Information Extraction (OIE) ahead to Open Knowledge Representation (OKR), aiming to represent information conveyed jointly in a set of texts in an open text-based manner. We do so by consolidating OIE extractions using entity and predicate coreference, while modeling information containment between coreferring elements via lexical entailment. We suggest that generating OKR structures can be a useful step in the NLP pipeline, to give semantic applications an easy handle on consolidated information across multiple texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0903/>Event-Related Features in <a href=https://en.wikipedia.org/wiki/Feedforward_neural_network>Feedforward Neural Networks</a> Contribute to Identifying Causal Relations in Discourse</a></strong><br><a href=/people/e/edoardo-maria-ponti/>Edoardo Maria Ponti</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0903><div class="card-body p-3 small">Causal relations play a key role in <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. Most of the times, their expression is ambiguous or implicit, i.e. without signals in the text. This makes their identification challenging. We aim to improve their identification by implementing a <a href=https://en.wikipedia.org/wiki/Feedforward_neural_network>Feedforward Neural Network</a> with a novel set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In particular, these are based on the position of event mentions and the semantics of events and participants. The resulting classifier outperforms strong baselines on two datasets (the Penn Discourse Treebank and the CSTNews corpus) annotated with different schemes and containing examples in two languages, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>. This result demonstrates the importance of <a href=https://en.wikipedia.org/wiki/Event_(philosophy)>events</a> for identifying <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0904/>Stance Detection in <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook Posts</a> of a German Right-wing Party<span class=acl-fixed-case>F</span>acebook Posts of a <span class=acl-fixed-case>G</span>erman Right-wing Party</a></strong><br><a href=/people/m/manfred-klenner/>Manfred Klenner</a>
|
<a href=/people/d/don-tuggener/>Don Tuggener</a>
|
<a href=/people/s/simon-clematide/>Simon Clematide</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0904><div class="card-body p-3 small">We argue that in order to detect stance, not only the explicit attitudes of the stance holder towards the targets are crucial. It is the whole narrative the writer drafts that counts, including the way he hypostasizes the discourse referents : as benefactors or villains, as victims or beneficiaries. We exemplify the ability of our system to identify targets and detect the writer&#8217;s stance towards them on the basis of about 100 000 Facebook posts of a German right-wing party. A reader and writer model on top of our verb-based attitude extraction directly reveal stance conflicts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0905/>Behind the Scenes of an Evolving Event Cloze Test</a></strong><br><a href=/people/n/nathanael-chambers/>Nathanael Chambers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0905><div class="card-body p-3 small">This paper analyzes the narrative event cloze test and its recent evolution. The <a href=https://en.wikipedia.org/wiki/Software_testing>test</a> removes one event from a document&#8217;s chain of events, and systems predict the missing event. Originally proposed to evaluate learned knowledge of <a href=https://en.wikipedia.org/wiki/Event_(computing)>event scenarios</a> (e.g., <a href=https://en.wikipedia.org/wiki/Scripting_language>scripts</a> and frames), most recent work now builds ngram-like language models (LM) to beat the test. This paper argues that the <a href=https://en.wikipedia.org/wiki/Test_(assessment)>test</a> has slowly / unknowingly been altered to accommodate LMs.5 Most notably, tests are auto-generated rather than by hand, and no effort is taken to include core script events. Recent work is not clear on evaluation goals and contains contradictory results. We implement several <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, and show that the test&#8217;s bias to high-frequency events explains the inconsistencies. We conclude with recommendations on how to return to the test&#8217;s original intent, and offer brief suggestions on a path forward.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0906/>LSDSem 2017 Shared Task : The Story Cloze Test<span class=acl-fixed-case>LSDS</span>em 2017 Shared Task: The Story Cloze Test</a></strong><br><a href=/people/n/nasrin-mostafazadeh/>Nasrin Mostafazadeh</a>
|
<a href=/people/m/michael-roth/>Michael Roth</a>
|
<a href=/people/a/annie-louis/>Annie Louis</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/j/james-allen/>James Allen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0906><div class="card-body p-3 small">The LSDSem&#8217;17 shared task is the Story Cloze Test, a new evaluation for story understanding and script learning. This test provides a <a href=https://en.wikipedia.org/wiki/System>system</a> with a four-sentence story and two possible endings, and the system must choose the correct ending to the story. Successful narrative understanding (getting closer to human performance of 100 %) requires systems to link various levels of <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> to <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a>. A total of eight systems participated in the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a>, with a variety of <a href=https://en.wikipedia.org/wiki/Software_development_process>approaches</a> including.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0909/>Sentiment Analysis and Lexical Cohesion for the Story Cloze Task</a></strong><br><a href=/people/m/michael-flor/>Michael Flor</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0909><div class="card-body p-3 small">We present two NLP components for the Story Cloze Task dictionary-based sentiment analysis and lexical cohesion. While previous research found no contribution from sentiment analysis to the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on this task, we demonstrate that sentiment is an important aspect. We describe a new approach, using a <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rule</a> that estimates sentiment congruence in a story. Our sentiment-based system achieves strong results on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Our lexical cohesion system achieves <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> comparable to previously published baseline results. A combination of the two <a href=https://en.wikipedia.org/wiki/System>systems</a> achieves better <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than published baselines. We argue that <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> should be considered an integral part of narrative comprehension.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0910.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0910 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0910 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0910/>Resource-Lean Modeling of Coherence in Commonsense Stories</a></strong><br><a href=/people/n/niko-schenk/>Niko Schenk</a>
|
<a href=/people/c/christian-chiarcos/>Christian Chiarcos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0910><div class="card-body p-3 small">We present a resource-lean neural recognizer for modeling coherence in commonsense stories. Our lightweight system is inspired by successful attempts to modeling discourse relations and stands out due to its simplicity and easy optimization compared to prior approaches to narrative script learning. We evaluate our <a href=https://en.wikipedia.org/wiki/Software_development_process>approach</a> in the Story Cloze Test demonstrating an absolute improvement in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 4.7 % over state-of-the-art implementations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0912/>IIT (BHU): System Description for LSDSem’17 Shared Task<span class=acl-fixed-case>IIT</span> (<span class=acl-fixed-case>BHU</span>): System Description for <span class=acl-fixed-case>LSDS</span>em’17 Shared Task</a></strong><br><a href=/people/p/pranav-goel/>Pranav Goel</a>
|
<a href=/people/a/anil-kumar-singh/>Anil Kumar Singh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0912><div class="card-body p-3 small">This paper describes an ensemble system submitted as part of the LSDSem Shared Task 2017-the Story Cloze Test. The main conclusion from our results is that an approach based on <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> alone may not be enough for this task. We test various approaches and compare them with two ensemble systems. One is based on <a href=https://en.wikipedia.org/wiki/Voting>voting</a> and the other on <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression based classifier</a>. Our final <a href=https://en.wikipedia.org/wiki/System>system</a> is able to outperform the previous <a href=https://en.wikipedia.org/wiki/State_(computer_science)>state</a> of the art for the Story Cloze test. Another very interesting observation is the performance of sentiment based approach which works almost as well on its own as our final ensemble system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-0913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-0913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-0913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-0913/>Story Cloze Ending Selection Baselines and Data Examination</a></strong><br><a href=/people/t/todor-mihaylov/>Todor Mihaylov</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-0913><div class="card-body p-3 small">This paper describes two supervised baseline systems for the Story Cloze Test Shared Task (Mostafazadeh et al., 2016a). We first build a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> using <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> based on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity computation</a>. We further implement a neural LSTM system with different encoding strategies that try to model the relation between the story and the provided endings. Our experiments show that a model using representation features based on average word embedding vectors over the given story words and the candidate ending sentences words, joint with similarity features between the story and candidate ending representations performed better than the neural models. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 72.42, ranking 3rd in the official evaluation.</div></div></div><hr><div id=w17-10><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-10.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-10/>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1000/>Proceedings of the <span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>L</span>ing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</a></strong><br><a href=/people/g/george-giannakopoulos/>George Giannakopoulos</a>
|
<a href=/people/e/elena-lloret/>Elena Lloret</a>
|
<a href=/people/j/john-conroy/>John M. Conroy</a>
|
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>
|
<a href=/people/m/marina-litvak/>Marina Litvak</a>
|
<a href=/people/p/peter-a-rankel/>Peter Rankel</a>
|
<a href=/people/b/benoit-favre/>Benoit Favre</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1002/>Decoupling Encoder and Decoder Networks for Abstractive Document Summarization</a></strong><br><a href=/people/y/ying-xu/>Ying Xu</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1002><div class="card-body p-3 small">Abstractive document summarization seeks to automatically generate a summary for a document, based on some abstract understanding of the original document. State-of-the-art techniques traditionally use attentive encoderdecoder architectures. However, due to the large number of parameters in these <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, they require large training datasets and long training times. In this paper, we propose decoupling the encoder and decoder networks, and training them separately. We encode documents using an unsupervised document encoder, and then feed the document vector to a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network decoder</a>. With this decoupled architecture, we decrease the number of parameters in the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> substantially, and shorten its <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training time</a>. Experiments show that the decoupled model achieves comparable performance with state-of-the-art models for in-domain documents, but less well for out-of-domain documents.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-1003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-1003/>Centroid-based Text Summarization through Compositionality of Word Embeddings</a></strong><br><a href=/people/g/gaetano-rossiello/>Gaetano Rossiello</a>
|
<a href=/people/p/pierpaolo-basile/>Pierpaolo Basile</a>
|
<a href=/people/g/giovanni-semeraro/>Giovanni Semeraro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1003><div class="card-body p-3 small">The textual similarity is a crucial aspect for many extractive text summarization methods. A bag-of-words representation does not allow to grasp the semantic relationships between concepts when comparing strongly related sentences with no words in common. To overcome this issue, in this paper we propose a centroid-based method for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>text summarization</a> that exploits the compositional capabilities of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. The evaluations on multi-document and multilingual datasets prove the effectiveness of the continuous vector representation of words compared to the <a href=https://en.wikipedia.org/wiki/Bag-of-words_model>bag-of-words model</a>. Despite its simplicity, our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves good performance even in comparison to more complex <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>. Our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> is unsupervised and <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can be adopted in other summarization tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1004/>Query-based summarization using MDL principle<span class=acl-fixed-case>MDL</span> principle</a></strong><br><a href=/people/m/marina-litvak/>Marina Litvak</a>
|
<a href=/people/n/natalia-vanetik/>Natalia Vanetik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1004><div class="card-body p-3 small">Query-based text summarization is aimed at extracting essential information that answers the query from original text. The answer is presented in a minimal, often predefined, number of words. In this paper we introduce a new unsupervised approach for query-based extractive summarization, based on the minimum description length (MDL) principle that employs Krimp compression algorithm (Vreeken et al., 2011). The key idea of our approach is to select frequent word sets related to a given query that compress document sentences better and therefore describe the document better. A summary is extracted by selecting sentences that best cover query-related frequent word sets. The approach is evaluated based on the DUC 2005 and DUC 2006 datasets which are specifically designed for query-based summarization (DUC, 2005 2006). It competes with the best results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1005/>Word Embedding and Topic Modeling Enhanced Multiple Features for Content Linking and Argument / Sentiment Labeling in Online Forums</a></strong><br><a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/l/liyuan-mao/>Liyuan Mao</a>
|
<a href=/people/m/moye-chen/>Moye Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1005><div class="card-body p-3 small">Multiple grammatical and semantic features are adopted in content linking and argument / sentiment labeling for online forums in this paper. There are mainly two different <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> for content linking. First, we utilize the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep feature</a> obtained from Word Embedding Model in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and compute sentence similarity. Second, we use multiple traditional features to locate candidate linking sentences, and then adopt a voting method to obtain the final result. LDA topic modeling is used to mine latent semantic feature and <a href=https://en.wikipedia.org/wiki/K-means_clustering>K-means clustering</a> is implemented for argument labeling, while features from sentiment dictionaries and rule-based sentiment analysis are integrated for sentiment labeling. Experimental results have shown that our <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> are valid.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1006/>Ultra-Concise Multi-genre Summarisation of <a href=https://en.wikipedia.org/wiki/Web_2.0>Web2.0</a> : towards Intelligent Content Generation</a></strong><br><a href=/people/e/elena-lloret/>Elena Lloret</a>
|
<a href=/people/e/ester-boldrini/>Ester Boldrini</a>
|
<a href=/people/p/patricio-martinez-barco/>Patricio Martínez-Barco</a>
|
<a href=/people/m/manuel-palomar/>Manuel Palomar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1006><div class="card-body p-3 small">The electronic Word of Mouth has become the most powerful communication channel thanks to the wide usage of the <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a>. Our research proposes an approach towards the production of automatic ultra-concise summaries from multiple <a href=https://en.wikipedia.org/wiki/Web_2.0>Web 2.0 sources</a>. We exploit <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> from reviews and microblogs in different domains, and compile and analyse four types of ultra-concise summaries : a)positive information, b) negative information ; c) both or d) objective information. The appropriateness and usefulness of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is demonstrated by its successful results and great potential in real-life applications, thus meaning a relevant advancement of the state-of-the-art approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1007/>Machine Learning Approach to Evaluate MultiLingual Summaries<span class=acl-fixed-case>M</span>ulti<span class=acl-fixed-case>L</span>ingual Summaries</a></strong><br><a href=/people/s/samira-ellouze/>Samira Ellouze</a>
|
<a href=/people/m/maher-jaoua/>Maher Jaoua</a>
|
<a href=/people/l/lamia-hadrich-belguith/>Lamia Hadrich Belguith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1007><div class="card-body p-3 small">The present paper introduces a new MultiLing text summary evaluation method. This method relies on machine learning approach which operates by combining multiple <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to build <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> that predict the human score (overall responsiveness) of a new summary. We have tried several single and ensemble learning classifiers to build the best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We have experimented our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> in summary level evaluation where we evaluate each text summary separately. The correlation between built models and human score is better than the correlation between baselines and <a href=https://en.wikipedia.org/wiki/Score_(game)>manual score</a>.</div></div></div><hr><div id=w17-11><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-11.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-11/>Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1100/>Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media</a></strong><br><a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a>
|
<a href=/people/c/cheng-te-li/>Cheng-Te Li</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1101/>A Survey on Hate Speech Detection using <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/a/anna-schmidt/>Anna Schmidt</a>
|
<a href=/people/m/michael-wiegand/>Michael Wiegand</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1101><div class="card-body p-3 small">This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of <a href=https://en.wikipedia.org/wiki/Online_hate_speech>online hate speech</a> is also increasing. Due to the massive scale of the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, methods that automatically detect <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. We also discuss limits of those <a href=https://en.wikipedia.org/wiki/Theory_of_forms>approaches</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1102/>Facebook sentiment : Reactions and Emojis<span class=acl-fixed-case>F</span>acebook sentiment: Reactions and Emojis</a></strong><br><a href=/people/y/ye-tian/>Ye Tian</a>
|
<a href=/people/t/thiago-galery/>Thiago Galery</a>
|
<a href=/people/g/giulio-dulcinati/>Giulio Dulcinati</a>
|
<a href=/people/e/emilia-molimpakis/>Emilia Molimpakis</a>
|
<a href=/people/c/chao-sun/>Chao Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1102><div class="card-body p-3 small">Emojis are used frequently in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. A widely assumed view is that <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> express the emotional state of the user, which has led to research focusing on the expressiveness of <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> independent from the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>linguistic context</a>. We argue that <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> and the <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>linguistic texts</a> can modify the meaning of each other. The overall communicated meaning is not a simple sum of the two channels. In order to study the meaning interplay, we need data indicating the overall sentiment of the entire message as well as the sentiment of the <a href=https://en.wikipedia.org/wiki/Emoji>emojis stand-alone</a>. We propose that Facebook Reactions are a good data source for such a purpose. FB reactions (e.g. Love and Angry) indicate the readers&#8217; overall sentiment, against which we can investigate the types of <a href=https://en.wikipedia.org/wiki/Emoji>emojis</a> used the comments under different reaction profiles. We present a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> of 21,000 FB posts (57 million reactions and 8 million comments) from public media pages across four countries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1103/>Potential and Limitations of Cross-Domain Sentiment Classification</a></strong><br><a href=/people/j/jan-milan-deriu/>Jan Milan Deriu</a>
|
<a href=/people/m/martin-weilenmann/>Martin Weilenmann</a>
|
<a href=/people/d/dirk-von-gruenigen/>Dirk Von Gruenigen</a>
|
<a href=/people/m/mark-cieliebak/>Mark Cieliebak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1103><div class="card-body p-3 small">In this paper we investigate the cross-domain performance of a current state-of-the-art <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis systems</a>. For this purpose we train a convolutional neural network (CNN) on data from different domains and evaluate its performance on other domains. Furthermore, we evaluate the usefulness of combining a large amount of different smaller annotated corpora to a large corpus. Our results show that more sophisticated approaches are required to train a <a href=https://en.wikipedia.org/wiki/System>system</a> that works equally well on various domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1104/>Aligning Entity Names with Online Aliases on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/k/kevin-mckelvey/>Kevin McKelvey</a>
|
<a href=/people/p/peter-goutzounis/>Peter Goutzounis</a>
|
<a href=/people/s/stephen-da-cruz/>Stephen da Cruz</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1104><div class="card-body p-3 small">This paper presents new <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that automatically align <a href=https://en.wikipedia.org/wiki/Pseudonym>online aliases</a> with their real entity names. Many research applications rely on identifying entity names in text, but people often refer to entities with unexpected nicknames and <a href=https://en.wikipedia.org/wiki/Pseudonym>aliases</a>. For example, The King and King James are <a href=https://en.wikipedia.org/wiki/Pseudonym>aliases</a> for Lebron James, a professional basketball player. Recent work on <a href=https://en.wikipedia.org/wiki/Entity_linking>entity linking</a> attempts to resolve mentions to knowledge base entries, like a wikipedia page, but linking is unfortunately limited to well-known entities with pre-built pages. This paper asks a more basic question : can <a href=https://en.wikipedia.org/wiki/Pseudonym>aliases</a> be aligned without background knowledge of the entity? Further, can the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> surrounding alias mentions be used to inform alignments? We describe <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical models</a> that make decisions based on the lexicographic properties of the <a href=https://en.wikipedia.org/wiki/Pseudonym>aliases</a> with their semantic context in a large corpus of tweets. We experiment on a database of Twitter users and their <a href=https://en.wikipedia.org/wiki/User_(computing)>usernames</a>, and present the first human evaluation for this task. Alignment accuracy approaches human performance at 81 %, and we show that while lexicographic features are most important, the semantic context of an alias further improves classification accuracy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-1105" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-1105/>Character-based Neural Embeddings for Tweet Clustering</a></strong><br><a href=/people/s/svitlana-vakulenko/>Svitlana Vakulenko</a>
|
<a href=/people/l/lyndon-nixon/>Lyndon Nixon</a>
|
<a href=/people/m/mihai-lupu/>Mihai Lupu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1105><div class="card-body p-3 small">In this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks. The proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content. Our evaluation results and code are available on-line :.<url>https://github.com/vendi12/tweet2vec_clustering</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1106/>A Twitter Corpus and Benchmark Resources for German Sentiment Analysis<span class=acl-fixed-case>T</span>witter Corpus and Benchmark Resources for <span class=acl-fixed-case>G</span>erman Sentiment Analysis</a></strong><br><a href=/people/m/mark-cieliebak/>Mark Cieliebak</a>
|
<a href=/people/j/jan-milan-deriu/>Jan Milan Deriu</a>
|
<a href=/people/d/dominic-egger/>Dominic Egger</a>
|
<a href=/people/f/fatih-uzdilli/>Fatih Uzdilli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1106><div class="card-body p-3 small">In this paper we present SB10k, a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> with approx. 10,000 German tweets. We use this new corpus and two existing corpora to provide state-of-the-art benchmarks for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in <a href=https://en.wikipedia.org/wiki/German_language>German</a> : we implemented a CNN (based on the winning system of SemEval-2016) and a feature-based SVM and compare their performance on all three corpora. For the <a href=https://en.wikipedia.org/wiki/CNN>CNN</a>, we also created German word embeddings trained on 300 M tweets. These <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> were then optimized for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> using distant-supervised learning. The new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, the German word embeddings (plain and optimized), and source code to re-run the <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a> are publicly available.</div></div></div><hr><div id=w17-12><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-12.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-12/>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1200/>Proceedings of the Fourth Workshop on <span class=acl-fixed-case>NLP</span> for Similar Languages, Varieties and Dialects (<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial)</a></strong><br><a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/s/shervin-malmasi/>Shevin Malmasi</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1201/>Findings of the VarDial Evaluation Campaign 2017<span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial Evaluation Campaign 2017</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/a/ahmed-ali/>Ahmed Ali</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/n/noemi-aepli/>Noëmi Aepli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1201><div class="card-body p-3 small">We present the results of the VarDial Evaluation Campaign on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects, which we organized as part of the fourth edition of the VarDial workshop at EACL&#8217;2017. This year, we included four shared tasks : Discriminating between Similar Languages (DSL), Arabic Dialect Identification (ADI), German Dialect Identification (GDI), and Cross-lingual Dependency Parsing (CLP). A total of 19 teams submitted runs across the four <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>, and 15 of them wrote system description papers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1203/>Computational analysis of Gondi dialects<span class=acl-fixed-case>G</span>ondi dialects</a></strong><br><a href=/people/t/taraka-rama/>Taraka Rama</a>
|
<a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/p/pavel-sofroniev/>Pavel Sofroniev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1203><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational analysis</a> of <a href=https://en.wikipedia.org/wiki/Gondi_language>Gondi dialects</a> spoken in central India. We present a digitized data set of the dialect area, and analyze the <a href=https://en.wikipedia.org/wiki/Data>data</a> using different techniques from <a href=https://en.wikipedia.org/wiki/Dialectometry>dialectometry</a>, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, and <a href=https://en.wikipedia.org/wiki/Computational_biology>computational biology</a>. We show that the <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> largely agree with each other and with the earlier non-computational analyses of the language group.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1204/>Investigating Diatopic Variation in a Historical Corpus</a></strong><br><a href=/people/s/stefanie-dipper/>Stefanie Dipper</a>
|
<a href=/people/s/sandra-waldenberger/>Sandra Waldenberger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1204><div class="card-body p-3 small">This paper investigates diatopic variation in a historical corpus of German. Based on equivalent word forms from different language areas, replacement rules and <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> are derived which describe the relations between these word forms. These rules and <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a> are then interpreted as reflections of morphological, phonological or graphemic variation. Based on sample rules and <a href=https://en.wikipedia.org/wiki/Map_(mathematics)>mappings</a>, we show that our approach can replicate results from <a href=https://en.wikipedia.org/wiki/Historical_linguistics>historical linguistics</a>. While previous studies were restricted to predefined word lists, or confined to single authors or texts, our approach uses a much wider range of data available in historical corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1206/>The similarity and Mutual Intelligibility between Amharic and Tigrigna Varieties<span class=acl-fixed-case>A</span>mharic and <span class=acl-fixed-case>T</span>igrigna Varieties</a></strong><br><a href=/people/t/tekabe-legesse-feleke/>Tekabe Legesse Feleke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1206><div class="card-body p-3 small">The present study has examined the similarity and the <a href=https://en.wikipedia.org/wiki/Mutual_intelligibility>mutual intelligibility</a> between <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> and Tigrigna using three tools namely <a href=https://en.wikipedia.org/wiki/Levenshtein_distance>Levenshtein distance</a>, intelligibility test and questionnaires. The study has shown that both <a href=https://en.wikipedia.org/wiki/Tigrigna_language>Tigrigna varieties</a> have almost equal phonetic and lexical distances from Amharic. The study also indicated that <a href=https://en.wikipedia.org/wiki/Amharic>Amharic speakers</a> understand less than 50 % of the two varieties. Furthermore, the study showed that Amharic speakers are more positive about the Ethiopian Tigrigna variety than the Eritrean Variety. However, their attitude towards the two <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>varieties</a> does not have an impact on their intelligibility. The <a href=https://en.wikipedia.org/wiki/Amharic>Amharic speakers</a>&#8217; familiarity to the <a href=https://en.wikipedia.org/wiki/Tigrinya_language>Tigrigna varieties</a> is largely dependent on the genealogical relation between <a href=https://en.wikipedia.org/wiki/Amharic>Amharic</a> and the two <a href=https://en.wikipedia.org/wiki/Tigrinya_language>Tigrigna varieties</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1207/>Why Catalan-Spanish Neural Machine Translation? Analysis, comparison and combination with standard Rule and Phrase-based technologies<span class=acl-fixed-case>C</span>atalan-<span class=acl-fixed-case>S</span>panish Neural Machine Translation? Analysis, comparison and combination with standard Rule and Phrase-based technologies</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1207><div class="card-body p-3 small">Catalan and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> are two related languages given that both derive from <a href=https://en.wikipedia.org/wiki/Latin>Latin</a>. They share similarities in several linguistic levels including <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a>, <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. This makes them particularly interesting for the MT task. Given the recent appearance and popularity of neural MT, this paper analyzes the performance of this new approach compared to the well-established rule-based and phrase-based MT systems. Experiments are reported on a large database of 180 million words. Results, in terms of standard automatic measures, show that neural MT clearly outperforms the rule-based and phrase-based MT system on in-domain test set, but it is worst in the out-of-domain test set. A naive system combination specially works for the latter. In-domain manual analysis shows that neural MT tends to improve both adequacy and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>, for example, by being able to generate more natural translations instead of literal ones, choosing to the adequate target word when the source word has several translations and improving gender agreement. However, out-of-domain manual analysis shows how neural MT is more affected by unknown words or contexts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1208/>Kurdish Interdialect Machine Translation<span class=acl-fixed-case>K</span>urdish Interdialect Machine Translation</a></strong><br><a href=/people/h/hossein-hassani/>Hossein Hassani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1208><div class="card-body p-3 small">This research suggests a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> among two <a href=https://en.wikipedia.org/wiki/Kurdish_languages>Kurdish dialects</a>. We chose the two widely spoken dialects, <a href=https://en.wikipedia.org/wiki/Kurmanji>Kurmanji</a> and <a href=https://en.wikipedia.org/wiki/Sorani>Sorani</a>, which are considered to be mutually unintelligible. Also, despite being spoken by about 30 million people in different countries, <a href=https://en.wikipedia.org/wiki/Kurdish_languages>Kurdish</a> is among less-resourced languages. The research used bi-dialectal dictionaries and showed that the lack of <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> is not a major obstacle in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> between the two dialects. The experiments showed that the machine translated texts are comprehensible to those who do not speak the dialect. The research is the first attempt for inter-dialect machine translation in <a href=https://en.wikipedia.org/wiki/Kurdish_languages>Kurdish</a> and particularly could help in making online texts in one dialect comprehensible to those who only speak the target dialect. The results showed that the translated texts are in 71 % and 79 % cases rated as understandable for <a href=https://en.wikipedia.org/wiki/Kurmanji>Kurmanji</a> and <a href=https://en.wikipedia.org/wiki/Sorani>Sorani</a> respectively. They are rated as slightly-understandable in 29 % cases for <a href=https://en.wikipedia.org/wiki/Kurmanji>Kurmanji</a> and 21 % for <a href=https://en.wikipedia.org/wiki/Sorani>Sorani</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1209/>Twitter Language Identification Of Similar Languages And Dialects Without Ground Truth<span class=acl-fixed-case>T</span>witter Language Identification Of Similar Languages And Dialects Without Ground Truth</a></strong><br><a href=/people/j/jennifer-williams/>Jennifer Williams</a>
|
<a href=/people/c/charlie-dagli/>Charlie Dagli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1209><div class="card-body p-3 small">We present a new method to bootstrap filter Twitter language ID labels in our dataset for automatic language identification (LID). Our method combines geo-location, original Twitter LID labels, and <a href=https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk>Amazon Mechanical Turk</a> to resolve missing and unreliable labels. We are the first to compare LID classification performance using the MIRA algorithm and langid.py. We show <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> performance on different versions of our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> using only Twitter data, without <a href=https://en.wikipedia.org/wiki/Ground_truth>ground truth</a>, and very few training examples. We also show how <a href=https://en.wikipedia.org/wiki/Platt_scaling>Platt Scaling</a> can be use to calibrate MIRA classifier output values into a <a href=https://en.wikipedia.org/wiki/Probability_distribution>probability distribution</a> over candidate classes, making the output more intuitive. Our method allows for fine-grained distinctions between similar languages and dialects and allows us to rediscover the language composition of our Twitter dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1210/>Multi-source morphosyntactic tagging for spoken Rusyn<span class=acl-fixed-case>R</span>usyn</a></strong><br><a href=/people/y/yves-scherrer/>Yves Scherrer</a>
|
<a href=/people/a/achim-rabus/>Achim Rabus</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1210><div class="card-body p-3 small">This paper deals with the development of <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphosyntactic taggers</a> for <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>spoken varieties</a> of the Slavic minority language Rusyn. As neither annotated corpora nor parallel corpora are electronically available for <a href=https://en.wikipedia.org/wiki/Rusyn_language>Rusyn</a>, we propose to combine existing resources from the etymologically close Slavic languages Russian, <a href=https://en.wikipedia.org/wiki/Ukrainian_language>Ukrainian</a>, <a href=https://en.wikipedia.org/wiki/Slovak_language>Slovak</a>, and <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and adapt them to <a href=https://en.wikipedia.org/wiki/Rusyn_language>Rusyn</a>. Using MarMoT as tagging toolkit, we show that a tagger trained on a balanced set of the four source languages outperforms single language taggers by about 9 %, and that additional automatically induced morphosyntactic lexicons lead to further improvements. The best observed accuracies for Rusyn are 82.4 % for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a> and 75.5 % for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>full morphological tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1211/>Identifying dialects with textual and acoustic cues</a></strong><br><a href=/people/a/abualsoud-hanani/>Abualsoud Hanani</a>
|
<a href=/people/a/aziz-qaroush/>Aziz Qaroush</a>
|
<a href=/people/s/stephen-taylor/>Stephen Taylor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1211><div class="card-body p-3 small">We describe several systems for identifying short samples of Arabic or Swiss-German dialects, which were prepared for the shared task of the 2017 DSL Workshop (Zampieri et al., 2017). The Arabic data comprises both text and acoustic files, and our best run combined both. The Swiss-German data is text-only. Coincidently, our best runs achieved a <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of nearly 63 % on both the Swiss-German and Arabic dialects tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1212/>Evaluating HeLI with Non-Linear Mappings<span class=acl-fixed-case>H</span>e<span class=acl-fixed-case>LI</span> with Non-Linear Mappings</a></strong><br><a href=/people/t/tommi-jauhiainen/>Tommi Jauhiainen</a>
|
<a href=/people/k/krister-linden/>Krister Lindén</a>
|
<a href=/people/h/heidi-jauhiainen/>Heidi Jauhiainen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1212><div class="card-body p-3 small">In this paper we describe the non-linear mappings we used with the Helsinki language identification method, HeLI, in the 4th edition of the Discriminating between Similar Languages (DSL) shared task, which was organized as part of the VarDial 2017 workshop. Our SUKI team participated on the closed track together with 10 other teams. Our <a href=https://en.wikipedia.org/wiki/System>system</a> reached the 7th position in the track. We describe the HeLI method and the non-linear mappings in <a href=https://en.wikipedia.org/wiki/Mathematical_notation>mathematical notation</a>. The HeLI method uses a <a href=https://en.wikipedia.org/wiki/Statistical_model>probabilistic model</a> with character n-grams and word-based backoff. We also describe our trials using the non-linear mappings instead of relative frequencies and we present statistics about the back-off function of the HeLI method.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1213/>A Perplexity-Based Method for Similar Languages Discrimination</a></strong><br><a href=/people/p/pablo-gamallo/>Pablo Gamallo</a>
|
<a href=/people/j/jose-ramom-pichel-campos/>Jose Ramom Pichel</a>
|
<a href=/people/i/inaki-alegria/>Iñaki Alegria</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1213><div class="card-body p-3 small">This article describes the system submitted by the Citius_Ixa_Imaxin team to the VarDial 2017 (DSL and GDI tasks). The strategy underlying our <a href=https://en.wikipedia.org/wiki/System>system</a> is based on a <a href=https://en.wikipedia.org/wiki/Language_distance>language distance</a> computed by means of model perplexity. The best model configuration we have tested is a <a href=https://en.wikipedia.org/wiki/Electoral_system>voting system</a> making use of several n-grams models of both words and characters, even if word unigrams turned out to be a very competitive <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with reasonable results in the tasks we have participated. An <a href=https://en.wikipedia.org/wiki/Error_analysis_(linguistics)>error analysis</a> has been performed in which we identified many test examples with no linguistic evidences to distinguish among the variants.<tex-math>n</tex-math>-grams models of both words and characters, even if word unigrams turned out to be a very competitive model with reasonable results in the tasks we have participated. An error analysis has been performed in which we identified many test examples with no linguistic evidences to distinguish among the variants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1214/>Improving the Character Ngram Model for the DSL Task with BM25 Weighting and Less Frequently Used Feature Sets<span class=acl-fixed-case>DSL</span> Task with <span class=acl-fixed-case>BM</span>25 Weighting and Less Frequently Used Feature Sets</a></strong><br><a href=/people/y/yves-bestgen/>Yves Bestgen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1214><div class="card-body p-3 small">This paper describes the system developed by the Centre for English Corpus Linguistics (CECL) to discriminating similar languages, <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language varieties</a> and dialects. Based on a <a href=https://en.wikipedia.org/wiki/Symmetric_multiprocessing>SVM</a> with character and POStag n-grams as features and the BM25 weighting scheme, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> achieved 92.7 % accuracy in the Discriminating between Similar Languages (DSL) task, ranking first among eleven systems but with a lead over the next three teams of only 0.2 %. A simpler version of the <a href=https://en.wikipedia.org/wiki/System>system</a> ranked second in the German Dialect Identification (GDI) task thanks to several ad hoc postprocessing steps. Complementary analyses carried out by a <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>cross-validation procedure</a> suggest that the BM25 weighting scheme could be competitive in this type of tasks, at least in comparison with the sublinear TF-IDF. POStag n-grams also improved the <a href=https://en.wikipedia.org/wiki/System>system</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1215/>Discriminating between Similar Languages with Word-level Convolutional Neural Networks</a></strong><br><a href=/people/m/marcelo-criscuolo/>Marcelo Criscuolo</a>
|
<a href=/people/s/sandra-aluisio/>Sandra Maria Aluísio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1215><div class="card-body p-3 small">Discriminating between Similar Languages (DSL) is a challenging task addressed at the VarDial Workshop series. We report on our participation in the DSL shared task with a <a href=https://en.wikipedia.org/wiki/Multithreading_(computer_architecture)>two-stage system</a>. In the first stage, character n-grams are used to separate <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language groups</a>, then specialized classifiers distinguish similar language varieties. We have conducted experiments with three system configurations and submitted one run for each. Our main approach is a word-level convolutional neural network (CNN) that learns task-specific vectors with minimal text preprocessing. We also experiment with multi-layer perceptron (MLP) networks and another hybrid configuration. Our best run achieved an accuracy of 90.76 %, ranking 8th among 11 participants and getting very close to the system that ranked first (less than 2 points). Even though the CNN model could not achieve the best results, it still makes a viable approach to discriminating between similar languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1216/>Cross-lingual dependency parsing for closely related languages-Helsinki’s submission to VarDial 2017<span class=acl-fixed-case>H</span>elsinki’s submission to <span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2017</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1216><div class="card-body p-3 small">This paper describes the submission from the University of Helsinki to the shared task on cross-lingual dependency parsing at VarDial 2017. We present work on annotation projection and treebank translation that gave good results for all three target languages in the test set. In particular, <a href=https://en.wikipedia.org/wiki/Slovak_language>Slovak</a> seems to work well with information coming from the Czech treebank, which is in line with related work. The attachment scores for cross-lingual models even surpass the <a href=https://en.wikipedia.org/wiki/Supervised_learning>fully supervised models</a> trained on the target language treebank. Croatian is the most difficult language in the test set and the improvements over the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> are rather modest. Norwegian works best with information coming from <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a> whereas <a href=https://en.wikipedia.org/wiki/Danish_language>Danish</a> contributes surprisingly little.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1217/>Discriminating between Similar Languages Using a Combination of Typed and Untyped Character N-grams and Words</a></strong><br><a href=/people/h/helena-gomez/>Helena Gomez</a>
|
<a href=/people/i/ilia-markov/>Ilia Markov</a>
|
<a href=/people/j/jorge-baptista/>Jorge Baptista</a>
|
<a href=/people/g/grigori-sidorov/>Grigori Sidorov</a>
|
<a href=/people/d/david-pinto/>David Pinto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1217><div class="card-body p-3 small">This paper presents the cic_ualg&#8217;s system that took part in the Discriminating between Similar Languages (DSL) shared task, held at the VarDial 2017 Workshop. This year&#8217;s task aims at identifying 14 languages across 6 language groups using a corpus of excerpts of journalistic texts. Two classification approaches were compared : a single-step (all languages) approach and a two-step (language group and then languages within the group) approach. Features exploited include lexical features (unigrams of words) and character n-grams. Besides traditional (untyped) character n-grams, we introduce typed character n-grams in the DSL task. Experiments were carried out with different feature representation methods (binary and raw term frequency), frequency threshold values, and machine-learning algorithms Support Vector Machines (SVM) and Multinomial Naive Bayes (MNB). Our best run in the DSL task achieved 91.46 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1218 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1218/>Tbingen system in VarDial 2017 shared task : experiments with <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a> and cross-lingual parsing<span class=acl-fixed-case>T</span>übingen system in <span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial 2017 shared task: experiments with language identification and cross-lingual parsing</a></strong><br><a href=/people/c/cagri-coltekin/>Çağrı Çöltekin</a>
|
<a href=/people/t/taraka-rama/>Taraka Rama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1218><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>systems</a> and results on VarDial 2017 shared tasks. Besides three language / dialect discrimination tasks, we also participated in the cross-lingual dependency parsing (CLP) task using a simple methodology which we also briefly describe in this paper. For all the discrimination tasks, we used linear SVMs with character and word features. The <a href=https://en.wikipedia.org/wiki/System>system</a> achieves competitive results among other <a href=https://en.wikipedia.org/wiki/System>systems</a> in the shared <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We also report additional experiments with neural network models. The performance of neural network models was close but always below the corresponding SVM classifiers in the discrimination tasks. For the cross-lingual parsing task, we experimented with an approach based on automatically translating the source treebank to the target language, and training a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> on the translated treebank. We used off-the-shelf tools for both <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Despite achieving better-than-baseline results, our scores in CLP tasks were substantially lower than the scores of the other participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1219 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1219/>When Sparse Traditional Models Outperform Dense Neural Networks : the Curious Case of Discriminating between Similar Languages</a></strong><br><a href=/people/m/maria-medvedeva/>Maria Medvedeva</a>
|
<a href=/people/m/martin-kroon/>Martin Kroon</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1219><div class="card-body p-3 small">We present the results of our participation in the VarDial 4 shared task on discriminating closely related languages. Our submission includes simple traditional models using linear support vector machines (SVMs) and a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network (NN)</a>. The main idea was to leverage language group information. We did so with a two-layer approach in the traditional <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and a multi-task objective in the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network case</a>. Our results confirm earlier findings : simple traditional models outperform <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> consistently for this task, at least given the amount of systems we could examine in the available time. Our two-layer linear SVM ranked 2nd in the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1220 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1220/>German Dialect Identification in Interview Transcriptions<span class=acl-fixed-case>G</span>erman Dialect Identification in Interview Transcriptions</a></strong><br><a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1220><div class="card-body p-3 small">This paper presents three systems submitted to the German Dialect Identification (GDI) task at the VarDial Evaluation Campaign 2017. The task consists of training <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> to identify the dialect of Swiss-German speech transcripts. The dialects included in the GDI dataset are <a href=https://en.wikipedia.org/wiki/Canton_of_Basel-Stadt>Basel</a>, <a href=https://en.wikipedia.org/wiki/Canton_of_Bern>Bern</a>, Lucerne, and <a href=https://en.wikipedia.org/wiki/Canton_of_Z&#252;rich>Zurich</a>. The three systems we submitted are based on : a plurality ensemble, a mean probability ensemble, and a meta-classifier trained on character and word n-grams. The best results were obtained by the <a href=https://en.wikipedia.org/wiki/Meta-analysis>meta-classifier</a> achieving 68.1 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and 66.2 % <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a>, ranking first among the 10 teams which participated in the GDI shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1221 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1221/>CLUZH at VarDial GDI 2017 : Testing a Variety of Machine Learning Tools for the Classification of Swiss German Dialects<span class=acl-fixed-case>CLUZH</span> at <span class=acl-fixed-case>V</span>ar<span class=acl-fixed-case>D</span>ial <span class=acl-fixed-case>GDI</span> 2017: Testing a Variety of Machine Learning Tools for the Classification of <span class=acl-fixed-case>S</span>wiss <span class=acl-fixed-case>G</span>erman Dialects</a></strong><br><a href=/people/s/simon-clematide/>Simon Clematide</a>
|
<a href=/people/p/peter-makarov/>Peter Makarov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1221><div class="card-body p-3 small">Our submissions for the GDI 2017 Shared Task are the results from three different types of classifiers : Nave Bayes, Conditional Random Fields (CRF), and Support Vector Machine (SVM). Our CRF-based run achieves a weighted F1 score of 65 % (third rank) being beaten by the best system by 0.9 %. Measured by classification accuracy, our ensemble run (Nave Bayes, CRF, SVM) reaches 67 % (second rank) being 1 % lower than the best system. We also describe our experiments with Recurrent Neural Network (RNN) architectures. Since they performed worse than our non-neural approaches we did not include them in the submission.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1222 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1222/>Arabic Dialect Identification Using iVectors and ASR Transcripts<span class=acl-fixed-case>A</span>rabic Dialect Identification Using i<span class=acl-fixed-case>V</span>ectors and <span class=acl-fixed-case>ASR</span> Transcripts</a></strong><br><a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1222><div class="card-body p-3 small">This paper presents the systems submitted by the MAZA team to the Arabic Dialect Identification (ADI) shared task at the VarDial Evaluation Campaign 2017. The goal of the task is to evaluate <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational models</a> to identify the <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>dialect of Arabic utterances</a> using both <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>audio and text transcriptions</a>. The ADI shared task dataset included Modern Standard Arabic (MSA) and four Arabic dialects : <a href=https://en.wikipedia.org/wiki/Egyptian_Arabic>Egyptian</a>, <a href=https://en.wikipedia.org/wiki/Gulf_Arabic>Gulf</a>, <a href=https://en.wikipedia.org/wiki/Levantine_Arabic>Levantine</a>, and <a href=https://en.wikipedia.org/wiki/North_African_Arabic>North-African</a>. The three systems submitted by MAZA are based on combinations of multiple machine learning classifiers arranged as (1) voting ensemble ; (2) mean probability ensemble ; (3) meta-classifier. The best results were obtained by the <a href=https://en.wikipedia.org/wiki/Meta-analysis>meta-classifier</a> achieving 71.7 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, ranking second among the six teams which participated in the ADI shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1224 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1224/>Exploring Lexical and Syntactic Features for Language Variety Identification</a></strong><br><a href=/people/c/chris-van-der-lee/>Chris van der Lee</a>
|
<a href=/people/a/antal-van-den-bosch/>Antal van den Bosch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1224><div class="card-body p-3 small">We present a method to discriminate between texts written in either the Netherlandic or the Flemish variant of the <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch language</a>. The method draws on a feature bundle representing text statistics, <a href=https://en.wikipedia.org/wiki/Syntax>syntactic features</a>, and <a href=https://en.wikipedia.org/wiki/N-gram>word n-grams</a>. Text statistics include average word length and <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>, while syntactic features include ratios of function words and part-of-speech n-grams. The effectiveness of the <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifier</a> was measured by classifying <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch subtitles</a> developed for either <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> or <a href=https://en.wikipedia.org/wiki/Vlaamse_Radio-_en_Televisieomroeporganisatie>Flemish television</a>. Several <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning algorithms</a> were compared as well as feature combination methods in order to find the optimal generalization performance. A machine-learning meta classifier based on <a href=https://en.wikipedia.org/wiki/AdaBoost>AdaBoost</a> attained the best <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.92.<tex-math>n</tex-math>-grams. Text statistics include average word length and sentence length, while syntactic features include ratios of function words and part-of-speech <tex-math>n</tex-math>-grams. The effectiveness of the classifier was measured by classifying Dutch subtitles developed for either Dutch or Flemish television. Several machine learning algorithms were compared as well as feature combination methods in order to find the optimal generalization performance. A machine-learning meta classifier based on AdaBoost attained the best F-score of 0.92.</div></div></div><hr><div id=w17-13><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-13.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-13/>Proceedings of the Third Arabic Natural Language Processing Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1300/>Proceedings of the Third <span class=acl-fixed-case>A</span>rabic Natural Language Processing Workshop</a></strong><br><a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a>
|
<a href=/people/w/wassim-el-hajj/>Wassim El-Hajj</a>
|
<a href=/people/h/hend-al-khalifa/>Hend Al-Khalifa</a>
|
<a href=/people/h/houda-bouamor/>Houda Bouamor</a>
|
<a href=/people/n/nadi-tomeh/>Nadi Tomeh</a>
|
<a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>
|
<a href=/people/w/wajdi-zaghouani/>Wajdi Zaghouani</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1301/>Identification of Languages in Algerian Arabic Multilingual Documents<span class=acl-fixed-case>A</span>lgerian <span class=acl-fixed-case>A</span>rabic Multilingual Documents</a></strong><br><a href=/people/w/wafia-adouane/>Wafia Adouane</a>
|
<a href=/people/s/simon-dobnik/>Simon Dobnik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1301><div class="card-body p-3 small">This paper presents a language identification system designed to detect the language of each word, in its context, in a multilingual documents as generated in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> by bilingual / multilingual communities, in our case speakers of Algerian Arabic. We frame the task as a sequence tagging problem and use <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised machine learning</a> with standard methods like HMM and Ngram classification tagging. We also experiment with a lexicon-based method. Combining all the methods in a fall-back mechanism and introducing some <a href=https://en.wikipedia.org/wiki/Rule_of_inference>linguistic rules</a>, to deal with unseen tokens and <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguous words</a>, gives an overall <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 93.14 %. Finally, we introduced <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> for language identification from sequences of recognised words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1302/>Arabic Diacritization : <a href=https://en.wikipedia.org/wiki/Statistics>Stats</a>, <a href=https://en.wikipedia.org/wiki/Regulation>Rules</a>, and Hacks<span class=acl-fixed-case>A</span>rabic Diacritization: Stats, Rules, and Hacks</a></strong><br><a href=/people/k/kareem-darwish/>Kareem Darwish</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1302><div class="card-body p-3 small">In this paper, we present a new and fast state-of-the-art Arabic diacritizer that guesses the diacritics of words and then their <a href=https://en.wikipedia.org/wiki/Grammatical_case>case endings</a>. We employ a <a href=https://en.wikipedia.org/wiki/Viterbi_decoder>Viterbi decoder</a> at word-level with back-off to stem, morphological patterns, and <a href=https://en.wikipedia.org/wiki/Transliteration>transliteration</a> and sequence labeling based diacritization of named entities. For case endings, we use Support Vector Machine (SVM) based ranking coupled with <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological patterns</a> and <a href=https://en.wikipedia.org/wiki/Rule_of_inference>linguistic rules</a> to properly guess case endings. We achieve a low word level diacritization error of 3.29 % and 12.77 % without and with case endings respectively on a new multi-genre free of copyright test set. We are making the diacritizer available for free for research purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1303/>Semantic Similarity of Arabic Sentences with Word Embeddings<span class=acl-fixed-case>A</span>rabic Sentences with Word Embeddings</a></strong><br><a href=/people/e/el-moatez-billah-nagoudi/>El Moatez Billah Nagoudi</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1303><div class="card-body p-3 small">Semantic textual similarity is the basis of countless applications and plays an important role in diverse areas, such as <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, <a href=https://en.wikipedia.org/wiki/Plagiarism_detection>plagiarism detection</a>, <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. This article proposes an innovative word embedding-based system devoted to calculate the <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> in <a href=https://en.wikipedia.org/wiki/Arabic_grammar>Arabic sentences</a>. The main idea is to exploit <a href=https://en.wikipedia.org/wiki/Vector_space>vectors</a> as word representations in a <a href=https://en.wikipedia.org/wiki/Dimension_(vector_space)>multidimensional space</a> in order to capture the semantic and syntactic properties of words. IDF weighting and Part-of-Speech tagging are applied on the examined sentences to support the identification of words that are highly descriptive in each sentence. The performance of our proposed <a href=https://en.wikipedia.org/wiki/System>system</a> is confirmed through the <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson correlation</a> between our assigned semantic similarity scores and <a href=https://en.wikipedia.org/wiki/Judgement>human judgments</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1304/>Morphological Analysis for the <a href=https://en.wikipedia.org/wiki/Maltese_language>Maltese Language</a> : The challenges of a <a href=https://en.wikipedia.org/wiki/Hybrid_system>hybrid system</a><span class=acl-fixed-case>M</span>altese Language: The challenges of a hybrid system</a></strong><br><a href=/people/c/claudia-borg/>Claudia Borg</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1304><div class="card-body p-3 small">Maltese is a morphologically rich language with a hybrid morphological system which features both concatenative and non-concatenative processes. This paper analyses the impact of this <a href=https://en.wikipedia.org/wiki/Hybridity>hybridity</a> on the performance of machine learning techniques for <a href=https://en.wikipedia.org/wiki/Morphology_(biology)>morphological labelling</a> and <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering</a>. In particular, we analyse a dataset of morphologically related word clusters to evaluate the difference in results for concatenative and non-concatenative clusters. We also describe research carried out in <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological labelling</a>, with a particular focus on the verb category. Two evaluations were carried out, one using an unseen dataset, and another one using a gold standard dataset which was manually labelled. The <a href=https://en.wikipedia.org/wiki/Gold_standard_(test)>gold standard dataset</a> was split into concatenative and non-concatenative to analyse the difference in results between the two <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1305/>A <a href=https://en.wikipedia.org/wiki/Morphological_analysis>Morphological Analyzer</a> for Gulf Arabic Verbs<span class=acl-fixed-case>G</span>ulf <span class=acl-fixed-case>A</span>rabic Verbs</a></strong><br><a href=/people/s/salam-khalifa/>Salam Khalifa</a>
|
<a href=/people/s/sara-hassan/>Sara Hassan</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1305><div class="card-body p-3 small">We present CALIMAGLF, a Gulf Arabic morphological analyzer currently covering over 2,600 verbal lemmas. We describe in detail the process of building the analyzer starting from phonetic dictionary entries to fully inflected orthographic paradigms and associated lexicon and orthographic variants. We evaluate the coverage of CALIMA-GLF against Modern Standard Arabic and Egyptian Arabic analyzers on part of a Gulf Arabic novel. CALIMA-GLF verb analysis token recall for identifying correct POS tag outperforms both the Modern Standard Arabic and Egyptian Arabic analyzers by over 27.4 % and 16.9 % absolute, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1306/>A Neural Architecture for Dialectal Arabic Segmentation<span class=acl-fixed-case>A</span>rabic Segmentation</a></strong><br><a href=/people/y/younes-samih/>Younes Samih</a>
|
<a href=/people/m/mohammed-attia/>Mohammed Attia</a>
|
<a href=/people/m/mohamed-eldesouki/>Mohamed Eldesouki</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/l/laura-kallmeyer/>Laura Kallmeyer</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1306><div class="card-body p-3 small">The automated processing of <a href=https://en.wikipedia.org/wiki/Arabic_dialects>Arabic Dialects</a> is challenging due to the lack of spelling standards and to the scarcity of annotated data and resources in general. Segmentation of words into its constituent parts is an important processing building block. In this paper, we show how a segmenter can be trained using only 350 annotated tweets using <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> without any <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> or use of lexical features or lexical resources. We deal with segmentation as a sequence labeling problem at the <a href=https://en.wikipedia.org/wiki/Character_(computing)>character level</a>. We show experimentally that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can rival state-of-the-art methods that rely on additional resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-1307" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-1307/>Sentiment Analysis of Tunisian Dialects : Linguistic Ressources and Experiments<span class=acl-fixed-case>T</span>unisian Dialects: Linguistic Ressources and Experiments</a></strong><br><a href=/people/s/salima-medhaffar/>Salima Medhaffar</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/y/yannick-esteve/>Yannick Estève</a>
|
<a href=/people/l/lamia-hadrich-belguith/>Lamia Hadrich-Belguith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1307><div class="card-body p-3 small">Dialectal Arabic (DA) is significantly different from the <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a> taught in schools and used in <a href=https://en.wikipedia.org/wiki/Writing>written communication</a> and formal speech (broadcast news, <a href=https://en.wikipedia.org/wiki/Religion>religion</a>, <a href=https://en.wikipedia.org/wiki/Politics>politics</a>, etc.). There are many existing researches in the field of Arabic language Sentiment Analysis (SA) ; however, they are generally restricted to Modern Standard Arabic (MSA) or some dialects of economic or political interest. In this paper we are interested in the <a href=https://en.wikipedia.org/wiki/Arabic>SA</a> of the <a href=https://en.wikipedia.org/wiki/Tunisian_Arabic>Tunisian Dialect</a>. We utilize Machine Learning techniques to determine the <a href=https://en.wikipedia.org/wiki/Polarity_(linguistics)>polarity</a> of comments written in <a href=https://en.wikipedia.org/wiki/Tunisian_Arabic>Tunisian Dialect</a>. First, we evaluate the SA systems performances with <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained using freely available MSA and Multi-dialectal data sets. We then collect and annotate a Tunisian Dialect corpus of 17.000 comments from <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a>. This <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> allows us a significant accuracy improvement compared to the best <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on other <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> or MSA data. We believe that this first freely available corpus will be valuable to researchers working in the field of Tunisian Sentiment Analysis and similar areas.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1308/>CAT : Credibility Analysis of Arabic Content on Twitter<span class=acl-fixed-case>CAT</span>: Credibility Analysis of <span class=acl-fixed-case>A</span>rabic Content on <span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/r/rim-el-ballouli/>Rim El Ballouli</a>
|
<a href=/people/w/wassim-el-hajj/>Wassim El-Hajj</a>
|
<a href=/people/a/ahmad-ghandour/>Ahmad Ghandour</a>
|
<a href=/people/s/shady-elbassuoni/>Shady Elbassuoni</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a>
|
<a href=/people/k/khaled-shaban/>Khaled Shaban</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1308><div class="card-body p-3 small">Data generated on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> has become a rich source for various data mining tasks. Those data analysis tasks that are dependent on the tweet semantics, such as <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, emotion mining, and rumor detection among others, suffer considerably if the tweet is not credible, not real, or spam. In this paper, we perform an extensive analysis on credibility of Arabic content on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. We also build a classification model (CAT) to automatically predict the <a href=https://en.wikipedia.org/wiki/Credibility>credibility</a> of a given Arabic tweet. Of particular originality is the inclusion of <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> extracted directly or indirectly from the author&#8217;s profile and timeline. To train and test CAT, we annotated for <a href=https://en.wikipedia.org/wiki/Credibility>credibility</a> a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> of 9,000 <a href=https://en.wikipedia.org/wiki/Twitter>Arabic tweets</a> that are topic independent. CAT achieved consistent improvements in predicting the credibility of the tweets when compared to several baselines and when compared to the state-of-the-art approach with an improvement of 21 % in weighted average F-measure. We also conducted experiments to highlight the importance of the user-based features as opposed to the content-based features. We conclude our work with a feature reduction experiment that highlights the best indicative features of credibility.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1309/>A New <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>Error Annotation</a> for Dyslexic texts in Arabic<span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/m/maha-alamri/>Maha Alamri</a>
|
<a href=/people/w/william-j-teahan/>William J Teahan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1309><div class="card-body p-3 small">This paper aims to develop a new classification of errors made in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> by those suffering from <a href=https://en.wikipedia.org/wiki/Dyslexia>dyslexia</a> to be used in the annotation of the Arabic dyslexia corpus (BDAC). The dyslexic error classification for Arabic texts (DECA) comprises a list of spelling errors extracted from previous studies and a collection of texts written by people with <a href=https://en.wikipedia.org/wiki/Dyslexia>dyslexia</a> that can provide a framework to help analyse specific errors committed by dyslexic writers. The <a href=https://en.wikipedia.org/wiki/Categorization>classification</a> comprises 37 types of errors, grouped into nine categories. The paper also discusses building a corpus of dyslexic Arabic texts that uses the error annotation scheme and provides an analysis of the errors that were found in the texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1311/>SHAKKIL : An Automatic Diacritization System for Modern Standard Arabic Texts<span class=acl-fixed-case>SHAKKIL</span>: An Automatic Diacritization System for <span class=acl-fixed-case>M</span>odern <span class=acl-fixed-case>S</span>tandard <span class=acl-fixed-case>A</span>rabic Texts</a></strong><br><a href=/people/a/amany-fashwan/>Amany Fashwan</a>
|
<a href=/people/s/sameh-alansary/>Sameh Alansary</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1311><div class="card-body p-3 small">This paper sheds light on a <a href=https://en.wikipedia.org/wiki/System>system</a> that would be able to diacritize Arabic texts automatically (SHAKKIL). In this system, the diacritization problem will be handled through two levels ; morphological and syntactic processing levels. The adopted morphological disambiguation algorithm depends on four layers ; Uni-morphological form layer, rule-based morphological disambiguation layer, statistical-based disambiguation layer and Out Of Vocabulary (OOV) layer. The adopted syntactic disambiguation algorithms is concerned with detecting the case ending diacritics depending on a rule based approach simulating the shallow parsing technique. This will be achieved using an annotated corpus for extracting the <a href=https://en.wikipedia.org/wiki/Arabic_grammar>Arabic linguistic rules</a>, building the <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> and testing the system output. This system is considered as a good trial of the interaction between rule-based approach and statistical approach, where the rules can help the statistics in detecting the right <a href=https://en.wikipedia.org/wiki/Diacritic>diacritization</a> and vice versa. At this point, the morphological Word Error Rate (WER) is 4.56 % while the morphological Diacritic Error Rate (DER) is 1.88 % and the syntactic WER is 9.36 %. The best WER is 14.78 % compared to the best-published results, of (Abandah, 2015) ; 11.68 %, (Rashwan, et al., 2015) ; 12.90 % and (Metwally, Rashwan, & Atiya, 2016) ; 13.70 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1312 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1312/>Arabic Tweets Treebanking and Parsing : A Bootstrapping Approach<span class=acl-fixed-case>A</span>rabic Tweets Treebanking and Parsing: A Bootstrapping Approach</a></strong><br><a href=/people/f/fahad-albogamy/>Fahad Albogamy</a>
|
<a href=/people/a/allan-ramsay/>Allan Ramsay</a>
|
<a href=/people/h/hanady-ahmed/>Hanady Ahmed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1312><div class="card-body p-3 small">In this paper, we propose using a bootstrapping method for constructing a dependency treebank of <a href=https://en.wikipedia.org/wiki/Twitter>Arabic tweets</a>. This method uses a rule-based parser to create a small treebank of one thousand Arabic tweets and a data-driven parser to create a larger <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a> by using the small treebank as a seed training set. We are able to create a dependency treebank from unlabelled tweets without any manual intervention. Experiments results show that this method can improve the speed of training the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> and the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the resulting <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1313/>Identifying Effective Translations for Cross-lingual Arabic-to-English User-generated Speech Search<span class=acl-fixed-case>A</span>rabic-to-<span class=acl-fixed-case>E</span>nglish User-generated Speech Search</a></strong><br><a href=/people/a/ahmad-khwileh/>Ahmad Khwileh</a>
|
<a href=/people/h/haithem-afli/>Haithem Afli</a>
|
<a href=/people/g/gareth-jones/>Gareth Jones</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1313><div class="card-body p-3 small">Cross Language Information Retrieval (CLIR) systems are a valuable tool to enable speakers of one language to search for content of interest expressed in a different language. A group for whom this is of particular interest is bilingual Arabic speakers who wish to search for English language content using information needs expressed in Arabic queries. A key challenge in <a href=https://en.wikipedia.org/wiki/CLIR>CLIR</a> is crossing the language barrier between the query and the documents. The most common approach to bridging this gap is automated query translation, which can be unreliable for vague or short queries. In this work, we examine the potential for improving CLIR effectiveness by predicting the translation effectiveness using Query Performance Prediction (QPP) techniques. We propose a novel QPP method to estimate the quality of translation for an Arabic-English Cross-lingual User-generated Speech Search (CLUGS) task. We present an empirical evaluation that demonstrates the quality of our method on alternative translation outputs extracted from an Arabic-to-English Machine Translation system developed for this task. Finally, we show how this <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can be integrated in CLUGS to find relevant translations for improved retrieval performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1314/>A Characterization Study of Arabic Twitter Data with a Benchmarking for State-of-the-Art Opinion Mining Models<span class=acl-fixed-case>A</span>rabic <span class=acl-fixed-case>T</span>witter Data with a Benchmarking for State-of-the-Art Opinion Mining Models</a></strong><br><a href=/people/r/ramy-baly/>Ramy Baly</a>
|
<a href=/people/g/gilbert-badaro/>Gilbert Badaro</a>
|
<a href=/people/g/georges-el-khoury/>Georges El-Khoury</a>
|
<a href=/people/r/rawan-moukalled/>Rawan Moukalled</a>
|
<a href=/people/r/rita-aoun/>Rita Aoun</a>
|
<a href=/people/h/hazem-hajj/>Hazem Hajj</a>
|
<a href=/people/w/wassim-el-hajj/>Wassim El-Hajj</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/k/khaled-shaban/>Khaled Shaban</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1314><div class="card-body p-3 small">Opinion mining in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> is a challenging task given the <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>rich morphology of the language</a>. The task becomes more challenging when it is applied to Twitter data, which contains additional sources of noise, such as the use of unstandardized dialectal variations, the nonconformation to grammatical rules, the use of Arabizi and code-switching, and the use of non-text objects such as images and <a href=https://en.wikipedia.org/wiki/URL>URLs</a> to express opinion. In this paper, we perform an analytical study to observe how such linguistic phenomena vary across different <a href=https://en.wikipedia.org/wiki/Arab_world>Arab regions</a>. This study of Arabic Twitter characterization aims at providing better understanding of Arabic Tweets, and fostering advanced research on the topic. Furthermore, we explore the performance of the two schools of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> on Arabic Twitter, namely the <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering approach</a> and the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning approach</a>. We consider <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that have achieved state-of-the-art performance for opinion mining in English. Results highlight the advantages of using <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning-based models</a>, and confirm the importance of using <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological abstractions</a> to address <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>Arabic&#8217;s complex morphology</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1315/>Robust Dictionary Lookup in Multiple Noisy Orthographies</a></strong><br><a href=/people/l/lingliang-zhang/>Lingliang Zhang</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/g/godfried-toussaint/>Godfried Toussaint</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1315><div class="card-body p-3 small">We present the MultiScript Phonetic Search algorithm to address the problem of <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learners</a> looking up unfamiliar words that they heard. We apply it to Arabic dictionary lookup with noisy queries done using both the <a href=https://en.wikipedia.org/wiki/Arabic_script>Arabic and Roman scripts</a>. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is based on a computational phonetic distance metric that can be optionally machine learned. To benchmark our performance, we created the ArabScribe dataset, containing 10,000 noisy transcriptions of random Arabic dictionary words. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> outperforms Google Translate&#8217;s did you mean feature, as well as the Yamli smart Arabic keyboard.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1318/>Not All Segments are Created Equal : Syntactically Motivated Sentiment Analysis in Lexical Space</a></strong><br><a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1318><div class="card-body p-3 small">Although there is by now a considerable amount of research on subjectivity and sentiment analysis on morphologically-rich languages, it is still unclear how lexical information can best be modeled in these <a href=https://en.wikipedia.org/wiki/Language>languages</a>. To bridge this gap, we build effective models exploiting exclusively gold- and machine-segmented lexical input and successfully employ syntactically motivated feature selection to improve <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. Our best <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> achieve significantly above the baselines, with 67.93 % and 69.37 % accuracies for subjectivity and sentiment classification respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1319 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1319/>An enhanced automatic speech recognition system for Arabic<span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/m/mohamed-amine-menacer/>Mohamed Amine Menacer</a>
|
<a href=/people/o/odile-mella/>Odile Mella</a>
|
<a href=/people/d/dominique-fohr/>Dominique Fohr</a>
|
<a href=/people/d/denis-jouvet/>Denis Jouvet</a>
|
<a href=/people/d/david-langlois/>David Langlois</a>
|
<a href=/people/k/kamel-smaili/>Kamel Smaili</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1319><div class="card-body p-3 small">Automatic speech recognition for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> is a very challenging task. Despite all the classical techniques for Automatic Speech Recognition (ASR), which can be efficiently applied to Arabic speech recognition, it is essential to take into consideration the language specificities to improve the system performance. In this article, we focus on Modern Standard Arabic (MSA) speech recognition. We introduce the challenges related to <a href=https://en.wikipedia.org/wiki/Arabic>Arabic language</a>, namely the complex morphology nature of the language and the absence of the short vowels in written text, which leads to several potential vowelization for each graphemes, which is often conflicting. We develop an ASR system for MSA by using <a href=https://en.wikipedia.org/wiki/Kaldi_(software)>Kaldi toolkit</a>. Several acoustic and language models are trained. We obtain a <a href=https://en.wikipedia.org/wiki/Word_error_rate>Word Error Rate (WER)</a> of 14.42 for the baseline system and 12.2 relative improvement by rescoring the <a href=https://en.wikipedia.org/wiki/Lattice_(group)>lattice</a> and by rewriting the output with the right Z hamoza above or below Alif.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1320/>Universal Dependencies for Arabic<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies for <span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/d/dima-taji/>Dima Taji</a>
|
<a href=/people/n/nizar-habash/>Nizar Habash</a>
|
<a href=/people/d/daniel-zeman/>Daniel Zeman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1320><div class="card-body p-3 small">We describe the process of creating NUDAR, a Universal Dependency treebank for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. We present the conversion from the Penn Arabic Treebank to the Universal Dependency syntactic representation through an intermediate dependency representation. We discuss the challenges faced in the <a href=https://en.wikipedia.org/wiki/Data_conversion>conversion of the trees</a>, the decisions we made to solve them, and the validation of our <a href=https://en.wikipedia.org/wiki/Data_conversion>conversion</a>. We also present initial <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> results on NUDAR.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1321/>A Layered Language Model based Hybrid Approach to Automatic Full Diacritization of Arabic<span class=acl-fixed-case>A</span>rabic</a></strong><br><a href=/people/m/mohamed-al-badrashiny/>Mohamed Al-Badrashiny</a>
|
<a href=/people/a/abdelati-hawwari/>Abdelati Hawwari</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1321><div class="card-body p-3 small">In this paper we present a system for automatic Arabic text diacritization using three levels of analysis granularity in a layered back off manner. We build and exploit diacritized language models (LM) for each of three different levels of granularity : surface form, morphologically segmented into prefix / stem / suffix, and character level. For each of the passes, we use <a href=https://en.wikipedia.org/wiki/Viterbi_search>Viterbi search</a> to pick the most probable diacritization per word in the input. We start with the surface form LM, followed by the morphological level, then finally we leverage the character level LM. Our <a href=https://en.wikipedia.org/wiki/System>system</a> outperforms all of the published <a href=https://en.wikipedia.org/wiki/System>systems</a> evaluated against the same <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and test data</a>. It achieves a 10.87 % <a href=https://en.wikipedia.org/wiki/Word_error_rate>WER</a> for complete full diacritization including lexical and syntactic diacritization, and 3.0 % <a href=https://en.wikipedia.org/wiki/Word_error_rate>WER</a> for lexical diacritization, ignoring syntactic diacritization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1322/>Arabic Textual Entailment with Word Embeddings<span class=acl-fixed-case>A</span>rabic Textual Entailment with Word Embeddings</a></strong><br><a href=/people/n/nada-almarwani/>Nada Almarwani</a>
|
<a href=/people/m/mona-diab/>Mona Diab</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1322><div class="card-body p-3 small">Determining the <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a> between texts is important in many NLP tasks, such as <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a>, <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, and <a href=https://en.wikipedia.org/wiki/Information_retrieval>information extraction and retrieval</a>. Various methods have been suggested based on external knowledge sources ; however, such resources are not always available in all languages and their acquisition is typically laborious and very costly. Distributional word representations such as <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> learned over large corpora have been shown to capture syntactic and semantic word relationships. Such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> have contributed to improving the performance of several <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a>. In this paper, we address the problem of <a href=https://en.wikipedia.org/wiki/Textual_entailment>textual entailment</a> in <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>. We employ both traditional <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and distributional representations. Crucially, we do not depend on any external resources in the process. Our suggested approach yields state of the art performance on a standard data set, ArbTE, achieving an accuracy of 76.2 % compared to state of the art of 69.3 %.</div></div></div><hr><div id=w17-14><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-14.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-14/>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1400/>Proceedings of the 6th Workshop on <span class=acl-fixed-case>B</span>alto-<span class=acl-fixed-case>S</span>lavic Natural Language Processing</a></strong><br><a href=/people/t/tomaz-erjavec/>Tomaž Erjavec</a>
|
<a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a>
|
<a href=/people/j/josef-steinberger/>Josef Steinberger</a>
|
<a href=/people/r/roman-yangarber/>Roman Yangarber</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1401/>Toward Pan-Slavic NLP : Some Experiments with Language Adaptation<span class=acl-fixed-case>S</span>lavic <span class=acl-fixed-case>NLP</span>: Some Experiments with Language Adaptation</a></strong><br><a href=/people/s/serge-sharoff/>Serge Sharoff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1401><div class="card-body p-3 small">There is great variation in the amount of NLP resources available for <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavonic languages</a>. For example, the Universal Dependency treebank (Nivre et al., 2016) has about 2 MW of training resources for <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, more than 1 MW for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, while only 950 words for <a href=https://en.wikipedia.org/wiki/Ukrainian_language>Ukrainian</a> and nothing for <a href=https://en.wikipedia.org/wiki/Belarusian_language>Belorussian</a>, <a href=https://en.wikipedia.org/wiki/Bosnian_language>Bosnian</a> or <a href=https://en.wikipedia.org/wiki/Macedonian_language>Macedonian</a>. Similarly, the Autodesk Machine Translation dataset only covers three <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavonic languages</a> (Czech, <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a> and Russian). In this talk I will discuss a general approach, which can be called Language Adaptation, similarly to <a href=https://en.wikipedia.org/wiki/Domain_adaptation>Domain Adaptation</a>. In this approach, a model for a particular language processing task is built by lexical transfer of cognate words and by learning a new <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>feature representation</a> for a lesser-resourced (recipient) language starting from a better-resourced (donor) language. More specifically, I will demonstrate how language adaptation works in such training scenarios as Translation Quality Estimation, Part-of-Speech tagging and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1402/>Clustering of Russian Adjective-Noun Constructions using Word Embeddings<span class=acl-fixed-case>R</span>ussian Adjective-Noun Constructions using Word Embeddings</a></strong><br><a href=/people/a/andrey-kutuzov/>Andrey Kutuzov</a>
|
<a href=/people/e/elizaveta-kuzmenko/>Elizaveta Kuzmenko</a>
|
<a href=/people/l/lidia-pivovarova/>Lidia Pivovarova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1402><div class="card-body p-3 small">This paper presents a method of automatic construction extraction from a large corpus of Russian. The term &#8216;construction&#8217; here means a multi-word expression in which a variable can be replaced with another word from the same <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic class</a>, for example, &#8216;a glass of [ water / juice / milk ]&#8217;. We deal with <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>constructions</a> that consist of a <a href=https://en.wikipedia.org/wiki/Noun>noun</a> and its adjective modifier. We propose a method of grouping such constructions into <a href=https://en.wikipedia.org/wiki/Semantic_class>semantic classes</a> via 2-step clustering of word vectors in <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional models</a>. We compare it with other clustering techniques and evaluate it against A Russian-English Collocational Dictionary of the Human Body that contains manually annotated groups of constructions with nouns meaning human body parts. The best performing method is used to cluster all adjective-noun bigrams in the <a href=https://en.wikipedia.org/wiki/Russian_National_Corpus>Russian National Corpus</a>. Results of this procedure are publicly available and can be used for building Russian construction dictionary as well as to accelerate theoretical studies of constructions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1403/>A Preliminary Study of Croatian Lexical Substitution<span class=acl-fixed-case>C</span>roatian Lexical Substitution</a></strong><br><a href=/people/d/domagoj-alagic/>Domagoj Alagić</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1403><div class="card-body p-3 small">Lexical substitution is a task of determining a meaning-preserving replacement for a word in context. We report on a preliminary study of this task for the <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian language</a> on a small-scale lexical sample dataset, manually annotated using three different annotation schemes. We compare the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>, analyze the inter-annotator agreement, and observe a number of interesting language specific details in the obtained lexical substitutes. Furthermore, we apply a recently-proposed, dependency-based lexical substitution model to our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a <a href=https://en.wikipedia.org/wiki/P-value>P@3 score</a> of 0.35, which indicates the difficulty of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1404/>Projecting Multiword Expression Resources on a Polish Treebank<span class=acl-fixed-case>P</span>olish Treebank</a></strong><br><a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/j/jakub-waszczuk/>Jakub Waszczuk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1404><div class="card-body p-3 small">Multiword expressions (MWEs) are <a href=https://en.wikipedia.org/wiki/Object_(philosophy)>linguistic objects</a> containing two or more words and showing <a href=https://en.wikipedia.org/wiki/Idiosyncrasy>idiosyncratic behavior</a> at different levels. Treebanks with annotated MWEs enable studies of such properties, as well as training and evaluation of MWE-aware parsers. However, few <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> contain full-fledged MWE annotations. We show how this gap can be bridged in Polish by projecting 3 MWE resources on a constituency treebank.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1405/>Lexicon Induction for Spoken Rusyn Challenges and Results<span class=acl-fixed-case>R</span>usyn – Challenges and Results</a></strong><br><a href=/people/a/achim-rabus/>Achim Rabus</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1405><div class="card-body p-3 small">This paper reports on challenges and results in developing NLP resources for <a href=https://en.wikipedia.org/wiki/Rusyn_language>spoken Rusyn</a>. Being a <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic minority language</a>, <a href=https://en.wikipedia.org/wiki/Rusyn_language>Rusyn</a> does not have any resources to make use of. We propose to build a morphosyntactic dictionary for <a href=https://en.wikipedia.org/wiki/Rusyn_language>Rusyn</a>, combining existing resources from the etymologically close Slavic languages Russian, <a href=https://en.wikipedia.org/wiki/Ukrainian_language>Ukrainian</a>, <a href=https://en.wikipedia.org/wiki/Slovak_language>Slovak</a>, and <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>. We adapt these resources to <a href=https://en.wikipedia.org/wiki/Rusyn_language>Rusyn</a> by using vowel-sensitive Levenshtein distance, hand-written language-specific transformation rules, and combinations of the two. Compared to an exact match baseline, we increase the coverage of the resulting <a href=https://en.wikipedia.org/wiki/Morphological_dictionary>morphological dictionary</a> by up to 77.4 % relative (42.9 % absolute), which results in a tagging recall increased by 11.6 % relative (9.1 % absolute). Our research confirms and expands the results of previous studies showing the efficiency of using NLP resources from neighboring languages for low-resourced languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1407/>Universal Dependencies for <a href=https://en.wikipedia.org/wiki/Serbian_language>Serbian</a> in Comparison with Croatian and Other Slavic Languages<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies for <span class=acl-fixed-case>S</span>erbian in Comparison with <span class=acl-fixed-case>C</span>roatian and Other <span class=acl-fixed-case>S</span>lavic Languages</a></strong><br><a href=/people/t/tanja-samardzic/>Tanja Samardžić</a>
|
<a href=/people/m/mirjana-starovic/>Mirjana Starović</a>
|
<a href=/people/z/zeljko-agic/>Željko Agić</a>
|
<a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1407><div class="card-body p-3 small">The paper documents the procedure of building a new Universal Dependencies (UDv2) treebank for <a href=https://en.wikipedia.org/wiki/Serbian_language>Serbian</a> starting from an existing Croatian UDv1 treebank and taking into account the other Slavic UD annotation guidelines. We describe the automatic and manual annotation procedures, discuss the annotation of Slavic-specific categories (case governing quantifiers, reflexive pronouns, question particles) and propose an approach to handling deverbal nouns in <a href=https://en.wikipedia.org/wiki/Slavic_languages>Slavic languages</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1408/>Spelling Correction for Morphologically Rich Language : a Case Study of <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a><span class=acl-fixed-case>R</span>ussian</a></strong><br><a href=/people/a/alexey-sorokin/>Alexey Sorokin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1408><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for automatic correction of spelling errors on the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level</a>, which uses noisy channel model and feature-based reranking of hypotheses. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is designed for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and clearly outperforms the winner of SpellRuEval-2016 competition. We show that language model size has the greatest influence on spelling correction quality. We also experiment with different types of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> and show that morphological and semantic information also improves the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of <a href=https://en.wikipedia.org/wiki/Spell_checker>spellchecking</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1409/>Debunking Sentiment Lexicons : A Case of Domain-Specific Sentiment Classification for Croatian<span class=acl-fixed-case>C</span>roatian</a></strong><br><a href=/people/p/paula-gombar/>Paula Gombar</a>
|
<a href=/people/z/zoran-medic/>Zoran Medić</a>
|
<a href=/people/d/domagoj-alagic/>Domagoj Alagić</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1409><div class="card-body p-3 small">Sentiment lexicons are widely used as an intuitive and inexpensive way of tackling sentiment classification, often within a simple lexicon word-counting approach or as part of a supervised model. However, it is an open question whether these approaches can compete with <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised models</a> that use only word-representation features. We address this question in the context of domain-specific sentiment classification for <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>. We experiment with the graph-based acquisition of sentiment lexicons, analyze their quality, and investigate how effectively they can be used in sentiment classification. Our results indicate that, even with as few as 500 labeled instances, a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> substantially outperforms a word-counting model. We also observe that adding lexicon-based features does not significantly improve supervised sentiment classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1410 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1410/>Adapting a State-of-the-Art <a href=https://en.wikipedia.org/wiki/Tagger>Tagger</a> for <a href=https://en.wikipedia.org/wiki/South_Slavic_languages>South Slavic Languages</a> to Non-Standard Text<span class=acl-fixed-case>S</span>outh <span class=acl-fixed-case>S</span>lavic Languages to Non-Standard Text</a></strong><br><a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/t/tomaz-erjavec/>Tomaž Erjavec</a>
|
<a href=/people/d/darja-fiser/>Darja Fišer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1410><div class="card-body p-3 small">In this paper we present the adaptations of a state-of-the-art <a href=https://en.wikipedia.org/wiki/Tagger>tagger</a> for <a href=https://en.wikipedia.org/wiki/South_Slavic_languages>South Slavic languages</a> to <a href=https://en.wikipedia.org/wiki/Nonstandard_dialect>non-standard texts</a> on the example of the <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovene language</a>. We investigate the impact of introducing in-domain training data as well as additional supervision through external resources or tools like word clusters and <a href=https://en.wikipedia.org/wiki/Word_normalization>word normalization</a>. We remove more than half of the error of the standard tagger when applied to non-standard texts by training it on a combination of standard and non-standard training data, while enriching the data representation with external resources removes additional 11 percent of the error. The final <a href=https://en.wikipedia.org/wiki/Computer_configuration>configuration</a> achieves tagging accuracy of 87.41 % on the full <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphosyntactic description</a>, which is, nevertheless, still quite far from the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 94.27 % achieved on standard text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1414 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1414/>Language-Independent Named Entity Analysis Using <a href=https://en.wikipedia.org/wiki/Parallel_projection>Parallel Projection</a> and Rule-Based Disambiguation</a></strong><br><a href=/people/j/james-mayfield/>James Mayfield</a>
|
<a href=/people/p/paul-mcnamee/>Paul McNamee</a>
|
<a href=/people/c/cash-costello/>Cash Costello</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1414><div class="card-body p-3 small">The 2017 shared task at the Balto-Slavic NLP workshop requires identifying coarse-grained named entities in seven languages, identifying each entity&#8217;s base form, and clustering name mentions across the multilingual set of documents. The fact that no training data is provided to systems for building <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifiers</a> further adds to the <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a>. To complete the task we first use publicly available parallel texts to project <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> capability from <a href=https://en.wikipedia.org/wiki/English_language>English</a> to each evaluation language. We ignore entirely the subtask of identifying non-inflected forms of names. Finally, we create cross-document entity identifiers by clustering named mentions using a procedure-based approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1416/>Stylometric Analysis of Parliamentary Speeches : Gender Dimension</a></strong><br><a href=/people/j/justina-mandravickaite/>Justina Mandravickaitė</a>
|
<a href=/people/t/tomas-krilavicius/>Tomas Krilavičius</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1416><div class="card-body p-3 small">Relation between <a href=https://en.wikipedia.org/wiki/Gender>gender</a> and language has been studied by many authors, however, there is still some uncertainty left regarding gender influence on <a href=https://en.wikipedia.org/wiki/Usage_(language)>language usage</a> in the professional environment. Often, the studied data sets are too small or texts of individual authors are too short in order to capture differences of language usage wrt gender successfully. This study draws from a larger corpus of speeches transcripts of the Lithuanian Parliament (1990-2013) to explore language differences of political debates by gender via stylometric analysis. Experimental set up consists of stylistic features that indicate lexical style and do not require external linguistic tools, namely the most frequent words, in combination with unsupervised machine learning algorithms. Results show that gender differences in the language use remain in professional environment not only in usage of <a href=https://en.wikipedia.org/wiki/Function_word>function words</a>, preferred <a href=https://en.wikipedia.org/wiki/Construct_(philosophy)>linguistic constructions</a>, but in the presented topics as well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1418/>Gender Profiling for Slovene Twitter communication : the Influence of Gender Marking, Content and Style<span class=acl-fixed-case>S</span>lovene <span class=acl-fixed-case>T</span>witter communication: the Influence of Gender Marking, Content and Style</a></strong><br><a href=/people/b/ben-verhoeven/>Ben Verhoeven</a>
|
<a href=/people/i/iza-skrjanec/>Iza Škrjanec</a>
|
<a href=/people/s/senja-pollak/>Senja Pollak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1418><div class="card-body p-3 small">We present results of the first <a href=https://en.wikipedia.org/wiki/Sex_and_gender_distinction>gender classification</a> experiments on <a href=https://en.wikipedia.org/wiki/Slovene_language>Slovene text</a> to our knowledge. Inspired by the TwiSty corpus and experiments (Verhoeven et al., 2016), we employed the Janes corpus (Erjavec et al., 2016) and its gender annotations to perform gender classification experiments on Twitter text comparing a token-based and a lemma-based approach. We find that the token-based approach (92.6 % accuracy), containing <a href=https://en.wikipedia.org/wiki/Gender>gender markings</a> related to the author, outperforms the lemma-based approach by about 5 %. Especially in the lemmatized version, we also observe stylistic and content-based differences in writing between men (e.g. more profane language, <a href=https://en.wikipedia.org/wiki/Numeral_(linguistics)>numerals</a> and beer mentions) and <a href=https://en.wikipedia.org/wiki/Woman>women</a> (e.g. more <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>, <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a> and character flooding). Many of our findings corroborate previous research on other languages.</div></div></div><hr><div id=w17-15><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-15.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-15/>Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1500/>Proceedings of the 2nd Workshop on Coreference Resolution Beyond <span class=acl-fixed-case>O</span>nto<span class=acl-fixed-case>N</span>otes (<span class=acl-fixed-case>CORBON</span> 2017)</a></strong><br><a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1501/>Use Generalized Representations, But Do Not Forget Surface Features</a></strong><br><a href=/people/n/nafise-sadat-moosavi/>Nafise Sadat Moosavi</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1501><div class="card-body p-3 small">Only a year ago, all state-of-the-art coreference resolvers were using an extensive amount of surface features. Recently, there was a paradigm shift towards using <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>, where the use of surface features is very limited. In this paper, we show that a simple SVM model with <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>surface features</a> outperforms more complex neural models for detecting anaphoric mentions. Our analysis suggests that using generalized representations and surface features have different strength that should be both taken into account for improving <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1502/>Enriching Basque Coreference Resolution System using Semantic Knowledge sources<span class=acl-fixed-case>B</span>asque Coreference Resolution System using Semantic Knowledge sources</a></strong><br><a href=/people/a/ander-soraluze/>Ander Soraluze</a>
|
<a href=/people/o/olatz-arregi/>Olatz Arregi</a>
|
<a href=/people/x/xabier-arregi/>Xabier Arregi</a>
|
<a href=/people/a/arantza-diaz-de-ilarraza/>Arantza Díaz de Ilarraza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1502><div class="card-body p-3 small">In this paper we present a Basque coreference resolution system enriched with semantic knowledge. An <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error analysis</a> carried out revealed the deficiencies that the <a href=https://en.wikipedia.org/wiki/System>system</a> had in resolving coreference cases in which semantic or world knowledge is needed. We attempt to improve the deficiencies using two semantic knowledge sources, specifically <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> and <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1503/>Improving Polish Mention Detection with Valency Dictionary<span class=acl-fixed-case>P</span>olish Mention Detection with Valency Dictionary</a></strong><br><a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a>
|
<a href=/people/b/bartlomiej-niton/>Bartłomiej Nitoń</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1503><div class="card-body p-3 small">This paper presents results of an experiment integrating information from valency dictionary of Polish into a mention detection system. Two types of information is acquired : positions of syntactic schemata for nominal and verbal constructs and secondary prepositions present in schemata. The syntactic schemata are used to prevent (for verbal realizations) or encourage (for nominal groups) constructing mentions from phrases filling multiple schema positions, the secondary prepositions to filter out artificial mentions created from their nominal components. Mention detection is evaluated against the manual annotation of the Polish Coreference Corpus in two settings : taking into account only mention heads or exact borders.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1504/>A Google-Proof Collection of French Winograd Schemas<span class=acl-fixed-case>G</span>oogle-Proof Collection of <span class=acl-fixed-case>F</span>rench <span class=acl-fixed-case>W</span>inograd Schemas</a></strong><br><a href=/people/p/pascal-amsili/>Pascal Amsili</a>
|
<a href=/people/o/olga-seminck/>Olga Seminck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1504><div class="card-body p-3 small">This article presents the first collection of French Winograd Schemas. Winograd Schemas form anaphora resolution problems that can only be resolved with extensive <a href=https://en.wikipedia.org/wiki/World_knowledge>world knowledge</a>. For this reason the <a href=https://en.wikipedia.org/wiki/Winograd_Schema_Challenge>Winograd Schema Challenge</a> has been proposed as an alternative to the <a href=https://en.wikipedia.org/wiki/Turing_test>Turing Test</a>. A very important feature of Winograd Schemas is that it should be impossible to resolve them with statistical information about word co-occurrences : they should be Google-proof. We propose a measure of Google-proofness based on <a href=https://en.wikipedia.org/wiki/Mutual_information>Mutual Information</a>, and demonstrate the method on our collection of French Winograd Schemas.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1506/>Multi-source annotation projection of coreference chains : assessing strategies and testing opportunities</a></strong><br><a href=/people/y/yulia-grishina/>Yulia Grishina</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1506><div class="card-body p-3 small">In this paper, we examine the possibility of using annotation projection from multiple sources for automatically obtaining coreference annotations in the target language. We implement a multi-source annotation projection algorithm and apply it on an English-German-Russian parallel corpus in order to transfer coreference chains from two sources to the target side. Operating in two settings a low-resource and a more linguistically-informed one we show that automatic coreference transfer could benefit from combining information from multiple languages, and assess the quality of both the extraction and the linking of target coreference mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1507/>CORBON 2017 Shared Task : Projection-Based Coreference Resolution<span class=acl-fixed-case>CORBON</span> 2017 Shared Task: Projection-Based Coreference Resolution</a></strong><br><a href=/people/y/yulia-grishina/>Yulia Grishina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1507><div class="card-body p-3 small">The CORBON 2017 Shared Task, organised as part of the Coreference Resolution Beyond OntoNotes workshop at EACL 2017, presented a new challenge for multilingual coreference resolution : we offer a projection-based setting in which one is supposed to build a coreference resolver for a new language exploiting little or even no knowledge of it, with our languages of interest being <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. We additionally offer a more traditional setting, targeting the development of a multilingual coreference resolver without any restrictions on the resources and methods used. In this paper, we describe the task setting and provide the results of one participant who successfully completed the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, comparing their results to the closely related previous research. Analysing the task setting and the results, we discuss the major challenges and make suggestions on the future directions of coreference evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1508/>Projection-based Coreference Resolution Using Deep Syntax</a></strong><br><a href=/people/m/michal-novak/>Michal Novák</a>
|
<a href=/people/a/anna-nedoluzhko/>Anna Nedoluzhko</a>
|
<a href=/people/z/zdenek-zabokrtsky/>Zdeněk Žabokrtský</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1508><div class="card-body p-3 small">The paper describes the system for <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> in <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, trained exclusively on coreference relations project ed through a parallel corpus from <a href=https://en.wikipedia.org/wiki/English_language>English</a>. The <a href=https://en.wikipedia.org/wiki/Resolver>resolver</a> operates on the level of deep syntax and makes use of multiple specialized models. It achieves 32 and 22 points in terms of CoNLL score for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, respectively. Analysis of the evaluation results show that the <a href=https://en.wikipedia.org/wiki/Resolver>resolver</a> for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> is able to preserve 66 % of the English resolver&#8217;s quality in terms of CoNLL score. The <a href=https://en.wikipedia.org/wiki/System>system</a> was submitted to the Closed track of the CORBON 2017 Shared task.</div></div></div><hr><div id=w17-16><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-16.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-16/>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1600/>Proceedings of the First <span class=acl-fixed-case>ACL</span> Workshop on Ethics in Natural Language Processing</a></strong><br><a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/s/shannon-l-spruit/>Shannon Spruit</a>
|
<a href=/people/m/margaret-mitchell/>Margaret Mitchell</a>
|
<a href=/people/e/emily-m-bender/>Emily M. Bender</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a>
|
<a href=/people/h/hanna-wallach/>Hanna Wallach</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1601/>Gender as a Variable in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural-Language Processing</a> : Ethical Considerations</a></strong><br><a href=/people/b/brian-larson/>Brian Larson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1601><div class="card-body p-3 small">Researchers and practitioners in natural-language processing (NLP) and related fields should attend to ethical principles in study design, ascription of categories / variables to study participants, and reporting of findings or results. This paper discusses theoretical and ethical frameworks for using <a href=https://en.wikipedia.org/wiki/Gender>gender</a> as a variable in NLP studies and proposes four guidelines for researchers and practitioners. The principles outlined here should guide practitioners, researchers, and peer reviewers, and they may be applicable to other <a href=https://en.wikipedia.org/wiki/Social_class>social categories</a>, such as <a href=https://en.wikipedia.org/wiki/Race_(human_categorization)>race</a>, applied to human beings connected to <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP research</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1602/>These are not the Stereotypes You are Looking For : Bias and Fairness in Authorial Gender Attribution</a></strong><br><a href=/people/c/corina-koolen/>Corina Koolen</a>
|
<a href=/people/a/andreas-van-cranenburgh/>Andreas van Cranenburgh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1602><div class="card-body p-3 small">Stylometric and text categorization results show that author gender can be discerned in texts with relatively high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. However, it is difficult to explain what gives rise to these results and there are many possible <a href=https://en.wikipedia.org/wiki/Confounding>confounding factors</a>, such as the domain, genre, and target audience of a text. More fundamentally, such classification efforts risk invoking <a href=https://en.wikipedia.org/wiki/Stereotype>stereotyping</a> and <a href=https://en.wikipedia.org/wiki/Essentialism>essentialism</a>. We explore this issue in two datasets of Dutch literary novels, using commonly used descriptive (LIWC, topic modeling) and predictive (machine learning) methods. Our results show the importance of controlling for variables in the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and we argue for taking care not to overgeneralize from the results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1603/>A Quantitative Study of Data in the NLP community<span class=acl-fixed-case>NLP</span> community</a></strong><br><a href=/people/m/margot-mieskes/>Margot Mieskes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1603><div class="card-body p-3 small">We present results on a quantitative analysis of publications in the NLP domain on collecting, publishing and availability of research data. We find that a wide range of publications rely on data crawled from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a>, but few give details on how potentially sensitive data was treated. Additionally, we find that while links to repositories of data are given, they often do not work even a short time after publication. We put together several suggestions on how to improve this situation based on publications from the <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP domain</a>, but also other research areas.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1604/>Ethical by Design : Ethics Best Practices for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a></a></strong><br><a href=/people/j/jochen-l-leidner/>Jochen L. Leidner</a>
|
<a href=/people/v/vassilis-plachouras/>Vassilis Plachouras</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1604><div class="card-body p-3 small">Natural language processing (NLP) systems analyze and/or generate <a href=https://en.wikipedia.org/wiki/Human_language>human language</a>, typically on users&#8217; behalf. One natural and necessary question that needs to be addressed in this context, both in research projects and in production settings, is the question how ethical the work is, both regarding the process and its outcome. Towards this end, we articulate a set of issues, propose a set of best practices, notably a process featuring an ethics review board, and sketch and how they could be meaningfully applied. Our main argument is that ethical outcomes ought to be achieved by design, i.e. by following a process aligned by <a href=https://en.wikipedia.org/wiki/Value_(ethics)>ethical values</a>. We also offer some response options for those facing <a href=https://en.wikipedia.org/wiki/Ethics>ethics issues</a>. While a number of previous works exist that discuss ethical issues, in particular around <a href=https://en.wikipedia.org/wiki/Big_data>big data</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, to the authors&#8217; knowledge this is the first account of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Ethics>ethics</a> from the perspective of a principled process.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1606/>Gender and Dialect Bias in YouTube’s Automatic Captions<span class=acl-fixed-case>Y</span>ou<span class=acl-fixed-case>T</span>ube’s Automatic Captions</a></strong><br><a href=/people/r/rachael-tatman/>Rachael Tatman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1606><div class="card-body p-3 small">This project evaluates the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of YouTube&#8217;s automatically-generated captions across two genders and five dialect groups. Speakers&#8217; dialect and gender was controlled for by using videos uploaded as part of the accent tag challenge, where speakers explicitly identify their language background. The results show robust differences in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> across both gender and dialect, with lower <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> for 1) women and 2) speakers from Scotland. This finding builds on earlier research finding that speaker&#8217;s sociolinguistic identity may negatively impact their ability to use automatic speech recognition, and demonstrates the need for sociolinguistically-stratified validation of systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1607/>Integrating the Management of Personal Data Protection and <a href=https://en.wikipedia.org/wiki/Open_science>Open Science</a> with Research Ethics</a></strong><br><a href=/people/d/dave-lewis/>Dave Lewis</a>
|
<a href=/people/j/joss-moorkens/>Joss Moorkens</a>
|
<a href=/people/k/kaniz-fatema/>Kaniz Fatema</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1607><div class="card-body p-3 small">We examine the impact of the <a href=https://en.wikipedia.org/wiki/General_Data_Protection_Regulation>EU General Data Protection Regulation</a> and the push from research funders to provide open access research data on the current practices in Language Technology Research. We analyse the challenges that arise and the opportunities to address many of them through the use of existing open data practices. We discuss the impact of this also on current practice in <a href=https://en.wikipedia.org/wiki/Research_ethics>research ethics</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1608 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1608/>Ethical Considerations in NLP Shared Tasks<span class=acl-fixed-case>NLP</span> Shared Tasks</a></strong><br><a href=/people/c/carla-parra-escartin/>Carla Parra Escartín</a>
|
<a href=/people/w/wessel-reijers/>Wessel Reijers</a>
|
<a href=/people/t/teresa-lynn/>Teresa Lynn</a>
|
<a href=/people/j/joss-moorkens/>Joss Moorkens</a>
|
<a href=/people/a/andy-way/>Andy Way</a>
|
<a href=/people/c/chao-hong-liu/>Chao-Hong Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1608><div class="card-body p-3 small">Shared tasks are increasingly common in our field, and new challenges are suggested at almost every conference and workshop. However, as this has become an established way of pushing research forward, it is important to discuss how we researchers organise and participate in shared tasks, and make that information available to the community to allow further research improvements. In this paper, we present a number of <a href=https://en.wikipedia.org/wiki/Ethics>ethical issues</a> along with other areas of concern that are related to the competitive nature of shared tasks. As such issues could potentially impact on research ethics in the Natural Language Processing community, we also propose the development of a framework for the organisation of and participation in shared tasks that can help mitigate against these issues arising.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-1609" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-1609/>Social Bias in Elicited Natural Language Inferences</a></strong><br><a href=/people/r/rachel-rudinger/>Rachel Rudinger</a>
|
<a href=/people/c/chandler-may/>Chandler May</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1609><div class="card-body p-3 small">We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The SNLI human-elicitation protocol makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1610/>A Short Review of Ethical Challenges in Clinical Natural Language Processing</a></strong><br><a href=/people/s/simon-suster/>Simon Šuster</a>
|
<a href=/people/s/stephan-tulkens/>Stéphan Tulkens</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1610><div class="card-body p-3 small">Clinical NLP has an immense potential in contributing to how clinical practice will be revolutionized by the advent of large scale processing of clinical records. However, this potential has remained largely untapped due to slow progress primarily caused by strict data access policies for researchers. In this paper, we discuss the concern for <a href=https://en.wikipedia.org/wiki/Privacy>privacy</a> and the measures it entails. We also suggest sources of less sensitive data. Finally, we draw attention to biases that can compromise the validity of empirical research and lead to socially harmful applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1611/>Goal-Oriented Design for Ethical Machine Learning and NLP<span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/t/tyler-schnoebelen/>Tyler Schnoebelen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1611><div class="card-body p-3 small">The argument made in this paper is that to act ethically in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> requires focusing on goals. NLP projects are often classificatory systems that deal with <a href=https://en.wikipedia.org/wiki/Human_subject_research>human subjects</a>, which means that goals from people affected by the <a href=https://en.wikipedia.org/wiki/System>systems</a> should be included. The paper takes as its core example a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that detects <a href=https://en.wikipedia.org/wiki/Crime>criminality</a>, showing the problems of training data, categories, and outcomes. The paper is oriented to the kinds of critiques on <a href=https://en.wikipedia.org/wiki/Power_(social_and_political)>power</a> and the reproduction of inequality that are found in <a href=https://en.wikipedia.org/wiki/Social_theory>social theory</a>, but it also includes concrete suggestions on how to put goal-oriented design into practice.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1612.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1612 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1612/>Ethical Research Protocols for Social Media Health Research</a></strong><br><a href=/people/a/adrian-benton/>Adrian Benton</a>
|
<a href=/people/g/glen-coppersmith/>Glen Coppersmith</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1612><div class="card-body p-3 small">Social media have transformed data-driven research in <a href=https://en.wikipedia.org/wiki/Political_science>political science</a>, the <a href=https://en.wikipedia.org/wiki/Social_science>social sciences</a>, <a href=https://en.wikipedia.org/wiki/Health>health</a>, and <a href=https://en.wikipedia.org/wiki/Medicine>medicine</a>. Since <a href=https://en.wikipedia.org/wiki/Medical_research>health research</a> often touches on sensitive topics that relate to ethics of treatment and patient privacy, similar ethical considerations should be acknowledged when using social media data in <a href=https://en.wikipedia.org/wiki/Medical_research>health research</a>. While much has been said regarding the ethical considerations of social media research, <a href=https://en.wikipedia.org/wiki/Medical_research>health research</a> leads to an additional set of concerns. We provide practical suggestions in the form of guidelines for researchers working with social media data in <a href=https://en.wikipedia.org/wiki/Medical_research>health research</a>. These guidelines can inform an IRB proposal for researchers new to social media health research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1613.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1613 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1613 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1613/>Say the Right Thing Right : Ethics Issues in Natural Language Generation Systems</a></strong><br><a href=/people/c/charese-smiley/>Charese Smiley</a>
|
<a href=/people/f/frank-schilder/>Frank Schilder</a>
|
<a href=/people/v/vassilis-plachouras/>Vassilis Plachouras</a>
|
<a href=/people/j/jochen-l-leidner/>Jochen L. Leidner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1613><div class="card-body p-3 small">We discuss the ethical implications of <a href=https://en.wikipedia.org/wiki/Natural-language_generation>Natural Language Generation systems</a>. We use one particular <a href=https://en.wikipedia.org/wiki/System>system</a> as a case study to identify and classify issues, and we provide an ethics checklist, in the hope that future system designers may benefit from conducting their own ethics reviews based on our checklist.</div></div></div><hr><div id=w17-17><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-17.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-17/>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1700/>Proceedings of the 13th Workshop on Multiword Expressions (<span class=acl-fixed-case>MWE</span> 2017)</a></strong><br><a href=/people/s/stella-markantonatou/>Stella Markantonatou</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/v/veronika-vincze/>Veronika Vincze</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1702 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1702/>Multi-word Entity Classification in a Highly Multilingual Environment</a></strong><br><a href=/people/s/sophie-chesney/>Sophie Chesney</a>
|
<a href=/people/g/guillaume-jacquet/>Guillaume Jacquet</a>
|
<a href=/people/r/ralf-steinberger/>Ralf Steinberger</a>
|
<a href=/people/j/jakub-piskorski/>Jakub Piskorski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1702><div class="card-body p-3 small">This paper describes an approach for the classification of millions of existing multi-word entities (MWEntities), such as organisation or event names, into thirteen category types, based only on the tokens they contain. In order to classify our very large in-house collection of multilingual MWEntities into an application-oriented set of entity categories, we trained and tested distantly-supervised classifiers in 43 languages based on MWEntities extracted from <a href=https://en.wikipedia.org/wiki/BabelNet>BabelNet</a>. The best-performing <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> was the multi-class SVM using a TF.IDF-weighted data representation. Interestingly, one unique <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> trained on a mix of all languages consistently performed better than <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained for individual languages, reaching an averaged F1-value of 88.8 %. In this paper, we present the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and test data</a>, including a human evaluation of its <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, describe the methods used to train the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>, and discuss the results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1703/>Using bilingual word-embeddings for multilingual collocation extraction</a></strong><br><a href=/people/m/marcos-garcia/>Marcos Garcia</a>
|
<a href=/people/m/marcos-garcia-salido/>Marcos García-Salido</a>
|
<a href=/people/m/margarita-alonso-ramos/>Margarita Alonso-Ramos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1703><div class="card-body p-3 small">This paper presents a new strategy for multilingual collocation extraction which takes advantage of parallel corpora to learn bilingual word-embeddings. Monolingual collocation candidates are retrieved using Universal Dependencies, while the distributional models are then applied to search for equivalents of the elements of each collocation in the target languages. The proposed method extracts not only collocation equivalents with direct translation between languages, but also other cases where the <a href=https://en.wikipedia.org/wiki/Collocation>collocations</a> in the two languages are not literal translations of each other. Several experiments -evaluating collocations with three syntactic patterns- in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, and <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> show that our approach can effectively extract large pairs of bilingual equivalents with an average precision of about 90 %. Moreover, preliminary results on comparable corpora suggest that the distributional models can be applied for identifying new bilingual collocations in different domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1704.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1704 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1704 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1704/>The PARSEME Shared Task on Automatic Identification of Verbal Multiword Expressions<span class=acl-fixed-case>PARSEME</span> Shared Task on Automatic Identification of Verbal Multiword Expressions</a></strong><br><a href=/people/a/agata-savary/>Agata Savary</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/s/silvio-cordeiro/>Silvio Cordeiro</a>
|
<a href=/people/f/federico-sangati/>Federico Sangati</a>
|
<a href=/people/v/veronika-vincze/>Veronika Vincze</a>
|
<a href=/people/b/behrang-qasemizadeh/>Behrang QasemiZadeh</a>
|
<a href=/people/m/marie-candito/>Marie Candito</a>
|
<a href=/people/f/fabienne-cap/>Fabienne Cap</a>
|
<a href=/people/v/voula-giouli/>Voula Giouli</a>
|
<a href=/people/i/ivelina-stoyanova/>Ivelina Stoyanova</a>
|
<a href=/people/a/antoine-doucet/>Antoine Doucet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1704><div class="card-body p-3 small">Multiword expressions (MWEs) are known as a pain in the neck for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> due to their idiosyncratic behaviour. While some categories of MWEs have been addressed by many studies, verbal MWEs (VMWEs), such as to take a decision, to break one&#8217;s heart or to turn off, have been rarely modelled. This is notably due to their syntactic variability, which hinders treating them as words with spaces. We describe an initiative meant to bring about substantial progress in understanding, modelling and processing VMWEs. It is a joint effort, carried out within a European research network, to elaborate universal terminologies and annotation guidelines for 18 languages. Its main outcome is a multilingual 5-million-word annotated corpus which underlies a shared task on automatic identification of VMWEs. This paper presents the corpus annotation methodology and outcome, the shared task organisation and the results of the participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1707 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1707/>Neural Networks for Multi-Word Expression Detection</a></strong><br><a href=/people/n/natalia-klyueva/>Natalia Klyueva</a>
|
<a href=/people/a/antoine-doucet/>Antoine Doucet</a>
|
<a href=/people/m/milan-straka/>Milan Straka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1707><div class="card-body p-3 small">In this paper we describe the MUMULS system that participated to the 2017 shared task on automatic identification of verbal multiword expressions (VMWEs). The MUMULS system was implemented using a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised approach</a> based on <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> using the open source library <a href=https://en.wikipedia.org/wiki/TensorFlow>TensorFlow</a>. The <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> was trained on a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> containing annotated VMWEs as well as <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological and syntactic information</a>. The MUMULS system performed the identification of VMWEs in 15 languages, it was one of few systems that could categorize VMWEs type in nearly all languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1708/>Factoring Ambiguity out of the Prediction of Compositionality for German Multi-Word Expressions<span class=acl-fixed-case>G</span>erman Multi-Word Expressions</a></strong><br><a href=/people/s/stefan-bott/>Stefan Bott</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1708><div class="card-body p-3 small">Ambiguity represents an obstacle for distributional semantic models(DSMs), which typically subsume the contexts of all word senses within one vector. While individual vector space approaches have been concerned with sense discrimination (e.g., Schtze 1998, Erk 2009, Erk and Pado 2010), such <a href=https://en.wikipedia.org/wiki/Discrimination>discrimination</a> has rarely been integrated into DSMs across <a href=https://en.wikipedia.org/wiki/Semantic_analysis_(linguistics)>semantic tasks</a>. This paper presents a soft-clustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1709/>Multiword expressions and lexicalism : the view from LFG<span class=acl-fixed-case>LFG</span></a></strong><br><a href=/people/j/jamie-y-findlay/>Jamie Y. Findlay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1709><div class="card-body p-3 small">Multiword expressions (MWEs) pose a problem for lexicalist theories like Lexical Functional Grammar (LFG), since they are prima facie counterexamples to a strong form of the lexical integrity principle, which entails that a lexical item can only be realised as a single, syntactically atomic word. In this paper, I demonstrate some of the problems facing any strongly lexicalist account of MWEs, and argue that the lexical integrity principle must be weakened. I conclude by sketching a <a href=https://en.wikipedia.org/wiki/Formalism_(philosophy_of_mathematics)>formalism</a> which integrates a Tree Adjoining Grammar into the LFG architecture, taking advantage of this relaxation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1710 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1710/>Understanding Idiomatic Variation</a></strong><br><a href=/people/k/kristina-geeraert/>Kristina Geeraert</a>
|
<a href=/people/h/harald-baayen/>R. Harald Baayen</a>
|
<a href=/people/j/john-newman/>John Newman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1710><div class="card-body p-3 small">This study investigates the processing of <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idiomatic variants</a> through an eye-tracking experiment. Four types of idiom variants were included, in addition to the <a href=https://en.wikipedia.org/wiki/Canonical_form>canonical form</a> and the <a href=https://en.wikipedia.org/wiki/Literal_and_figurative_language>literal meaning</a>. Results suggest that modifications to <a href=https://en.wikipedia.org/wiki/Idiom_(language_structure)>idioms</a>, modulo obvious effects of length differences, are not more difficult to process than the canonical forms themselves. This fits with recent corpus findings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1711/>Discovering Light Verb Constructions and their Translations from Parallel Corpora without Word Alignment</a></strong><br><a href=/people/n/natalie-vargas/>Natalie Vargas</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a>
|
<a href=/people/h/helena-caseli/>Helena Caseli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1711><div class="card-body p-3 small">We propose a method for joint unsupervised discovery of multiword expressions (MWEs) and their translations from parallel corpora. First, we apply independent monolingual MWE extraction in source and target languages simultaneously. Then, we calculate translation probability, association score and distributional similarity of co-occurring pairs. Finally, we rank all <a href=https://en.wikipedia.org/wiki/Translation_(geometry)>translations</a> of a given MWE using a linear combination of these features. Preliminary experiments on light verb constructions show promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1713 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1713/>Show Me Your Variance and I Tell You Who You Are-Deriving Compound Compositionality from Word Alignments<span class=acl-fixed-case>I</span> Tell You Who You Are - Deriving Compound Compositionality from Word Alignments</a></strong><br><a href=/people/f/fabienne-cap/>Fabienne Cap</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1713><div class="card-body p-3 small">We use word alignment variance as an indicator for the <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>non-compositionality</a> of German and English noun compounds. Our work-in-progress results are on their own not competitive with state-of-the art approaches, but they show that alignment variance is correlated with <a href=https://en.wikipedia.org/wiki/Compositionality>compositionality</a> and thus worth a closer look in the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1714 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1714/>Semantic annotation to characterize contextual variation in terminological noun compounds : a pilot study</a></strong><br><a href=/people/m/melania-cabezas-garcia/>Melania Cabezas-García</a>
|
<a href=/people/a/antonio-san-martin/>Antonio San Martín</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1714><div class="card-body p-3 small">Noun compounds (NCs) are semantically complex and not fully compositional, as is often assumed. This paper presents a pilot study regarding the semantic annotation of environmental NCs with a view to accessing their <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and exploring their domain-based contextual variation. Our results showed that the semantic annotation of NCs afforded important insights into how context impacts their conceptualization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1716 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1716/>A data-driven approach to verbal multiword expression detection. PARSEME Shared Task system description paper<span class=acl-fixed-case>PARSEME</span> Shared Task system description paper</a></strong><br><a href=/people/t/tiberiu-boros/>Tiberiu Boros</a>
|
<a href=/people/s/sonia-pipa/>Sonia Pipa</a>
|
<a href=/people/v/verginica-barbu-mititelu/>Verginica Barbu Mititelu</a>
|
<a href=/people/d/dan-tufis/>Dan Tufis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1716><div class="card-body p-3 small">Multiword expressions are groups of words acting as a morphologic, syntactic and semantic unit in <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic analysis</a>. Verbal multiword expressions represent the subgroup of <a href=https://en.wikipedia.org/wiki/Multiword_expression>multiword expressions</a>, namely that in which a verb is the syntactic head of the group considered in its canonical (or dictionary) form. All multiword expressions are a great challenge for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, but the verbal ones are particularly interesting for tasks such as <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>, as the verb is the central element in the syntactic organization of a sentence. In this paper we introduce our data-driven approach to verbal multiword expressions which was objectively validated during the PARSEME shared task on verbal multiword expressions identification. We tested our approach on 12 languages, and we provide detailed information about corpora composition, feature selection process, validation procedure and performance on all languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1717.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1717 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1717 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1717/>The ATILF-LLF System for Parseme Shared Task : a Transition-based Verbal Multiword Expression Tagger<span class=acl-fixed-case>ATILF</span>-<span class=acl-fixed-case>LLF</span> System for Parseme Shared Task: a Transition-based Verbal Multiword Expression Tagger</a></strong><br><a href=/people/h/hazem-al-saied/>Hazem Al Saied</a>
|
<a href=/people/m/matthieu-constant/>Matthieu Constant</a>
|
<a href=/people/m/marie-candito/>Marie Candito</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1717><div class="card-body p-3 small">We describe the ATILF-LLF system built for the MWE 2017 Shared Task on automatic identification of verbal multiword expressions. We participated in the closed track only, for all the 18 available languages. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is a robust greedy transition-based system, in which MWE are identified through a MERGE transition. The system was meant to accommodate the variety of linguistic resources provided for each language, in terms of accompanying <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological and syntactic information</a>. Using per-MWE Fscore, the <a href=https://en.wikipedia.org/wiki/System>system</a> was ranked first for all but two languages (Hungarian and Romanian).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1719.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1719 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1719 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1719/>Compositionality in Verb-Particle Constructions</a></strong><br><a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/c/choh-man-teng/>Choh Man Teng</a>
|
<a href=/people/j/james-allen/>James Allen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1719><div class="card-body p-3 small">We are developing a broad-coverage deep semantic lexicon for a system that parses sentences into a logical form expressed in a rich ontology that supports <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. In this paper we look at verb-particle constructions (VPCs), and the extent to which they can be treated compositionally vs idiomatically. First we distinguish between the different types of VPCs based on their compositionality and then present a set of <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> for classifying specific instances as compositional or not. We then identify a small set of general sense classes for <a href=https://en.wikipedia.org/wiki/Grammatical_particle>particles</a> when used compositionally and discuss the resulting lexical representations that are being added to the lexicon. By treating VPCs as compositional whenever possible, we attain broad coverage in a compact way, and also enable interpretations of novel VPC usages not explicitly present in the lexicon.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1720.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1720 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1720 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1720/>Rule-Based Translation of Spanish Verb-Noun Combinations into <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a><span class=acl-fixed-case>S</span>panish Verb-Noun Combinations into <span class=acl-fixed-case>B</span>asque</a></strong><br><a href=/people/u/uxoa-inurrieta/>Uxoa Iñurrieta</a>
|
<a href=/people/i/itziar-aduriz/>Itziar Aduriz</a>
|
<a href=/people/a/arantza-diaz-de-ilarraza/>Arantza Díaz de Ilarraza</a>
|
<a href=/people/g/gorka-labaka/>Gorka Labaka</a>
|
<a href=/people/k/kepa-sarasola/>Kepa Sarasola</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1720><div class="card-body p-3 small">This paper presents a method to improve the translation of Verb-Noun Combinations (VNCs) in a rule-based Machine Translation (MT) system for Spanish-Basque. Linguistic information about a set of <a href=https://en.wikipedia.org/wiki/Virtual_Network_Computing>VNCs</a> is gathered from the public database Konbitzul, and it is integrated into the MT system, leading to an improvement in <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, NIST and TER scores, as well as the results being evidently better according to human evaluators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1721.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1721 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1721 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1721/>Verb-Particle Constructions in Questions</a></strong><br><a href=/people/v/veronika-vincze/>Veronika Vincze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1721><div class="card-body p-3 small">In this paper, we investigate the behavior of <a href=https://en.wikipedia.org/wiki/Grammatical_conjugation>verb-particle constructions</a> in English questions. We present a small dataset that contains <a href=https://en.wikipedia.org/wiki/Questionnaire_construction>questions</a> and verb-particle construction candidates. We demonstrate that there are significant differences in the distribution of WH-words, verbs and prepositions / particles in sentences that contain VPCs and sentences that contain only verb + prepositional phrase combinations both by statistical means and in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> experiments. Hence, VPCs and non-VPCs can be effectively separated from each other by using a rich feature set, containing several novel features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1722.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1722 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1722 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1722/>Simple Compound Splitting for German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/m/marion-weller-di-marco/>Marion Weller-Di Marco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1722><div class="card-body p-3 small">This paper presents a simple method for German compound splitting that combines a basic frequency-based approach with a form-to-lemma mapping to approximate morphological operations. With the exception of a small set of hand-crafted rules for modeling transitional elements, this approach is resource-poor. In our evaluation, the simple splitter outperforms a splitter relying on rich morphological resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1723.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1723 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1723 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1723/>Identification of Ambiguous Multiword Expressions Using Sequence Models and Lexical Resources</a></strong><br><a href=/people/m/manon-scholivet/>Manon Scholivet</a>
|
<a href=/people/c/carlos-ramisch/>Carlos Ramisch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1723><div class="card-body p-3 small">We present a simple and efficient <a href=https://en.wikipedia.org/wiki/Tagger>tagger</a> capable of identifying highly ambiguous multiword expressions (MWEs) in French texts. It is based on conditional random fields (CRF), using local context information as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. We show that this approach can obtain results that, in some cases, approach more sophisticated parser-based MWE identification methods without requiring syntactic trees from a <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a>. Moreover, we study how well the CRF can take into account external information coming from a lexicon.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1724.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1724 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1724 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1724/>Comparing Recurring Lexico-Syntactic Trees (RLTs) and Ngram Techniques for Extended Phraseology Extraction<span class=acl-fixed-case>RLT</span>s) and Ngram Techniques for Extended Phraseology Extraction</a></strong><br><a href=/people/a/agnes-tutin/>Agnès Tutin</a>
|
<a href=/people/o/olivier-kraif/>Olivier Kraif</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1724><div class="card-body p-3 small">This paper aims at assessing to what extent a syntax-based method (Recurring Lexico-syntactic Trees (RLT) extraction) allows us to extract large phraseological units such as prefabricated routines, e.g. as previously said or as far as we / I know in <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific writing</a>. In order to evaluate this method, we compare it to the classical ngram extraction technique, on a subset of recurring segments including speech verbs in a French corpus of scientific writing. Results show that the LRT extraction technique is far more efficient for extended MWEs such as <a href=https://en.wikipedia.org/wiki/Subroutine>routines</a> or <a href=https://en.wikipedia.org/wiki/Collocation>collocations</a> but performs more poorly for surface phenomena such as syntactic constructions or fully frozen expressions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1725.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1725 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1725 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1725/>Benchmarking Joint Lexical and Syntactic Analysis on Multiword-Rich Data</a></strong><br><a href=/people/m/matthieu-constant/>Matthieu Constant</a>
|
<a href=/people/h/hector-martinez-alonso/>Héctor Martinez Alonso</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1725><div class="card-body p-3 small">This article evaluates the extension of a <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parser</a> that performs joint syntactic analysis and multiword expression identification. We show that, given sufficient training data, the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> benefits from explicit multiword information and improves overall labeled accuracy score in eight of the ten evaluation cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1726.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1726 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1726 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-1726" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-1726/>Semi-Automated Resolution of Inconsistency for a Harmonized Multiword Expression and Dependency Parse Annotation</a></strong><br><a href=/people/k/king-chan/>King Chan</a>
|
<a href=/people/j/julian-brooke/>Julian Brooke</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1726><div class="card-body p-3 small">This paper presents a methodology for identifying and resolving various kinds of inconsistency in the context of merging dependency and multiword expression (MWE) annotations, to generate a dependency treebank with comprehensive MWE annotations. Candidates for correction are identified using a variety of <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>, including an entirely novel one which identifies violations of MWE constituency in the dependency tree, and resolved by <a href=https://en.wikipedia.org/wiki/Arbitration>arbitration</a> with minimal human intervention. Using this technique, we identified and corrected several hundred errors across both parse and MWE annotations, representing changes to a significant percentage (well over 10 %) of the MWE instances in the joint corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1727.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1727 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1727 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1727/>Combining <a href=https://en.wikipedia.org/wiki/Linguistic_feature>Linguistic Features</a> for the Detection of Croatian Multiword Expressions<span class=acl-fixed-case>C</span>roatian Multiword Expressions</a></strong><br><a href=/people/m/maja-buljan/>Maja Buljan</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1727><div class="card-body p-3 small">As multiword expressions (MWEs) exhibit a range of <a href=https://en.wikipedia.org/wiki/Idiosyncrasy>idiosyncrasies</a>, their automatic detection warrants the use of many different features. Tsvetkov and Wintner (2014) proposed a Bayesian network model that combines linguistically motivated features and also models their interactions. In this paper, we extend their model with new <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> and apply it to <a href=https://en.wikipedia.org/wiki/Croatian_language>Croatian</a>, a <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphologically complex</a> and a relatively free word order language, achieving a satisfactory performance of 0.823 F1-score. Furthermore, by comparing against (semi)naive Bayes models, we demonstrate that manually modeling feature interactions is indeed important. We make our annotated dataset of Croatian MWEs freely available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1728.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1728 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1728 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1728/>Complex Verbs are Different : Exploring the Visual Modality in Multi-Modal Models to Predict Compositionality</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1728><div class="card-body p-3 small">This paper compares a neural network DSM relying on textual co-occurrences with a multi-modal model integrating visual information. We focus on <a href=https://en.wikipedia.org/wiki/Compound_(linguistics)>nominal vs. verbal compounds</a>, and zoom into lexical, empirical and perceptual target properties to explore the contribution of the <a href=https://en.wikipedia.org/wiki/Visual_system>visual modality</a>. Our experiments show that (i) visual features contribute differently for verbs than for nouns, and (ii) <a href=https://en.wikipedia.org/wiki/Image>images</a> complement textual information, if (a) the textual modality by itself is poor and appropriate image subsets are used, or (b) the textual modality by itself is rich and large (potentially noisy) images are added.</div></div></div><hr><div id=w17-18><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-18.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-18/>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1800/>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</a></strong><br><a href=/people/e/eduardo-blanco/>Eduardo Blanco</a>
|
<a href=/people/r/roser-morante/>Roser Morante</a>
|
<a href=/people/r/roser-sauri/>Roser Saurí</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1801/>Understanding the Semantics of Narratives of Interpersonal Violence through Reader Annotations and Physiological Reactions</a></strong><br><a href=/people/a/alexander-calderwood/>Alexander Calderwood</a>
|
<a href=/people/e/elizabeth-a-pruett/>Elizabeth A. Pruett</a>
|
<a href=/people/r/raymond-ptucha/>Raymond Ptucha</a>
|
<a href=/people/c/christopher-homan/>Christopher Homan</a>
|
<a href=/people/c/cecilia-ovesdotter-alm/>Cecilia Ovesdotter Alm</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1801><div class="card-body p-3 small">Interpersonal violence (IPV) is a prominent sociological problem that affects people of all demographic backgrounds. By analyzing how readers interpret, perceive, and react to experiences narrated in social media posts, we explore an understudied source for <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> about abuse. We asked readers to annotate Reddit posts about relationships with vs. without IPV for stakeholder roles and emotion, while measuring their <a href=https://en.wikipedia.org/wiki/Electrodermal_activity>galvanic skin response (GSR)</a>, <a href=https://en.wikipedia.org/wiki/Pulse>pulse</a>, and <a href=https://en.wikipedia.org/wiki/Facial_expression>facial expression</a>. We map annotations to coreference resolution output to obtain a labeled coreference chain for stakeholders in texts, and apply automated semantic role labeling for analyzing IPV discourse. Findings provide insights into how readers process roles and emotion in narratives. For example, abusers tend to be linked with <a href=https://en.wikipedia.org/wiki/Violence>violent actions</a> and certain <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect states</a>. We train classifiers to predict stakeholder categories of coreference chains. We also find that subjects&#8217; GSR noticeably changed for IPV texts, suggesting that co-collected measurement-based data about annotators can be used to support text annotation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1802/>Intension, <a href=https://en.wikipedia.org/wiki/Attitude_(psychology)>Attitude</a>, and <a href=https://en.wikipedia.org/wiki/Tense&#8211;aspect&#8211;mood>Tense Annotation</a> in a High-Fidelity Semantic Representation</a></strong><br><a href=/people/g/gene-kim/>Gene Kim</a>
|
<a href=/people/l/lenhart-schubert/>Lenhart Schubert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1802><div class="card-body p-3 small">This paper describes current efforts in developing an annotation schema and guidelines for sentences in Episodic Logic (EL). We focus on important distinctions for representing <a href=https://en.wikipedia.org/wiki/Modal_logic>modality</a>, <a href=https://en.wikipedia.org/wiki/Attitude_(psychology)>attitudes</a>, and <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and present an annotation schema that makes these distinctions. EL has proved competitive with other logical formulations in speed and inference-enablement, while expressing a wider array of natural language phenomena including intensional modification of predicates and sentences, propositional attitudes, and <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a> and <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1803/>Towards a lexicon of event-selecting predicates for a French FactBank<span class=acl-fixed-case>F</span>rench <span class=acl-fixed-case>F</span>act<span class=acl-fixed-case>B</span>ank</a></strong><br><a href=/people/i/ingrid-falk/>Ingrid Falk</a>
|
<a href=/people/f/fabienne-martin/>Fabienne Martin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1803><div class="card-body p-3 small">This paper presents ongoing work for the construction of a French FactBank and a lexicon of French event-selecting predicates (ESPs), by applying the factuality detection algorithm introduced in (Saur and Pustejovsky, 2012). This algorithm relies on a lexicon of ESPs, specifying how these predicates influence the polarity of their embedded events. For this pilot study, we focused on French factive and implicative verbs, and capitalised on a lexical resource for the English counterparts of these verbs provided by the CLSI Group (Nairn et al., 2006 ; Karttunen, 2012).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1804/>Universal Dependencies to <a href=https://en.wikipedia.org/wiki/Logical_form>Logical Form</a> with Negation Scope<span class=acl-fixed-case>U</span>niversal <span class=acl-fixed-case>D</span>ependencies to Logical Form with Negation Scope</a></strong><br><a href=/people/f/federico-fancellu/>Federico Fancellu</a>
|
<a href=/people/s/siva-reddy/>Siva Reddy</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a>
|
<a href=/people/b/bonnie-webber/>Bonnie Webber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1804><div class="card-body p-3 small">Many language technology applications would benefit from the ability to represent <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> and its scope on top of widely-used linguistic resources. In this paper, we investigate the possibility of obtaining a <a href=https://en.wikipedia.org/wiki/First-order_logic>first-order logic representation</a> with <a href=https://en.wikipedia.org/wiki/Scope_(computer_science)>negation scope</a> marked using Universal Dependencies. To do so, we enhance UDepLambda, a <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> that converts <a href=https://en.wikipedia.org/wiki/Dependency_graph>dependency graphs</a> to <a href=https://en.wikipedia.org/wiki/Logical_form>logical forms</a>. The resulting UDepLambda is able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to <a href=https://en.wikipedia.org/wiki/Negation>negation</a> but also to <a href=https://en.wikipedia.org/wiki/Universal_quantification>universal quantification</a> and other complex semantic phenomena. The initial conversion we did for <a href=https://en.wikipedia.org/wiki/English_language>English</a> is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as <a href=https://en.wikipedia.org/wiki/Universal_quantification>universal quantifiers</a>.<i>Universal Dependencies</i>. To do so, we enhance <i>UDepLambda</i>, a framework that converts dependency graphs to logical forms. The resulting <i>UDepLambda<tex-math>\\lnot</tex-math>\n </i>\n\nis able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to negation but also to universal quantification and other complex semantic phenomena. The initial conversion we did for English is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as universal quantifiers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1805/>Meaning Banking beyond Events and Roles</a></strong><br><a href=/people/j/johan-bos/>Johan Bos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1805><div class="card-body p-3 small">In this talk I will discuss the analysis of several semantic phenomena that need meaning representations that can describe <a href=https://en.wikipedia.org/wiki/Context_(language_use)>attributes of propositional contexts</a>. I will do this in a version of <a href=https://en.wikipedia.org/wiki/Discourse_representation_theory>Discourse Representation Theory</a>, using a universal semantic tagset developed as part of a project that aims to produce a large meaning bank (a semantically-annotated corpus) for four languages (English, Dutch, German and Italian).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1806/>The Scope and Focus of Negation : A Complete Annotation Framework for <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a><span class=acl-fixed-case>I</span>talian</a></strong><br><a href=/people/b/begona-altuna/>Begoña Altuna</a>
|
<a href=/people/a/anne-lyse-minard/>Anne-Lyse Minard</a>
|
<a href=/people/m/manuela-speranza/>Manuela Speranza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1806><div class="card-body p-3 small">In this paper we present a complete framework for the annotation of negation in <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, which accounts for both negation scope and negation focus, and also for language-specific phenomena such as negative concord. In our view, the annotation of negation complements more comprehensive Natural Language Processing tasks, such as temporal information processing and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. We applied the proposed framework and the guidelines built on top of it to the annotation of written texts, namely news articles and <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, thus producing annotated data for a total of over 36,000 tokens.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1808 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1808/>Annotating Negation in Spanish Clinical Texts<span class=acl-fixed-case>S</span>panish Clinical Texts</a></strong><br><a href=/people/n/noa-p-cruz-diaz/>Noa Cruz</a>
|
<a href=/people/r/roser-morante/>Roser Morante</a>
|
<a href=/people/m/manuel-j-mana-lopez/>Manuel J. Maña López</a>
|
<a href=/people/j/jacinto-mata-vazquez/>Jacinto Mata Vázquez</a>
|
<a href=/people/c/carlos-l-parra-calderon/>Carlos L. Parra Calderón</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1808><div class="card-body p-3 small">In this paper we present on-going work on annotating <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> in Spanish clinical documents. A corpus of anamnesis and radiology reports has been annotated by two domain expert annotators with negation markers and negated events. The <a href=https://en.wikipedia.org/wiki/Dice_coefficient>Dice coefficient</a> for <a href=https://en.wikipedia.org/wiki/Inter-annotator_agreement>inter-annotator agreement</a> is higher than 0.94 for negation markers and higher than 0.72 for negated events. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> will be publicly released when the annotation process is finished, constituting the first <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> annotated with <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a> for Spanish clinical reports available for the NLP community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1810.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1810 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1810 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-1810" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-1810/>An open-source tool for negation detection : a maximum-margin approach</a></strong><br><a href=/people/m/martine-enger/>Martine Enger</a>
|
<a href=/people/e/erik-velldal/>Erik Velldal</a>
|
<a href=/people/l/lilja-ovrelid/>Lilja Øvrelid</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1810><div class="card-body p-3 small">This paper presents an <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source toolkit</a> for negation detection. It identifies negation cues and their corresponding scope in either raw or parsed text using maximum-margin classification. The <a href=https://en.wikipedia.org/wiki/System>system</a> design draws on best practice from the existing literature on negation detection, aiming for a simple and portable system that still achieves competitive performance. Pre-trained models and experimental results are provided for <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div></div><hr><div id=w17-19><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-19.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-19/>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1900/>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</a></strong><br><a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1901.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1901 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1901/>Compositional Semantics using Feature-Based Models from <a href=https://en.wikipedia.org/wiki/WordNet>WordNet</a><span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et</a></strong><br><a href=/people/p/pablo-gamallo/>Pablo Gamallo</a>
|
<a href=/people/m/martin-pereira-farina/>Martín Pereira-Fariña</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1901><div class="card-body p-3 small">This article describes a method to build semantic representations of composite expressions in a compositional way by using WordNet relations to represent the meaning of words. The meaning of a target word is modelled as a vector in which its semantically related words are assigned weights according to both the type of the relationship and the distance to the target word. Word vectors are compositionally combined by syntactic dependencies. Each syntactic dependency triggers two complementary compositional functions : the named head function and dependent function. The experiments show that the proposed compositional method outperforms the state-of-the-art for both intransitive subject-verb and transitive subject-verb-object constructions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-1902" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-1902/>Automated WordNet Construction Using Word Embeddings<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et Construction Using Word Embeddings</a></strong><br><a href=/people/m/mikhail-khodak/>Mikhail Khodak</a>
|
<a href=/people/a/andrej-risteski/>Andrej Risteski</a>
|
<a href=/people/c/christiane-fellbaum/>Christiane Fellbaum</a>
|
<a href=/people/s/sanjeev-arora/>Sanjeev Arora</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1902><div class="card-body p-3 small">We present a fully unsupervised method for automated construction of WordNets based upon recent advances in distributional representations of sentences and word-senses combined with readily available machine translation tools. The approach requires very few linguistic resources and is thus extensible to multiple target languages. To evaluate our method we construct two 600-word testsets for word-to-synset matching in <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> using native speakers and evaluate the performance of our method along with several other recent approaches. Our method exceeds the best language-specific and multi-lingual automated WordNets in <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> for both languages. The databases we construct for <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, both languages without large publicly available manually constructed WordNets, will be publicly released along with the testsets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1903/>Improving Verb Metaphor Detection by Propagating Abstractness to Words, Phrases and Individual Senses</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1903><div class="card-body p-3 small">Abstract words refer to things that can not be seen, heard, felt, smelled, or tasted as opposed to concrete words. Among other <a href=https://en.wikipedia.org/wiki/Application_software>applications</a>, the degree of <a href=https://en.wikipedia.org/wiki/Abstraction>abstractness</a> has been shown to be a useful information for metaphor detection. Our contribution to this topic are as follows : i) we compare supervised techniques to learn and extend <a href=https://en.wikipedia.org/wiki/Abstraction>abstractness ratings</a> for huge vocabularies ii) we learn and investigate norms for larger units by propagating <a href=https://en.wikipedia.org/wiki/Abstraction>abstractness</a> to verb-noun pairs which lead to better metaphor detection iii) we overcome the limitation of learning a single rating per word and show that multi-sense abstractness ratings are potentially useful for metaphor detection. Finally, with this paper we publish automatically created abstractness norms for 3million English words and multi-words as well as automatically created sense specific abstractness ratings</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1904/>Improving Clinical Diagnosis Inference through Integration of Structured and Unstructured Knowledge</a></strong><br><a href=/people/y/yuan-ling/>Yuan Ling</a>
|
<a href=/people/y/yuan-an/>Yuan An</a>
|
<a href=/people/s/sadid-a-hasan/>Sadid Hasan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1904><div class="card-body p-3 small">This paper presents a novel approach to the task of automatically inferring the most probable diagnosis from a given clinical narrative. Structured Knowledge Bases (KBs) can be useful for such complex <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> but not sufficient. Hence, we leverage a vast amount of unstructured free text to integrate with structured KBs. The key innovative ideas include building a concept graph from both structured and unstructured knowledge sources and ranking the diagnosis concepts using the enhanced word embedding vectors learned from integrated sources. Experiments on the TREC CDS and HumanDx datasets showed that our methods improved the results of clinical diagnosis inference.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1905/>Classifying Lexical-semantic Relationships by Exploiting Sense / Concept Representations</a></strong><br><a href=/people/k/kentaro-kanada/>Kentaro Kanada</a>
|
<a href=/people/t/tetsunori-kobayashi/>Tetsunori Kobayashi</a>
|
<a href=/people/y/yoshihiko-hayashi/>Yoshihiko Hayashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1905><div class="card-body p-3 small">This paper proposes a method for classifying the type of lexical-semantic relation between a given pair of words. Given an inventory of target relationships, this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> can be seen as a multi-class classification problem. We train a supervised classifier by assuming : (1) a specific type of lexical-semantic relation between a pair of words would be indicated by a carefully designed set of relation-specific similarities associated with the words ; and (2) the similarities could be effectively computed by sense representations (sense / concept embeddings). The experimental results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> clearly outperforms an existing state-of-the-art method that does not utilize sense / concept embeddings, thereby demonstrating the effectiveness of the sense representations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1906/>Supervised and unsupervised approaches to measuring usage similarity</a></strong><br><a href=/people/m/milton-king/>Milton King</a>
|
<a href=/people/p/paul-cook/>Paul Cook</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1906><div class="card-body p-3 small">Usage similarity (USim) is an approach to determining word meaning in context that does not rely on a sense inventory. Instead, pairs of usages of a target lemma are rated on a scale. In this paper we propose unsupervised approaches to USim based on embeddings for words, contexts, and sentences, and achieve state-of-the-art results over two USim datasets. We further consider supervised approaches to USim, and find that although they outperform unsupervised approaches, they are unable to generalize to lemmas that are unseen in the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1908/>Creating and Validating Multilingual Semantic Representations for Six Languages : Expert versus Non-Expert Crowds</a></strong><br><a href=/people/m/mahmoud-el-haj/>Mahmoud El-Haj</a>
|
<a href=/people/p/paul-rayson/>Paul Rayson</a>
|
<a href=/people/s/scott-s-l-piao/>Scott Piao</a>
|
<a href=/people/s/stephen-wattam/>Stephen Wattam</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1908><div class="card-body p-3 small">Creating high-quality wide-coverage multilingual semantic lexicons to support knowledge-based approaches is a challenging time-consuming manual task. This has traditionally been performed by <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic experts</a> : a slow and expensive process. We present an experiment in which we adapt and evaluate crowdsourcing methods employing native speakers to generate a list of coarse-grained senses under a common multilingual semantic taxonomy for sets of words in six languages. 451 non-experts (including 427 Mechanical Turk workers) and 15 expert participants semantically annotated 250 words manually for <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a>, <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, Portuguese and Urdu lexicons. In order to avoid erroneous (spam) crowdsourced results, we used a novel task-specific two-phase filtering process where users were asked to identify synonyms in the target language, and remove erroneous senses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1910.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1910 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1910 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-1910" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-1910/>One Representation per Word-Does it make Sense for Composition?</a></strong><br><a href=/people/t/thomas-kober/>Thomas Kober</a>
|
<a href=/people/j/julie-weeds/>Julie Weeds</a>
|
<a href=/people/j/john-wilkie/>John Wilkie</a>
|
<a href=/people/j/jeremy-reffin/>Jeremy Reffin</a>
|
<a href=/people/d/david-weir/>David Weir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1910><div class="card-body p-3 small">In this paper, we investigate whether an a priori disambiguation of word senses is strictly necessary or whether the meaning of a word in context can be disambiguated through <a href=https://en.wikipedia.org/wiki/Composition_(language)>composition</a> alone. We evaluate the performance of off-the-shelf single-vector and multi-sense vector models on a benchmark phrase similarity task and a novel task for word-sense discrimination. We find that single-sense vector models perform as well or better than multi-sense vector models despite arguably less clean elementary representations. Our findings furthermore show that simple composition functions such as <a href=https://en.wikipedia.org/wiki/Pointwise_addition>pointwise addition</a> are able to recover sense specific information from a single-sense vector model remarkably well.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1911/>Elucidating Conceptual Properties from Word Embeddings</a></strong><br><a href=/people/k/kyoung-rok-jang/>Kyoung-Rok Jang</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-Hyon Myaeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1911><div class="card-body p-3 small">In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> of identifying the components (i.e. dimensions) of word embeddings that strongly signifies properties of a word. By elucidating such properties hidden in <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, we could make <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> more interpretable, and also could perform property-based meaning comparison. With the capability, we can answer questions like To what degree a given word has the property cuteness? or In what perspective two words are similar?. We verify our method by examining how the strength of property-signifying components correlates with the degree of prototypicality of a target word.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1912/>TTCS^ : a Vectorial Resource for Computing Conceptual Similarity<span class=acl-fixed-case>TTCS</span><span class=tex-math><sup>ℰ</sup></span>: a Vectorial Resource for Computing Conceptual Similarity</a></strong><br><a href=/people/e/enrico-mensa/>Enrico Mensa</a>
|
<a href=/people/d/daniele-p-radicioni/>Daniele P. Radicioni</a>
|
<a href=/people/a/antonio-lieto/>Antonio Lieto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1912><div class="card-body p-3 small">In this paper we introduce the TTCS^, a linguistic resource that relies on <a href=https://en.wikipedia.org/wiki/BabelNet>BabelNet</a>, NASARI and <a href=https://en.wikipedia.org/wiki/ConceptNet>ConceptNet</a>, that has now been used to compute the conceptual similarity between concept pairs. The conceptual representation herein provides uniform access to concepts based on BabelNet synset IDs, and consists of a vector-based semantic representation which is compliant with the Conceptual Spaces, a geometric framework for common-sense knowledge representation and reasoning. The TTCS^ has been evaluated in a preliminary experimentation on a conceptual similarity task.<tex-math>^{\\mathcal{E}}</tex-math>, a linguistic resource that relies on BabelNet, NASARI and ConceptNet, that has now been used to compute the conceptual similarity between concept pairs. The conceptual representation herein provides uniform access to concepts based on BabelNet synset IDs, and consists of a vector-based semantic representation which is compliant with the Conceptual Spaces, a geometric framework for common-sense knowledge representation and reasoning. The TTCS<tex-math>^{\\mathcal{E}}</tex-math> has been evaluated in a preliminary experimentation on a conceptual similarity task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1913/>Measuring the Italian-English lexical gap for <a href=https://en.wikipedia.org/wiki/Action_verb>action verbs</a> and its impact on translation<span class=acl-fixed-case>I</span>talian-<span class=acl-fixed-case>E</span>nglish lexical gap for action verbs and its impact on translation</a></strong><br><a href=/people/l/lorenzo-gregori/>Lorenzo Gregori</a>
|
<a href=/people/a/alessandro-panunzi/>Alessandro Panunzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1913><div class="card-body p-3 small">This paper describes a method to measure the lexical gap of action verbs in <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> by using the IMAGACT ontology of action. The fine-grained categorization of action concepts of the data source allowed to have wide overview of the relation between concepts in the two languages. The calculated <a href=https://en.wikipedia.org/wiki/Lexical_gap>lexical gap</a> for both <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> is about 30 % of the action concepts, much higher than previous results. Beyond this general numbers a deeper analysis has been performed in order to evaluate the impact that <a href=https://en.wikipedia.org/wiki/Lexical_gap>lexical gaps</a> can have on <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In particular a distinction has been made between the cases in which the presence of a <a href=https://en.wikipedia.org/wiki/Lexical_gap>lexical gap</a> affects translation correctness and <a href=https://en.wikipedia.org/wiki/Completeness_(logic)>completeness</a> at a <a href=https://en.wikipedia.org/wiki/Semantics>semantic level</a>. The results highlight a high percentage of concepts that can be considered hard to translate (about 18 % from English to <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and 20 % from <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> to English) and confirms that <a href=https://en.wikipedia.org/wiki/Action_verb>action verbs</a> are a critical lexical class for translation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1914/>Word Sense Filtering Improves Embedding-Based Lexical Substitution</a></strong><br><a href=/people/a/anne-cocos/>Anne Cocos</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1914><div class="card-body p-3 small">The role of <a href=https://en.wikipedia.org/wiki/Word_sense_disambiguation>word sense disambiguation</a> in <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> has been questioned due to the high performance of vector space models which propose good substitutes without explicitly accounting for <a href=https://en.wikipedia.org/wiki/Word_sense>sense</a>. We show that a filtering mechanism based on a sense inventory optimized for <a href=https://en.wikipedia.org/wiki/Substituent>substitutability</a> can improve the results of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our sense inventory is constructed using a clustering method which generates paraphrase clusters that are congruent with lexical substitution annotations in a development set. The results show that <a href=https://en.wikipedia.org/wiki/Lexical_substitution>lexical substitution</a> can still benefit from <a href=https://en.wikipedia.org/wiki/Word_sense>senses</a> which can improve the output of vector space paraphrase ranking models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-1915.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-1915 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-1915 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-1915/>Supervised and Unsupervised Word Sense Disambiguation on Word Embedding Vectors of Unambigous Synonyms</a></strong><br><a href=/people/a/aleksander-wawer/>Aleksander Wawer</a>
|
<a href=/people/a/agnieszka-mykowiecka/>Agnieszka Mykowiecka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-1915><div class="card-body p-3 small">This paper compares two approaches to <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> using <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> trained on unambiguous synonyms. The first is <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> based on computing log probability from sequences of word embedding vectors, taking into account ambiguous word senses and guessing correct sense from context. The second method is supervised. We use a multilayer neural network model to learn a context-sensitive transformation that maps an input vector of ambiguous word into an output vector representing its sense. We evaluate both methods on corpora with manual annotations of word senses from the Polish wordnet (plWordnet).</div></div></div><hr><div id=w17-20><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-20.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-20/>Proceedings of the Sixth Workshop on Vision and Language</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2000/>Proceedings of the Sixth Workshop on Vision and Language</a></strong><br><a href=/people/a/anja-belz/>Anya Belz</a>
|
<a href=/people/e/erkut-erdem/>Erkut Erdem</a>
|
<a href=/people/k/katerina-pastra/>Katerina Pastra</a>
|
<a href=/people/k/krystian-mikolajczyk/>Krystian Mikolajczyk</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2003/>Learning to Recognize Animals by Watching Documentaries : Using <a href=https://en.wikipedia.org/wiki/Subtitle_(titling)>Subtitles</a> as Weak Supervision</a></strong><br><a href=/people/a/aparna-nurani-venkitasubramanian/>Aparna Nurani Venkitasubramanian</a>
|
<a href=/people/t/tinne-tuytelaars/>Tinne Tuytelaars</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2003><div class="card-body p-3 small">We investigate animal recognition models learned from wildlife video documentaries by using the weak supervision of the textual subtitles. This is a particularly challenging setting, since i) the animals occur in their natural habitat and are often largely occluded and ii) subtitles are to a large degree complementary to the visual content, providing a very weak supervisory signal. This is in contrast to most work on integrated vision and language in the literature, where textual descriptions are tightly linked to the image content, and often generated in a curated fashion for the task at hand. In particular, we investigate different image representations and models, including a support vector machine on top of activations of a pretrained convolutional neural network, as well as a Naive Bayes framework on a &#8216;bag-of-activations&#8217; image representation, where each element of the bag is considered separately. This representation allows key components in the image to be isolated, in spite of largely varying backgrounds and image clutter, without an <a href=https://en.wikipedia.org/wiki/Object_detection>object detection</a> or image segmentation step. The methods are evaluated based on how well they transfer to unseen camera-trap images captured across diverse topographical regions under different environmental conditions and illumination settings, involving a large domain shift.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2004/>Human Evaluation of Multi-modal Neural Machine Translation : A Case-Study on E-Commerce Listing Titles<span class=acl-fixed-case>E</span>-Commerce Listing Titles</a></strong><br><a href=/people/i/iacer-calixto/>Iacer Calixto</a>
|
<a href=/people/d/daniel-stein/>Daniel Stein</a>
|
<a href=/people/e/evgeny-matusov/>Evgeny Matusov</a>
|
<a href=/people/s/sheila-castilho/>Sheila Castilho</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2004><div class="card-body p-3 small">In this paper, we study how humans perceive the use of <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> as an additional knowledge source to machine-translate <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated product listings</a> in an <a href=https://en.wikipedia.org/wiki/E-commerce>e-commerce company</a>. We conduct a human evaluation where we assess how a multi-modal neural machine translation (NMT) model compares to two text-only approaches : a conventional state-of-the-art attention-based NMT and a phrase-based statistical machine translation (PBSMT) model. We evaluate translations obtained with different systems and also discuss the data set of user-generated product listings, which in our case comprises both product listings and associated images. We found that humans preferred translations obtained with a PBSMT system to both text-only and multi-modal NMT over 56 % of the time. Nonetheless, human evaluators ranked translations from a multi-modal NMT model as better than those of a text-only NMT over 88 % of the time, which suggests that <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> do help <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> in this use-case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2005/>The BreakingNews Dataset<span class=acl-fixed-case>B</span>reaking<span class=acl-fixed-case>N</span>ews Dataset</a></strong><br><a href=/people/a/arnau-ramisa/>Arnau Ramisa</a>
|
<a href=/people/f/fei-yan/>Fei Yan</a>
|
<a href=/people/f/francesc-moreno-noguer/>Francesc Moreno-Noguer</a>
|
<a href=/people/k/krystian-mikolajczyk/>Krystian Mikolajczyk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2005><div class="card-body p-3 small">We present BreakingNews, a novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with approximately 100 K news articles including images, text and captions, and enriched with heterogeneous meta-data (e.g. GPS coordinates and popularity metrics). The tenuous connection between the images and text in news data is appropriate to take work at the intersection of <a href=https://en.wikipedia.org/wiki/Computer_vision>Computer Vision</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> to the next step, hence we hope this dataset will help spur progress in the field.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2007/>Multi-Modal Fashion Product Retrieval</a></strong><br><a href=/people/a/antonio-rubio-romano/>Antonio Rubio Romano</a>
|
<a href=/people/l/longlong-yu/>LongLong Yu</a>
|
<a href=/people/e/edgar-simo-serra/>Edgar Simo-Serra</a>
|
<a href=/people/f/francesc-moreno-noguer/>Francesc Moreno-Noguer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2007><div class="card-body p-3 small">Finding a product in the <a href=https://en.wikipedia.org/wiki/Fashion>fashion world</a> can be a daunting task. Everyday, e-commerce sites are updating with thousands of <a href=https://en.wikipedia.org/wiki/Digital_image>images</a> and their associated metadata (textual information), deepening the problem. In this paper, we leverage both the images and textual metadata and propose a joint multi-modal embedding that maps both the <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> and images into a common latent space. Distances in the latent space correspond to similarity between products, allowing us to effectively perform retrieval in this latent space. We compare against existing approaches and show significant improvements in retrieval tasks on a large-scale e-commerce dataset.</div></div></div><hr><div id=w17-22><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-22.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-22/>Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2200/>Proceedings of the Joint <span class=acl-fixed-case>SIGHUM</span> Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</a></strong><br><a href=/people/b/beatrice-alex/>Beatrice Alex</a>
|
<a href=/people/s/stefania-degaetano-ortlieb/>Stefania Degaetano-Ortlieb</a>
|
<a href=/people/a/anna-feldman/>Anna Feldman</a>
|
<a href=/people/a/anna-kazantseva/>Anna Kazantseva</a>
|
<a href=/people/n/nils-reiter/>Nils Reiter</a>
|
<a href=/people/s/stan-szpakowicz/>Stan Szpakowicz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2201/>Metaphor Detection in a Poetry Corpus</a></strong><br><a href=/people/v/vaibhav-kesarwani/>Vaibhav Kesarwani</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a>
|
<a href=/people/s/stan-szpakowicz/>Stan Szpakowicz</a>
|
<a href=/people/c/chris-tanasescu/>Chris Tanasescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2201><div class="card-body p-3 small">Metaphor is indispensable in <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a>. It showcases the poet&#8217;s creativity, and contributes to the overall emotional pertinence of the poem while honing its specific rhetorical impact. Previous work on metaphor detection relies on either rule-based or statistical models, none of them applied to <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a>. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> focuses on metaphor detection in a <a href=https://en.wikipedia.org/wiki/Text_corpus>poetry corpus</a>. It combines rule-based and statistical models (word embeddings) to develop a new classification system. Our <a href=https://en.wikipedia.org/wiki/System>system</a> has achieved a <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a> of 0.759 and a <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>recall</a> of 0.804 in identifying one type of <a href=https://en.wikipedia.org/wiki/Metaphor>metaphor</a> in <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2202/>Machine Translation and Automated Analysis of the Sumerian Language<span class=acl-fixed-case>S</span>umerian Language</a></strong><br><a href=/people/e/emilie-page-perron/>Émilie Pagé-Perron</a>
|
<a href=/people/m/maria-sukhareva/>Maria Sukhareva</a>
|
<a href=/people/i/ilya-khait/>Ilya Khait</a>
|
<a href=/people/c/christian-chiarcos/>Christian Chiarcos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2202><div class="card-body p-3 small">This paper presents a newly funded international project for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and automated analysis of ancient cuneiform languages where NLP specialists and Assyriologists collaborate to create an information retrieval system for <a href=https://en.wikipedia.org/wiki/Sumerian_language>Sumerian</a>. This research is conceived in response to the need to translate large numbers of administrative texts that are only available in transcription, in order to make them accessible to a wider audience. The <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> includes creation of a specialized NLP pipeline and also the use of linguistic linked open data to increase access to the results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2203/>Investigating the Relationship between <a href=https://en.wikipedia.org/wiki/Literary_genre>Literary Genres</a> and Emotional Plot Development</a></strong><br><a href=/people/e/evgeny-kim/>Evgeny Kim</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2203><div class="card-body p-3 small">Literary genres are commonly viewed as being defined in terms of content and <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>stylistic features</a>. In this paper, we focus on one particular class of <a href=https://en.wikipedia.org/wiki/Lexicon>lexical features</a>, namely <a href=https://en.wikipedia.org/wiki/Emotion>emotion information</a>, and investigate the hypothesis that <a href=https://en.wikipedia.org/wiki/Emotion>emotion-related information</a> correlates with particular <a href=https://en.wikipedia.org/wiki/Genre>genres</a>. Using genre classification as a testbed, we compare a model that computes lexicon-based emotion scores globally for complete stories with a model that tracks emotion arcs through stories on a subset of Project Gutenberg with five genres. Our main findings are : (a), the global emotion model is competitive with a large-vocabulary bag-of-words genre classifier (80%F1) ; (b), the emotion arc model shows a lower performance (59 % F1) but shows complementary behavior to the global model, as indicated by a very good performance of an oracle model (94 % F1) and an improved performance of an ensemble model (84 % F1) ; (c), genres differ in the extent to which stories follow the same emotional arcs, with particularly uniform behavior for anger (mystery) and fear (adventures, romance, humor, science fiction).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2204/>Enjambment Detection in a Large Diachronic Corpus of Spanish Sonnets<span class=acl-fixed-case>S</span>panish Sonnets</a></strong><br><a href=/people/p/pablo-ruiz/>Pablo Ruiz</a>
|
<a href=/people/c/clara-martinez-canton/>Clara Martínez Cantón</a>
|
<a href=/people/t/thierry-poibeau/>Thierry Poibeau</a>
|
<a href=/people/e/elena-gonzalez-blanco/>Elena González-Blanco</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2204><div class="card-body p-3 small">Enjambment takes place when a <a href=https://en.wikipedia.org/wiki/Syntax>syntactic unit</a> is broken up across two lines of <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a>, giving rise to different <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>stylistic effects</a>. In <a href=https://en.wikipedia.org/wiki/Spanish_literature>Spanish literary studies</a>, there are unclear points about the types of stylistic effects that can arise, and under which linguistic conditions. To systematically gather evidence about this, we developed a system to automatically identify <a href=https://en.wikipedia.org/wiki/Enjambment>enjambment</a> (and its type) in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. For evaluation, we manually annotated a <a href=https://en.wikipedia.org/wiki/Text_corpus>reference corpus</a> covering different periods. As a scholarly corpus to apply the tool, from public HTML sources we created a diachronic corpus covering four centuries of <a href=https://en.wikipedia.org/wiki/Sonnet>sonnets</a> (3750 poems), and we analyzed the occurrence of <a href=https://en.wikipedia.org/wiki/Enjambment>enjambment</a> across stanzaic boundaries in different periods. Besides, we found examples that highlight limitations in current definitions of <a href=https://en.wikipedia.org/wiki/Enjambment>enjambment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2205/>Plotting Markson’s Mistress<span class=acl-fixed-case>M</span>arkson’s “Mistress”</a></strong><br><a href=/people/c/conor-kelleher/>Conor Kelleher</a>
|
<a href=/people/m/mark-keane/>Mark Keane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2205><div class="card-body p-3 small">The post-modern novel Wittgenstein&#8217;s Mistress by David Markson (1988) presents the reader with a very challenging <a href=https://en.wikipedia.org/wiki/Nonlinear_narrative>non-linear narrative</a>, that itself appears to one of the novel&#8217;s themes. We present a distant reading of this work designed to complement a close reading of it by David Foster Wallace (1990). Using a combination of text analysis, entity recognition and networks, we plot repetitive structures in the novel&#8217;s narrative relating them to its critical analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2206/>Annotation Challenges for Reconstructing the Structural Elaboration of Middle Low German<span class=acl-fixed-case>M</span>iddle <span class=acl-fixed-case>L</span>ow <span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/n/nina-seemann/>Nina Seemann</a>
|
<a href=/people/m/marie-luis-merten/>Marie-Luis Merten</a>
|
<a href=/people/m/michaela-geierhos/>Michaela Geierhos</a>
|
<a href=/people/d/doris-tophinke/>Doris Tophinke</a>
|
<a href=/people/e/eyke-hullermeier/>Eyke Hüllermeier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2206><div class="card-body p-3 small">In this paper, we present the annotation challenges we have encountered when working on a <a href=https://en.wikipedia.org/wiki/Historical_language>historical language</a> that was undergoing elaboration processes. We especially focus on <a href=https://en.wikipedia.org/wiki/Syntactic_ambiguity>syntactic ambiguity</a> and <a href=https://en.wikipedia.org/wiki/Gradience>gradience</a> in <a href=https://en.wikipedia.org/wiki/Middle_Low_German>Middle Low German</a>, which causes uncertainty to some extent. Since current annotation tools consider construction contexts and the dynamics of the grammaticalization only partially, we plan to extend CorA-a web-based annotation tool for historical and other non-standard language data-to capture elaboration phenomena and annotator unsureness. Moreover, we seek to interactively learn morphological as well as syntactic annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2207/>Phonological Soundscapes in Medieval Poetry</a></strong><br><a href=/people/c/christopher-hench/>Christopher Hench</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2207><div class="card-body p-3 small">The oral component of <a href=https://en.wikipedia.org/wiki/Medieval_poetry>medieval poetry</a> was integral to its performance and reception. Yet many believe that the medieval voice has been forever lost, and any attempts at rediscovering it are doomed to failure due to <a href=https://en.wikipedia.org/wiki/Scribe>scribal practices</a>, manuscript mouvance, and linguistic normalization in editing practices. This paper offers a method to abstract from this noise and better understand relative differences in phonological soundscapes by considering syllable qualities. The presented <a href=https://en.wikipedia.org/wiki/Syllabification>syllabification method</a> and soundscape analysis offer themselves as cross-disciplinary tools for low-resource languages. As a case study, we examine medieval German lyric and argue that the heavily debated lyrical &#8216;I&#8217; follows a unique trajectory through soundscapes, shedding light on the performance and practice of these poets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2209/>Modeling intra-textual variation with <a href=https://en.wikipedia.org/wiki/Entropy>entropy</a> and surprisal : topical vs. stylistic patterns</a></strong><br><a href=/people/s/stefania-degaetano-ortlieb/>Stefania Degaetano-Ortlieb</a>
|
<a href=/people/e/elke-teich/>Elke Teich</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2209><div class="card-body p-3 small">We present a data-driven approach to investigate intra-textual variation by combining <a href=https://en.wikipedia.org/wiki/Entropy_(information_theory)>entropy</a> and surprisal. With this approach we detect <a href=https://en.wikipedia.org/wiki/Variation_(linguistics)>linguistic variation</a> based on phrasal lexico-grammatical patterns across sections of research articles. Entropy is used to detect patterns typical of specific sections. Surprisal is used to differentiate between more and less informationally-loaded patterns as well as type of information (topical vs. stylistic). While we here focus on <a href=https://en.wikipedia.org/wiki/Article_(publishing)>research articles</a> in biology / genetics, the <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is especially interesting for <a href=https://en.wikipedia.org/wiki/Digital_humanities>digital humanities scholars</a>, as it can be applied to any text type or domain and combined with additional variables (e.g. time, author or social group).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2212/>Speeding up corpus development for linguistic research : <a href=https://en.wikipedia.org/wiki/Language_documentation>language documentation</a> and acquisition in Romansh Tuatschin<span class=acl-fixed-case>R</span>omansh Tuatschin</a></strong><br><a href=/people/g/geraldine-walther/>Géraldine Walther</a>
|
<a href=/people/b/benoit-sagot/>Benoît Sagot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2212><div class="card-body p-3 small">In this paper, we present ongoing work for developing language resources and basic NLP tools for an undocumented variety of Romansh, in the context of a language documentation and language acquisition project. Our tools are meant to improve the speed and reliability of corpus annotations for noisy data involving large amounts of <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>, occurrences of child-speech and orthographic noise. Being able to increase the efficiency of language resource development for language documentation and acquisition research also constitutes a step towards solving the data sparsity issues with which researchers have been struggling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2214/>A Dataset for Sanskrit Word Segmentation<span class=acl-fixed-case>S</span>anskrit Word Segmentation</a></strong><br><a href=/people/a/amrith-krishna/>Amrith Krishna</a>
|
<a href=/people/p/pavankumar-satuluri/>Pavan Kumar Satuluri</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2214><div class="card-body p-3 small">The last decade saw a surge in digitisation efforts for ancient manuscripts in <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>. Due to various linguistic peculiarities inherent to the language, even the preliminary tasks such as <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> are non-trivial in <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>. Elegant models for <a href=https://en.wikipedia.org/wiki/Word_segmentation>Word Segmentation</a> in <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a> are indispensable for further syntactic and semantic processing of the manuscripts. Current works in <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> for <a href=https://en.wikipedia.org/wiki/Sanskrit>Sanskrit</a>, though commendable in their novelty, often have variations in their objective and evaluation criteria. In this work, we set the record straight. We formally define the objectives and the requirements for the word segmentation task. In order to encourage research in the field and to alleviate the time and effort required in pre-processing, we release a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 115,000 sentences for <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>. For each sentence in the dataset we include the input character sequence, ground truth segmentation, and additionally lexical and morphological information about all the phonetically possible segments for the given sentence. In this work, we also discuss the linguistic considerations made while generating the candidate space of the possible segments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2215/>Lexical Correction of Polish Twitter Political Data<span class=acl-fixed-case>P</span>olish <span class=acl-fixed-case>T</span>witter Political Data</a></strong><br><a href=/people/m/maciej-ogrodniczuk/>Maciej Ogrodniczuk</a>
|
<a href=/people/m/mateusz-kopec/>Mateusz Kopeć</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2215><div class="card-body p-3 small">Language processing architectures are often evaluated in near-to-perfect conditions with respect to processed content. The tools which perform sufficiently well on <a href=https://en.wikipedia.org/wiki/Electronic_publishing>electronic press</a>, <a href=https://en.wikipedia.org/wiki/Book>books</a> and other type of non-interactive content may poorly handle littered, colloquial and multilingual textual data which make the majority of communication today. This paper aims at investigating how <a href=https://en.wikipedia.org/wiki/Polish_language>Polish Twitter data</a> (in a slightly controlled &#8216;political&#8217; flavour) differs from expectation of linguistic tools and how they could be corrected to be ready for processing by standard language processing chains available for <a href=https://en.wikipedia.org/wiki/Polish_language>Polish</a>. The setting includes specialised components for spelling correction of tweets as well as hashtag and username decoding.</div></div></div><hr><div id=w17-23><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-23.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-23/>BioNLP 2017</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2300/><span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>NLP</span> 2017</a></strong><br><a href=/people/k/k-bretonnel-cohen/>Kevin Bretonnel Cohen</a>
|
<a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a>
|
<a href=/people/j/junichi-tsujii/>Junichi Tsujii</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2301/>Target word prediction and paraphasia classification in spoken discourse</a></strong><br><a href=/people/j/joel-adams/>Joel Adams</a>
|
<a href=/people/s/steven-bedrick/>Steven Bedrick</a>
|
<a href=/people/g/gerasimos-fergadiotis/>Gerasimos Fergadiotis</a>
|
<a href=/people/k/kyle-gorman/>Kyle Gorman</a>
|
<a href=/people/j/jan-van-santen/>Jan van Santen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2301><div class="card-body p-3 small">We present a system for automatically detecting and classifying phonologically anomalous productions in the <a href=https://en.wikipedia.org/wiki/Speech>speech</a> of individuals with <a href=https://en.wikipedia.org/wiki/Aphasia>aphasia</a>. Working from transcribed discourse samples, our system identifies <a href=https://en.wikipedia.org/wiki/Neologism>neologisms</a>, and uses a combination of string alignment and <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> to produce a lattice of plausible words that the speaker may have intended to produce. We then score this <a href=https://en.wikipedia.org/wiki/Lattice_(order)>lattice</a> according to various features, and attempt to determine whether the anomalous production represented a phonemic error or a genuine <a href=https://en.wikipedia.org/wiki/Neologism>neologism</a>. This approach has the potential to be expanded to consider other types of paraphasic errors, and could be applied to a wide variety of screening and therapeutic applications.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2302/>Extracting Drug-Drug Interactions with Attention CNNs<span class=acl-fixed-case>CNN</span>s</a></strong><br><a href=/people/m/masaki-asada/>Masaki Asada</a>
|
<a href=/people/m/makoto-miwa/>Makoto Miwa</a>
|
<a href=/people/y/yutaka-sasaki/>Yutaka Sasaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2302><div class="card-body p-3 small">We propose a novel attention mechanism for a Convolutional Neural Network (CNN)-based Drug-Drug Interaction (DDI) extraction model. CNNs have been shown to have a great potential on DDI extraction tasks ; however, <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a>, which emphasize important words in the sentence of a target-entity pair, have not been investigated with the CNNs despite the fact that <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanisms</a> are shown to be effective for a general domain relation classification task. We evaluated our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the Task 9.2 of the DDIExtraction-2013 shared task. As a result, our attention mechanism improved the performance of our base CNN-based DDI model, and the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieved an F-score of 69.12 %, which is competitive with the state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2303" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2303/>Insights into Analogy Completion from the Biomedical Domain</a></strong><br><a href=/people/d/denis-newman-griffis/>Denis Newman-Griffis</a>
|
<a href=/people/a/albert-m-lai/>Albert Lai</a>
|
<a href=/people/e/eric-fosler-lussier/>Eric Fosler-Lussier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2303><div class="card-body p-3 small">Analogy completion has been a popular task in recent years for evaluating the semantic properties of word embeddings, but the standard methodology makes a number of assumptions about <a href=https://en.wikipedia.org/wiki/Analogy>analogies</a> that do not always hold, either in recent benchmark datasets or when expanding into other domains. Through an analysis of analogies in the biomedical domain, we identify three assumptions : that of a Single Answer for any given <a href=https://en.wikipedia.org/wiki/Analogy>analogy</a>, that the pairs involved describe the Same Relationship, and that each pair is Informative with respect to the other. We propose modifying the standard <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to relax these assumptions by allowing for multiple correct answers, reporting MAP and MRR in addition to <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, and using multiple example pairs. We further present BMASS, a novel dataset for evaluating linguistic regularities in biomedical embeddings, and demonstrate that the relationships described in the dataset pose significant semantic challenges to current word embedding methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2304/>Deep learning for extracting protein-protein interactions from biomedical literature</a></strong><br><a href=/people/y/yifan-peng/>Yifan Peng</a>
|
<a href=/people/z/zhiyong-lu/>Zhiyong Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2304><div class="card-body p-3 small">State-of-the-art methods for protein-protein interaction (PPI) extraction are primarily feature-based or kernel-based by leveraging lexical and syntactic information. But how to incorporate such knowledge in the recent <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> remains an open question. In this paper, we propose a multichannel dependency-based convolutional neural network model (McDepCNN). It applies one channel to the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>embedding vector</a> of each word in the sentence, and another channel to the <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>embedding vector</a> of the head of the corresponding word. Therefore, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can use richer information obtained from different channels. Experiments on two public benchmarking datasets, AIMed and BioInfer, demonstrate that McDepCNN provides up to 6 % F1-score improvement over rich feature-based methods and single-kernel methods. In addition, McDepCNN achieves 24.4 % relative improvement in <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> over the state-of-the-art methods on cross-corpus evaluation and 12 % improvement in <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> over kernel-based methods on difficult instances. These results suggest that McDepCNN generalizes more easily over different corpora, and is capable of capturing long distance features in the sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2305/>Stacking With Auxiliary Features for Entity Linking in the Medical Domain</a></strong><br><a href=/people/n/nazneen-fatema-rajani/>Nazneen Fatema Rajani</a>
|
<a href=/people/m/mihaela-bornea/>Mihaela Bornea</a>
|
<a href=/people/k/ken-barker/>Ken Barker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2305><div class="card-body p-3 small">Linking spans of natural language text to concepts in a structured source is an important task for many problems. It allows intelligent systems to leverage rich knowledge available in those sources (such as concept properties and relations) to enhance the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of the mentions of these concepts in text. In the medical domain, it is common to link text spans to medical concepts in large, curated knowledge repositories such as the <a href=https://en.wikipedia.org/wiki/Unified_Medical_Language_System>Unified Medical Language System</a>. Different approaches have different strengths : some are precision-oriented, some recall-oriented ; some better at considering context but more prone to <a href=https://en.wikipedia.org/wiki/Hallucination>hallucination</a>. The variety of techniques suggests that <a href=https://en.wikipedia.org/wiki/Assembly_language>ensembling</a> could outperform component technologies at this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. In this paper, we describe our process for building a Stacking ensemble using additional, auxiliary features for Entity Linking in the medical domain. We report experiments that show that naive ensembling does not always outperform component Entity Linking systems, that stacking usually outperforms naive ensembling, and that auxiliary features added to the stacker further improve its performance on three distinct datasets. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> produces state-of-the-art results on several <a href=https://en.wikipedia.org/wiki/Data_set>medical datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2307/>Tackling Biomedical Text Summarization : OAQA at BioASQ 5B<span class=acl-fixed-case>OAQA</span> at <span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>ASQ</span> 5<span class=acl-fixed-case>B</span></a></strong><br><a href=/people/k/khyathi-chandu/>Khyathi Chandu</a>
|
<a href=/people/a/aakanksha-naik/>Aakanksha Naik</a>
|
<a href=/people/a/aditya-chandrasekar/>Aditya Chandrasekar</a>
|
<a href=/people/z/zi-yang/>Zi Yang</a>
|
<a href=/people/n/niloy-gupta/>Niloy Gupta</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2307><div class="card-body p-3 small">In this paper, we describe our participation in phase B of task 5b of the fifth edition of the annual BioASQ challenge, which includes answering factoid, list, yes-no and summary questions from biomedical data. We describe our techniques with an emphasis on ideal answer generation, where the goal is to produce a relevant, precise, non-redundant, query-oriented summary from multiple relevant documents. We make use of extractive summarization techniques to address this task and experiment with different biomedical ontologies and various algorithms including <a href=https://en.wikipedia.org/wiki/Agglomerative_clustering>agglomerative clustering</a>, Maximum Marginal Relevance (MMR) and sentence compression. We propose a novel word embedding based tf-idf similarity metric and a soft positional constraint which improve our system performance. We evaluate our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>techniques</a> on test batch 4 from the fourth edition of the challenge. Our best system achieves a ROUGE-2 score of 0.6534 and ROUGE-SU4 score of 0.6536.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2308/>Macquarie University at BioASQ 5b Query-based Summarisation Techniques for Selecting the Ideal Answers<span class=acl-fixed-case>M</span>acquarie <span class=acl-fixed-case>U</span>niversity at <span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>ASQ</span> 5b – Query-based Summarisation Techniques for Selecting the Ideal Answers</a></strong><br><a href=/people/d/diego-molla/>Diego Mollá</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2308><div class="card-body p-3 small">Macquarie University&#8217;s contribution to the BioASQ challenge (Task 5b Phase B) focused on the use of query-based extractive summarisation techniques for the generation of the ideal answers. Four runs were submitted, with approaches ranging from a <a href=https://en.wikipedia.org/wiki/Triviality_(mathematics)>trivial system</a> that selected the first n snippets, to the use of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning approaches</a> under a <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression framework</a>. Our experiments and the ROUGE results of the five test batches of BioASQ indicate surprisingly good results for the trivial approach. Overall, most of our runs on the first three test batches achieved the best ROUGE-SU4 results in the challenge.<tex-math>n</tex-math> snippets, to the use of deep learning approaches under a regression framework. Our experiments and the ROUGE results of the five test batches of BioASQ indicate surprisingly good results for the trivial approach. Overall, most of our runs on the first three test batches achieved the best ROUGE-SU4 results in the challenge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2309.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-2309/>Neural Question Answering at BioASQ 5B<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>ASQ</span> 5<span class=acl-fixed-case>B</span></a></strong><br><a href=/people/g/georg-wiese/>Georg Wiese</a>
|
<a href=/people/d/dirk-weissenborn/>Dirk Weissenborn</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2309><div class="card-body p-3 small">This paper describes our submission to the 2017 BioASQ challenge. We participated in Task B, Phase B which is concerned with biomedical question answering (QA). We focus on factoid and list question, using an extractive QA model, that is, we restrict our system to output substrings of the provided text snippets. At the core of our <a href=https://en.wikipedia.org/wiki/System>system</a>, we use FastQA, a state-of-the-art neural QA system. We extended it with biomedical word embeddings and changed its answer layer to be able to answer list questions in addition to factoid questions. We pre-trained the model on a large-scale open-domain QA dataset, SQuAD, and then fine-tuned the parameters on the BioASQ training set. With our approach, we achieve state-of-the-art results on factoid questions and competitive results on list questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2310" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2310/>End-to-End System for Bacteria Habitat Extraction</a></strong><br><a href=/people/f/farrokh-mehryary/>Farrokh Mehryary</a>
|
<a href=/people/k/kai-hakala/>Kai Hakala</a>
|
<a href=/people/s/suwisa-kaewphan/>Suwisa Kaewphan</a>
|
<a href=/people/j/jari-bjorne/>Jari Björne</a>
|
<a href=/people/t/tapio-salakoski/>Tapio Salakoski</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2310><div class="card-body p-3 small">We introduce an end-to-end system capable of <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named-entity detection</a>, <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> and relation extraction for extracting information about bacteria and their habitats from <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical literature</a>. Our system is based on <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a>, CRF classifiers and <a href=https://en.wikipedia.org/wiki/Vector_space_model>vector space models</a>. We train and evaluate the <a href=https://en.wikipedia.org/wiki/System>system</a> on the BioNLP 2016 Shared Task Bacteria Biotope data. The official evaluation shows that the joint performance of our entity detection and relation extraction models outperforms the winning team of the Shared Task by 19pp on F1-score, establishing a new top score for the task. We also achieve state-of-the-art results in the <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization task</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is open source and freely available at.<url>https://github.com/TurkuNLP/BHE</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2312 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2312/>Representation of complex terms in a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> structured by an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> for a normalization task</a></strong><br><a href=/people/a/arnaud-ferre/>Arnaud Ferré</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a>
|
<a href=/people/c/claire-nedellec/>Claire Nédellec</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2312><div class="card-body p-3 small">We propose in this paper a <a href=https://en.wikipedia.org/wiki/Semi-supervised_learning>semi-supervised method</a> for labeling terms of texts with concepts of a <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>domain ontology</a>. The method generates continuous vector representations of complex terms in a <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> structured by the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a>. The proposed method relies on a distributional semantics approach, which generates initial vectors for each of the extracted terms. Then these <a href=https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)>vectors</a> are embedded in the <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> constructed from the structure of the ontology. This embedding is carried out by training a <a href=https://en.wikipedia.org/wiki/Linear_model>linear model</a>. Finally, we apply a distance calculation to determine the proximity between vectors of terms and vectors of concepts and thus to assign <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology labels</a> to terms. We have evaluated the quality of these representations for a normalization task by using the concepts of an <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> as semantic labels. Normalization of terms is an important step to extract a part of the information containing in texts, but the <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a> generated might find other applications. The performance of this <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is comparable to that of the state of the art for this task of standardization, opening up encouraging prospects.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2313/>Improving Correlation with Human Judgments by Integrating Semantic Similarity with SecondOrder Vectors</a></strong><br><a href=/people/b/bridget-mcinnes/>Bridget McInnes</a>
|
<a href=/people/t/ted-pedersen/>Ted Pedersen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2313><div class="card-body p-3 small">Vector space methods that measure semantic similarity and relatedness often rely on distributional information such as cooccurrence frequencies or <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>statistical measures of association</a> to weight the importance of particular cooccurrences. In this paper, we extend these methods by incorporating a measure of <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> based on a human curated taxonomy into a secondorder vector representation. This results in a measure of <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a> that combines both the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> available in a corpusbased vector space representation with the semantic knowledge found in a biomedical ontology. Our results show that incorporating <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a> into a second order co-occurrence matrices improves correlation with human judgments for both similarity and relatedness, and that our method compares favorably to various different word embedding methods that have recently been evaluated on the same reference standards we have used.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2314/>Proactive Learning for Named Entity Recognition</a></strong><br><a href=/people/m/maolin-li/>Maolin Li</a>
|
<a href=/people/n/nhung-nguyen/>Nhung Nguyen</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2314><div class="card-body p-3 small">The goal of <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> is to minimise the cost of producing an <a href=https://en.wikipedia.org/wiki/Annotation>annotated dataset</a>, in which annotators are assumed to be perfect, i.e., they always choose the correct labels. However, in practice, annotators are not infallible, and they are likely to assign incorrect labels to some instances. Proactive learning is a generalisation of active learning that can model different kinds of <a href=https://en.wikipedia.org/wiki/Annotation>annotators</a>. Although <a href=https://en.wikipedia.org/wiki/Proactive_learning>proactive learning</a> has been applied to certain labelling tasks, such as text classification, there is little work on its application to named entity (NE) tagging. In this paper, we propose a proactive learning method for producing NE annotated corpora, using two annotators with different levels of expertise, and who charge different amounts based on their levels of experience. To optimise both cost and annotation quality, we also propose a mechanism to present multiple sentences to annotators at each iteration. Experimental results for several corpora show that our method facilitates the construction of high-quality NE labelled datasets at minimal cost.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2315/>Biomedical Event Extraction using Abstract Meaning Representation<span class=acl-fixed-case>A</span>bstract <span class=acl-fixed-case>M</span>eaning <span class=acl-fixed-case>R</span>epresentation</a></strong><br><a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/d/daniel-marcu/>Daniel Marcu</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2315><div class="card-body p-3 small">We propose a novel, Abstract Meaning Representation (AMR) based approach to identifying molecular events / interactions in biomedical text. Our key contributions are : (1) an empirical validation of our hypothesis that an event is a subgraph of the AMR graph, (2) a neural network-based model that identifies such an event subgraph given an AMR, and (3) a distant supervision based approach to gather additional training data. We evaluate our approach on the 2013 Genia Event Extraction dataset and show promising results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2316.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2316 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2316 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2316/>Detecting Personal Medication Intake in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> : An Annotated Corpus and Baseline Classification System<span class=acl-fixed-case>T</span>witter: An Annotated Corpus and Baseline Classification System</a></strong><br><a href=/people/a/ari-klein/>Ari Klein</a>
|
<a href=/people/a/abeed-sarker/>Abeed Sarker</a>
|
<a href=/people/m/masoud-rouhizadeh/>Masoud Rouhizadeh</a>
|
<a href=/people/k/karen-oconnor/>Karen O’Connor</a>
|
<a href=/people/g/graciela-gonzalez/>Graciela Gonzalez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2316><div class="card-body p-3 small">Social media sites (e.g., <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>) have been used for surveillance of drug safety at the population level, but studies that focus on the effects of medications on specific sets of individuals have had to rely on other sources of data. Mining social media data for this in-formation would require the ability to distinguish indications of personal medication in-take in this <a href=https://en.wikipedia.org/wiki/Media_(communication)>media</a>. Towards that end, this paper presents an annotated corpus that can be used to train <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning systems</a> to determine whether a tweet that mentions a medication indicates that the individual posting has taken that medication at a specific time. To demonstrate the utility of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> as a training set, we present baseline results of <a href=https://en.wikipedia.org/wiki/Supervised_classification>supervised classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2317.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2317 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2317 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2317/>Unsupervised Context-Sensitive Spelling Correction of Clinical Free-Text with Word and Character N-Gram Embeddings</a></strong><br><a href=/people/p/pieter-fivez/>Pieter Fivez</a>
|
<a href=/people/s/simon-suster/>Simon Šuster</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2317><div class="card-body p-3 small">We present an unsupervised context-sensitive spelling correction method for clinical free-text that uses word and character n-gram embeddings. Our method generates misspelling replacement candidates and ranks them according to their semantic fit, by calculating a weighted cosine similarity between the vectorized representation of a candidate and the misspelling context. We greatly outperform two baseline off-the-shelf spelling correction tools on a manually annotated MIMIC-III test set, and counter the frequency bias of an optimized noisy channel model, showing that neural embeddings can be successfully exploited to include context-awareness in a spelling correction model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2318/>Characterization of Divergence in Impaired Speech of ALS Patients<span class=acl-fixed-case>ALS</span> Patients</a></strong><br><a href=/people/a/archna-bhatia/>Archna Bhatia</a>
|
<a href=/people/b/bonnie-dorr/>Bonnie Dorr</a>
|
<a href=/people/k/kristy-hollingshead/>Kristy Hollingshead</a>
|
<a href=/people/s/samuel-l-phillips/>Samuel L. Phillips</a>
|
<a href=/people/b/barbara-mckenzie/>Barbara McKenzie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2318><div class="card-body p-3 small">Approximately 80 % to 95 % of patients with Amyotrophic Lateral Sclerosis (ALS) eventually develop speech impairments, such as defective articulation, slow laborious speech and <a href=https://en.wikipedia.org/wiki/Hypernasal_speech>hypernasality</a>. The relationship between <a href=https://en.wikipedia.org/wiki/Speech_disorder>impaired speech</a> and asymptomatic speech may be seen as a divergence from a baseline. This relationship can be characterized in terms of measurable combinations of <a href=https://en.wikipedia.org/wiki/Phonology>phonological characteristics</a> that are indicative of the degree to which the two diverge. We demonstrate that divergence measurements based on <a href=https://en.wikipedia.org/wiki/Phonology>phonological characteristics of speech</a> correlate with <a href=https://en.wikipedia.org/wiki/Physiology>physiological assessments of ALS</a>. Speech-based assessments offer benefits over commonly-used physiological assessments in that they are inexpensive, non-intrusive, and do not require trained clinical personnel for administering and interpreting the results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2319.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2319 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2319 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2319/>Deep Learning for Punctuation Restoration in Medical Reports</a></strong><br><a href=/people/w/wael-salloum/>Wael Salloum</a>
|
<a href=/people/g/gregory-finley/>Greg Finley</a>
|
<a href=/people/e/erik-edwards/>Erik Edwards</a>
|
<a href=/people/m/mark-miller/>Mark Miller</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2319><div class="card-body p-3 small">In clinical dictation, speakers try to be as concise as possible to save time, often resulting in utterances without explicit punctuation commands. Since the end product of a dictated report, e.g. an out-patient letter, does require correct <a href=https://en.wikipedia.org/wiki/Orthography>orthography</a>, including exact <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>, the latter need to be restored, preferably by automated means. This paper describes a method for punctuation restoration based on a state-of-the-art stack of NLP and machine learning techniques including B-RNNs with an attention mechanism and late fusion, as well as a feature extraction technique tailored to the processing of medical terminology using a novel vocabulary reduction model. To the best of our knowledge, the resulting performance is superior to that reported in prior art on similar <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2320.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2320 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2320 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2320/>Unsupervised Domain Adaptation for Clinical Negation Detection</a></strong><br><a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/h/hadi-amiri/>Hadi Amiri</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2320><div class="card-body p-3 small">Detecting negated concepts in clinical texts is an important part of NLP information extraction systems. However, generalizability of negation systems is lacking, as cross-domain experiments suffer dramatic performance losses. We examine the performance of multiple unsupervised domain adaptation algorithms on clinical negation detection, finding only modest gains that fall well short of in-domain performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2321.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2321 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2321 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2321/>BioCreative VI Precision Medicine Track : creating a training corpus for mining protein-protein interactions affected by mutations<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>C</span>reative <span class=acl-fixed-case>VI</span> Precision Medicine Track: creating a training corpus for mining protein-protein interactions affected by mutations</a></strong><br><a href=/people/r/rezarta-islamaj-dogan/>Rezarta Islamaj Doğan</a>
|
<a href=/people/a/andrew-chatr-aryamontri/>Andrew Chatr-aryamontri</a>
|
<a href=/people/s/sun-kim/>Sun Kim</a>
|
<a href=/people/c/chih-hsuan-wei/>Chih-Hsuan Wei</a>
|
<a href=/people/y/yifan-peng/>Yifan Peng</a>
|
<a href=/people/d/donald-c-comeau/>Donald Comeau</a>
|
<a href=/people/z/zhiyong-lu/>Zhiyong Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2321><div class="card-body p-3 small">The Precision Medicine Track in BioCre-ative VI aims to bring together the Bi-oNLP community for a novel challenge focused on mining the biomedical litera-ture in search of mutations and protein-protein interactions (PPI). In order to support this track with an effective train-ing dataset with limited curator time, the track organizers carefully reviewed Pub-Med articles from two different sources : curated public PPI databases, and the re-sults of state-of-the-art public text mining tools. We detail here the <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a>, manual review and annotation process and describe this training corpus charac-teristics. We also describe a corpus per-formance baseline. This analysis will provide useful information to developers and researchers for comparing and devel-oping innovative text mining approaches for the BioCreative VI challenge and other Precision Medicine related applica-tions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2322.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2322 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2322 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2322/>Painless Relation Extraction with Kindred</a></strong><br><a href=/people/j/jake-lever/>Jake Lever</a>
|
<a href=/people/s/steven-jm-jones/>Steven Jones</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2322><div class="card-body p-3 small">Relation extraction methods are essential for creating robust text mining tools to help researchers find useful knowledge in the vast published literature. Easy-to-use and generalizable methods are needed to encourage an ecosystem in which researchers can easily use shared resources and build upon each others&#8217; methods. We present the Kindred Python package for <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a>. It builds upon methods from the most successful tools in the recent BioNLP Shared Task to predict high-quality predictions with low computational cost. It also integrates with PubAnnotation, PubTator, and BioNLP Shared Task data in order to allow easy development and application of relation extraction models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2323.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2323 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2323 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2323/>Noise Reduction Methods for Distantly Supervised Biomedical Relation Extraction</a></strong><br><a href=/people/g/gang-li/>Gang Li</a>
|
<a href=/people/c/cathy-wu/>Cathy Wu</a>
|
<a href=/people/k/k-vijay-shanker/>K. Vijay-Shanker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2323><div class="card-body p-3 small">Distant supervision has been applied to automatically generate labeled data for biomedical relation extraction. Noise exists in both positively and negatively-labeled data and affects the performance of supervised machine learning methods. In this paper, we propose three novel <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> based on the notion of proximity, trigger word and confidence of patterns to leverage lexical and syntactic information to reduce the level of noise in the distantly labeled data. Experiments on three different tasks, extraction of protein-protein-interaction, miRNA-gene regulation relation and protein-localization event, show that the proposed methods can improve the <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> over the baseline by 6, 10 and 14 points for the three tasks, respectively. We also show that when the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are configured to output high-confidence results, high precisions can be obtained using the proposed methods, making them promising for facilitating manual curation for <a href=https://en.wikipedia.org/wiki/Database>databases</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2324.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2324 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2324 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2324/>Role-Preserving Redaction of Medical Records to Enable Ontology-Driven Processing</a></strong><br><a href=/people/s/seth-polsley/>Seth Polsley</a>
|
<a href=/people/a/atif-tahir/>Atif Tahir</a>
|
<a href=/people/m/muppala-raju/>Muppala Raju</a>
|
<a href=/people/a/akintayo-akinleye/>Akintayo Akinleye</a>
|
<a href=/people/d/duane-steward/>Duane Steward</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2324><div class="card-body p-3 small">Electronic medical records (EMR) have largely replaced hand-written patient files in healthcare. The growing pool of <a href=https://en.wikipedia.org/wiki/Electronic_health_record>EMR data</a> presents a significant resource in <a href=https://en.wikipedia.org/wiki/Medical_research>medical research</a>, but the U.S. Health Insurance Portability and Accountability Act (HIPAA) mandates redacting <a href=https://en.wikipedia.org/wiki/Medical_record>medical records</a> before performing any analysis on the same. This process complicates obtaining <a href=https://en.wikipedia.org/wiki/Medical_record>medical data</a> and can remove much useful information from the record. As part of a larger project involving ontology-driven medical processing, we employ a method of recognizing protected health information (PHI) that maps to <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological terms</a>. We then use the relationships defined in the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> to redact medical texts so that roles and semantics of terms are retained without compromising <a href=https://en.wikipedia.org/wiki/Anonymity>anonymity</a>. The method is evaluated by clinical experts on several hundred <a href=https://en.wikipedia.org/wiki/Medical_record>medical documents</a>, achieving up to a 98.8 % <a href=https://en.wikipedia.org/wiki/F-score>f-score</a>, and has already shown promise for retaining semantic information in later processing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2325.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2325 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2325 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2325/>Annotation of pain and anesthesia events for surgery-related processes and outcomes extraction</a></strong><br><a href=/people/w/wen-wai-yim/>Wen-wai Yim</a>
|
<a href=/people/d/dario-tedesco/>Dario Tedesco</a>
|
<a href=/people/c/catherine-curtin/>Catherine Curtin</a>
|
<a href=/people/t/tina-hernandez-boussard/>Tina Hernandez-Boussard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2325><div class="card-body p-3 small">Pain and anesthesia information are crucial elements to identifying surgery-related processes and outcomes. However <a href=https://en.wikipedia.org/wiki/Pain>pain</a> is not consistently recorded in the <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic medical record</a>. Even when recorded, the rich complex granularity of the pain experience may be lost. Similarly, anesthesia information is recorded using local electronic collection systems ; though the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and completeness of the information is unknown. We propose an annotation schema to capture <a href=https://en.wikipedia.org/wiki/Pain>pain</a>, <a href=https://en.wikipedia.org/wiki/Pain_management>pain management</a>, and anesthesia event information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2326.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2326 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2326 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2326/>Identifying Comparative Structures in Biomedical Text</a></strong><br><a href=/people/s/samir-gupta/>Samir Gupta</a>
|
<a href=/people/a/a-s-m-ashique-mahmood/>A.S.M. Ashique Mahmood</a>
|
<a href=/people/k/karen-ross/>Karen Ross</a>
|
<a href=/people/c/cathy-wu/>Cathy Wu</a>
|
<a href=/people/k/k-vijay-shanker/>K. Vijay-Shanker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2326><div class="card-body p-3 small">Comparison sentences are very commonly used by authors in <a href=https://en.wikipedia.org/wiki/Medical_literature>biomedical literature</a> to report results of experiments. In such comparisons, authors typically make observations under two different scenarios. In this paper, we present a <a href=https://en.wikipedia.org/wiki/System>system</a> to automatically identify such comparative sentences and their components i.e. the compared entities, the scale of the comparison and the aspect on which the entities are being compared. Our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is based on <a href=https://en.wikipedia.org/wiki/Coupling_(computer_programming)>dependencies</a> obtained by applying a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> to extract a wide range of comparison structures. We evaluated our <a href=https://en.wikipedia.org/wiki/System>system</a> for its effectiveness in identifying comparisons and their components. The <a href=https://en.wikipedia.org/wiki/System>system</a> achieved a F-score of 0.87 for comparison sentence identification and 0.77-0.81 for identifying its components.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2327.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2327 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2327 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2327/>Tagging Funding Agencies and Grants in Scientific Articles using Sequential Learning Models</a></strong><br><a href=/people/s/subhradeep-kayal/>Subhradeep Kayal</a>
|
<a href=/people/z/zubair-afzal/>Zubair Afzal</a>
|
<a href=/people/g/george-tsatsaronis/>George Tsatsaronis</a>
|
<a href=/people/s/sophia-katrenko/>Sophia Katrenko</a>
|
<a href=/people/p/pascal-coupet/>Pascal Coupet</a>
|
<a href=/people/m/marius-doornenbal/>Marius Doornenbal</a>
|
<a href=/people/m/michelle-gregory/>Michelle Gregory</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2327><div class="card-body p-3 small">In this paper we present a solution for tagging funding bodies and grants in scientific articles using a combination of trained sequential learning models, namely conditional random fields (CRF), hidden markov models (HMM) and maximum entropy models (MaxEnt), on a benchmark set created in-house. We apply the trained models to address the BioASQ challenge 5c, which is a newly introduced task that aims to solve the problem of funding information extraction from scientific articles. Results in the dry-run data set of BioASQ task 5c show that the suggested approach can achieve a micro-recall of more than 85 % in tagging both funding bodies and grants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2328.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2328 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2328 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2328/>Deep Learning for Biomedical Information Retrieval : Learning Textual Relevance from Click Logs</a></strong><br><a href=/people/s/sunil-mohan/>Sunil Mohan</a>
|
<a href=/people/n/nicolas-fiorini/>Nicolas Fiorini</a>
|
<a href=/people/s/sun-kim/>Sun Kim</a>
|
<a href=/people/z/zhiyong-lu/>Zhiyong Lu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2328><div class="card-body p-3 small">We describe a Deep Learning approach to modeling the relevance of a document&#8217;s text to a query, applied to biomedical literature. Instead of mapping each document and query to a common semantic space, we compute a variable-length difference vector between the query and document which is then passed through a deep convolution stage followed by a deep regression network to produce the estimated probability of the document&#8217;s relevance to the query. Despite the small amount of training data, this approach produces a more robust predictor than computing similarities between semantic vector representations of the query and document, and also results in significant improvements over traditional IR text factors. In the future, we plan to explore its application in improving <a href=https://en.wikipedia.org/wiki/PubMed>PubMed search</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2329.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2329 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2329 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2329" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2329/>Detecting Dementia through Retrospective Analysis of Routine Blog Posts by Bloggers with Dementia</a></strong><br><a href=/people/v/vaden-masrani/>Vaden Masrani</a>
|
<a href=/people/g/gabriel-murray/>Gabriel Murray</a>
|
<a href=/people/t/thalia-field/>Thalia Field</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2329><div class="card-body p-3 small">We investigate if writers with dementia can be automatically distinguished from those without by analyzing <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>linguistic markers</a> in <a href=https://en.wikipedia.org/wiki/Writing>written text</a>, in the form of <a href=https://en.wikipedia.org/wiki/Blog>blog posts</a>. We have built a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of several thousand <a href=https://en.wikipedia.org/wiki/Blog>blog posts</a>, some by people with dementia and others by people with loved ones with dementia. We use this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to train and test several <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a>, and achieve <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> performance at a level far above the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2330.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2330 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2330 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2330/>Protein Word Detection using Text Segmentation Techniques</a></strong><br><a href=/people/d/devi-ganesan/>Devi Ganesan</a>
|
<a href=/people/a/ashish-v-tendulkar/>Ashish V. Tendulkar</a>
|
<a href=/people/s/sutanu-chakraborti/>Sutanu Chakraborti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2330><div class="card-body p-3 small">Literature in <a href=https://en.wikipedia.org/wiki/Molecular_biology>Molecular Biology</a> is abundant with linguistic metaphors. There have been works in the past that attempt to draw parallels between <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> and <a href=https://en.wikipedia.org/wiki/Biology>biology</a>, driven by the fundamental premise that <a href=https://en.wikipedia.org/wiki/Protein>proteins</a> have a language of their own. Since word detection is crucial to the decipherment of any unknown language, we attempt to establish a problem mapping from <a href=https://en.wikipedia.org/wiki/Natural_language>natural language text</a> to <a href=https://en.wikipedia.org/wiki/Protein_primary_structure>protein sequences</a> at the level of words. Towards this end, we explore the use of an unsupervised text segmentation algorithm to the task of extracting biological words from <a href=https://en.wikipedia.org/wiki/Protein_primary_structure>protein sequences</a>. In particular, we demonstrate the effectiveness of using <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> to complement data driven approaches in the text segmentation task, as well as in its biological counterpart. We also propose a novel extrinsic evaluation measure for protein words through protein family classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2331.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2331 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2331 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2331" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2331/>External Evaluation of Event Extraction Classifiers for Automatic Pathway Curation : An extended study of the <a href=https://en.wikipedia.org/wiki/MTOR_pathway>mTOR pathway</a><span class=acl-fixed-case>TOR</span> pathway</a></strong><br><a href=/people/w/wojciech-kusa/>Wojciech Kusa</a>
|
<a href=/people/m/michael-spranger/>Michael Spranger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2331><div class="card-body p-3 small">This paper evaluates the impact of various event extraction systems on automatic pathway curation using the popular <a href=https://en.wikipedia.org/wiki/MTOR_pathway>mTOR pathway</a>. We quantify the impact of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data sets</a> as well as different <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classifiers</a> and show that some improve the quality of automatically extracted pathways.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2332.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2332 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2332 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2332" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2332/>Toward Automated Early Sepsis Alerting : Identifying Infection Patients from Nursing Notes</a></strong><br><a href=/people/e/emilia-apostolova/>Emilia Apostolova</a>
|
<a href=/people/t/tom-velez/>Tom Velez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2332><div class="card-body p-3 small">Severe sepsis and <a href=https://en.wikipedia.org/wiki/Septic_shock>septic shock</a> are conditions that affect millions of patients and have close to 50 % <a href=https://en.wikipedia.org/wiki/Mortality_rate>mortality rate</a>. Early identification of at-risk patients significantly improves outcomes. Electronic surveillance tools have been developed to monitor structured Electronic Medical Records and automatically recognize early signs of sepsis. However, many sepsis risk factors (e.g. symptoms and signs of infection) are often captured only in free text clinical notes. In this study, we developed a method for automatic monitoring of nursing notes for signs and symptoms of infection. We utilized a creative approach to automatically generate an annotated dataset. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> was used to create a <a href=https://en.wikipedia.org/wiki/Machine_learning>Machine Learning model</a> that achieved an <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> ranging from 79 to 96 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2333.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2333 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2333 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2333/>Enhancing Automatic ICD-9-CM Code Assignment for Medical Texts with PubMed<span class=acl-fixed-case>ICD</span>-9-<span class=acl-fixed-case>CM</span> Code Assignment for Medical Texts with <span class=acl-fixed-case>P</span>ub<span class=acl-fixed-case>M</span>ed</a></strong><br><a href=/people/d/danchen-zhang/>Danchen Zhang</a>
|
<a href=/people/d/daqing-he/>Daqing He</a>
|
<a href=/people/s/sanqiang-zhao/>Sanqiang Zhao</a>
|
<a href=/people/l/lei-li/>Lei Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2333><div class="card-body p-3 small">Assigning a standard ICD-9-CM code to <a href=https://en.wikipedia.org/wiki/Symptom>disease symptoms</a> in <a href=https://en.wikipedia.org/wiki/Medical_literature>medical texts</a> is an important task in the <a href=https://en.wikipedia.org/wiki/Medicine>medical domain</a>. Automating this <a href=https://en.wikipedia.org/wiki/Process_(engineering)>process</a> could greatly reduce the costs. However, the effectiveness of an automatic ICD-9-CM code classifier faces a serious problem, which can be triggered by <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>unbalanced training data</a>. Frequent diseases often have more <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a>, which helps its <a href=https://en.wikipedia.org/wiki/Medical_classification>classification</a> to perform better than that of an infrequent disease. However, a <a href=https://en.wikipedia.org/wiki/Incidence_(epidemiology)>disease&#8217;s frequency</a> does not necessarily reflect its importance. To resolve this training data shortage problem, we propose to strategically draw data from <a href=https://en.wikipedia.org/wiki/PubMed>PubMed</a> to enrich the training data when there is such need. We validate our method on the CMC dataset, and the evaluation results indicate that our method can significantly improve the code assignment classifiers&#8217; performance at the macro-averaging level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2334.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2334 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2334 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2334/>Evaluating Feature Extraction Methods for Knowledge-based Biomedical Word Sense Disambiguation</a></strong><br><a href=/people/s/sam-henry/>Sam Henry</a>
|
<a href=/people/c/clint-cuffy/>Clint Cuffy</a>
|
<a href=/people/b/bridget-mcinnes/>Bridget McInnes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2334><div class="card-body p-3 small">In this paper, we present an analysis of <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extraction methods</a> via <a href=https://en.wikipedia.org/wiki/Dimensionality_reduction>dimensionality reduction</a> for the task of biomedical Word Sense Disambiguation (WSD). We modify the vector representations in the 2-MRD WSD algorithm, and evaluate four dimensionality reduction methods : Word Embeddings using Continuous Bag of Words and Skip Gram, Singular Value Decomposition (SVD), and Principal Component Analysis (PCA). We also evaluate the effects of <a href=https://en.wikipedia.org/wiki/Vector_space>vector size</a> on the performance of each of these <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a>. Results are evaluated on five standard evaluation datasets (Abbrev.100, Abbrev.200, Abbrev.300, NLM-WSD, and MSH-WSD). We find that <a href=https://en.wikipedia.org/wiki/Norm_(mathematics)>vector sizes</a> of 100 are sufficient for all techniques except <a href=https://en.wikipedia.org/wiki/Singular_value_decomposition>SVD</a>, for which a <a href=https://en.wikipedia.org/wiki/Vector_space>vector size</a> of 1500 is referred. We also show that <a href=https://en.wikipedia.org/wiki/Word_embedding>SVD</a> performs on par with <a href=https://en.wikipedia.org/wiki/Word_embedding>Word Embeddings</a> for all but one <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2336.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2336 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2336 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2336/>Automated Preamble Detection in Dictated Medical Reports</a></strong><br><a href=/people/w/wael-salloum/>Wael Salloum</a>
|
<a href=/people/g/gregory-finley/>Greg Finley</a>
|
<a href=/people/e/erik-edwards/>Erik Edwards</a>
|
<a href=/people/m/mark-miller/>Mark Miller</a>
|
<a href=/people/d/david-suendermann-oeft/>David Suendermann-Oeft</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2336><div class="card-body p-3 small">Dictated medical reports very often feature a <a href=https://en.wikipedia.org/wiki/Preamble>preamble</a> containing <a href=https://en.wikipedia.org/wiki/Metainformation>metainformation</a> about the report such as patient and physician names, location and name of the clinic, date of procedure, and so on. In the medical transcription process, the <a href=https://en.wikipedia.org/wiki/Preamble>preamble</a> is usually omitted from the final report, as it contains information already available in the <a href=https://en.wikipedia.org/wiki/Electronic_health_record>electronic medical record</a>. We present a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> which is able to automatically identify <a href=https://en.wikipedia.org/wiki/Preamble>preambles</a> in <a href=https://en.wikipedia.org/wiki/Dictation_(exercise)>medical dictations</a>. The method makes use of state-of-the-art NLP techniques including <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and Bi-LSTMs and achieves preamble detection performance superior to humans.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2337.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2337 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2337 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2337/>A Biomedical Question Answering System in BioASQ 2017<span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>ASQ</span> 2017</a></strong><br><a href=/people/m/mourad-sarrouti/>Mourad Sarrouti</a>
|
<a href=/people/s/said-ouatik-el-alaoui/>Said Ouatik El Alaoui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2337><div class="card-body p-3 small">Question answering, the identification of short accurate answers to users questions, is a longstanding challenge widely studied over the last decades in the <a href=https://en.wikipedia.org/wiki/Open_domain>open domain</a>. However, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> still requires further efforts in the <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedical domain</a>. In this paper, we describe our participation in phase B of task 5b in the 2017 BioASQ challenge using our biomedical question answering system. Our <a href=https://en.wikipedia.org/wiki/System>system</a>, dealing with four types of <a href=https://en.wikipedia.org/wiki/Question>questions</a> (i.e., yes / no, <a href=https://en.wikipedia.org/wiki/Factoid>factoid</a>, list, and summary), is based on (1) a dictionary-based approach for generating the exact answers of yes / no questions, (2) UMLS metathesaurus and term frequency metric for extracting the exact answers of <a href=https://en.wikipedia.org/wiki/Factoid>factoid and list questions</a>, and (3) the BM25 model and <a href=https://en.wikipedia.org/wiki/Unified_Modeling_Language>UMLS concepts</a> for retrieving the ideal answers (i.e., paragraph-sized summaries). Preliminary results show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves good and competitive results in both exact and ideal answers extraction tasks as compared with the participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2338.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2338 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2338 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2338/>Adapting Pre-trained Word Embeddings For Use In Medical Coding</a></strong><br><a href=/people/k/kevin-patel/>Kevin Patel</a>
|
<a href=/people/d/divya-patel/>Divya Patel</a>
|
<a href=/people/m/mansi-golakiya/>Mansi Golakiya</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a>
|
<a href=/people/n/nilesh-birari/>Nilesh Birari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2338><div class="card-body p-3 small">Word embeddings are a crucial component in modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a>. Pre-trained embeddings released by different groups have been a major reason for their popularity. However, they are trained on generic corpora, which limits their direct use for domain specific tasks. In this paper, we propose a method to add task specific information to pre-trained word embeddings. Such <a href=https://en.wikipedia.org/wiki/Information>information</a> can improve their utility. We add information from medical coding data, as well as the first level from the hierarchy of ICD-10 medical code set to different pre-trained word embeddings. We adapt CBOW algorithm from the <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec package</a> for our purpose. We evaluated our approach on five different pre-trained word embeddings. Both the original word embeddings, and their modified versions (the ones with added information) were used for automated review of medical coding. The modified <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> give an improvement in <a href=https://en.wikipedia.org/wiki/F-score>f-score</a> by 1 % on the 5-fold evaluation on a private medical claims dataset. Our results show that adding extra information is possible and beneficial for the task at hand.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2340.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2340 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2340 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2340" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2340/>Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models</a></strong><br><a href=/people/r/rahul-v-s-s-patchigolla/>Rahul V S S Patchigolla</a>
|
<a href=/people/s/sunil-sahu/>Sunil Sahu</a>
|
<a href=/people/a/ashish-anand/>Ashish Anand</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2340><div class="card-body p-3 small">Biomedical events describe complex interactions between various biomedical entities. Event trigger is a word or a phrase which typically signifies the occurrence of an event. Event trigger identification is an important first step in all event extraction methods. However many of the current approaches either rely on complex hand-crafted features or consider features only within a window. In this paper we propose a method that takes the advantage of <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network (RNN)</a> to extract higher level features present across the sentence. Thus hidden state representation of RNN along with word and entity type embedding as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> avoid relying on the complex hand-crafted features generated using various NLP toolkits. Our experiments have shown to achieve state-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We have also performed category-wise analysis of the result and discussed the importance of various <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> in trigger identification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2341.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2341 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2341 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2341/>Representations of Time Expressions for Temporal Relation Extraction with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/c/chen-lin/>Chen Lin</a>
|
<a href=/people/t/timothy-miller/>Timothy Miller</a>
|
<a href=/people/d/dmitriy-dligach/>Dmitriy Dligach</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2341><div class="card-body p-3 small">Token sequences are often used as the input for Convolutional Neural Networks (CNNs) in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. However, they might not be an ideal representation for time expressions, which are long, highly varied, and semantically complex. We describe a <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> for representing time expressions with single pseudo-tokens for CNNs. With this <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, we establish a new state-of-the-art result for a clinical temporal relation extraction task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2342.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2342 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2342 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2342/>Automatic Diagnosis Coding of Radiology Reports : A Comparison of <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Conventional Classification Methods</a></strong><br><a href=/people/s/sarvnaz-karimi/>Sarvnaz Karimi</a>
|
<a href=/people/x/xiang-dai/>Xiang Dai</a>
|
<a href=/people/h/hamed-hassanzadeh/>Hamed Hassanzadeh</a>
|
<a href=/people/a/anthony-nguyen/>Anthony Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2342><div class="card-body p-3 small">Diagnosis autocoding services and research intend to both improve the productivity of clinical coders and the accuracy of the coding. It is an important step in <a href=https://en.wikipedia.org/wiki/Data_analysis>data analysis</a> for funding and reimbursement, as well as health services planning and <a href=https://en.wikipedia.org/wiki/Resource_allocation>resource allocation</a>. We investigate the applicability of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> at autocoding of radiology reports using International Classification of Diseases (ICD). Deep learning methods are known to require <a href=https://en.wikipedia.org/wiki/Big_data>large training data</a>. Our goal is to explore how to use these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> when the training data is sparse, skewed and relatively small, and how their effectiveness compares to conventional methods. We identify optimal parameters that could be used in setting up a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> for <a href=https://en.wikipedia.org/wiki/Autocoding>autocoding</a> with comparable results to that of conventional methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2343.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2343 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2343 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2343/>Automatic classification of doctor-patient questions for a virtual patient record query task</a></strong><br><a href=/people/l/leonardo-campillos-llanos/>Leonardo Campillos Llanos</a>
|
<a href=/people/s/sophie-rosset/>Sophie Rosset</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2343><div class="card-body p-3 small">We present the work-in-progress of automating the classification of doctor-patient questions in the context of a simulated consultation with a virtual patient. We classify questions according to the computational strategy (rule-based or other) needed for looking up data in the <a href=https://en.wikipedia.org/wiki/Medical_record>clinical record</a>. We compare &#8216;traditional&#8217; machine learning methods (Gaussian and Multinomial Naive Bayes, and Support Vector Machines) and a neural network classifier (FastText). We obtained the best results with the SVM using <a href=https://en.wikipedia.org/wiki/Semantic_annotation>semantic annotations</a>, whereas the neural classifier achieved promising results without it.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2345.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2345 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2345 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2345/>Clinical Event Detection with Hybrid Neural Architecture</a></strong><br><a href=/people/a/adyasha-maharana/>Adyasha Maharana</a>
|
<a href=/people/m/meliha-yetisgen-yildiz/>Meliha Yetisgen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2345><div class="card-body p-3 small">Event detection from clinical notes has been traditionally solved with rule based and statistical natural language processing (NLP) approaches that require extensive domain knowledge and feature engineering. In this paper, we have explored the feasibility of approaching this task with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>, clinical word embeddings and introduced a hybrid architecture to improve detection for entities with smaller representation in the dataset. A comparative analysis is also done which reveals the complementary behavior of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> and <a href=https://en.wikipedia.org/wiki/Conditional_random_field>conditional random fields</a> in clinical entity detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2346.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2346 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2346 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2346/>Extracting Personal Medical Events for User Timeline Construction using Minimal Supervision</a></strong><br><a href=/people/a/aakanksha-naik/>Aakanksha Naik</a>
|
<a href=/people/c/christopher-bogart/>Chris Bogart</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rose</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2346><div class="card-body p-3 small">In this paper, we describe a <a href=https://en.wikipedia.org/wiki/System>system</a> for automatic construction of user disease progression timelines from their posts in online support groups using minimal supervision. In recent years, several online support groups have been established which has led to a huge increase in the amount of patient-authored text available. Creating systems which can automatically extract important medical events and create disease progression timelines for users from such text can help in patient health monitoring as well as studying links between <a href=https://en.wikipedia.org/wiki/Disease>medical events</a> and users&#8217; participation in <a href=https://en.wikipedia.org/wiki/Support_group>support groups</a>. Prior work in this domain has used manually constructed keyword sets to detect medical events. In this work, our aim is to perform medical event detection using minimal supervision in order to develop a more general timeline construction system. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 55.17 %, which is 92 % of the performance achieved by a supervised baseline system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2348.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2348 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2348 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2348/>A Multi-strategy Query Processing Approach for Biomedical Question Answering : USTB_PRIR at BioASQ 2017 Task 5B<span class=acl-fixed-case>USTB</span>_<span class=acl-fixed-case>PRIR</span> at <span class=acl-fixed-case>B</span>io<span class=acl-fixed-case>ASQ</span> 2017 Task 5<span class=acl-fixed-case>B</span></a></strong><br><a href=/people/z/zan-xia-jin/>Zan-Xia Jin</a>
|
<a href=/people/b/bo-wen-zhang/>Bo-Wen Zhang</a>
|
<a href=/people/f/fan-fang/>Fan Fang</a>
|
<a href=/people/l/le-le-zhang/>Le-Le Zhang</a>
|
<a href=/people/x/xu-cheng-yin/>Xu-Cheng Yin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2348><div class="card-body p-3 small">This paper describes the participation of USTB_PRIR team in the 2017 BioASQ 5B on <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>, including <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a>, snippet retrieval, and concept retrieval task. We introduce different multimodal query processing strategies to enrich query terms and assign different weights to them. Specifically, sequential dependence model (SDM), pseudo-relevance feedback (PRF), fielded sequential dependence model (FSDM) and Divergence from Randomness model (DFRM) are respectively performed on different fields of PubMed articles, sentences extracted from relevant articles, the five terminologies or ontologies (MeSH, GO, Jochem, Uniprot and DO) to achieve better search performances. Preliminary results show that our <a href=https://en.wikipedia.org/wiki/System>systems</a> outperform others in the document and snippet retrieval task in the first two batches.</div></div></div><hr><div id=w17-24><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-24.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-24/>Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2400/>Proceedings of <span class=acl-fixed-case>T</span>ext<span class=acl-fixed-case>G</span>raphs-11: the Workshop on Graph-based Methods for Natural Language Processing</a></strong><br><a href=/people/m/martin-riedl/>Martin Riedl</a>
|
<a href=/people/s/swapna-somasundaran/>Swapna Somasundaran</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2401/>On the Calligraphy of Books</a></strong><br><a href=/people/v/vanessa-queiroz-marinho/>Vanessa Queiroz Marinho</a>
|
<a href=/people/h/henrique-ferraz-de-arruda/>Henrique Ferraz de Arruda</a>
|
<a href=/people/t/thales-sinelli/>Thales Sinelli</a>
|
<a href=/people/l/luciano-da-fontoura-costa/>Luciano da Fontoura Costa</a>
|
<a href=/people/d/diego-raphael-amancio/>Diego Raphael Amancio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2401><div class="card-body p-3 small">Authorship attribution is a natural language processing task that has been widely studied, often by considering small order statistics. In this paper, we explore a complex network approach to assign the authorship of texts based on their mesoscopic representation, in an attempt to capture the flow of the narrative. Indeed, as reported in this work, such an approach allowed the identification of the dominant narrative structure of the studied authors. This has been achieved due to the ability of the mesoscopic approach to take into account relationships between different, not necessarily adjacent, parts of the text, which is able to capture the story flow. The potential of the proposed approach has been illustrated through <a href=https://en.wikipedia.org/wiki/Principal_component_analysis>principal component analysis</a>, a comparison with the chance baseline method, and <a href=https://en.wikipedia.org/wiki/Network_visualization>network visualization</a>. Such <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualizations</a> reveal individual characteristics of the authors, which can be understood as a kind of <a href=https://en.wikipedia.org/wiki/Calligraphy>calligraphy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2402/>Adapting predominant and novel sense discovery algorithms for identifying corpus-specific sense differences</a></strong><br><a href=/people/b/binny-mathew/>Binny Mathew</a>
|
<a href=/people/s/suman-kalyan-maity/>Suman Kalyan Maity</a>
|
<a href=/people/p/pratip-sarkar/>Pratip Sarkar</a>
|
<a href=/people/a/animesh-mukherjee/>Animesh Mukherjee</a>
|
<a href=/people/p/pawan-goyal/>Pawan Goyal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2402><div class="card-body p-3 small">Word senses are not static and may have temporal, spatial or corpus-specific scopes. Identifying such <a href=https://en.wikipedia.org/wiki/Scope_(computer_science)>scopes</a> might benefit the existing WSD systems largely. In this paper, while studying corpus specific word senses, we adapt three existing predominant and novel-sense discovery algorithms to identify these corpus-specific senses. We make use of text data available in the form of millions of digitized books and newspaper archives as two different sources of corpora and propose automated methods to identify corpus-specific word senses at various time points. We conduct an extensive and thorough human judgement experiment to rigorously evaluate and compare the performance of these approaches. Post adaptation, the output of the three <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> are in the same format and the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> results are also comparable, with roughly 45-60 % of the reported corpus-specific senses being judged as genuine.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2403/>Merging knowledge bases in different languages</a></strong><br><a href=/people/j/jeronimo-hernandez-gonzalez/>Jerónimo Hernández-González</a>
|
<a href=/people/e/estevam-r-hruschka-jr/>Estevam R. Hruschka Jr.</a>
|
<a href=/people/t/tom-mitchell/>Tom M. Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2403><div class="card-body p-3 small">Recently, different systems which learn to populate and extend a knowledge base (KB) from the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>web</a> in different languages have been presented. Although a large set of concepts should be learnt independently from the language used to read, there are facts which are expected to be more easily gathered in <a href=https://en.wikipedia.org/wiki/Local_language>local language</a> (e.g., culture or geography). A system that merges KBs learnt in different languages will benefit from the complementary information as long as common beliefs are identified, as well as from <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundancy</a> present in web pages written in different languages. In this paper, we deal with the problem of identifying equivalent beliefs (or concepts) across language specific KBs, assuming that they share the same ontology of categories and relations. In a case study with two KBs independently learnt from different inputs, namely <a href=https://en.wikipedia.org/wiki/Web_page>web pages</a> written in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Web_page>web pages</a> written in <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> respectively, we report on the results of two methodologies : an approach based on personalized PageRank and an inference technique to find out common relevant paths through the KBs. The proposed inference technique efficiently identifies relevant paths, outperforming the baseline (a dictionary-based classifier) in the vast majority of tested categories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2404/>Parameter Free Hierarchical Graph-Based Clustering for Analyzing Continuous Word Embeddings</a></strong><br><a href=/people/t/thomas-alexander-trost/>Thomas Alexander Trost</a>
|
<a href=/people/d/dietrich-klakow/>Dietrich Klakow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2404><div class="card-body p-3 small">Word embeddings are high-dimensional vector representations of words and are thus difficult to interpret. In order to deal with this, we introduce an unsupervised parameter free method for creating a hierarchical graphical clustering of the full ensemble of word vectors and show that this structure is a geometrically meaningful representation of the original relations between the words. This newly obtained representation can be used for better understanding and thus improving the <a href=https://en.wikipedia.org/wiki/Embedding>embedding algorithm</a> and exhibits <a href=https://en.wikipedia.org/wiki/Semantics>semantic meaning</a>, so it can also be utilized in a variety of language processing tasks like <a href=https://en.wikipedia.org/wiki/Categorization>categorization</a> or <a href=https://en.wikipedia.org/wiki/Similarity_measure>measuring similarity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2405/>Spectral Graph-Based Method of Multimodal Word Embedding</a></strong><br><a href=/people/k/kazuki-fukui/>Kazuki Fukui</a>
|
<a href=/people/t/takamasa-oshikiri/>Takamasa Oshikiri</a>
|
<a href=/people/h/hidetoshi-shimodaira/>Hidetoshi Shimodaira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2405><div class="card-body p-3 small">In this paper, we propose a novel method for multimodal word embedding, which exploit a generalized framework of multi-view spectral graph embedding to take into account visual appearances or scenes denoted by words in a corpus. We evaluated our method through word similarity tasks and a concept-to-image search task, having found that it provides word representations that reflect visual information, while somewhat trading-off the performance on the word similarity tasks. Moreover, we demonstrate that our method captures multimodal linguistic regularities, which enable recovering relational similarities between words and images by vector arithmetics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2407/>Extract with Order for Coherent Multi-Document Summarization</a></strong><br><a href=/people/m/mir-tafseer-nayeem/>Mir Tafseer Nayeem</a>
|
<a href=/people/y/yllias-chali/>Yllias Chali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2407><div class="card-body p-3 small">In this work, we aim at developing an extractive summarizer in the multi-document setting. We implement a rank based sentence selection using continuous vector representations along with key-phrases. Furthermore, we propose a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to tackle summary coherence for increasing <a href=https://en.wikipedia.org/wiki/Readability>readability</a>. We conduct experiments on the Document Understanding Conference (DUC) 2004 datasets using ROUGE toolkit. Our experiments demonstrate that the <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> bring significant improvements over the state of the art <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> in terms of <a href=https://en.wikipedia.org/wiki/Informatics>informativity</a> and <a href=https://en.wikipedia.org/wiki/Coherence_(physics)>coherence</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2410 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2410.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-2410/>Evaluating text coherence based on semantic similarity graph</a></strong><br><a href=/people/j/jan-wira-gotama-putra/>Jan Wira Gotama Putra</a>
|
<a href=/people/t/takenobu-tokunaga/>Takenobu Tokunaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2410><div class="card-body p-3 small">Coherence is a crucial feature of text because it is indispensable for conveying its communication purpose and meaning to its readers. In this paper, we propose an unsupervised text coherence scoring based on <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph construction</a> in which <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edges</a> are established between semantically similar sentences represented by vertices. The sentence similarity is calculated based on the cosine similarity of semantic vectors representing sentences. We provide three graph construction methods establishing an <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>edge</a> from a given <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>vertex</a> to a preceding adjacent vertex, to a single similar vertex, or to multiple similar vertices. We evaluated our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> in the document discrimination task and the insertion task by comparing our proposed <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> to the supervised (Entity Grid) and unsupervised (Entity Graph) baselines. In the document discrimination task, our method outperformed the unsupervised baseline but could not do the supervised baseline, while in the insertion task, our method outperformed both baselines.</div></div></div><hr><div id=w17-25><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-25.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-25/>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2500/>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</a></strong><br><a href=/people/s/serge-sharoff/>Serge Sharoff</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a>
|
<a href=/people/r/reinhard-rapp/>Reinhard Rapp</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2501/>Users and Data : The Two Neglected Children of Bilingual Natural Language Processing Research</a></strong><br><a href=/people/p/philippe-langlais/>Phillippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2501><div class="card-body p-3 small">Despite numerous studies devoted to mining parallel material from bilingual data, we have yet to see the resulting <a href=https://en.wikipedia.org/wiki/Technology>technologies</a> wholeheartedly adopted by professional translators and terminologists alike. I argue that this state of affairs is mainly due to two factors : the emphasis published authors put on <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> (even though data is as important), and the conspicuous lack of concern for actual end-users.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2502.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2502" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2502/>Deep Investigation of Cross-Language Plagiarism Detection Methods</a></strong><br><a href=/people/j/jeremy-ferrero/>Jérémy Ferrero</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/d/didier-schwab/>Didier Schwab</a>
|
<a href=/people/f/frederic-agnes/>Frédéric Agnès</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2502><div class="card-body p-3 small">This paper is a deep investigation of cross-language plagiarism detection methods on a new recently introduced <a href=https://en.wikipedia.org/wiki/Open_data>open dataset</a>, which contains parallel and comparable collections of documents with multiple characteristics (different genres, languages and sizes of texts). We investigate cross-language plagiarism detection methods for 6 language pairs on 2 granularities of text units in order to draw robust conclusions on the best methods while deeply analyzing correlations across document styles and languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2503/>Sentence Alignment using Unfolding Recursive Autoencoders</a></strong><br><a href=/people/j/jeenu-grover/>Jeenu Grover</a>
|
<a href=/people/p/pabitra-mitra/>Pabitra Mitra</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2503><div class="card-body p-3 small">In this paper, we propose a novel two step algorithm for sentence alignment in <a href=https://en.wikipedia.org/wiki/Text_corpus>monolingual corpora</a> using Unfolding Recursive Autoencoders. First, we use unfolding recursive auto-encoders (RAE) to learn feature vectors for phrases in syntactical tree of the sentence. To compare two sentences we use a <a href=https://en.wikipedia.org/wiki/Similarity_matrix>similarity matrix</a> which has dimensions proportional to the size of the two sentences. Since the <a href=https://en.wikipedia.org/wiki/Similarity_matrix>similarity matrix</a> generated to compare two sentences has varying dimension due to different sentence lengths, a dynamic pooling layer is used to map it to a matrix of fixed dimension. The resulting <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a> is used to calculate the similarity scores between the two sentences. The second step of the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> captures the contexts in which the sentences occur in the document by using a dynamic programming algorithm for global alignment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2504.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-2504/>Acquisition of Translation Lexicons for Historically Unwritten Languages via Bridging Loanwords</a></strong><br><a href=/people/m/michael-bloodgood/>Michael Bloodgood</a>
|
<a href=/people/b/benjamin-strauss/>Benjamin Strauss</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2504><div class="card-body p-3 small">With the advent of informal electronic communications such as <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, <a href=https://en.wikipedia.org/wiki/Colloquialism>colloquial languages</a> that were historically unwritten are being written for the first time in heavily <a href=https://en.wikipedia.org/wiki/Code-switching>code-switched environments</a>. We present a method for inducing portions of translation lexicons through the use of expert knowledge in these settings where there are approximately zero resources available other than a language informant, potentially not even large amounts of monolingual data. We investigate inducing a Moroccan Darija-English translation lexicon via French loanwords bridging into English and find that a useful lexicon is induced for human-assisted translation and <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2505/>Toward a Comparable Corpus of Latvian, Russian and English Tweets<span class=acl-fixed-case>L</span>atvian, <span class=acl-fixed-case>R</span>ussian and <span class=acl-fixed-case>E</span>nglish Tweets</a></strong><br><a href=/people/d/dmitrijs-milajevs/>Dmitrijs Milajevs</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2505><div class="card-body p-3 small">Twitter has become a rich source for linguistic data. Here, a possibility of building a trilingual Latvian-Russian-English corpus of tweets from Riga, Latvia is investigated. Such a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, once constructed, might be of great use for multiple purposes including training <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation models</a>, examining cross-lingual phenomena and studying the population of Riga. This pilot study shows that it is feasible to build such a resource by collecting and analysing a pilot corpus, which is made publicly available and can be used to construct a large comparable corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2506.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-2506/>Automatic Extraction of Parallel Speech Corpora from Dubbed Movies</a></strong><br><a href=/people/a/alp-oktem/>Alp Öktem</a>
|
<a href=/people/m/mireia-farrus/>Mireia Farrús</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2506><div class="card-body p-3 small">This paper presents a methodology to extract parallel speech corpora based on any language pair from dubbed movies, together with an application framework in which some corresponding prosodic parameters are extracted. The obtained <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> are especially suitable for speech-to-speech translation applications when a <a href=https://en.wikipedia.org/wiki/Prosody_(linguistics)>prosody transfer</a> between source and target languages is desired.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2507.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-2507/>A parallel collection of clinical trials in <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and English<span class=acl-fixed-case>P</span>ortuguese and <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/m/mariana-neves/>Mariana Neves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2507><div class="card-body p-3 small">Parallel collections of documents are crucial resources for training and evaluating machine translation (MT) systems. Even though large collections are available for certain domains and language pairs, these are still scarce in the biomedical domain. We developed a parallel corpus of clinical trials in <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a>. The documents are derived from the Brazilian Clinical Trials Registry and the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> currently contains a total of 1188 documents. In this paper, we describe the corpus construction and discuss the quality of the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and the sentence alignment that we obtained.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2508/>Weighted Set-Theoretic Alignment of Comparable Sentences</a></strong><br><a href=/people/a/andoni-azpeitia/>Andoni Azpeitia</a>
|
<a href=/people/t/thierry-etchegoyhen/>Thierry Etchegoyhen</a>
|
<a href=/people/e/eva-martinez-garcia/>Eva Martínez Garcia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2508><div class="card-body p-3 small">This article presents the STACCw system for the BUCC 2017 shared task on parallel sentence extraction from comparable corpora. The original STACC approach, based on set-theoretic operations over bags of words, had been previously shown to be efficient and portable across domains and alignment scenarios. Wedescribe an extension of this approach with a new weighting scheme and show that it provides significant improvements on the datasets provided for the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2509/>BUCC 2017 Shared Task : a First Attempt Toward a Deep Learning Framework for Identifying Parallel Sentences in Comparable Corpora<span class=acl-fixed-case>BUCC</span> 2017 Shared Task: a First Attempt Toward a Deep Learning Framework for Identifying Parallel Sentences in Comparable Corpora</a></strong><br><a href=/people/f/francis-gregoire/>Francis Grégoire</a>
|
<a href=/people/p/philippe-langlais/>Philippe Langlais</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2509><div class="card-body p-3 small">This paper describes our participation in BUCC 2017 shared task : identifying parallel sentences in comparable corpora. Our goal is to leverage continuous vector representations and <a href=https://en.wikipedia.org/wiki/Distributional_semantics>distributional semantics</a> with a minimal use of external preprocessing and postprocessing tools. We report experiments that were conducted after transmitting our results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2510/>zNLP : Identifying Parallel Sentences in Chinese-English Comparable Corpora<span class=acl-fixed-case>NLP</span>: Identifying Parallel Sentences in <span class=acl-fixed-case>C</span>hinese-<span class=acl-fixed-case>E</span>nglish Comparable Corpora</a></strong><br><a href=/people/z/zheng-zhang/>Zheng Zhang</a>
|
<a href=/people/p/pierre-zweigenbaum/>Pierre Zweigenbaum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2510><div class="card-body p-3 small">This paper describes the zNLP system for the BUCC 2017 shared task. Our system identifies parallel sentence pairs in Chinese-English comparable corpora by translating word-by-word Chinese sentences into English, using the search engine Solr to select near-parallel sentences and then by using an SVM classifier to identify true parallel sentences from the previous results. It obtains an F1-score of 45 % (resp. 32 %) on the test (training) set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2511 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2511/>BUCC2017 : A Hybrid Approach for Identifying Parallel Sentences in Comparable Corpora<span class=acl-fixed-case>BUCC</span>2017: A Hybrid Approach for Identifying Parallel Sentences in Comparable Corpora</a></strong><br><a href=/people/s/sainik-mahata/>Sainik Mahata</a>
|
<a href=/people/d/dipankar-das/>Dipankar Das</a>
|
<a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2511><div class="card-body p-3 small">A Statistical Machine Translation (SMT) system is always trained using large parallel corpus to produce effective <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. Not only is the corpus scarce, it also involves a lot of manual labor and cost. Parallel corpus can be prepared by employing comparable corpora where a pair of corpora is in two different languages pointing to the same domain. In the present work, we try to build a <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a> for French-English language pair from a given comparable corpus. The data and the problem set are provided as part of the shared task organized by BUCC 2017. We have proposed a system that first translates the sentences by heavily relying on <a href=https://en.wikipedia.org/wiki/Moses>Moses</a> and then group the sentences based on sentence length similarity. Finally, the one to one sentence selection was done based on Cosine Similarity algorithm.</div></div></div><hr><div id=w17-26><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-26.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-26/>Proceedings of the 2nd Workshop on Representation Learning for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2600/>Proceedings of the 2nd Workshop on Representation Learning for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/p/phil-blunsom/>Phil Blunsom</a>
|
<a href=/people/a/antoine-bordes/>Antoine Bordes</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a>
|
<a href=/people/s/shay-b-cohen/>Shay Cohen</a>
|
<a href=/people/c/chris-dyer/>Chris Dyer</a>
|
<a href=/people/e/edward-grefenstette/>Edward Grefenstette</a>
|
<a href=/people/k/karl-moritz-hermann/>Karl Moritz Hermann</a>
|
<a href=/people/l/laura-rimell/>Laura Rimell</a>
|
<a href=/people/j/jason-weston/>Jason Weston</a>
|
<a href=/people/s/scott-yih/>Scott Yih</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2601/>Sense Contextualization in a Dependency-Based Compositional Distributional Model</a></strong><br><a href=/people/p/pablo-gamallo/>Pablo Gamallo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2601><div class="card-body p-3 small">Little attention has been paid to distributional compositional methods which employ syntactically structured vector models. As word vectors belonging to different <a href=https://en.wikipedia.org/wiki/Syntactic_category>syntactic categories</a> have incompatible syntactic distributions, no trivial compositional operation can be applied to combine them into a new compositional vector. In this article, we generalize the method described by Erk and Pad (2009) by proposing a dependency-base framework that contextualize not only lemmas but also selectional preferences. The main contribution of the article is to expand their <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to a fully compositional framework in which syntactic dependencies are put at the core of semantic composition. We claim that semantic composition is mainly driven by syntactic dependencies. Each syntactic dependency generates two new compositional vectors representing the contextualized sense of the two related lemmas. The sequential application of the compositional operations associated to the dependencies results in as many contextualized vectors as lemmas the composite expression contains. At the end of the semantic process, we do not obtain a single compositional vector representing the semantic denotation of the whole composite expression, but one contextualized vector for each lemma of the whole expression. Our method avoids the troublesome high-order tensor representations by defining lemmas and selectional restrictions as <a href=https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)>first-order tensors</a> (i.e. standard vectors). A corpus-based experiment is performed to both evaluate the quality of the compositional vectors built with our strategy, and to compare them to other approaches on distributional compositional semantics. The experiments show that our dependency-based compositional method performs as (or even better than) the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2602.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2602 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2602 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2602" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2602/>Context encoders as a simple but powerful extension of <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a></a></strong><br><a href=/people/f/franziska-horn/>Franziska Horn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2602><div class="card-body p-3 small">With a strikingly simple architecture and the ability to learn meaningful <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> efficiently from texts containing billions of words, <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> remains one of the most popular neural language models used today. However, as only a single <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings and, additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model&#8217;s negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word&#8217;s average context vector, out-of-vocabulary (OOV) embeddings and representations for words with multiple meanings can be created based on the words&#8217; local contexts. The benefits of this approach are illustrated by using these <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> as <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a> in the CoNLL 2003 named entity recognition (NER) task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2603" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2603/>Machine Comprehension by Text-to-Text Neural Question Generation</a></strong><br><a href=/people/x/xingdi-yuan/>Xingdi Yuan</a>
|
<a href=/people/t/tong-wang/>Tong Wang</a>
|
<a href=/people/c/caglar-gulcehre/>Caglar Gulcehre</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/p/philip-bachman/>Philip Bachman</a>
|
<a href=/people/s/saizheng-zhang/>Saizheng Zhang</a>
|
<a href=/people/s/sandeep-subramanian/>Sandeep Subramanian</a>
|
<a href=/people/a/adam-trischler/>Adam Trischler</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2603><div class="card-body p-3 small">We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> using a combination of supervised and reinforcement learning. After teacher forcing for standard <a href=https://en.wikipedia.org/wiki/Maximum_likelihood_estimation>maximum likelihood training</a>, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a <a href=https://en.wikipedia.org/wiki/Question_answering>question-answering system</a>. We motivate question generation as a means to improve the performance of <a href=https://en.wikipedia.org/wiki/Question_answering>question answering systems</a>. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is trained and evaluated on the recent question-answering dataset SQuAD.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2604/>Emergent Predication Structure in Hidden State Vectors of Neural Readers</a></strong><br><a href=/people/h/hai-wang/>Hai Wang</a>
|
<a href=/people/t/takashi-onishi/>Takeshi Onishi</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a>
|
<a href=/people/d/david-mcallester/>David McAllester</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2604><div class="card-body p-3 small">A significant number of neural architectures for <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the emergence of <a href=https://en.wikipedia.org/wiki/Predicate_(mathematical_logic)>predication structure</a> in the hidden state vectors of these readers. More specifically, we provide evidence that the hidden state vectors represent atomic formulas [ c ] where is a semantic property (predicate) and c is a constant symbol entity identifier.<tex-math>\\Phi[c]</tex-math> where <tex-math>\\Phi</tex-math> is a semantic property (predicate) and <tex-math>c</tex-math> is a constant symbol entity identifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2606/>Combining Word-Level and Character-Level Representations for Relation Classification of Informal Text</a></strong><br><a href=/people/d/dongyun-liang/>Dongyun Liang</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a>
|
<a href=/people/y/yinge-zhao/>Yinge Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2606><div class="card-body p-3 small">Word representation models have achieved great success in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing tasks</a>, such as <a href=https://en.wikipedia.org/wiki/Binary_relation>relation classification</a>. However, it does not always work on informal text, and the morphemes of some misspelling words may carry important short-distance semantic information. We propose a hybrid model, combining the merits of word-level and character-level representations to learn better <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> on informal text. Experiments on two dataset of relation classification, SemEval-2010 Task8 and a large-scale one we compile from informal text, show that our model achieves a competitive result in the former and state-of-the-art with the other.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2607/>Transfer Learning for Neural Semantic Parsing</a></strong><br><a href=/people/x/xing-fan/>Xing Fan</a>
|
<a href=/people/e/emilio-monti/>Emilio Monti</a>
|
<a href=/people/l/lambert-mathias/>Lambert Mathias</a>
|
<a href=/people/m/markus-dreyer/>Markus Dreyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2607><div class="card-body p-3 small">The goal of <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> is to map <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> to a machine interpretable meaning representation language (MRL). One of the constraints that limits full exploration of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning technologies</a> for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> is the lack of sufficient <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>annotation training data</a>. In this paper, we propose using sequence-to-sequence in a multi-task setup for <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a> with focus on <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>. We explore three multi-task architectures for sequence-to-sequence model and compare their performance with the independently trained <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Our experiments show that the multi-task setup aids <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from an auxiliary task with large labeled data to the target <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> with smaller labeled data. We see an absolute accuracy gain ranging from 1.0 % to 4.4 % in in our in-house data set and we also see good gains ranging from 2.5 % to 7.0 % on the ATIS semantic parsing tasks with syntactic and semantic auxiliary tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2608 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2608/>Modeling Large-Scale Structured Relationships with <a href=https://en.wikipedia.org/wiki/Shared_memory>Shared Memory</a> for Knowledge Base Completion</a></strong><br><a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/p/po-sen-huang/>Po-Sen Huang</a>
|
<a href=/people/m/ming-wei-chang/>Ming-Wei Chang</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2608><div class="card-body p-3 small">Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>, learning multi-step relations directly on top of observed triplets could be costly. Hence, a manually designed procedure is often used when training the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform multi-step inference implicitly through a <a href=https://en.wikipedia.org/wiki/Controller_(computing)>controller</a> and <a href=https://en.wikipedia.org/wiki/Shared_memory>shared memory</a>. Without a human-designed inference procedure, IRNs use training data to learn to perform multi-step inference in an embedding neural space through the <a href=https://en.wikipedia.org/wiki/Shared_memory>shared memory</a> and controller. While the <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference procedure</a> does not explicitly operate on top of observed triplets, our proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms all previous <a href=https://en.wikipedia.org/wiki/Statistical_inference>approaches</a> on the popular FB15k benchmark by more than 5.7 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2610.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2610 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2610 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2610/>Sequential Attention : A Context-Aware Alignment Function for <a href=https://en.wikipedia.org/wiki/Machine_reading>Machine Reading</a></a></strong><br><a href=/people/s/sebastian-brarda/>Sebastian Brarda</a>
|
<a href=/people/p/philip-yeres/>Philip Yeres</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2610><div class="card-body p-3 small">In this paper we propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in a way that takes into account not just how well that word matches a query, but how well surrounding words match. We evaluate this approach on the task of <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a> (on the Who did What and CNN datasets) and show that it dramatically improves a strong baselinethe Stanford Readerand is competitive with the state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2611.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2611 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2611 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2611/>Semantic Vector Encoding and <a href=https://en.wikipedia.org/wiki/Similarity_search>Similarity Search</a> Using Fulltext Search Engines</a></strong><br><a href=/people/j/jan-rygl/>Jan Rygl</a>
|
<a href=/people/j/jan-pomikalek/>Jan Pomikálek</a>
|
<a href=/people/r/radim-rehurek/>Radim Řehůřek</a>
|
<a href=/people/m/michal-ruzicka/>Michal Růžička</a>
|
<a href=/people/v/vit-novotny/>Vít Novotný</a>
|
<a href=/people/p/petr-sojka/>Petr Sojka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2611><div class="card-body p-3 small">Vector representations and vector space modeling (VSM) play a central role in modern <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>. We propose a novel approach to &#8216;vector similarity searching&#8217; over dense semantic representations of words and documents that can be deployed on top of traditional inverted-index-based fulltext engines, taking advantage of their robustness, stability, scalability and ubiquity. We show that this approach allows the indexing and querying of dense vectors in text domains. This opens up exciting avenues for major efficiency gains, along with simpler deployment, scaling and monitoring. The end result is a fast and scalable vector database with a tunable trade-off between vector search performance and quality, backed by a standard <a href=https://en.wikipedia.org/wiki/Full-text_search>fulltext engine</a> such as <a href=https://en.wikipedia.org/wiki/Elasticsearch>Elasticsearch</a>. We empirically demonstrate its querying performance and quality by applying this solution to the task of semantic searching over a dense vector representation of the entire <a href=https://en.wikipedia.org/wiki/English_Wikipedia>English Wikipedia</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2612.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2612 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2612 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2612/>Multi-task Domain Adaptation for Sequence Tagging</a></strong><br><a href=/people/n/nanyun-peng/>Nanyun Peng</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2612><div class="card-body p-3 small">Many domain adaptation approaches rely on learning cross domain shared representations to transfer the knowledge learned in one domain to other domains. Traditional <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> only considers adapting for one task. In this paper, we explore multi-task representation learning under the domain adaptation scenario. We propose a neural network framework that supports <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for multiple tasks simultaneously, and learns shared representations that better generalize for <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>. We apply the proposed framework to <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for sequence tagging problems considering two tasks : <a href=https://en.wikipedia.org/wiki/Chinese_word_segmentation>Chinese word segmentation</a> and <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. Experiments show that multi-task domain adaptation works better than disjoint domain adaptation for each task, and achieves the state-of-the-art results for both tasks in the <a href=https://en.wikipedia.org/wiki/Social_media>social media domain</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2613.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2613 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2613 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2613/>Beyond Bilingual : Multi-sense Word Embeddings using Multilingual Context</a></strong><br><a href=/people/s/shyam-upadhyay/>Shyam Upadhyay</a>
|
<a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/m/matt-taddy/>Matt Taddy</a>
|
<a href=/people/a/adam-kalai/>Adam Kalai</a>
|
<a href=/people/j/james-zou/>James Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2613><div class="card-body p-3 small">Word embeddings, which represent a word as a point in a <a href=https://en.wikipedia.org/wiki/Vector_space>vector space</a>, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense wor d embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields comparable performance to a state of the art <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual model</a> trained on five times more training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2617.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2617 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2617 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2617/>Learning Bilingual Projections of Embeddings for Vocabulary Expansion in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/p/pranava-swaroop-madhyastha/>Pranava Swaroop Madhyastha</a>
|
<a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2617><div class="card-body p-3 small">We propose a simple log-bilinear softmax-based model to deal with vocabulary expansion in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Our model uses word embeddings trained on significantly large unlabelled monolingual corpora and learns over a fairly small, word-to-word bilingual dictionary. Given an out-of-vocabulary source word, the model generates a probabilistic list of possible translations in the target language using the trained bilingual embeddings. We integrate these translation options into a standard phrase-based statistical machine translation system and obtain consistent improvements in translation quality on the EnglishSpanish language pair. When tested over an out-of-domain testset, we get a significant improvement of 3.9 <a href=https://en.wikipedia.org/wiki/Point_(typography)>BLEU points</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2618.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2618 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2618 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2618/>Prediction of Frame-to-Frame Relations in the FrameNet Hierarchy with Frame Embeddings<span class=acl-fixed-case>F</span>rame<span class=acl-fixed-case>N</span>et Hierarchy with Frame Embeddings</a></strong><br><a href=/people/t/teresa-botschen/>Teresa Botschen</a>
|
<a href=/people/h/hatem-mousselly-sergieh/>Hatem Mousselly-Sergieh</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2618><div class="card-body p-3 small">Automatic completion of frame-to-frame (F2F) relations in the FrameNet (FN) hierarchy has received little attention, although they incorporate meta-level commonsense knowledge and are used in downstream approaches. We address the problem of sparsely annotated F2F relations. First, we examine whether the manually defined F2F relations emerge from text by learning text-based frame embeddings. Our analysis reveals insights about the difficulty of reconstructing F2F relations purely from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>. Second, we present different systems for predicting F2F relations ; our best-performing one uses the FN hierarchy to train on and to ground embeddings in. A comparison of systems and <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> exposes the crucial influence of knowledge-based embeddings to a <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s performance in predicting F2F relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2619.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2619 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2619 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2619/>Learning Joint Multilingual Sentence Representations with <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/h/holger-schwenk/>Holger Schwenk</a>
|
<a href=/people/m/matthijs-douze/>Matthijs Douze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2619><div class="card-body p-3 small">In this paper, we use the framework of <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> to learn joint sentence representations across six very different languages. Our aim is that a <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representation</a> which is independent of the language, is likely to capture the underlying <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. We define a new cross-lingual similarity measure, compare up to 1.4 M <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence representations</a> and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2621.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2621 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2621 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2621/>Gradual Learning of Matrix-Space Models of Language for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/s/shima-asaadi/>Shima Asaadi</a>
|
<a href=/people/s/sebastian-rudolph/>Sebastian Rudolph</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2621><div class="card-body p-3 small">Learning word representations to capture the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> and compositionality of language has received much research interest in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Beyond the popular vector space models, matrix representations for words have been proposed, since then, <a href=https://en.wikipedia.org/wiki/Matrix_multiplication>matrix multiplication</a> can serve as natural composition operation. In this work, we investigate the problem of learning <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix representations of words</a>. We present a learning approach for compositional matrix-space models for the task of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. We show that our approach, which learns the matrices gradually in two steps, outperforms other approaches and a gradient-descent baseline in terms of <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a> and <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2622.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2622 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2622 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2622/>Improving <a href=https://en.wikipedia.org/wiki/Language_model>Language Modeling</a> using Densely Connected Recurrent Neural Networks</a></strong><br><a href=/people/f/frederic-godin/>Fréderic Godin</a>
|
<a href=/people/j/joni-dambre/>Joni Dambre</a>
|
<a href=/people/w/wesley-de-neve/>Wesley De Neve</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2622><div class="card-body p-3 small">In this paper, we introduce the novel concept of densely connected layers into <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. We evaluate our proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> on the Penn Treebank language modeling task. We show that we can obtain similar perplexity scores with six times fewer parameters compared to a standard stacked 2-layer LSTM model trained with dropout (Zaremba et al., 2014). In contrast with the current usage of skip connections, we show that densely connecting only a few stacked layers with skip connections already yields significant perplexity reductions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2623.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2623 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2623 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2623/>NewsQA : A Machine Comprehension Dataset<span class=acl-fixed-case>N</span>ews<span class=acl-fixed-case>QA</span>: A Machine Comprehension Dataset</a></strong><br><a href=/people/a/adam-trischler/>Adam Trischler</a>
|
<a href=/people/t/tong-wang/>Tong Wang</a>
|
<a href=/people/x/xingdi-yuan/>Xingdi Yuan</a>
|
<a href=/people/j/justin-harris/>Justin Harris</a>
|
<a href=/people/a/alessandro-sordoni/>Alessandro Sordoni</a>
|
<a href=/people/p/philip-bachman/>Philip Bachman</a>
|
<a href=/people/k/kaheer-suleman/>Kaheer Suleman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2623><div class="card-body p-3 small">We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> from <a href=https://en.wikipedia.org/wiki/CNN>CNN</a>, with answers consisting of spans of text in the articles. We collect this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> through a four-stage process designed to solicit exploratory questions that require <a href=https://en.wikipedia.org/wiki/Reason>reasoning</a>. Analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure <a href=https://en.wikipedia.org/wiki/Human>human</a> performance on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and compare it to several strong neural models. The performance gap between <a href=https://en.wikipedia.org/wiki/Human>humans</a> and <a href=https://en.wikipedia.org/wiki/Machine>machines</a> (13.3 % F1) indicates that significant progress can be made on NewsQA through future research. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is freely available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2624.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2624 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2624 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2624/>Intrinsic and Extrinsic Evaluation of Spatiotemporal Text Representations in Twitter Streams<span class=acl-fixed-case>T</span>witter Streams</a></strong><br><a href=/people/l/lawrence-phillips/>Lawrence Phillips</a>
|
<a href=/people/k/kyle-shaffer/>Kyle Shaffer</a>
|
<a href=/people/d/dustin-arendt/>Dustin Arendt</a>
|
<a href=/people/n/nathan-hodas/>Nathan Hodas</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2624><div class="card-body p-3 small">Language in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is a dynamic system, constantly evolving and adapting, with words and concepts rapidly emerging, disappearing, and changing their meaning. These changes can be estimated using word representations in context, over time and across locations. A number of methods have been proposed to track these spatiotemporal changes but no general method exists to evaluate the quality of these representations. Previous work largely focused on qualitative evaluation, which we improve by proposing a set of <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualizations</a> that highlight changes in text representation over both space and time. We demonstrate usefulness of novel spatiotemporal representations to explore and characterize specific aspects of the <a href=https://en.wikipedia.org/wiki/Twitter>corpus of tweets</a> collected from European countries over a two-week period centered around the <a href=https://en.wikipedia.org/wiki/2016_Brussels_bombings>terrorist attacks</a> in Brussels in March 2016. In addition, we quantitatively evaluate spatiotemporal representations by feeding them into a downstream classification task event type prediction. Thus, our work is the first to provide both intrinsic (qualitative) and extrinsic (quantitative) evaluation of text representations for spatiotemporal trends.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2625.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2625 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2625 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2625.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-2625/>Rethinking Skip-thought : A Neighborhood based Approach</a></strong><br><a href=/people/s/shuai-tang/>Shuai Tang</a>
|
<a href=/people/h/hailin-jin/>Hailin Jin</a>
|
<a href=/people/c/chen-fang/>Chen Fang</a>
|
<a href=/people/z/zhaowen-wang/>Zhaowen Wang</a>
|
<a href=/people/v/virginia-de-sa/>Virginia de Sa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2625><div class="card-body p-3 small">We study the skip-thought model with neighborhood information as weak supervision. More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood. We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic relatedness</a>, <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a>, and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification benchmarks</a>. Both quantitative comparison and qualitative investigation are conducted. We empirically show that, our skip-thought neighbor model performs as well as the skip-thought model on evaluation tasks. In addition, we found that, incorporating an autoencoder path in our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> did n&#8217;t aid our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to perform better, while it hurts the performance of the skip-thought model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2629.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2629 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2629 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2629/>Adversarial Generation of Natural Language</a></strong><br><a href=/people/s/sandeep-subramanian/>Sandeep Subramanian</a>
|
<a href=/people/s/sai-rajeswar/>Sai Rajeswar</a>
|
<a href=/people/f/francis-dutil/>Francis Dutil</a>
|
<a href=/people/c/christopher-pal/>Chris Pal</a>
|
<a href=/people/a/aaron-courville/>Aaron Courville</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2629><div class="card-body p-3 small">Generative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding impressive results for image generation. Advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images, and still lag far behind likelihood based methods. In this paper, we take a step towards generating <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a> with a GAN objective alone. We introduce a simple baseline that addresses the discrete output space problem without relying on gradient estimators and show that it is able to achieve state-of-the-art results on a Chinese poem generation dataset. We present quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2630.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2630 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2630 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2630" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2630/>Deep Active Learning for Named Entity Recognition</a></strong><br><a href=/people/y/yanyao-shen/>Yanyao Shen</a>
|
<a href=/people/h/hyokun-yun/>Hyokun Yun</a>
|
<a href=/people/z/zachary-c-lipton/>Zachary Lipton</a>
|
<a href=/people/y/yakov-kronrod/>Yakov Kronrod</a>
|
<a href=/people/a/animashree-anandkumar/>Animashree Anandkumar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2630><div class="card-body p-3 small">Deep neural networks have advanced the state of the art in <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>. However, under typical <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training procedures</a>, advantages over <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>classical methods</a> emerge only with <a href=https://en.wikipedia.org/wiki/Data_set>large datasets</a>. As a result, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> is employed only when large public datasets or a large budget for manually labeling data is available. In this work, we show otherwise : by combining <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> with active learning, we can outperform classical methods even with a significantly smaller amount of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2631.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2631 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2631 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2631/>Learning when to skim and when to read</a></strong><br><a href=/people/a/alexander-johansen/>Alexander Johansen</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2631><div class="card-body p-3 small">Many recent advances in <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> have come at increasing <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a>, but the power of these state-of-the-art <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> is not needed for every example in a dataset. We demonstrate two approaches to reducing unnecessary computation in cases where a fast but weak baseline classier and a stronger, slower model are both available. Applying an AUC-based metric to the task of sentiment classification, we find significant efficiency gains with both a probability-threshold method for reducing computational cost and one that uses a secondary decision network.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2632.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2632 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2632 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2632/>Learning to Embed Words in Context for Syntactic Tasks</a></strong><br><a href=/people/l/lifu-tu/>Lifu Tu</a>
|
<a href=/people/k/kevin-gimpel/>Kevin Gimpel</a>
|
<a href=/people/k/karen-livescu/>Karen Livescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2632><div class="card-body p-3 small">We present <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for embedding words in the context of surrounding words. Such models, which we refer to as token embeddings, represent the characteristics of a word that are specific to a given context, such as <a href=https://en.wikipedia.org/wiki/Word_sense>word sense</a>, <a href=https://en.wikipedia.org/wiki/Syntactic_category>syntactic category</a>, and <a href=https://en.wikipedia.org/wiki/Semantic_role>semantic role</a>. We explore simple, efficient token embedding models based on standard neural network architectures. We learn token embeddings on a large amount of unannotated text and evaluate them as features for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech taggers</a> and dependency parsers trained on much smaller amounts of annotated data. We find that <a href=https://en.wikipedia.org/wiki/Dependent_and_independent_variables>predictors</a> endowed with token embeddings consistently outperform baseline predictors across a range of context window and training set sizes.</div></div></div><hr><div id=w17-27><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-27.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-27/>Proceedings of the Events and Stories in the News Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2700/>Proceedings of the Events and Stories in the News Workshop</a></strong><br><a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/b/ben-miller/>Ben Miller</a>
|
<a href=/people/m/marieke-van-erp/>Marieke van Erp</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a>
|
<a href=/people/d/david-caswell/>David Caswell</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2702 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2702/>Detecting Changes in Twitter Streams using Temporal Clusters of Hashtags<span class=acl-fixed-case>T</span>witter Streams using Temporal Clusters of Hashtags</a></strong><br><a href=/people/y/yunli-wang/>Yunli Wang</a>
|
<a href=/people/c/cyril-goutte/>Cyril Goutte</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2702><div class="card-body p-3 small">Detecting events from social media data has important applications in <a href=https://en.wikipedia.org/wiki/Public_security>public security</a>, <a href=https://en.wikipedia.org/wiki/Politics>political issues</a>, and <a href=https://en.wikipedia.org/wiki/Public_health>public health</a>. Many studies have focused on detecting specific or unspecific events from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter streams</a>. However, not much attention has been paid to detecting changes, and their impact, in online conversations related to an event. We propose methods for detecting such changes, using clustering of temporal profiles of hashtags, and three change point detection algorithms. The methods were tested on two Twitter datasets : one covering the 2014 Ottawa shooting event, and one covering the <a href=https://en.wikipedia.org/wiki/2014_Winter_Olympics>Sochi winter Olympics</a>. We compare our approach to a <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> consisting of detecting change from raw counts in the conversation. We show that our method produces large gains in change detection accuracy on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2703/>Event Detection Using Frame-Semantic Parser</a></strong><br><a href=/people/e/evangelia-spiliopoulou/>Evangelia Spiliopoulou</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/t/teruko-mitamura/>Teruko Mitamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2703><div class="card-body p-3 small">Recent methods for Event Detection focus on <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> for automatic feature generation and feature ranking. However, most of those approaches fail to exploit rich semantic information, which results in relatively poor <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a>. This paper is a small & focused contribution, where we introduce an Event Detection and classification system, based on deep semantic information retrieved from a frame-semantic parser. Our experiments show that our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves higher <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> than state-of-the-art systems. Further, we claim that enhancing our <a href=https://en.wikipedia.org/wiki/System>system</a> with deep learning techniques like feature ranking can achieve even better results, as it can benefit from both approaches.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2708/>Inference of Fine-Grained Event Causality from Blogs and Films</a></strong><br><a href=/people/z/zhichao-hu/>Zhichao Hu</a>
|
<a href=/people/e/elahe-rahimtoroghi/>Elahe Rahimtoroghi</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2708><div class="card-body p-3 small">Human understanding of narrative is mainly driven by reasoning about <a href=https://en.wikipedia.org/wiki/Causality>causal relations</a> between events and thus recognizing them is a key capability for computational models of language understanding. Computational work in this area has approached this via two different routes : by focusing on acquiring a knowledge base of common causal relations between events, or by attempting to understand a particular story or macro-event, along with its storyline. In this position paper, we focus on knowledge acquisition approach and claim that <a href=https://en.wikipedia.org/wiki/News_agency>newswire</a> is a relatively poor source for learning fine-grained causal relations between everyday events. We describe experiments using an <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> to learn <a href=https://en.wikipedia.org/wiki/Causality>causal relations</a> between events in the narrative genres of <a href=https://en.wikipedia.org/wiki/First-person_narrative>first-person narratives</a> and film scene descriptions. We show that our method learns fine-grained causal relations, judged by humans as likely to be causal over 80 % of the time. We also demonstrate that the learned event pairs do not exist in publicly available event-pair datasets extracted from <a href=https://en.wikipedia.org/wiki/News_agency>newswire</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2709/>On the Creation of a Security-Related Event Corpus</a></strong><br><a href=/people/m/martin-atkinson/>Martin Atkinson</a>
|
<a href=/people/j/jakub-piskorski/>Jakub Piskorski</a>
|
<a href=/people/h/hristo-tanev/>Hristo Tanev</a>
|
<a href=/people/v/vanni-zavarella/>Vanni Zavarella</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2709><div class="card-body p-3 small">This paper reports on an effort of creating a corpus of structured information on security-related events automatically extracted from <a href=https://en.wikipedia.org/wiki/Online_newspaper>on-line news</a>, part of which has been manually curated. The main motivation behind this effort is to provide material to the NLP community working on <a href=https://en.wikipedia.org/wiki/Event_extraction>event extraction</a> that could be used both for training and evaluation purposes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2710.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2710 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2710 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2710/>Inducing Event Types and Roles in Reverse : Using <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>Function</a> to Discover Theme</a></strong><br><a href=/people/n/natalie-ahn/>Natalie Ahn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2710><div class="card-body p-3 small">With growing interest in automated event extraction, there is an increasing need to overcome the labor costs of hand-written event templates, entity lists, and annotated corpora. In the last few years, more inductive approaches have emerged, seeking to discover unknown event types and roles in <a href=https://en.wikipedia.org/wiki/Text_corpus>raw text</a>. The main recent efforts use probabilistic generative models, as in <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a>, which are formally concise but do not always yield stable or easily interpretable results. We argue that event schema induction can benefit from greater structure in the process and in linguistic features that distinguish words&#8217; functions and <a href=https://en.wikipedia.org/wiki/Theme_(narrative)>themes</a>. To maximize our use of limited data, we reverse the typical schema induction steps and introduce new similarity measures, building an intuitive process for inducing the structure of unknown events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2711.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2711 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2711 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2711/>The Event StoryLine Corpus : A New Benchmark for Causal and Temporal Relation Extraction<span class=acl-fixed-case>S</span>tory<span class=acl-fixed-case>L</span>ine Corpus: A New Benchmark for Causal and Temporal Relation Extraction</a></strong><br><a href=/people/t/tommaso-caselli/>Tommaso Caselli</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2711><div class="card-body p-3 small">This paper reports on the Event StoryLine Corpus (ESC) v1.0, a new benchmark dataset for the temporal and causal relation detection. By developing this dataset, we also introduce a new task, the StoryLine Extraction from news data, which aims at extracting and classifying events relevant for stories, from across news documents spread in time and clustered around a single seminal event or topic. In addition to describing the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, we also report on three baselines systems whose results show the <a href=https://en.wikipedia.org/wiki/Complexity>complexity</a> of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and suggest directions for the development of more robust systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2712.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2712 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2712 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2712/>The Rich Event Ontology</a></strong><br><a href=/people/s/susan-windisch-brown/>Susan Brown</a>
|
<a href=/people/c/claire-bonial/>Claire Bonial</a>
|
<a href=/people/l/leo-obrst/>Leo Obrst</a>
|
<a href=/people/m/martha-palmer/>Martha Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2712><div class="card-body p-3 small">In this paper we describe a new lexical semantic resource, The Rich Event On-tology, which provides an independent conceptual backbone to unify existing semantic role labeling (SRL) schemas and augment them with event-to-event causal and temporal relations. By unifying the FrameNet, VerbNet, Automatic Content Extraction, and Rich Entities, Relations and Events resources, the ontology serves as a shared hub for the disparate annotation schemas and therefore enables the combination of SRL training data into a larger, more diverse corpus. By adding temporal and causal relational information not found in any of the independent resources, the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> facilitates reasoning on and across documents, revealing relationships between events that come together in temporal and causal chains to build more complex scenarios. We envision the open resource serving as a valuable tool for both moving from the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> to text to query for event types and scenarios of interest, and for moving from text to the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> to access interpretations of events using the combined semantic information housed there.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2713 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2713/>Integrating Decompositional Event Structures into Storylines</a></strong><br><a href=/people/w/william-croft/>William Croft</a>
|
<a href=/people/p/pavlina-peskova/>Pavlína Pešková</a>
|
<a href=/people/m/michael-regan/>Michael Regan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2713><div class="card-body p-3 small">Storyline research links together events in stories and specifies shared participants in those stories. In these analyses, an <a href=https://en.wikipedia.org/wiki/Atomic_event>atomic event</a> is assumed to be a single clause headed by a single verb. However, many analyses of verbal semantics assume a decompositional analysis of events expressed in single clauses. We present a formalization of a decompositional analysis of events in which each participant in a clausal event has their own temporally extended subevent, and the subevents are related through causal and other interactions. This decomposition allows us to represent <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>storylines</a> as an evolving set of interactions between participants over time.</div></div></div><hr><div id=w17-28><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-28.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-28/>Proceedings of the First Workshop on Language Grounding for Robotics</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2800/>Proceedings of the First Workshop on Language Grounding for Robotics</a></strong><br><a href=/people/m/mohit-bansal/>Mohit Bansal</a>
|
<a href=/people/c/cynthia-matuszek/>Cynthia Matuszek</a>
|
<a href=/people/j/jacob-andreas/>Jacob Andreas</a>
|
<a href=/people/y/yoav-artzi/>Yoav Artzi</a>
|
<a href=/people/y/yonatan-bisk/>Yonatan Bisk</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2801.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2801.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-2801/>Grounding Language for Interactive Task Learning</a></strong><br><a href=/people/p/peter-lindes/>Peter Lindes</a>
|
<a href=/people/a/aaron-mininger/>Aaron Mininger</a>
|
<a href=/people/j/james-r-kirk/>James R. Kirk</a>
|
<a href=/people/j/john-e-laird/>John E. Laird</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2801><div class="card-body p-3 small">This paper describes how <a href=https://en.wikipedia.org/wiki/Language>language</a> is grounded by a comprehension system called Lucia within a robotic agent called Rosie that can manipulate objects and navigate indoors. The whole system is built within the Soar cognitive architecture and uses Embodied Construction Grammar (ECG) as a formalism for describing linguistic knowledge. Grounding is performed using knowledge from the <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> itself, from the linguistic context, from the agents perception, and from an ontology of long-term knowledge about object categories and properties and actions the agent can perform. The paper also describes a benchmark corpus of 200 sentences in this domain along with test versions of the <a href=https://en.wikipedia.org/wiki/World_model>world model</a> and <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> and gold-standard meanings for each of the sentences. The benchmark is contained in the supplemental materials.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2802/>Learning how to Learn : An Adaptive Dialogue Agent for Incrementally Learning Visually Grounded Word Meanings</a></strong><br><a href=/people/y/yanchao-yu/>Yanchao Yu</a>
|
<a href=/people/a/arash-eshghi/>Arash Eshghi</a>
|
<a href=/people/o/oliver-lemon/>Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2802><div class="card-body p-3 small">We present an optimised multi-modal dialogue agent for interactive learning of visually grounded word meanings from a human tutor, trained on real human-human tutoring data. Within a life-long interactive learning period, the <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a>, trained using Reinforcement Learning (RL), must be able to handle natural conversations with human users, and achieve good learning performance (i.e. accuracy) while minimising human effort in the <a href=https://en.wikipedia.org/wiki/Learning>learning process</a>. We train and evaluate this system in interaction with a simulated human tutor, which is built on the BURCHAK corpus a Human-Human Dialogue dataset for the visual learning task. The results show that : 1) The learned <a href=https://en.wikipedia.org/wiki/Policy>policy</a> can coherently interact with the simulated user to achieve the goal of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> (i.e. learning visual attributes of objects, e.g. colour and shape) ; and 2) it finds a better trade-off between classifier accuracy and tutoring costs than hand-crafted rule-based policies, including ones with dynamic policies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2803/>Guiding Interaction Behaviors for Multi-modal Grounded Language Learning</a></strong><br><a href=/people/j/jesse-thomason/>Jesse Thomason</a>
|
<a href=/people/j/jivko-sinapov/>Jivko Sinapov</a>
|
<a href=/people/r/raymond-mooney/>Raymond Mooney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2803><div class="card-body p-3 small">Multi-modal grounded language learning connects language predicates to physical properties of objects in the world. Sensing with multiple modalities, such as audio, <a href=https://en.wikipedia.org/wiki/Haptic_perception>haptics</a>, and visual colors and shapes while performing interaction behaviors like lifting, dropping, and looking on objects enables a robot to ground non-visual predicates like empty as well as visual predicates like red. Previous work has established that grounding in <a href=https://en.wikipedia.org/wiki/Multimodal_interaction>multi-modal space</a> improves performance on object retrieval from <a href=https://en.wikipedia.org/wiki/Description>human descriptions</a>. In this work, we gather behavior annotations from humans and demonstrate that these improve language grounding performance by allowing a system to focus on relevant behaviors for words like white or half-full that can be understood by looking or lifting, respectively. We also explore adding modality annotations (whether to focus on audio or haptics when performing a behavior), which improves performance, and sharing information between linguistically related predicates (if green is a color, white is a color), which improves grounding recall but at the cost of precision.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2805/>Natural Language Grounding and Grammar Induction for Robotic Manipulation Commands</a></strong><br><a href=/people/m/muhannad-alomari/>Muhannad Alomari</a>
|
<a href=/people/p/paul-duckworth/>Paul Duckworth</a>
|
<a href=/people/m/majd-hawasly/>Majd Hawasly</a>
|
<a href=/people/d/david-c-hogg/>David C. Hogg</a>
|
<a href=/people/a/anthony-g-cohn/>Anthony G. Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2805><div class="card-body p-3 small">We present a cognitively plausible system capable of acquiring knowledge in language and vision from pairs of short video clips and <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic descriptions</a>. The aim of this work is to teach a robot manipulator how to execute <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language commands</a> by demonstration. This is achieved by first learning a set of visual &#8216;concepts&#8217; that abstract the visual feature spaces into concepts that have human-level meaning. Second, learning the mapping / grounding between words and the extracted visual concepts. Third, inducing <a href=https://en.wikipedia.org/wiki/Grammar>grammar rules</a> via a semantic representation known as Robot Control Language (RCL). We evaluate our approach against state-of-the-art supervised and unsupervised grounding and grammar induction systems, and show that a robot can learn to execute never seen-before commands from pairs of unlabelled linguistic and visual inputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2807/>Grounding Symbols in Multi-Modal Instructions</a></strong><br><a href=/people/y/yordan-hristov/>Yordan Hristov</a>
|
<a href=/people/s/svetlin-penkov/>Svetlin Penkov</a>
|
<a href=/people/a/alex-lascarides/>Alex Lascarides</a>
|
<a href=/people/s/subramanian-ramamoorthy/>Subramanian Ramamoorthy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2807><div class="card-body p-3 small">As robots begin to cohabit with humans in semi-structured environments, the need arises to understand instructions involving rich variabilityfor instance, learning to ground symbols in the physical world. Realistically, this task must cope with small datasets consisting of a particular users&#8217; contextual assignment of meaning to terms. We present a method for processing a raw stream of cross-modal inputi.e., <a href=https://en.wikipedia.org/wiki/Natural_language_processing>linguistic instructions</a>, visual perception of a scene and a concurrent trace of 3D eye tracking fixationsto produce the segmentation of objects with a correspondent association to <a href=https://en.wikipedia.org/wiki/High-_and_low-level>high-level concepts</a>. To test our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> we present experiments in a table-top object manipulation scenario. Our results show our model learns the user&#8217;s notion of colour and shape from a small number of physical demonstrations, generalising to identifying physical referents for novel combinations of the words.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2809 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2809" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2809/>A Tale of Two DRAGGNs : A Hybrid Approach for Interpreting Action-Oriented and Goal-Oriented Instructions<span class=acl-fixed-case>DRAGGN</span>s: A Hybrid Approach for Interpreting Action-Oriented and Goal-Oriented Instructions</a></strong><br><a href=/people/s/siddharth-karamcheti/>Siddharth Karamcheti</a>
|
<a href=/people/e/edward-clem-williams/>Edward Clem Williams</a>
|
<a href=/people/d/dilip-arumugam/>Dilip Arumugam</a>
|
<a href=/people/m/mina-rhee/>Mina Rhee</a>
|
<a href=/people/n/nakul-gopalan/>Nakul Gopalan</a>
|
<a href=/people/l/lawson-l-s-wong/>Lawson L.S. Wong</a>
|
<a href=/people/s/stefanie-tellex/>Stefanie Tellex</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2809><div class="card-body p-3 small">Robots operating alongside humans in diverse, <a href=https://en.wikipedia.org/wiki/Stochastic_process>stochastic environments</a> must be able to accurately interpret <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language commands</a>. These instructions often fall into one of two categories : those that specify a goal condition or target state, and those that specify explicit actions, or how to perform a given task. Recent approaches have used reward functions as a semantic representation of goal-based commands, which allows for the use of a state-of-the-art planner to find a <a href=https://en.wikipedia.org/wiki/Policy>policy</a> for the given <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. However, these reward functions can not be directly used to represent action-oriented commands. We introduce a new hybrid approach, the Deep Recurrent Action-Goal Grounding Network (DRAGGN), for task grounding and execution that handles <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a> from either category as input, and generalizes to unseen environments. Our robot-simulation results demonstrate that a system successfully interpreting both goal-oriented and action-oriented task specifications brings us closer to robust <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a> for <a href=https://en.wikipedia.org/wiki/Human&#8211;robot_interaction>human-robot interaction</a>.</div></div></div><hr><div id=w17-29><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-29.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-29/>Proceedings of the Second Workshop on NLP and Computational Social Science</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2900/>Proceedings of the Second Workshop on <span class=acl-fixed-case>NLP</span> and Computational Social Science</a></strong><br><a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/s/svitlana-volkova/>Svitlana Volkova</a>
|
<a href=/people/d/david-bamman/>David Bamman</a>
|
<a href=/people/d/david-jurgens/>David Jurgens</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a>
|
<a href=/people/o/oren-tsur/>Oren Tsur</a>
|
<a href=/people/a/a-seza-dogruoz/>A. Seza Doğruöz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2901.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2901 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2901/>Language-independent Gender Prediction on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/n/nikola-ljubesic/>Nikola Ljubešić</a>
|
<a href=/people/d/darja-fiser/>Darja Fišer</a>
|
<a href=/people/t/tomaz-erjavec/>Tomaž Erjavec</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2901><div class="card-body p-3 small">In this paper we present a set of experiments and analyses on predicting the gender of Twitter users based on language-independent features extracted either from the text or the metadata of users&#8217; tweets. We perform our experiments on the TwiSty dataset containing manual gender annotations for users speaking six different languages. Our classification results show that, while the prediction model based on language-independent features performs worse than the <a href=https://en.wikipedia.org/wiki/Bag-of-words_model>bag-of-words model</a> when training and testing on the same language, it regularly outperforms the <a href=https://en.wikipedia.org/wiki/Bag-of-words_model>bag-of-words model</a> when applied to different languages, showing very stable results across various languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> correspond to cultural distances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-2902" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-2902/>When does a compliment become sexist? Analysis and classification of ambivalent sexism using twitter data</a></strong><br><a href=/people/a/akshita-jha/>Akshita Jha</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2902><div class="card-body p-3 small">Sexism is prevalent in today&#8217;s society, both offline and online, and poses a credible threat to <a href=https://en.wikipedia.org/wiki/Social_equality>social equality</a> with respect to gender. According to ambivalent sexism theory (Glick and Fiske, 1996), it comes in two forms : Hostile and Benevolent. While <a href=https://en.wikipedia.org/wiki/Hostile_sexism>hostile sexism</a> is characterized by an explicitly negative attitude, <a href=https://en.wikipedia.org/wiki/Benevolent_sexism>benevolent sexism</a> is more subtle. Previous works on computationally detecting sexism present online are restricted to identifying the hostile form. Our objective is to investigate the less pronounced form of <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> demonstrated online. We achieve this by creating and analyzing a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> that exhibit <a href=https://en.wikipedia.org/wiki/Benevolent_sexism>benevolent sexism</a>. By using Support Vector Machines (SVM), sequence-to-sequence models and FastText classifier, we classify tweets into &#8216;Hostile&#8217;, &#8216;Benevolent&#8217; or &#8216;Others&#8217; class depending on the kind of sexism they exhibit. We have been able to achieve an <a href=https://en.wikipedia.org/wiki/F-score>F1-score</a> of 87.22 % using FastText classifier. Our work helps analyze and understand the much prevalent ambivalent sexism in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-2903.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-2903/>Personality Driven Differences in Paraphrase Preference</a></strong><br><a href=/people/d/daniel-preotiuc-pietro/>Daniel Preoţiuc-Pietro</a>
|
<a href=/people/j/jordan-carpenter/>Jordan Carpenter</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2903><div class="card-body p-3 small">Personality plays a decisive role in how people behave in different <a href=https://en.wikipedia.org/wiki/Scenario_analysis>scenarios</a>, including <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a>. Researchers have used such <a href=https://en.wikipedia.org/wiki/Data>data</a> to study how <a href=https://en.wikipedia.org/wiki/Personality>personality</a> can be predicted from <a href=https://en.wikipedia.org/wiki/Usage_(language)>language use</a>. In this paper, we study phrase choice as a particular stylistic linguistic difference, as opposed to the mostly topical differences identified previously. Building on previous work on demographic preferences, we quantify differences in paraphrase choice from a massive Facebook data set with posts from over 115,000 users. We quantify the predictive power of phrase choice in user profiling and use phrase choice to study psycholinguistic hypotheses. This work is relevant to future applications that aim to personalize <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a> to specific personality types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2904/>community2vec : Vector representations of online communities encode semantic relationships</a></strong><br><a href=/people/t/trevor-martin/>Trevor Martin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2904><div class="card-body p-3 small">Vector embeddings of words have been shown to encode meaningful semantic relationships that enable solving of complex analogies. This vector embedding concept has been extended successfully to many different domains and in this paper we both create and visualize vector representations of an unstructured collection of online communities based on user participation. Further, we quantitatively and qualitatively show that these representations allow solving of semantically meaningful community analogies and also other more general types of relationships. These results could help improve community recommendation engines and also serve as a tool for sociological studies of community relatedness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2905.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2905 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2905 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2905/>Telling Apart Tweets Associated with Controversial versus Non-Controversial Topics</a></strong><br><a href=/people/a/aseel-addawood/>Aseel Addawood</a>
|
<a href=/people/r/rezvaneh-rezapour/>Rezvaneh Rezapour</a>
|
<a href=/people/o/omid-abdar/>Omid Abdar</a>
|
<a href=/people/j/jana-diesner/>Jana Diesner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2905><div class="card-body p-3 small">In this paper, we evaluate the <a href=https://en.wikipedia.org/wiki/Predictability>predictability of tweets</a> associated with controversial versus non-controversial topics. As a first step, we crowd-sourced the scoring of a predefined set of topics on a <a href=https://en.wikipedia.org/wiki/Likert_scale>Likert scale</a> from non-controversial to controversial. Our <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature set</a> entails and goes beyond <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment features</a>, e.g., by leveraging empathic language and other <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that have been previously used but are new for this particular study. We find focusing on the structural characteristics of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> to be beneficial for this task. Using a combination of emphatic, language-specific, and Twitter-specific features for <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> resulted in 87 % accuracy (F1) for <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>cross-validation</a> of the training set and 63.4 % accuracy when using the test set. Our analysis shows that features specific to <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> or <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, in general, are more prevalent in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> on controversial topics than in non-controversial ones. To test the premise of the paper, we conducted two additional sets of experiments, which led to mixed results. This finding will inform our future investigations into the relationship between language use on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and the perceived controversiality of topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2906/>Cross-Lingual Classification of Topics in Political Texts</a></strong><br><a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/f/federico-nanni/>Federico Nanni</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2906><div class="card-body p-3 small">In this paper, we propose an approach for cross-lingual topical coding of sentences from electoral manifestos of political parties in different languages. To this end, we exploit continuous semantic text representations and induce a joint multilingual semantic vector spaces to enable <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> using manually-coded sentences across different languages. Our experimental results show that <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> trained on multilingual data yield performance boosts over monolingual topic classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2907/>Mining Social Science Publications for Survey Variables</a></strong><br><a href=/people/a/andrea-zielinski/>Andrea Zielinski</a>
|
<a href=/people/p/peter-mutschke/>Peter Mutschke</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2907><div class="card-body p-3 small">Research in <a href=https://en.wikipedia.org/wiki/Social_science>Social Science</a> is usually based on <a href=https://en.wikipedia.org/wiki/Survey_methodology>survey data</a> where individual research questions relate to observable concepts (variables). However, due to a lack of standards for data citations a reliable identification of the variables used is often difficult. In this paper, we present a work-in-progress study that seeks to provide a solution to the variable detection task based on supervised machine learning algorithms, using a linguistic analysis pipeline to extract a rich feature set, including terminological concepts and similarity metric scores. Further, we present preliminary results on a small dataset that has been specifically designed for this task, yielding a significant increase in performance over the random baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2908/>Linguistic Markers of Influence in Informal Interactions</a></strong><br><a href=/people/s/shrimai-prabhumoye/>Shrimai Prabhumoye</a>
|
<a href=/people/s/samridhi-choudhary/>Samridhi Choudhary</a>
|
<a href=/people/e/evangelia-spiliopoulou/>Evangelia Spiliopoulou</a>
|
<a href=/people/c/christopher-bogart/>Christopher Bogart</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rose</a>
|
<a href=/people/a/alan-w-black/>Alan W Black</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2908><div class="card-body p-3 small">There has been a long standing interest in understanding &#8216;<a href=https://en.wikipedia.org/wiki/Social_influence>Social Influence</a>&#8217; both in <a href=https://en.wikipedia.org/wiki/Social_science>Social Sciences</a> and in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>Computational Linguistics</a>. In this paper, we present a novel approach to study and measure <a href=https://en.wikipedia.org/wiki/Interpersonal_influence>interpersonal influence</a> in <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>daily interactions</a>. Motivated by the basic principles of <a href=https://en.wikipedia.org/wiki/Social_influence>influence</a>, we attempt to identify indicative linguistic features of the posts in an online knitting community. We present the <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a> used to operationalize and label the posts as influential or non-influential. Experiments with the identified <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> show an improvement in the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> of <a href=https://en.wikipedia.org/wiki/Social_influence>influence</a> by 3.15 %. Our results illustrate the important correlation between the structure of the language and its potential to influence others.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2909/>Non-lexical Features Encode Political Affiliation on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/r/rachael-tatman/>Rachael Tatman</a>
|
<a href=/people/l/leo-stewart/>Leo Stewart</a>
|
<a href=/people/a/amandalynne-paullada/>Amandalynne Paullada</a>
|
<a href=/people/e/emma-spiro/>Emma Spiro</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2909><div class="card-body p-3 small">Previous work on classifying Twitter users&#8217; political alignment has mainly focused on lexical and social network features. This study provides evidence that political affiliation is also reflected in features which have been previously overlooked : users&#8217; discourse patterns (proportion of Tweets that are retweets or replies) and their rate of use of <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a> and <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>. We find robust differences between politically left- and right-leaning communities with respect to these discourse and sub-lexical features, although they are not enough to train a high-accuracy classifier.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2912/>How Does Twitter User Behavior Vary Across Demographic Groups?<span class=acl-fixed-case>T</span>witter User Behavior Vary Across Demographic Groups?</a></strong><br><a href=/people/z/zach-wood-doughty/>Zach Wood-Doughty</a>
|
<a href=/people/m/michael-smith/>Michael Smith</a>
|
<a href=/people/d/david-broniatowski/>David Broniatowski</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2912><div class="card-body p-3 small">Demographically-tagged social media messages are a common source of data for <a href=https://en.wikipedia.org/wiki/Computational_social_science>computational social science</a>. While these messages can indicate differences in beliefs and behaviors between demographic groups, we do not have a clear understanding of how different demographic groups use platforms such as <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. This paper presents a preliminary analysis of how groups&#8217; differing behaviors may confound analyses of the groups themselves. We analyzed one million <a href=https://en.wikipedia.org/wiki/Twitter>Twitter users</a> by first inferring <a href=https://en.wikipedia.org/wiki/Demography>demographic attributes</a>, and then measuring several <a href=https://en.wikipedia.org/wiki/Indicator_(statistics)>indicators</a> of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter behavior</a>. We find differences in these indicators across demographic groups, suggesting that there may be underlying differences in how different demographic groups use <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-2913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-2913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-2913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-2913/>Ideological Phrase Indicators for Classification of Political Discourse Framing on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/k/kristen-johnson/>Kristen Johnson</a>
|
<a href=/people/i/i-ta-lee/>I-Ta Lee</a>
|
<a href=/people/d/dan-goldwasser/>Dan Goldwasser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-2913><div class="card-body p-3 small">Politicians carefully word their statements in order to influence how others view an issue, a <a href=https://en.wikipedia.org/wiki/Political_strategy>political strategy</a> called <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>framing</a>. Simultaneously, these <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>frames</a> may also reveal the beliefs or positions on an issue of the politician. Simple language features such as <a href=https://en.wikipedia.org/wiki/Unigram>unigrams</a>, <a href=https://en.wikipedia.org/wiki/Bigram>bigrams</a>, and <a href=https://en.wikipedia.org/wiki/Trigram>trigrams</a> are important indicators for identifying the general frame of a text, for both longer congressional speeches and shorter tweets of politicians. However, <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> may contain multiple <a href=https://en.wikipedia.org/wiki/Unigram>unigrams</a> across different frames which limits the effectiveness of this approach. In this paper, we present a joint model which uses both linguistic features of tweets and ideological phrase indicators extracted from a state-of-the-art embedding-based model to predict the general frame of political tweets.</div></div></div><hr><div id=w17-30><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-30.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-30/>Proceedings of the First Workshop on Abusive Language Online</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3000/>Proceedings of the First Workshop on Abusive Language Online</a></strong><br><a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/w/wendy-hui-kyong-chung/>Wendy Hui Kyong Chung</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3001 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3001/>Dimensions of Abusive Language on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/i/isobelle-clarke/>Isobelle Clarke</a>
|
<a href=/people/j/jack-grieve/>Jack Grieve</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3001><div class="card-body p-3 small">In this paper, we use a new categorical form of multidimensional register analysis to identify the main dimensions of functional linguistic variation in a corpus of abusive language, consisting of racist and sexist Tweets. By analysing the use of a wide variety of parts-of-speech and grammatical constructions, as well as various features related to <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Computer-mediated_communication>computer-mediated communication</a>, we discover three dimensions of linguistic variation in this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, which we interpret as being related to the degree of interactive, antagonistic and attitudinal language exhibited by individual Tweets. We then demonstrate that there is a significant functional difference between racist and sexist Tweets, with sexists Tweets tending to be more interactive and attitudinal than racist Tweets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3002/>Constructive Language in News Comments</a></strong><br><a href=/people/v/varada-kolhatkar/>Varada Kolhatkar</a>
|
<a href=/people/m/maite-taboada/>Maite Taboada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3002><div class="card-body p-3 small">We discuss the characteristics of constructive news comments, and present methods to identify them. First, we define the notion of constructiveness. Second, we annotate a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for <a href=https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_education)>constructiveness</a>. Third, we explore whether available argumentation corpora can be useful to identify constructiveness in news comments. Our <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation corpora</a> achieves a top <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 72.59 % (baseline=49.44 %) on our crowd-annotated test data. Finally, we examine the relation between constructiveness and <a href=https://en.wikipedia.org/wiki/Toxicity>toxicity</a>. In our crowd-annotated data, 21.42 % of the non-constructive comments and 17.89 % of the constructive comments are toxic, suggesting that non-constructive comments are not much more toxic than constructive comments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3003/>Rephrasing Profanity in Chinese Text<span class=acl-fixed-case>C</span>hinese Text</a></strong><br><a href=/people/h/hui-po-su/>Hui-Po Su</a>
|
<a href=/people/z/zhen-jie-huang/>Zhen-Jie Huang</a>
|
<a href=/people/h/hao-tsung-chang/>Hao-Tsung Chang</a>
|
<a href=/people/c/chuan-jie-lin/>Chuan-Jie Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3003><div class="card-body p-3 small">This paper proposes a <a href=https://en.wikipedia.org/wiki/System>system</a> that can detect and rephrase profanity in <a href=https://en.wikipedia.org/wiki/Written_Chinese>Chinese text</a>. Rather than just masking detected profanity, we want to revise the input sentence by using inoffensive words while keeping their original meanings. 29 of such rephrasing rules were invented after observing sentences on real-word social websites. The overall <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of the proposed <a href=https://en.wikipedia.org/wiki/System>system</a> is 85.56 %</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3004/>Deep Learning for User Comment Moderation</a></strong><br><a href=/people/j/john-pavlopoulos/>John Pavlopoulos</a>
|
<a href=/people/p/prodromos-malakasiotis/>Prodromos Malakasiotis</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3004><div class="card-body p-3 small">Experimenting with a new dataset of 1.6 M user comments from a Greek news portal and existing datasets of EnglishWikipedia comments, we show that an <a href=https://en.wikipedia.org/wiki/Random-access_memory>RNN</a> outperforms the previous state of the art in <a href=https://en.wikipedia.org/wiki/Moderation_system>moderation</a>. A deep, classification-specific attention mechanism improves further the overall performance of the <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>RNN</a>. We also compare against a <a href=https://en.wikipedia.org/wiki/CNN>CNN</a> and a word-list baseline, considering both fully automatic and semi-automatic moderation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3005/>Class-based Prediction Errors to Detect Hate Speech with Out-of-vocabulary Words</a></strong><br><a href=/people/j/joan-serra/>Joan Serrà</a>
|
<a href=/people/i/ilias-leontiadis/>Ilias Leontiadis</a>
|
<a href=/people/d/dimitris-spathis/>Dimitris Spathis</a>
|
<a href=/people/g/gianluca-stringhini/>Gianluca Stringhini</a>
|
<a href=/people/j/jeremy-blackburn/>Jeremy Blackburn</a>
|
<a href=/people/a/athena-vakali/>Athena Vakali</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3005><div class="card-body p-3 small">Common approaches to <a href=https://en.wikipedia.org/wiki/Categorization>text categorization</a> essentially rely either on <a href=https://en.wikipedia.org/wiki/N-gram>n-gram counts</a> or on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. This presents important difficulties in highly dynamic or quickly-interacting environments, where the appearance of new words and/or varied misspellings is the norm. A paradigmatic example of this situation is abusive online behavior, with <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> and <a href=https://en.wikipedia.org/wiki/Mass_media>media platforms</a> struggling to effectively combat uncommon or non-blacklisted hate words. To better deal with these issues in those fast-paced environments, we propose using the <a href=https://en.wikipedia.org/wiki/Error_signal>error signal</a> of class-based language models as input to text classification algorithms. In particular, we train a next-character prediction model for any given class and then exploit the error of such class-based models to inform a neural network classifier. This way, we shift from the &#8216;ability to describe&#8217; seen documents to the &#8216;ability to predict&#8217; unseen content. Preliminary studies using out-of-vocabulary splits from abusive tweet data show promising results, outperforming competitive text categorization strategies by 4-11 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3006/>One-step and Two-step Classification for Abusive Language Detection on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/j/ji-ho-park/>Ji Ho Park</a>
|
<a href=/people/p/pascale-fung/>Pascale Fung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3006><div class="card-body p-3 small">Automatic abusive language detection is a difficult but important task for <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a>. Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of <a href=https://en.wikipedia.org/wiki/Sexism>sexism</a> and <a href=https://en.wikipedia.org/wiki/Racism>racism</a>, our approach shows a promising performance of 0.827 <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> by using HybridCNN in one-step and 0.824 <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> by using <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> in two-steps.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3008/>Abusive Language Detection on Arabic Social Media<span class=acl-fixed-case>A</span>rabic Social Media</a></strong><br><a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a>
|
<a href=/people/w/walid-magdy/>Walid Magdy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3008><div class="card-body p-3 small">In this paper, we present our work on detecting abusive language on Arabic social media. We extract a list of <a href=https://en.wikipedia.org/wiki/Obscenity>obscene words</a> and <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> using common patterns used in offensive and rude communications. We also classify Twitter users according to whether they use any of these <a href=https://en.wikipedia.org/wiki/Word>words</a> or not in their tweets. We expand the list of obscene words using this classification, and we report results on a newly created dataset of classified Arabic tweets (obscene, offensive, and clean). We make this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> freely available for research, in addition to the list of obscene words and <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a>. We are also publicly releasing a large corpus of classified user comments that were deleted from a popular Arabic news site due to violations the site&#8217;s rules and guidelines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3009 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3009/>Vectors for Counterspeech on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/l/lucas-wright/>Lucas Wright</a>
|
<a href=/people/d/derek-ruths/>Derek Ruths</a>
|
<a href=/people/k/kelly-p-dillon/>Kelly P Dillon</a>
|
<a href=/people/h/haji-mohammad-saleem/>Haji Mohammad Saleem</a>
|
<a href=/people/s/susan-benesch/>Susan Benesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3009><div class="card-body p-3 small">A study of conversations on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> found that some arguments between strangers led to favorable change in discourse and even in <a href=https://en.wikipedia.org/wiki/Attitude_(psychology)>attitudes</a>. The authors propose that such exchanges can be usefully distinguished according to whether individuals or groups take part on each side, since the opportunity for a constructive exchange of views seems to vary accordingly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3010 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3010/>Detecting Nastiness in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/n/niloofar-safi-samghabadi/>Niloofar Safi Samghabadi</a>
|
<a href=/people/s/suraj-maharjan/>Suraj Maharjan</a>
|
<a href=/people/a/alan-sprague/>Alan Sprague</a>
|
<a href=/people/r/raquel-diaz-sprague/>Raquel Diaz-Sprague</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3010><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> has made it easy for people to connect on a virtually unlimited basis, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> has also opened doors to people who misuse it to undermine, harass, humiliate, threaten and bully others. There is a lack of adequate resources to detect and hinder its occurrence. In this paper, we present our initial NLP approach to detect invective posts as a first step to eventually detect and deter <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>. We crawl data containing <a href=https://en.wikipedia.org/wiki/Profanity>profanities</a> and then determine whether or not it contains invective. Annotations on this <a href=https://en.wikipedia.org/wiki/Data>data</a> are improved iteratively by in-lab annotations and <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a>. We pursue different NLP approaches containing various typical and some newer techniques to distinguish the use of <a href=https://en.wikipedia.org/wiki/Profanity>swear words</a> in a neutral way from those instances in which they are used in an insulting way. We also show that this <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> not only works for our <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>, but also can be successfully applied to different <a href=https://en.wikipedia.org/wiki/Data_set>data sets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3012/>Understanding Abuse : A Typology of Abusive Language Detection Subtasks</a></strong><br><a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/t/thomas-davidson/>Thomas Davidson</a>
|
<a href=/people/d/dana-warmsley/>Dana Warmsley</a>
|
<a href=/people/i/ingmar-weber/>Ingmar Weber</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3012><div class="card-body p-3 small">As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>, <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>, and <a href=https://en.wikipedia.org/wiki/Online_abuse>online abuse</a> we propose a typology that captures central similarities and differences between subtasks and discuss the implications of this for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3014 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3014/>Illegal is not a Noun : Linguistic Form for Detection of Pejorative Nominalizations</a></strong><br><a href=/people/a/alexis-palmer/>Alexis Palmer</a>
|
<a href=/people/m/melissa-robinson/>Melissa Robinson</a>
|
<a href=/people/k/kristy-k-phillips/>Kristy K. Phillips</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3014><div class="card-body p-3 small">This paper focuses on a particular type of abusive language, targeting expressions in which typically neutral adjectives take on pejorative meaning when used as nouns-compare &#8216;gay people&#8217; to &#8216;the gays&#8217;. We first collect and analyze a corpus of hand-curated, expert-annotated pejorative nominalizations for four target adjectives : female, gay, illegal, and poor. We then collect a second corpus of automatically-extracted and POS-tagged, crowd-annotated tweets. For both corpora, we find support for the hypothesis that some <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a>, when nominalized, take on negative meaning. The targeted constructions are non-standard yet widely-used, and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech taggers</a> mistag some nominal forms as <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a>. We implement a tool called NomCatcher to correct these mistaggings, and find that the same tool is effective for identifying new adjectives subject to transformation via <a href=https://en.wikipedia.org/wiki/Nominalization>nominalization</a> into abusive language.</div></div></div><hr><div id=w17-31><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-31.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-31/>Proceedings of the Fourth Workshop on Computational Linguistics and Clinical Psychology — From Linguistic Signal to Clinical Reality</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3100/>Proceedings of the Fourth Workshop on Computational Linguistics and Clinical Psychology — From Linguistic Signal to Clinical Reality</a></strong><br><a href=/people/k/kristy-hollingshead/>Kristy Hollingshead</a>
|
<a href=/people/m/molly-ireland/>Molly E. Ireland</a>
|
<a href=/people/k/kate-loveys/>Kate Loveys</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3101/>A Cross-modal Review of Indicators for Depression Detection Systems</a></strong><br><a href=/people/m/michelle-morales/>Michelle Morales</a>
|
<a href=/people/s/stefan-scherer/>Stefan Scherer</a>
|
<a href=/people/r/rivka-levitan/>Rivka Levitan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3101><div class="card-body p-3 small">Automatic detection of depression has attracted increasing attention from researchers in <a href=https://en.wikipedia.org/wiki/Psychology>psychology</a>, <a href=https://en.wikipedia.org/wiki/Computer_science>computer science</a>, <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a>, and related disciplines. As a result, promising <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depression detection systems</a> have been reported. This paper surveys these efforts by presenting the first cross-modal review of depression detection systems and discusses best practices and most promising approaches to this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3102/>In your wildest dreams : the language and psychological features of dreams</a></strong><br><a href=/people/k/kate-niederhoffer/>Kate Niederhoffer</a>
|
<a href=/people/j/jonathan-schler/>Jonathan Schler</a>
|
<a href=/people/p/patrick-crutchley/>Patrick Crutchley</a>
|
<a href=/people/k/kate-loveys/>Kate Loveys</a>
|
<a href=/people/g/glen-coppersmith/>Glen Coppersmith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3102><div class="card-body p-3 small">In this paper, we provide the first quantified exploration of the structure of the language of dreams, their linguistic style and <a href=https://en.wikipedia.org/wiki/Emotion>emotional content</a>. We present a collection of digital dream logs as a viable corpus for the growing study of <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a> through the lens of language, complementary to the work done examining more traditional <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. This paper is largely exploratory in nature to lay the groundwork for subsequent research in <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a>, rather than optimizing a particular text classification task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3103/>A Corpus Analysis of Social Connections and <a href=https://en.wikipedia.org/wiki/Social_isolation>Social Isolation</a> in Adolescents Suffering from Depressive Disorders</a></strong><br><a href=/people/j/jia-wen-guo/>Jia-Wen Guo</a>
|
<a href=/people/d/danielle-l-mowery/>Danielle L Mowery</a>
|
<a href=/people/d/djin-lai/>Djin Lai</a>
|
<a href=/people/k/katherine-sward/>Katherine Sward</a>
|
<a href=/people/m/mike-conway/>Mike Conway</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3103><div class="card-body p-3 small">Social connection and <a href=https://en.wikipedia.org/wiki/Social_isolation>social isolation</a> are associated with <a href=https://en.wikipedia.org/wiki/Major_depressive_disorder>depressive symptoms</a>, particularly in adolescents and young adults, but how these concepts are documented in <a href=https://en.wikipedia.org/wiki/Medical_record>clinical notes</a> is unknown. This pilot study aimed to identify the topics relevant to <a href=https://en.wikipedia.org/wiki/Interpersonal_relationship>social connection</a> and <a href=https://en.wikipedia.org/wiki/Social_isolation>isolation</a> by analyzing 145 clinical notes from patients with <a href=https://en.wikipedia.org/wiki/Major_depressive_disorder>depression diagnosis</a>. We found that providers, including <a href=https://en.wikipedia.org/wiki/Physician>physicians</a>, <a href=https://en.wikipedia.org/wiki/Nursing>nurses</a>, social workers, and psychologists, document descriptions of both <a href=https://en.wikipedia.org/wiki/Social_connection>social connection</a> and <a href=https://en.wikipedia.org/wiki/Social_isolation>social isolation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3105/>Investigating Patient Attitudes Towards the use of Social Media Data to Augment Depression Diagnosis and Treatment : a Qualitative Study</a></strong><br><a href=/people/j/jude-mikal/>Jude Mikal</a>
|
<a href=/people/s/samantha-hurst/>Samantha Hurst</a>
|
<a href=/people/m/mike-conway/>Mike Conway</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3105><div class="card-body p-3 small">In this paper, we use <a href=https://en.wikipedia.org/wiki/Qualitative_research>qualitative research methods</a> to investigate the attitudes of <a href=https://en.wikipedia.org/wiki/Social_media>social media users</a> towards the (opt-in) integration of social media data with routine mental health care and diagnosis. Our investigation was based on secondary analysis of a series of five focus groups with Twitter users, including three groups consisting of participants with a self-reported history of depression, and two groups consisting of participants without a self reported history of depression. Our results indicate that, overall, research participants were enthusiastic about the possibility of using <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> (in conjunction with automated Natural Language Processing algorithms) for mood tracking under the supervision of a mental health practitioner. However, for at least some participants, there was skepticism related to how well <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> represents the mental health of users, and hence its usefulness in the clinical context.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3106/>Natural-language Interactive Narratives in Imaginal Exposure Therapy for Obsessive-Compulsive Disorder</a></strong><br><a href=/people/m/melissa-roemmele/>Melissa Roemmele</a>
|
<a href=/people/p/paola-mardo/>Paola Mardo</a>
|
<a href=/people/a/andrew-gordon/>Andrew Gordon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3106><div class="card-body p-3 small">Obsessive-compulsive disorder (OCD) is an <a href=https://en.wikipedia.org/wiki/Anxiety_disorder>anxiety-based disorder</a> that affects around 2.5 % of the population. A common treatment for <a href=https://en.wikipedia.org/wiki/Obsessive&#8211;compulsive_disorder>OCD</a> is <a href=https://en.wikipedia.org/wiki/Exposure_therapy>exposure therapy</a>, where the patient repeatedly confronts a feared experience, which has the long-term effect of decreasing their anxiety. Some exposures consist of reading and writing stories about an imagined anxiety-provoking scenario. In this paper, we present a technology that enables patients to interactively contribute to exposure stories by supplying natural language input (typed or spoken) that advances a scenario. This <a href=https://en.wikipedia.org/wiki/Interactivity>interactivity</a> could potentially increase the patient&#8217;s sense of immersion in an exposure and contribute to its success. We introduce the NLP task behind processing inputs to predict new events in the <a href=https://en.wikipedia.org/wiki/Scenario>scenario</a>, and describe our initial approach. We then illustrate the future possibility of this work with an example of an exposure scenario authored with our <a href=https://en.wikipedia.org/wiki/Application_software>application</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3107/>Detecting Anxiety through Reddit<span class=acl-fixed-case>R</span>eddit</a></strong><br><a href=/people/j/judy-hanwen-shen/>Judy Hanwen Shen</a>
|
<a href=/people/f/frank-rudzicz/>Frank Rudzicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3107><div class="card-body p-3 small">Previous investigations into detecting mental illnesses through <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> have predominately focused on detecting depression through Twitter corpora. In this paper, we study <a href=https://en.wikipedia.org/wiki/Anxiety_disorder>anxiety disorders</a> through personal narratives collected through the popular social media website, <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a>. We build a substantial data set of typical and anxiety-related posts, and we apply N-gram language modeling, vector embeddings, topic analysis, and emotional norms to generate features that accurately classify posts related to binary levels of anxiety. We achieve an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 91 % with vector-space word embeddings, and an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 98 % when combined with lexicon-based features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3109/>A Dictionary-Based Comparison of <a href=https://en.wikipedia.org/wiki/Autobiography>Autobiographies</a> by People and Murderous Monsters</a></strong><br><a href=/people/m/micah-iserman/>Micah Iserman</a>
|
<a href=/people/m/molly-ireland/>Molly Ireland</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3109><div class="card-body p-3 small">People typically assume that killers are mentally ill or fundamentally different from the rest of humanity. Similarly, people often associate <a href=https://en.wikipedia.org/wiki/Mental_disorder>mental health conditions</a> (such as <a href=https://en.wikipedia.org/wiki/Schizophrenia>schizophrenia</a> or autism) with <a href=https://en.wikipedia.org/wiki/Violence>violence</a> and otherness-treatable perhaps, but not empathically understandable. We take a dictionary approach to explore word use in a set of <a href=https://en.wikipedia.org/wiki/Autobiography>autobiographies</a>, comparing the narratives of 2 killers (Adolf Hitler and Elliot Rodger) and 39 non-killers. Although results suggest several dimensions that differentiate these autobiographies-such as <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a>, temporal orientation, and references to death-they appear to reflect subject matter rather than psychology per se. Additionally, the Rodger text shows roughly typical developmental arcs in its use of words relating to <a href=https://en.wikipedia.org/wiki/Friendship>friends</a>, <a href=https://en.wikipedia.org/wiki/Family>family</a>, <a href=https://en.wikipedia.org/wiki/Sex>sex</a>, and <a href=https://en.wikipedia.org/wiki/Affect_(psychology)>affect</a>. From these data, we discuss the challenges of understanding <a href=https://en.wikipedia.org/wiki/Murder>killers</a> and people in general.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3110/>Small but Mighty : Affective Micropatterns for Quantifying Mental Health from Social Media Language</a></strong><br><a href=/people/k/kate-loveys/>Kate Loveys</a>
|
<a href=/people/p/patrick-crutchley/>Patrick Crutchley</a>
|
<a href=/people/e/emily-wyatt/>Emily Wyatt</a>
|
<a href=/people/g/glen-coppersmith/>Glen Coppersmith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3110><div class="card-body p-3 small">Many <a href=https://en.wikipedia.org/wiki/Phenomenon>psychological phenomena</a> occur in small time windows, measured in minutes or hours. However, most <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistic techniques</a> look at data on the order of weeks, months, or years. We explore micropatterns in sequences of messages occurring over a short time window for their prevalence and power for quantifying psychological phenomena, specifically, patterns in affect. We examine affective micropatterns in social media posts from users with anxiety, <a href=https://en.wikipedia.org/wiki/Eating_disorder>eating disorders</a>, <a href=https://en.wikipedia.org/wiki/Panic_attack>panic attacks</a>, <a href=https://en.wikipedia.org/wiki/Schizophrenia>schizophrenia</a>, <a href=https://en.wikipedia.org/wiki/Suicidality>suicidality</a>, and matched controls.</div></div></div><hr><div id=w17-32><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-32.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-32/>Proceedings of the First Workshop on Neural Machine Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3200/>Proceedings of the First Workshop on Neural Machine Translation</a></strong><br><a href=/people/m/minh-thang-luong/>Thang Luong</a>
|
<a href=/people/a/alexandra-birch/>Alexandra Birch</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/a/andrew-finch/>Andrew Finch</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3201/>An Empirical Study of Adequate <a href=https://en.wikipedia.org/wiki/Vision_span>Vision Span</a> for Attention-Based Neural Machine Translation</a></strong><br><a href=/people/r/raphael-shu/>Raphael Shu</a>
|
<a href=/people/h/hideki-nakayama/>Hideki Nakayama</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3201><div class="card-body p-3 small">Recently, the attention mechanism plays a key role to achieve high performance for Neural Machine Translation models. However, as it computes a <a href=https://en.wikipedia.org/wiki/Score_function>score function</a> for the encoder states in all positions at each decoding step, the attention model greatly increases the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a>. In this paper, we investigate the adequate vision span of attention models in the context of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, by proposing a novel attention framework that is capable of reducing redundant score computation dynamically. The term <a href=https://en.wikipedia.org/wiki/Vision_span>vision span</a>&#8217; means a window of the encoder states considered by the attention model in one step. In our experiments, we found that the average window size of vision span can be reduced by over 50 % with modest loss in accuracy on English-Japanese and German-English translation tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3202/>Analyzing Neural MT Search and Model Performance<span class=acl-fixed-case>MT</span> Search and Model Performance</a></strong><br><a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/e/eunah-cho/>Eunah Cho</a>
|
<a href=/people/t/thanh-le-ha/>Thanh-Le Ha</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3202><div class="card-body p-3 small">In this paper, we offer an in-depth analysis about the modeling and search performance. We address the question if a more complex <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithm</a> is necessary. Furthermore, we investigate the question if more complex <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> which might only be applicable during rescoring are promising. By separating the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> and the <a href=https://en.wikipedia.org/wiki/Mathematical_model>modeling</a> using n-best list reranking, we analyze the influence of both parts of an NMT system independently. By comparing differently performing NMT systems, we show that the better translation is already in the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> of the translation systems with less performance. This results indicate that the current <a href=https://en.wikipedia.org/wiki/Search_algorithm>search algorithms</a> are sufficient for the <a href=https://en.wikipedia.org/wiki/Network_topology>NMT systems</a>. Furthermore, we could show that even a relatively small n-best list of 50 hypotheses already contain notably better translations.<tex-math>n</tex-math>-best list of 50 hypotheses already contain notably better translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3203/>Stronger Baselines for Trustable Results in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/m/michael-denkowski/>Michael Denkowski</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3203><div class="card-body p-3 small">Interest in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> has grown rapidly as its effectiveness has been demonstrated across language and data scenarios. New research regularly introduces architectural and algorithmic improvements that lead to significant gains over vanilla NMT implementations. However, these new <a href=https://en.wikipedia.org/wiki/Software_development_process>techniques</a> are rarely evaluated in the context of previously published <a href=https://en.wikipedia.org/wiki/Software_development_process>techniques</a>, specifically those that are widely used in state-of-the-art production and shared-task systems. As a result, it is often difficult to determine whether improvements from research will carry over to <a href=https://en.wikipedia.org/wiki/System>systems</a> deployed for real-world use. In this work, we recommend three specific <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> that are relatively easy to implement and result in much stronger experimental systems. Beyond reporting significantly higher BLEU scores, we conduct an in-depth analysis of where improvements originate and what inherent weaknesses of basic NMT models are being addressed. We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen. This indicates that choosing a strong <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> is crucial for reporting reliable experimental results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3205/>Cost Weighting for Neural Machine Translation Domain Adaptation</a></strong><br><a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/c/colin-cherry/>Colin Cherry</a>
|
<a href=/people/g/george-foster/>George Foster</a>
|
<a href=/people/s/samuel-larkin/>Samuel Larkin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3205><div class="card-body p-3 small">In this paper, we propose a new domain adaptation technique for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> called cost weighting, which is appropriate for adaptation scenarios in which a small in-domain data set and a large general-domain data set are available. Cost weighting incorporates a domain classifier into the neural machine translation training algorithm, using features derived from the encoder representation in order to distinguish in-domain from out-of-domain data. Classifier probabilities are used to weight sentences according to their domain similarity when updating the parameters of the neural translation model. We compare cost weighting to two traditional domain adaptation techniques developed for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> : data selection and sub-corpus weighting. Experiments on two large-data tasks show that both the traditional techniques and our novel proposal lead to significant gains, with cost weighting outperforming the traditional methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3206/>Detecting Untranslated Content for Neural Machine Translation</a></strong><br><a href=/people/i/isao-goto/>Isao Goto</a>
|
<a href=/people/h/hideki-tanaka/>Hideki Tanaka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3206><div class="card-body p-3 small">Despite its promise, neural machine translation (NMT) has a serious problem in that source content may be mistakenly left untranslated. The ability to detect untranslated content is important for the practical use of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a>. We evaluate two types of <a href=https://en.wikipedia.org/wiki/Probability>probability</a> with which to detect untranslated content : the cumulative attention (ATN) probability and back translation (BT) probability from the target sentence to the source sentence. Experiments on detecting untranslated content in Japanese-English patent translations show that ATN and BT are each more effective than random choice, BT is more effective than ATN, and the combination of the two provides further improvements. We also confirmed the effectiveness of using <a href=https://en.wikipedia.org/wiki/Atrial_natriuretic_peptide>ATN</a> and <a href=https://en.wikipedia.org/wiki/Thiamine_triphosphate>BT</a> to rerank the n-best NMT outputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3207/>Beam Search Strategies for Neural Machine Translation</a></strong><br><a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/y/yaser-al-onaizan/>Yaser Al-Onaizan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3207><div class="card-body p-3 small">The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance on a given <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpus</a>. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to-right while keeping a fixed amount of active candidates at each time step. First, this simple <a href=https://en.wikipedia.org/wiki/Search_algorithm>search</a> is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the <a href=https://en.wikipedia.org/wiki/Beam_diameter>beam size</a> until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the <a href=https://en.wikipedia.org/wiki/Codec>decoder</a> by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43 % for the two language pairs <a href=https://en.wikipedia.org/wiki/German_language>German</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> to <a href=https://en.wikipedia.org/wiki/English_language>English</a> without losing any translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3209/>Detecting Cross-Lingual Semantic Divergence for Neural Machine Translation</a></strong><br><a href=/people/m/marine-carpuat/>Marine Carpuat</a>
|
<a href=/people/y/yogarshi-vyas/>Yogarshi Vyas</a>
|
<a href=/people/x/xing-niu/>Xing Niu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3209><div class="card-body p-3 small">Parallel corpora are often not as parallel as one might assume : non-literal translations and noisy translations abound, even in curated corpora routinely used for training and evaluation. We use a cross-lingual textual entailment system to distinguish sentence pairs that are parallel in meaning from those that are not, and show that filtering out divergent examples from training improves translation quality.</div></div></div><hr><div id=w17-34><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-34.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-34/>Proceedings of the 15th Meeting on the Mathematics of Language</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3400/>Proceedings of the 15th Meeting on the Mathematics of Language</a></strong><br><a href=/people/m/makoto-kanazawa/>Makoto Kanazawa</a>
|
<a href=/people/p/philippe-de-groote/>Philippe de Groote</a>
|
<a href=/people/m/mehrnoosh-sadrzadeh/>Mehrnoosh Sadrzadeh</a></span></p></div><hr><div id=w17-35><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-35.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-35/>Proceedings of the 10th International Conference on Natural Language Generation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3500/>Proceedings of the 10th International Conference on Natural Language Generation</a></strong><br><a href=/people/j/jose-m-alonso/>Jose M. Alonso</a>
|
<a href=/people/a/alberto-bugarin-diz/>Alberto Bugarín</a>
|
<a href=/people/e/ehud-reiter/>Ehud Reiter</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3501/>Linguistic realisation as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> : Comparing different MT models for AMR-to-text generation<span class=acl-fixed-case>MT</span> models for <span class=acl-fixed-case>AMR</span>-to-text generation</a></strong><br><a href=/people/t/thiago-castro-ferreira/>Thiago Castro Ferreira</a>
|
<a href=/people/i/iacer-calixto/>Iacer Calixto</a>
|
<a href=/people/s/sander-wubben/>Sander Wubben</a>
|
<a href=/people/e/emiel-krahmer/>Emiel Krahmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3501><div class="card-body p-3 small">In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrase-based and Neural MT). We systematically study the effects of 3 AMR preprocessing steps (Delexicalisation, <a href=https://en.wikipedia.org/wiki/Data_compression>Compression</a>, and Linearisation) applied before the MT phase. Our results show that <a href=https://en.wikipedia.org/wiki/Data_preprocessing>preprocessing</a> indeed helps, although the benefits differ for the two MT models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3502/>A Survey on Intelligent Poetry Generation : <a href=https://en.wikipedia.org/wiki/Language>Languages</a>, Features, <a href=https://en.wikipedia.org/wiki/Technology>Techniques</a>, Reutilisation and Evaluation</a></strong><br><a href=/people/h/hugo-goncalo-oliveira/>Hugo Gonçalo Oliveira</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3502><div class="card-body p-3 small">Poetry generation is becoming popular among researchers of <a href=https://en.wikipedia.org/wiki/Natural-language_generation>Natural Language Generation</a>, <a href=https://en.wikipedia.org/wiki/Computational_creativity>Computational Creativity</a> and, broadly, <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>Artificial Intelligence</a>. To produce text that may be regarded as <a href=https://en.wikipedia.org/wiki/Poetry>poetry</a>, <a href=https://en.wikipedia.org/wiki/Poetry>poetry generation systems</a> are typically knowledge-intensive and have to deal with several levels of language, from lexical to semantics. Interest on the topic resulted in the development of several <a href=https://en.wikipedia.org/wiki/Poetry_generator>poetry generators</a> described in the literature, with different features covered or handled differently, by a broad range of alternative approaches, as well as different perspectives on <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a>, another challenging aspect due the underlying subjectivity. This paper surveys intelligent poetry generators around a set of relevant axis for poetry generation targeted languages, form and content features, techniques, reutilisation of material, and evaluation and aims to organise work developed on this topic so far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3504/>Content Selection for Real-time Sports News Construction from Commentary Texts</a></strong><br><a href=/people/j/jin-ge-yao/>Jin-ge Yao</a>
|
<a href=/people/j/jianmin-zhang/>Jianmin Zhang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a>
|
<a href=/people/j/jianguo-xiao/>Jianguo Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3504><div class="card-body p-3 small">We study the task of constructing sports news report automatically from <a href=https://en.wikipedia.org/wiki/Sports_commentator>live commentary</a> and focus on content selection. Rather than receiving every piece of text of a sports match before news construction, as in previous related work, we novelly verify the feasibility of a more challenging but more useful setting to generate news report on the fly by treating live text input as a stream. Specifically, we design various scoring functions to address different requirements of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. The near submodularity of scoring functions makes it possible to adapt efficient <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy algorithms</a> even in stream data settings. Experiments suggest that our proposed framework can already produce comparable results compared with previous work that relies on a supervised learning-to-rank model with heavy feature engineering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3505/>Improving the Naturalness and Expressivity of <a href=https://en.wikipedia.org/wiki/Language_generation>Language Generation</a> for Spanish<span class=acl-fixed-case>S</span>panish</a></strong><br><a href=/people/c/cristina-barros/>Cristina Barros</a>
|
<a href=/people/d/dimitra-gkatzia/>Dimitra Gkatzia</a>
|
<a href=/people/e/elena-lloret/>Elena Lloret</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3505><div class="card-body p-3 small">We present a flexible Natural Language Generation approach for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, focused on the surface realisation stage, which integrates an inflection module in order to improve the naturalness and expressivity of the generated language. This inflection module inflects the verbs using an <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>ensemble of trainable algorithms</a> whereas the other types of words (e.g. nouns, <a href=https://en.wikipedia.org/wiki/Determiner>determiners</a>, etc) are inflected using hand-crafted rules. We show that our approach achieves 2 % higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> than two state-of-art inflection generation approaches. Furthermore, our proposed approach also predicts an extra feature : the inflection of the <a href=https://en.wikipedia.org/wiki/Imperative_mood>imperative mood</a>, which was not taken into account by previous work. We also present a user evaluation, where we demonstrate that the proposed method significantly improves the perceived naturalness of the generated language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-3506" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-3506/>What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?<span class=acl-fixed-case>RNN</span>s) in an Image Caption Generator?</a></strong><br><a href=/people/m/marc-tanti/>Marc Tanti</a>
|
<a href=/people/a/albert-gatt/>Albert Gatt</a>
|
<a href=/people/k/kenneth-camilleri/>Kenneth Camilleri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3506><div class="card-body p-3 small">Image captioning has evolved into a core task for Natural Language Generation and has also proved to be an important testbed for deep learning approaches to handling multimodal representations. Most contemporary approaches rely on a combination of a convolutional network to handle image features, and a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> to encode linguistic information. The latter is typically viewed as the primary generation component. Beyond this high-level characterisation, a CNN+RNN model supports a variety of architectural designs. The dominant model in the literature is one in which visual features encoded by a CNN are injected as part of the linguistic encoding process, driving the RNN&#8217;s linguistic choices. By contrast, it is possible to envisage an <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> in which visual and linguistic features are encoded separately, and merged at a subsequent stage. In this paper, we address two related questions : (1) Is direct injection the best way of combining multimodal information, or is a late merging alternative better for the image captioning task? (2) To what extent should a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> be viewed as actually generating, rather than simply encoding, linguistic information?</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3507/>Exploring the Behavior of Classic REG Algorithms in the Description of Characters in 3D Images<span class=acl-fixed-case>REG</span> Algorithms in the Description of Characters in 3<span class=acl-fixed-case>D</span> Images</a></strong><br><a href=/people/g/gonzalo-mendez/>Gonzalo Méndez</a>
|
<a href=/people/r/raquel-hervas/>Raquel Hervás</a>
|
<a href=/people/s/susana-bautista/>Susana Bautista</a>
|
<a href=/people/a/adrian-rabadan/>Adrián Rabadán</a>
|
<a href=/people/t/teresa-rodriguez/>Teresa Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3507><div class="card-body p-3 small">Describing people and characters can be very useful in different contexts, such as computational narrative or image description for the visually impaired. However, a review of the existing literature shows that the automatic generation of people descriptions has not received much attention. Our work focuses on the description of people in snapshots from a <a href=https://en.wikipedia.org/wiki/3D_computer_graphics>3D environment</a>. First, we have conducted a survey to identify the way in which people describe other people under different conditions. We have used the information extracted from this survey to design several Referring Expression Generation algorithms which produce similar results. We have evaluated these <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> with users in order to identify which ones generate the best description for specific characters in different situations. The evaluation has shown that, in order to generate good descriptions, a combination of different <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> has to be used depending on the features and situation of the person to be described.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3508/>Co-PoeTryMe : a Co-Creative Interface for the Composition of Poetry<span class=acl-fixed-case>P</span>oe<span class=acl-fixed-case>T</span>ry<span class=acl-fixed-case>M</span>e: a Co-Creative Interface for the Composition of Poetry</a></strong><br><a href=/people/h/hugo-goncalo-oliveira/>Hugo Gonçalo Oliveira</a>
|
<a href=/people/t/tiago-mendes/>Tiago Mendes</a>
|
<a href=/people/a/ana-boavida/>Ana Boavida</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3508><div class="card-body p-3 small">Co-PoeTryMe is a web application for poetry composition, guided by the user, though with the help of automatic features, such as the generation of full (editable) drafts, as well as the acquisition of additional well-formed lines, or semantically-related words, possibly constrained by the number of syllables, <a href=https://en.wikipedia.org/wiki/Rhyme>rhyme</a>, or polarity. Towards the final poem, the latter can replace lines or words in the draft.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3509/>Refer-iTTS : A System for Referring in Spoken Installments to Objects in Real-World Images<span class=acl-fixed-case>TTS</span>: A System for Referring in Spoken Installments to Objects in Real-World Images</a></strong><br><a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/m/m-soledad-lopez-gambino/>M. Soledad López Gambino</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3509><div class="card-body p-3 small">Current referring expression generation systems mostly deliver their output as one-shot, written expressions. We present on-going work on incremental generation of spoken expressions referring to objects in real-world images. This approach extends upon previous work using the words-as-classifier model for generation. We implement this generator in an incremental dialogue processing framework such that we can exploit an existing interface to incremental text-to-speech synthesis. Our <a href=https://en.wikipedia.org/wiki/System>system</a> generates and synthesizes <a href=https://en.wikipedia.org/wiki/Reference>referring expressions</a> while continuously observing <a href=https://en.wikipedia.org/wiki/Nonverbal_communication>non-verbal user reactions</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3510/>Finding the right answers for customers</a></strong><br><a href=/people/f/frank-schilder/>Frank Schilder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3510><div class="card-body p-3 small">This talk will present a few NLG systems developed within <a href=https://en.wikipedia.org/wiki/Thomson_Reuters>Thomson Reuters</a> providing information to professionals such as lawyers, accountants or traders. Based on the experience developing these system, I will discuss the usefulness of <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>, <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowd-sourced evaluation</a>, corpora studies and expert reviews. I will conclude with exploring the question of whether developers of NLG systems need to follow ethical guidelines and how those guidelines could be established.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3514/>Evaluation of a Runyankore grammar engine for healthcare messages<span class=acl-fixed-case>R</span>unyankore grammar engine for healthcare messages</a></strong><br><a href=/people/j/joan-byamugisha/>Joan Byamugisha</a>
|
<a href=/people/c/c-maria-keet/>C. Maria Keet</a>
|
<a href=/people/b/brian-derenzi/>Brian DeRenzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3514><div class="card-body p-3 small">Natural Language Generation (NLG) can be used to generate personalized health information, which is especially useful when provided in one&#8217;s own language. However, the NLG technique widely used in different domains and languagestemplateswas shown to be inapplicable to <a href=https://en.wikipedia.org/wiki/Bantu_languages>Bantu languages</a>, due to their characteristic agglutinative structure. We present here our use of the grammar engine NLG technique to generate text in <a href=https://en.wikipedia.org/wiki/Nkore_language>Runyankore</a>, a Bantu language indigenous to <a href=https://en.wikipedia.org/wiki/Uganda>Uganda</a>. Our grammar engine adds to previous work in this field with new rules for <a href=https://en.wikipedia.org/wiki/Cardinality>cardinality constraints</a>, prepositions in roles, the passive, and phonological conditioning. We evaluated the generated text with linguists and non-linguists, who regarded most text as grammatically correct and understandable ; and over 60 % of them regarded all the text generated by our system to have been authored by a human being.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3518 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3518/>The WebNLG Challenge : Generating Text from <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF Data</a><span class=acl-fixed-case>W</span>eb<span class=acl-fixed-case>NLG</span> Challenge: Generating Text from <span class=acl-fixed-case>RDF</span> Data</a></strong><br><a href=/people/c/claire-gardent/>Claire Gardent</a>
|
<a href=/people/a/anastasia-shimorina/>Anastasia Shimorina</a>
|
<a href=/people/s/shashi-narayan/>Shashi Narayan</a>
|
<a href=/people/l/laura-perez-beltrachini/>Laura Perez-Beltrachini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3518><div class="card-body p-3 small">The WebNLG challenge consists in mapping sets of <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF triples</a> to text. It provides a common benchmark on which to train, evaluate and compare <a href=https://en.wikipedia.org/wiki/Microplanners>microplanners</a>, i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation, aggregation, <a href=https://en.wikipedia.org/wiki/Lexicalization>lexicalisation</a>, surface realisation and <a href=https://en.wikipedia.org/wiki/Sentence_segmentation>sentence segmentation</a>. In this paper, we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3520 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3520/>Integrated sentence generation using charts</a></strong><br><a href=/people/a/alexander-koller/>Alexander Koller</a>
|
<a href=/people/n/nikos-engonopoulos/>Nikos Engonopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3520><div class="card-body p-3 small">Integrating surface realization and the generation of referring expressions into a single <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can improve the quality of the generated sentences. Existing <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> for doing this, such as <a href=https://en.wikipedia.org/wiki/SPUD>SPUD</a> and CRISP, are search-based and can be slow or incomplete. We offer a chart-based algorithm for integrated sentence generation and demonstrate its runtime efficiency.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3521.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3521 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3521 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3521/>Adapting SimpleNLG to Spanish<span class=acl-fixed-case>S</span>imple<span class=acl-fixed-case>NLG</span> to <span class=acl-fixed-case>S</span>panish</a></strong><br><a href=/people/a/alejandro-ramos-soto/>Alejandro Ramos-Soto</a>
|
<a href=/people/j/julio-janeiro-gallardo/>Julio Janeiro-Gallardo</a>
|
<a href=/people/a/alberto-bugarin-diz/>Alberto Bugarín Diz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3521><div class="card-body p-3 small">We describe SimpleNLG-ES, an adaptation of the SimpleNLG realization library for the <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish language</a>. Our <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> is based on the bilingual English-French SimpleNLG-EnFr adaptation. The <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a> has been tested using a battery of examples that ensure that the most common <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> and orthography rules for <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> are met. The <a href=https://en.wikipedia.org/wiki/Library_(computing)>library</a> is currently being used in three different projects for the development of data-to-text systems in the meteorological, statistical data information, and business intelligence application domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3522.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3522 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3522 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3522/>G-TUNA : a corpus of referring expressions in <a href=https://en.wikipedia.org/wiki/German_language>German</a>, including duration information<span class=acl-fixed-case>G</span>-<span class=acl-fixed-case>TUNA</span>: a corpus of referring expressions in <span class=acl-fixed-case>G</span>erman, including duration information</a></strong><br><a href=/people/d/david-m-howcroft/>David Howcroft</a>
|
<a href=/people/j/jorrig-vogels/>Jorrig Vogels</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3522><div class="card-body p-3 small">Corpora of referring expressions elicited from human participants in a controlled environment are an important resource for research on automatic referring expression generation. We here present G-TUNA, a new corpus of referring expressions for <a href=https://en.wikipedia.org/wiki/German_language>German</a>. Using the furniture stimuli set developed for the TUNA and D-TUNA corpora, our corpus extends on these <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a> by providing data collected in a simulated driving dual-task setting, and additionally provides exact duration annotations for the spoken referring expressions. This <a href=https://en.wikipedia.org/wiki/Speech_corpus>corpus</a> will hence allow researchers to analyze the interaction between referring expression length and speech rate, under conditions where the listener is under high vs. low cognitive load.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3523/>Toward an NLG System for <a href=https://en.wikipedia.org/wiki/Bantu_languages>Bantu languages</a> : first steps with Runyankore (demo)<span class=acl-fixed-case>NLG</span> System for <span class=acl-fixed-case>B</span>antu languages: first steps with <span class=acl-fixed-case>R</span>unyankore (demo)</a></strong><br><a href=/people/j/joan-byamugisha/>Joan Byamugisha</a>
|
<a href=/people/c/c-maria-keet/>C. Maria Keet</a>
|
<a href=/people/b/brian-derenzi/>Brian DeRenzi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3523><div class="card-body p-3 small">There are many domain-specific and language-specific NLG systems, of which it may be possible to adapt to related domains and languages. The languages in the <a href=https://en.wikipedia.org/wiki/Bantu_languages>Bantu language family</a> have their own set of features distinct from other major groups, which therefore severely limits the options to bootstrap an NLG system from existing ones. We present here our first proof-of-concept application for knowledge-to-text NLG as a plugin to the <a href=https://en.wikipedia.org/wiki/Protege>Protege 5.x ontology development system</a>, tailored to <a href=https://en.wikipedia.org/wiki/Nkore_language>Runyankore</a>, a <a href=https://en.wikipedia.org/wiki/Bantu_languages>Bantu language</a> indigenous to Uganda. It comprises a basic annotation model for linguistic information such as <a href=https://en.wikipedia.org/wiki/Noun_class>noun class</a>, an implementation of existing verbalisation rules and a CFG for verbs, and a basic interface for <a href=https://en.wikipedia.org/wiki/Data_entry_clerk>data entry</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3524 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3524/>A working, non-trivial, topically indifferent NLG System for 17 languages<span class=acl-fixed-case>NLG</span> System for 17 languages</a></strong><br><a href=/people/r/robert-weissgraeber/>Robert Weißgraeber</a>
|
<a href=/people/a/andreas-madsack/>Andreas Madsack</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3524><div class="card-body p-3 small">A fully fledged practical working application for a rule-based NLG system is presented that is able to create non-trivial, human sounding narrative from structured data, in any language and for any topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3525 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3525/>Generating titles for millions of browse pages on an e-Commerce site</a></strong><br><a href=/people/p/prashant-mathur/>Prashant Mathur</a>
|
<a href=/people/n/nicola-ueffing/>Nicola Ueffing</a>
|
<a href=/people/g/gregor-leusch/>Gregor Leusch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3525><div class="card-body p-3 small">We present two approaches to generate titles for browse pages in five different languages, namely <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/German_language>German</a>, <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. These browse pages are structured search pages in an e-commerce domain. We first present a rule-based approach to generate these browse page titles. In addition, we also present a hybrid approach which uses a phrase-based statistical machine translation engine on top of the rule-based system to assemble the best title. For the two languages <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a> we have access to a large amount of already available rule-based generated and curated titles. For these languages we present an automatic post-editing approach which learns how to post-edit the rule-based titles into curated titles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3526 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3526/>Towards Automatic Generation of Product Reviews from Aspect-Sentiment Scores</a></strong><br><a href=/people/h/hongyu-zang/>Hongyu Zang</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3526><div class="card-body p-3 small">Data-to-text generation is very essential and important in machine writing applications. The recent <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, like <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks (RNNs)</a>, have shown a bright future for relevant text generation tasks. However, rare work has been done for automatic generation of long reviews from user opinions. In this paper, we introduce a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network model</a> to generate long Chinese reviews from aspect-sentiment scores representing users&#8217; opinions. We conduct our study within the framework of encoder-decoder networks, and we propose a hierarchical structure with aligned attention in the Long-Short Term Memory (LSTM) decoder. Experiments show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms retrieval based baseline methods, and also beats the sequential generation models in qualitative evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3527 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3527/>A model of suspense for narrative generation</a></strong><br><a href=/people/r/richard-doust/>Richard Doust</a>
|
<a href=/people/p/paul-piwek/>Paul Piwek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3527><div class="card-body p-3 small">Most work on automatic generation of narratives, and more specifically <a href=https://en.wikipedia.org/wiki/Suspense>suspenseful narrative</a>, has focused on detailed domain-specific modelling of character psychology and <a href=https://en.wikipedia.org/wiki/Plot_(narrative)>plot structure</a>. Recent work in <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> on the automatic learning of narrative schemas suggests an alternative approach that exploits such <a href=https://en.wikipedia.org/wiki/Schema_(psychology)>schemas</a> as a starting point for modelling and measuring <a href=https://en.wikipedia.org/wiki/Suspense>suspense</a>. We propose a domain-independent model for tracking <a href=https://en.wikipedia.org/wiki/Suspense>suspense</a> in a story which can be used to predict the audience&#8217;s suspense response on a sentence-by-sentence basis at the content determination stage of narrative generation. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> lends itself as the theoretical foundation for a <a href=https://en.wikipedia.org/wiki/Suspense>suspense module</a> that is compatible with alternative narrative generation theories. The <a href=https://en.wikipedia.org/wiki/Proposal_(business)>proposal</a> is evaluated by human judges&#8217; normalised average scores correlate strongly with predicted values.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3528.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3528 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3528 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3528/>Data-Driven News Generation for Automated Journalism</a></strong><br><a href=/people/l/leo-leppanen/>Leo Leppänen</a>
|
<a href=/people/m/myriam-munezero/>Myriam Munezero</a>
|
<a href=/people/m/mark-granroth-wilding/>Mark Granroth-Wilding</a>
|
<a href=/people/h/hannu-toivonen/>Hannu Toivonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3528><div class="card-body p-3 small">Despite increasing amounts of data and ever improving <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation techniques</a>, work on <a href=https://en.wikipedia.org/wiki/Automated_journalism>automated journalism</a> is still relatively scarce. In this paper, we explore the field and challenges associated with building a journalistic natural language generation system. We present a set of requirements that should guide system design, including <a href=https://en.wikipedia.org/wiki/Transparency_and_translucency>transparency</a>, <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, modifiability and transferability. Guided by the requirements, we present a data-driven architecture for automated journalism that is largely domain and language independent. We illustrate its practical application in the production of <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> about the 2017 Finnish municipal elections in three languages, demonstrating the successfulness of the data-driven, modular approach of the design. We then draw some lessons for future <a href=https://en.wikipedia.org/wiki/Automated_journalism>automated journalism</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3529 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3529/>Data Augmentation for Visual Question Answering</a></strong><br><a href=/people/k/kushal-kafle/>Kushal Kafle</a>
|
<a href=/people/m/mohammed-yousefhussien/>Mohammed Yousefhussien</a>
|
<a href=/people/c/christopher-kanan/>Christopher Kanan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3529><div class="card-body p-3 small">Data augmentation is widely used to train <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> for image classification tasks. Simply flipping images can help learning tremendously by increasing the number of training images by a factor of two. However, little work has been done studying <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Here, we describe two methods for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> for Visual Question Answering (VQA). The <a href=https://en.wikipedia.org/wiki/First_Amendment_to_the_United_States_Constitution>first</a> uses existing <a href=https://en.wikipedia.org/wiki/Semantic_annotation>semantic annotations</a> to generate new questions. The second method is a generative approach using <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. Experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> improves performance of both <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> and state-of-the-art VQA algorithms.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3531 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-3531" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-3531/>A Comparison of Neural Models for Word Ordering</a></strong><br><a href=/people/e/eva-hasler/>Eva Hasler</a>
|
<a href=/people/f/felix-stahlberg/>Felix Stahlberg</a>
|
<a href=/people/m/marcus-tomalin/>Marcus Tomalin</a>
|
<a href=/people/a/adria-de-gispert/>Adrià de Gispert</a>
|
<a href=/people/b/bill-byrne/>Bill Byrne</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3531><div class="card-body p-3 small">We compare several language models for the word-ordering task and propose a new bag-to-sequence neural model based on attention-based sequence-to-sequence models. We evaluate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a large German WMT data set where it significantly outperforms existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We also describe a novel search strategy for LM-based word ordering and report results on the English Penn Treebank. Our best model setup outperforms prior work both in terms of speed and quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3532 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3532/>Investigating the content and form of referring expressions in <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin</a> : introducing the Mtuna corpus<span class=acl-fixed-case>M</span>andarin: introducing the Mtuna corpus</a></strong><br><a href=/people/k/kees-van-deemter/>Kees van Deemter</a>
|
<a href=/people/l/le-sun/>Le Sun</a>
|
<a href=/people/r/rint-sybesma/>Rint Sybesma</a>
|
<a href=/people/x/xiao-li/>Xiao Li</a>
|
<a href=/people/b/bo-chen/>Bo Chen</a>
|
<a href=/people/m/muyun-yang/>Muyun Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3532><div class="card-body p-3 small">East Asian languages are thought to handle reference differently from <a href=https://en.wikipedia.org/wiki/Language>languages</a> such as <a href=https://en.wikipedia.org/wiki/English_language>English</a>, particularly in terms of the marking of definiteness and <a href=https://en.wikipedia.org/wiki/Grammatical_number>number</a>. We present the first Data-Text corpus for Referring Expressions in <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin</a>, and we use this <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to test some initial hypotheses inspired by the theoretical linguistics literature. Our findings suggest that function words deserve more attention in Referring Expressions Generation than they have so far received, and they have a bearing on the debate about whether different languages make different trade-offs between clarity and brevity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3533 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3533/>Realization of long sentences using chunking</a></strong><br><a href=/people/e/ewa-muszynska/>Ewa Muszyńska</a>
|
<a href=/people/a/ann-copestake/>Ann Copestake</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3533><div class="card-body p-3 small">We propose sentence chunking as a way to reduce the time and memory costs of realization of long sentences. During chunking we divide the semantic representation of a sentence into smaller components which can be processed and recombined without loss of information. Our meaning representation of choice is the Dependency Minimal Recursion Semantics (DMRS). We show that realizing chunks of a sentence and combining the results of such realizations increases the coverage for long sentences, significantly reduces the resources required and does not affect the quality of the realization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3534 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3534/>SaToS : Assessing and Summarising Terms of Services from German Webshops<span class=acl-fixed-case>S</span>a<span class=acl-fixed-case>T</span>o<span class=acl-fixed-case>S</span>: Assessing and Summarising Terms of Services from <span class=acl-fixed-case>G</span>erman Webshops</a></strong><br><a href=/people/d/daniel-braun/>Daniel Braun</a>
|
<a href=/people/e/elena-scepankova/>Elena Scepankova</a>
|
<a href=/people/p/patrick-holl/>Patrick Holl</a>
|
<a href=/people/f/florian-matthes/>Florian Matthes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3534><div class="card-body p-3 small">Every time we buy something online, we are confronted with <a href=https://en.wikipedia.org/wiki/Terms_of_service>Terms of Services</a>. However, only a few people actually read these terms, before accepting them, often to their disadvantage. In this paper, we present the SaToS browser plugin which summarises and simplifies <a href=https://en.wikipedia.org/wiki/Terms_of_service>Terms of Services</a> from German webshops.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3535.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3535 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3535 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3535/>Textually Summarising Incomplete Data</a></strong><br><a href=/people/s/stephanie-inglis/>Stephanie Inglis</a>
|
<a href=/people/e/ehud-reiter/>Ehud Reiter</a>
|
<a href=/people/s/somayajulu-sripada/>Somayajulu Sripada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3535><div class="card-body p-3 small">Many data-to-text NLG systems work with data sets which are incomplete, ie some of the data is missing. We have worked with data journalists to understand how they describe incomplete data, and are building NLG algorithms based on these insights. A pilot evaluation showed mixed results, and highlighted several areas where we need to improve our <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3537.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3537 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3537 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3537/>Analysing Data-To-Text Generation Benchmarks</a></strong><br><a href=/people/l/laura-perez-beltrachini/>Laura Perez-Beltrachini</a>
|
<a href=/people/c/claire-gardent/>Claire Gardent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3537><div class="card-body p-3 small">A generation system can only be as good as the data it is trained on. In this short paper, we propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> for analysing data-to-text corpora used for training Natural Language Generation (NLG) systems. We apply this <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to three existing <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmarks</a>. We conclude by eliciting a set of criteria for the creation of a data-to-text benchmark which could help better support the development, evaluation and comparison of linguistically sophisticated data-to-text generators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3538.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3538 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3538 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3538/>Linguistic Description of Complex Phenomena with the rLDCP R Package<span class=acl-fixed-case>LDCP</span> <span class=acl-fixed-case>R</span> Package</a></strong><br><a href=/people/j/jose-m-alonso/>Jose Alonso</a>
|
<a href=/people/p/patricia-conde-clemente/>Patricia Conde-Clemente</a>
|
<a href=/people/g/gracian-trivino/>Gracian Trivino</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3538><div class="card-body p-3 small">Monitoring and analysis of complex phenomena attract the attention of both academy and industry. Dealing with data produced by <a href=https://en.wikipedia.org/wiki/Complex_system>complex phenomena</a> requires the use of advance <a href=https://en.wikipedia.org/wiki/Computational_intelligence>computational intelligence techniques</a>. Namely, linguistic description of complex phenomena constitutes a mature research line. It is supported by the Computational Theory of Perceptions grounded on the <a href=https://en.wikipedia.org/wiki/Fuzzy_set>Fuzzy Sets Theory</a>. Its aim is the development of computational systems with the ability to generate vague descriptions of the world in a similar way how humans do. This is a human-centric and multi-disciplinary research work. Moreover, its success is a matter of careful design ; thus, developers play a key role. The rLDCP R package was designed to facilitate the development of new applications. This demo introduces the use of rLDCP, for both beginners and advance developers, in practical use cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3539.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3539 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3539 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3539/>A demo of FORGe : the Pompeu Fabra Open Rule-based Generator<span class=acl-fixed-case>FORG</span>e: the <span class=acl-fixed-case>P</span>ompeu <span class=acl-fixed-case>F</span>abra Open Rule-based Generator</a></strong><br><a href=/people/s/simon-mille/>Simon Mille</a>
|
<a href=/people/l/leo-wanner/>Leo Wanner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3539><div class="card-body p-3 small">This demo paper presents the multilingual deep sentence generator developed by the TALN group at Universitat Pompeu Fabra, implemented as a series of rule-based graph-transducers for the syntacticization of the input graphs, the resolution of morphological agreements, and the linearization of the trees.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3540 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3540/>Referential Success of Set Referring Expressions with Fuzzy Properties</a></strong><br><a href=/people/n/nicolas-marin/>Nicolás Marín</a>
|
<a href=/people/g/gustavo-rivas-gervilla/>Gustavo Rivas-Gervilla</a>
|
<a href=/people/d/daniel-sanchez-cisneros/>Daniel Sánchez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3540><div class="card-body p-3 small">We introduce the <a href=https://en.wikipedia.org/wiki/Property_(philosophy)>properties</a> to be satisfied by measures of referential success of set referring expressions with fuzzy properties. We define families of measures on the basis of n-cardinality measures and we illustrate some of them with a toy example.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3541.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3541 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3541 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3541/>Neural Response Generation for <a href=https://en.wikipedia.org/wiki/Customer_service>Customer Service</a> based on Personality Traits</a></strong><br><a href=/people/j/jonathan-herzig/>Jonathan Herzig</a>
|
<a href=/people/m/michal-shmueli-scheuer/>Michal Shmueli-Scheuer</a>
|
<a href=/people/t/tommy-sandbank/>Tommy Sandbank</a>
|
<a href=/people/d/david-konopnicki/>David Konopnicki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3541><div class="card-body p-3 small">We present a neural response generation model that generates responses conditioned on a target personality. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> learns high level features based on the target personality, and uses them to update its hidden state. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves performance improvements in both perplexity and BLEU scores over a baseline sequence-to-sequence model, and is validated by human judges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3542.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-3542 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-3542 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3542/>Neural Paraphrase Generation using <a href=https://en.wikipedia.org/wiki/Transfer_learning>Transfer Learning</a></a></strong><br><a href=/people/f/florin-brad/>Florin Brad</a>
|
<a href=/people/t/traian-rebedea/>Traian Rebedea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-3542><div class="card-body p-3 small">Progress in statistical paraphrase generation has been hindered for a long time by the lack of large monolingual parallel corpora. In this paper, we adapt the neural machine translation approach to <a href=https://en.wikipedia.org/wiki/Paraphrase_generation>paraphrase generation</a> and perform <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> from the closely related task of <a href=https://en.wikipedia.org/wiki/Logical_consequence>entailment generation</a>. We evaluate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on the Microsoft Research Paraphrase (MSRP) corpus and show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to generate sentences that capture part of the original meaning, but fails to pick up on important words or to show large lexical variation.</div></div></div><hr><div id=w17-36><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-36.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-36/>Proceedings of the 6th Workshop on Recent Advances in RST and Related Formalisms</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3600/>Proceedings of the 6th Workshop on Recent Advances in <span class=acl-fixed-case>RST</span> and Related Formalisms</a></strong><br><a href=/people/m/maite-taboada/>M. Taboada</a>
|
<a href=/people/i/iria-da-cunha/>I. da Cunha</a>
|
<a href=/people/e/e-g-maziero/>E.G. Maziero</a>
|
<a href=/people/p/paula-cardoso/>P. Cardoso</a>
|
<a href=/people/j/juliano-d-antonio/>J.D. Antonio</a>
|
<a href=/people/m/mikel-iruskieta/>M. Iruskieta</a></span></p></div><hr><div id=w17-37><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-37.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-37/>Proceedings of the 1st Workshop on Explainable Computational Intelligence (XCI 2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3700/>Proceedings of the 1st Workshop on Explainable Computational Intelligence (<span class=acl-fixed-case>XCI</span> 2017)</a></strong><br><a href=/people/m/martin-pereira-farina/>M. Pereira-Fariña</a>
|
<a href=/people/c/chris-reed/>C. Reed</a></span></p></div><hr><div id=w17-38><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-38.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-38/>Proceedings of the Linguistic Resources for Automatic Natural Language Generation - LiRA@NLG</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3800/>Proceedings of the Linguistic Resources for Automatic Natural Language Generation - <span class=acl-fixed-case>L</span>i<span class=acl-fixed-case>RA</span>@<span class=acl-fixed-case>NLG</span></a></strong><br><a href=/people/k/kristina-kocijan/>Kristina Kocijan</a>
|
<a href=/people/p/peter-machonis/>Peter Machonis</a>
|
<a href=/people/m/max-silberztein/>Max Silberztein</a></span></p></div><hr><div id=w17-39><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-39.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-39/>Proceedings of the Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-3900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-3900/>Proceedings of the Workshop on Computational Creativity in Natural Language Generation (<span class=acl-fixed-case>CC</span>-<span class=acl-fixed-case>NLG</span> 2017)</a></strong><br><a href=/people/h/hugo-goncalo-oliveira/>Hugo Gonçalo Oliveira</a>
|
<a href=/people/b/ben-burtenshaw/>Ben Burtenshaw</a>
|
<a href=/people/m/mike-kestemont/>Mike Kestemont</a>
|
<a href=/people/t/tom-de-smedt/>Tom De Smedt</a></span></p></div><hr><div id=w17-40><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-40/>Proceedings of the 13th International Conference on Finite State Methods and Natural Language Processing (FSMNLP 2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4000/>Proceedings of the 13th International Conference on Finite State Methods and Natural Language Processing (<span class=acl-fixed-case>FSMNLP</span> 2017)</a></strong><br><a href=/people/f/frank-drewes/>Frank Drewes</a></span></p></div><hr><div id=w17-41><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-41.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-41/>Proceedings of the First Workshop on Subword and Character Level Models in NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4100/>Proceedings of the First Workshop on Subword and Character Level Models in <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/m/manaal-faruqui/>Manaal Faruqui</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schuetze</a>
|
<a href=/people/i/isabel-trancoso/>Isabel Trancoso</a>
|
<a href=/people/y/yadollah-yaghoobzadeh/>Yadollah Yaghoobzadeh</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4101/>Character and Subword-Based Word Representation for Neural Language Modeling Prediction</a></strong><br><a href=/people/m/matthieu-labeau/>Matthieu Labeau</a>
|
<a href=/people/a/alexandre-allauzen/>Alexandre Allauzen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4101><div class="card-body p-3 small">Most of neural language models use different kinds of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> for <a href=https://en.wikipedia.org/wiki/Word_prediction>word prediction</a>. While <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> can be associated to each word in the vocabulary or derived from <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> as well as factored morphological decomposition, these word representations are mainly used to parametrize the input, i.e. the context of prediction. This work investigates the effect of using subword units (character and factored morphological decomposition) to build output representations for neural language modeling. We present a case study on <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>, a morphologically-rich language, experimenting with different input and output representations. When working with the full training vocabulary, despite unstable training, our experiments show that augmenting the output word representations with character-based embeddings can significantly improve the performance of the model. Moreover, reducing the size of the output look-up table, to let the character-based embeddings represent rare words, brings further improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4102/>Learning variable length units for SMT between related languages via Byte Pair Encoding<span class=acl-fixed-case>SMT</span> between related languages via Byte Pair Encoding</a></strong><br><a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4102><div class="card-body p-3 small">We explore the use of <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>segments</a> learnt using Byte Pair Encoding (referred to as BPE units) as basic units for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units modestly outperform orthographic syllables as units of translation, showing up to 11 % increase in BLEU score. While orthographic syllables can be used only for languages whose <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a> use vowel representations, BPE is writing system independent and we show that BPE outperforms other units for non-vowel writing systems too. Our results are supported by extensive experimentation spanning multiple language families and <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4103/>Character Based Pattern Mining for Neology Detection</a></strong><br><a href=/people/g/gael-lejeune/>Gaël Lejeune</a>
|
<a href=/people/e/emmanuel-cartier/>Emmanuel Cartier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4103><div class="card-body p-3 small">Detecting neologisms is essential in real-time natural language processing applications. Not only can it enable to follow the lexical evolution of languages, but it is also essential for updating linguistic resources and <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a>. In this paper, neology detection is considered as a classification task where a system has to assess whether a given lexical item is an actual <a href=https://en.wikipedia.org/wiki/Neologism>neologism</a> or not. We propose a combination of an unsupervised data mining technique and a supervised machine learning approach. It is inspired by current researches in <a href=https://en.wikipedia.org/wiki/Stylometry>stylometry</a> and on token-level and character-level patterns. We train and evaluate our system on a manually designed reference dataset in <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. We show that this approach is able to largely outperform state-of-the-art neology detection systems. Furthermore, character-level patterns exhibit good properties for multilingual extensions of the <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4104/>Automated Word Stress Detection in Russian<span class=acl-fixed-case>R</span>ussian</a></strong><br><a href=/people/m/maria-ponomareva/>Maria Ponomareva</a>
|
<a href=/people/k/kirill-milintsevich/>Kirill Milintsevich</a>
|
<a href=/people/e/ekaterina-chernyak/>Ekaterina Chernyak</a>
|
<a href=/people/a/anatoli-starostin/>Anatoly Starostin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4104><div class="card-body p-3 small">In this study we address the problem of automated word stress detection in <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> using character level models and no part-speech-taggers. We use a simple bidirectional RNN with LSTM nodes and achieve <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 90 % or higher. We experiment with two training datasets and show that using the data from an annotated corpus is much more efficient than using only a <a href=https://en.wikipedia.org/wiki/Dictionary>dictionary</a>, since it allows to retain the context of the word and its <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4105/>A Syllable-based Technique for Word Embeddings of Korean Words<span class=acl-fixed-case>K</span>orean Words</a></strong><br><a href=/people/s/sanghyuk-choi/>Sanghyuk Choi</a>
|
<a href=/people/t/taeuk-kim/>Taeuk Kim</a>
|
<a href=/people/j/jinseok-seol/>Jinseok Seol</a>
|
<a href=/people/s/sang-goo-lee/>Sang-goo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4105><div class="card-body p-3 small">Word embedding has become a fundamental component to many NLP tasks such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a> and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. However, popular <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that learn such embeddings are unaware of the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology of words</a>, so it is not directly applicable to highly <a href=https://en.wikipedia.org/wiki/Agglutinative_language>agglutinative languages</a> such as <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a>. We propose a syllable-based learning model for <a href=https://en.wikipedia.org/wiki/Korean_language>Korean</a> using a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a>, in which word representation is composed of trained syllable vectors. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> successfully produces morphologically meaningful representation of Korean words compared to the original Skip-gram embeddings. The results also show that it is quite robust to the Out-of-Vocabulary problem.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4106/>Supersense Tagging with a Combination of Character, Subword, and Word-level Representations</a></strong><br><a href=/people/y/youhyun-shin/>Youhyun Shin</a>
|
<a href=/people/s/sang-goo-lee/>Sang-goo Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4106><div class="card-body p-3 small">Recently, there has been increased interest in utilizing <a href=https://en.wikipedia.org/wiki/Character_(symbol)>characters or subwords</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP) tasks</a>. However, the effect of utilizing <a href=https://en.wikipedia.org/wiki/Character_(symbol)>character</a>, subword, and word-level information simultaneously has not been examined so far. In this paper, we propose a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to leverage various levels of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>input features</a> to improve on the performance of an supersense tagging task. Detailed analysis of experimental results show that different levels of input representation offer distinct characteristics that explain performance discrepancy among different tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4107/>Weakly supervised learning of allomorphy</a></strong><br><a href=/people/m/miikka-silfverberg/>Miikka Silfverberg</a>
|
<a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4107><div class="card-body p-3 small">Most NLP resources that offer annotations at the word segment level provide morphological annotation that includes features indicating tense, <a href=https://en.wikipedia.org/wiki/Grammatical_aspect>aspect</a>, <a href=https://en.wikipedia.org/wiki/Linguistic_modality>modality</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_gender>gender</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_case>case</a>, and other inflectional information. Such <a href=https://en.wikipedia.org/wiki/Information>information</a> is rarely aligned to the relevant parts of the wordsi.e. the <a href=https://en.wikipedia.org/wiki/Allomorphism>allomorphs</a>, as such <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> would be very costly. These unaligned weak labelings are commonly provided by annotated NLP corpora such as <a href=https://en.wikipedia.org/wiki/Treebank>treebanks</a> in various languages. Although they lack alignment information, the presence / absence of labels at the word level is also consistent with the amount of <a href=https://en.wikipedia.org/wiki/Supervisor>supervision</a> assumed to be provided to L1 and L2 learners. In this paper, we explore several methods to learn this latent alignment between parts of word forms and the <a href=https://en.wikipedia.org/wiki/Grammaticality>grammatical information</a> provided. All the <a href=https://en.wikipedia.org/wiki/Linguistic_description>methods</a> under investigation favor hypotheses regarding allomorphs of morphemes that re-use a small inventory, i.e. implicitly minimize the number of <a href=https://en.wikipedia.org/wiki/Allomorphism>allomorphs</a> that a <a href=https://en.wikipedia.org/wiki/Morpheme>morpheme</a> can be realized as. We show that the provided <a href=https://en.wikipedia.org/wiki/Information>information</a> offers a significant advantage for both <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> and the learning of allomorphy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4108/>Character-based recurrent neural networks for morphological relational reasoning</a></strong><br><a href=/people/o/olof-mogren/>Olof Mogren</a>
|
<a href=/people/r/richard-johansson/>Richard Johansson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4108><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for predicting word forms based on morphological relational reasoning with <a href=https://en.wikipedia.org/wiki/Analogy>analogies</a>. While previous work has explored tasks such as <a href=https://en.wikipedia.org/wiki/Inflection>morphological inflection</a> and reinflection, these models rely on an explicit enumeration of morphological features, which may not be available in all cases. To address the task of predicting a word form given a demo relation (a pair of word forms) and a query word, we devise a character-based recurrent neural network architecture using three separate encoders and a decoder. We also investigate a multiclass learning setup, where the prediction of the relation type label is used as an auxiliary task. Our results show that the exact form can be predicted for <a href=https://en.wikipedia.org/wiki/English_language>English</a> with an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 94.7 %. For <a href=https://en.wikipedia.org/wiki/Swedish_language>Swedish</a>, which has a more complex morphology with more <a href=https://en.wikipedia.org/wiki/Inflection>inflectional patterns</a> for <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> and <a href=https://en.wikipedia.org/wiki/Verb>verbs</a>, the accuracy is 89.3 %. We also show that using the auxiliary task of learning the relation type speeds up convergence and improves the prediction accuracy for the word generation task.<i>morphological relational reasoning</i> with analogies. While previous work has explored\n tasks such as morphological inflection and reinflection, these models rely\n on an explicit enumeration of morphological features, which may not be\n available in all cases. To address the task of predicting a word form\n given a <i>demo relation</i> (a pair of word forms) and a <i>query word</i>, we devise a\n character-based recurrent neural network architecture using three separate\n encoders and a decoder. We also investigate a multiclass learning setup,\n where the prediction of the relation type label is used as an auxiliary\n task. Our results show that the exact form can be predicted for English\n with an accuracy of 94.7%. For Swedish, which has a more complex\n morphology with more inflectional patterns for nouns and verbs, the\n accuracy is 89.3%. We also show that using the auxiliary task of learning\n the relation type speeds up convergence and improves the prediction\n accuracy for the word generation task.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4110/>Exploring Cross-Lingual Transfer of Morphological Knowledge In Sequence-to-Sequence Models</a></strong><br><a href=/people/h/huiming-jin/>Huiming Jin</a>
|
<a href=/people/k/katharina-kann/>Katharina Kann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4110><div class="card-body p-3 small">Multi-task training is an effective method to mitigate the data sparsity problem. It has recently been applied for cross-lingual transfer learning for paradigm completionthe task of producing inflected forms of lemmatawith sequence-to-sequence networks. However, it is still vague how the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> transfers knowledge across languages, as well as if and which information is shared. To investigate this, we propose a set of data-dependent experiments using an existing encoder-decoder recurrent neural network for the task. Our results show that indeed the performance gains surpass a pure regularization effect and that knowledge about <a href=https://en.wikipedia.org/wiki/Language>language</a> and <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> can be transferred.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4111 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4111/>Unlabeled Data for Morphological Generation With Character-Based Sequence-to-Sequence Models</a></strong><br><a href=/people/k/katharina-kann/>Katharina Kann</a>
|
<a href=/people/h/hinrich-schutze/>Hinrich Schütze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4111><div class="card-body p-3 small">We present a semi-supervised way of training a character-based encoder-decoder recurrent neural network for morphological reinflectionthe task of generating one inflected wordform from another. This is achieved by using unlabeled tokens or random strings as training data for an autoencoding task, adapting a network for morphological reinflection, and performing multi-task training. We thus use limited labeled data more effectively, obtaining up to 9.92 % improvement over state-of-the-art <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> for 8 different languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4112.Attachment.rar data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-4112/>Vowel and Consonant Classification through <a href=https://en.wikipedia.org/wiki/Spectral_decomposition>Spectral Decomposition</a></a></strong><br><a href=/people/p/patricia-thaine/>Patricia Thaine</a>
|
<a href=/people/g/gerald-penn/>Gerald Penn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4112><div class="card-body p-3 small">We consider two related problems in this paper. Given an undeciphered alphabetic writing system or mono-alphabetic cipher, determine : (1) which of its letters are vowels and which are consonants ; and (2) whether the <a href=https://en.wikipedia.org/wiki/Writing_system>writing system</a> is a vocalic alphabet or an <a href=https://en.wikipedia.org/wiki/Abjad>abjad</a>. We are able to show that a very simple <a href=https://en.wikipedia.org/wiki/Spectral_decomposition>spectral decomposition</a> based on character co-occurrences provides nearly perfect performance with respect to answering both question types.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4113 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4113.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-4113/>Syllable-level Neural Language Model for Agglutinative Language</a></strong><br><a href=/people/s/seunghak-yu/>Seunghak Yu</a>
|
<a href=/people/n/nilesh-kulkarni/>Nilesh Kulkarni</a>
|
<a href=/people/h/haejun-lee/>Haejun Lee</a>
|
<a href=/people/j/jihie-kim/>Jihie Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4113><div class="card-body p-3 small">We introduce a novel method to diminish the problem of out of vocabulary words by introducing an embedding method which leverages the agglutinative property of language. We propose additional embedding derived from <a href=https://en.wikipedia.org/wiki/Syllable>syllables</a> and <a href=https://en.wikipedia.org/wiki/Morpheme>morphemes</a> for the words to improve the performance of <a href=https://en.wikipedia.org/wiki/Language_model>language model</a>. We apply the above method to input prediction tasks and achieve state of the art performance in terms of Key Stroke Saving (KSS) w.r.t. to existing device input prediction methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4115/>Word Representation Models for Morphologically Rich Languages in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/e/ekaterina-vylomova/>Ekaterina Vylomova</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/x/xuanli-he/>Xuanli He</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4115><div class="card-body p-3 small">Out-of-vocabulary words present a great challenge for <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>. Recently various character-level compositional models were proposed to address this issue. In current research we incorporate two most popular neural architectures, namely LSTM and CNN, into hard- and soft-attentional models of translation for character-level representation of the source. We propose semantic and morphological intrinsic evaluation of encoder-level representations. Our analysis of the learned representations reveals that character-based LSTM seems to be better at capturing morphological aspects compared to character-based CNN. We also show that hard-attentional model provides better character-level representations compared to vanilla one.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4116.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4116 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4116 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4116/>Spell-Checking based on <a href=https://en.wikipedia.org/wiki/Syllabification>Syllabification</a> and Character-level Graphs for a Peruvian Agglutinative Language<span class=acl-fixed-case>P</span>eruvian Agglutinative Language</a></strong><br><a href=/people/c/carlo-alva/>Carlo Alva</a>
|
<a href=/people/a/arturo-oncevay/>Arturo Oncevay</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4116><div class="card-body p-3 small">There are several <a href=https://en.wikipedia.org/wiki/Indigenous_languages_of_the_Americas>native languages</a> in Peru which are mostly agglutinative. These <a href=https://en.wikipedia.org/wiki/Language>languages</a> are transmitted from generation to generation mainly in <a href=https://en.wikipedia.org/wiki/Oral_tradition>oral form</a>, causing different forms of writing across different communities. For this reason, there are recent efforts to standardize the <a href=https://en.wikipedia.org/wiki/Spelling>spelling</a> in the written texts, and it would be beneficial to support these tasks with an automatic tool such as an <a href=https://en.wikipedia.org/wiki/Spell_checker>spell-checker</a>. In this way, this spelling corrector is being developed based on two steps : an automatic rule-based syllabification method and a character-level graph to detect the degree of error in a misspelled word. The experiments were realized on Shipibo-konibo, a highly agglutinative and amazonian language, and the results obtained have been promising in a dataset built for the purpose.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4117.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4117 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4117 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4117/>What do we need to know about an unknown word when parsing German<span class=acl-fixed-case>G</span>erman</a></strong><br><a href=/people/b/bich-ngoc-do/>Bich-Ngoc Do</a>
|
<a href=/people/i/ines-rehbein/>Ines Rehbein</a>
|
<a href=/people/a/anette-frank/>Anette Frank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4117><div class="card-body p-3 small">We propose a new type of subword embedding designed to provide more information about unknown compounds, a major source for OOV words in <a href=https://en.wikipedia.org/wiki/German_language>German</a>. We present an extrinsic evaluation where we use the compound embeddings as input to a neural dependency parser and compare the results to the ones obtained with other types of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. Our evaluation shows that adding compound embeddings yields a significant improvement of 2 % LAS over using word embeddings when no POS information is available. When adding POS embeddings to the input, however, the effect levels out. This suggests that it is not the missing information about the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of the unknown words that causes problems for parsing <a href=https://en.wikipedia.org/wiki/German_language>German</a>, but the lack of morphological information for unknown words. To augment our evaluation, we also test the new embeddings in a language modelling task that requires both syntactic and semantic information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4118 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4118/>A General-Purpose Tagger with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/x/xiang-yu/>Xiang Yu</a>
|
<a href=/people/a/agnieszka-falenska/>Agnieszka Falenska</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4118><div class="card-body p-3 small">We present a general-purpose tagger based on convolutional neural networks (CNN), used for both composing word vectors and encoding context information. The CNN tagger is robust across different tagging tasks : without task-specific tuning of hyper-parameters, it achieves state-of-the-art results in <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>, morphological tagging and supertagging. The CNN tagger is also robust against the out-of-vocabulary problem ; it performs well on artificially unnormalized texts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4119.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4119 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4119 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4119/>Reconstruction of Word Embeddings from Sub-Word Parameters</a></strong><br><a href=/people/k/karl-stratos/>Karl Stratos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4119><div class="card-body p-3 small">Pre-trained word embeddings improve the performance of a neural model at the cost of increasing the model size. We propose to benefit from this resource without paying the cost by operating strictly at the sub-lexical level. Our approach is quite simple : before task-specific training, we first optimize sub-word parameters to reconstruct pre-trained word embeddings using various distance measures. We report interesting results on a variety of tasks : word similarity, word analogy, and <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4120 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4120/>Inflection Generation for Spanish Verbs using <a href=https://en.wikipedia.org/wiki/Supervised_learning>Supervised Learning</a><span class=acl-fixed-case>S</span>panish Verbs using Supervised Learning</a></strong><br><a href=/people/c/cristina-barros/>Cristina Barros</a>
|
<a href=/people/d/dimitra-gkatzia/>Dimitra Gkatzia</a>
|
<a href=/people/e/elena-lloret/>Elena Lloret</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4120><div class="card-body p-3 small">We present a novel supervised approach to inflection generation for <a href=https://en.wikipedia.org/wiki/Spanish_verbs>verbs in Spanish</a>. Our system takes as input the verb&#8217;s lemma form and the desired features such as <a href=https://en.wikipedia.org/wiki/Grammatical_person>person</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_number>number</a>, <a href=https://en.wikipedia.org/wiki/Grammatical_tense>tense</a>, and is able to predict the appropriate <a href=https://en.wikipedia.org/wiki/Grammatical_conjugation>grammatical conjugation</a>. Even though our approach learns from fewer examples comparing to previous work, it is able to deal with all the Spanish moods (indicative, subjunctive and imperative) in contrast to previous work which only focuses on indicative and subjunctive moods. We show that in an intrinsic evaluation, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves 99 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, outperforming (although not significantly) two competitive state-of-art systems. The successful results obtained clearly indicate that our approach could be integrated into wider approaches related to <a href=https://en.wikipedia.org/wiki/Text_generator>text generation</a> in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4122.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4122 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4122 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4122/>Sub-character Neural Language Modelling in Japanese<span class=acl-fixed-case>J</span>apanese</a></strong><br><a href=/people/v/viet-nguyen/>Viet Nguyen</a>
|
<a href=/people/j/julian-brooke/>Julian Brooke</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4122><div class="card-body p-3 small">In <a href=https://en.wikipedia.org/wiki/Languages_of_East_Asia>East Asian languages</a> such as <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, the <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> of a character are (somewhat) reflected in its sub-character elements. This paper examines the effect of using <a href=https://en.wikipedia.org/wiki/Character_(computing)>sub-characters</a> for <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. This is achieved by decomposing <a href=https://en.wikipedia.org/wiki/Character_(computing)>characters</a> according to a range of character decomposition datasets, and training a neural language model over variously decomposed character representations. Our results indicate that <a href=https://en.wikipedia.org/wiki/Language_model>language modelling</a> can be improved through the inclusion of <a href=https://en.wikipedia.org/wiki/Character_(computing)>sub-characters</a>, though this result depends on a good choice of <a href=https://en.wikipedia.org/wiki/Data_set>decomposition dataset</a> and the appropriate granularity of decomposition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4123.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4123 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4123 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4123/>Byte-based Neural Machine Translation</a></strong><br><a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4123><div class="card-body p-3 small">This paper presents experiments comparing character-based and byte-based neural machine translation systems. The main motivation of the byte-based neural machine translation system is to build multi-lingual neural machine translation systems that can share the same vocabulary. We compare the performance of both systems in several language pairs and we see that the performance in test is similar for most language pairs while the training time is slightly reduced in the case of byte-based neural machine translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4124.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4124 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4124 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4124/>Improving Opinion-Target Extraction with Character-Level Word Embeddings</a></strong><br><a href=/people/s/soufian-jebbara/>Soufian Jebbara</a>
|
<a href=/people/p/philipp-cimiano/>Philipp Cimiano</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4124><div class="card-body p-3 small">Fine-grained sentiment analysis is receiving increasing attention in recent years. Extracting opinion target expressions (OTE) in reviews is often an important step in fine-grained, aspect-based sentiment analysis. Retrieving this information from <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated text</a>, however, can be difficult. Customer reviews, for instance, are prone to contain misspelled words and are difficult to process due to their <a href=https://en.wikipedia.org/wiki/Domain-specific_language>domain-specific language</a>. In this work, we investigate whether character-level models can improve the performance for the identification of opinion target expressions. We integrate information about the character structure of a word into a sequence labeling system using character-level word embeddings and show their positive impact on the <a href=https://en.wikipedia.org/wiki/System>system</a>&#8217;s performance. Specifically, we obtain an increase by 3.3 points <a href=https://en.wikipedia.org/wiki/F-score>F1-score</a> with respect to our baseline model. In further experiments, we reveal encoded character patterns of the learned embeddings and give a nuanced view of the performance differences of both models.</div></div></div><hr><div id=w17-42><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-42.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-42/>Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4200/>Proceedings of the 2017 <span class=acl-fixed-case>EMNLP</span> Workshop: Natural Language Processing meets Journalism</a></strong><br><a href=/people/o/octavian-popescu/>Octavian Popescu</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4201/>Predicting News Values from <a href=https://en.wikipedia.org/wiki/Headline>Headline Text</a> and Emotions</a></strong><br><a href=/people/m/maria-pia-di-buono/>Maria Pia di Buono</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a>
|
<a href=/people/b/bojana-dalbelo-basic/>Bojana Dalbelo Bašić</a>
|
<a href=/people/g/goran-glavas/>Goran Glavaš</a>
|
<a href=/people/m/martin-tutek/>Martin Tutek</a>
|
<a href=/people/n/natasa-milic-frayling/>Natasa Milic-Frayling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4201><div class="card-body p-3 small">We present a preliminary study on predicting news values from headline text and emotions. We perform a <a href=https://en.wikipedia.org/wiki/Multivariate_analysis>multivariate analysis</a> on a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> manually annotated with news values and emotions, discovering interesting correlations among them. We then train two competitive machine learning models an SVM and a CNN to predict news values from headline text and emotions as features. We find that, while both models yield a satisfactory performance, some news values are more difficult to detect than others, while some profit more from including emotion information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4202.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4202 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4202 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4202/>Predicting User Views in Online News</a></strong><br><a href=/people/d/daniel-hardt/>Daniel Hardt</a>
|
<a href=/people/o/owen-rambow/>Owen Rambow</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4202><div class="card-body p-3 small">We analyze user viewing behavior on an <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news site</a>. We collect data from 64,000 <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a>, and use text features to predict frequency of user views. We compare predictiveness of the headline and teaser (viewed before clicking) and the body (viewed after clicking). Both are predictive of clicking behavior, with the full article text being most predictive.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4204/>What to Write? A topic recommender for journalists</a></strong><br><a href=/people/a/alessandro-cucchiarelli/>Alessandro Cucchiarelli</a>
|
<a href=/people/c/christian-morbidoni/>Christian Morbidoni</a>
|
<a href=/people/g/giovanni-stilo/>Giovanni Stilo</a>
|
<a href=/people/p/paola-velardi/>Paola Velardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4204><div class="card-body p-3 small">In this paper we present a <a href=https://en.wikipedia.org/wiki/Recommender_system>recommender system</a>, What To Write and Why, capable of suggesting to a journalist, for a given event, the aspects still uncovered in news articles on which the readers focus their interest. The basic idea is to characterize an event according to the echo it receives in online news sources and associate it with the corresponding readers&#8217; communicative and informative patterns, detected through the analysis of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>, respectively. Our methodology temporally aligns the results of this analysis and recommends the concepts that emerge as topics of interest from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> andWikipedia, either not covered or poorly covered in the published news articles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4205/>Comparing Attitudes to Climate Change in the Media using <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> based on Latent Dirichlet Allocation<span class=acl-fixed-case>L</span>atent <span class=acl-fixed-case>D</span>irichlet <span class=acl-fixed-case>A</span>llocation</a></strong><br><a href=/people/y/ye-jiang/>Ye Jiang</a>
|
<a href=/people/x/xingyi-song/>Xingyi Song</a>
|
<a href=/people/j/jackie-harrison/>Jackie Harrison</a>
|
<a href=/people/s/shaun-quegan/>Shaun Quegan</a>
|
<a href=/people/d/diana-maynard/>Diana Maynard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4205><div class="card-body p-3 small">News media typically present biased accounts of news stories, and different publications present different angles on the same event. In this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. To understand these attitudes, we find sentiment targets by combining Latent Dirichlet Allocation (LDA) with SentiWordNet, a general sentiment lexicon. Using LDA, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using SentiWordNet before regrouping the articles based on topic similarity. Preliminary analysis identifies clearly different attitudes on the same issue presented in different <a href=https://en.wikipedia.org/wiki/Source_(journalism)>news sources</a>. Ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4206/>Language-based Construction of Explorable News Graphs for Journalists</a></strong><br><a href=/people/r/remi-bois/>Rémi Bois</a>
|
<a href=/people/g/guillaume-gravier/>Guillaume Gravier</a>
|
<a href=/people/e/eric-jamet/>Eric Jamet</a>
|
<a href=/people/e/emmanuel-morin/>Emmanuel Morin</a>
|
<a href=/people/p/pascale-sebillot/>Pascale Sébillot</a>
|
<a href=/people/m/maxime-robert/>Maxime Robert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4206><div class="card-body p-3 small">Faced with ever-growing news archives, media professionals are in need of advanced tools to explore the information surrounding specific events. This problem is most commonly answered by browsing news datasets, going from article to article and viewing unaltered original content. In this article, we introduce an efficient way to generate links between news items, allowing such browsing through an easily explorable graph, and enrich this <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> by automatically typing links in order to inform the user on the nature of the relation between two news pieces. User evaluations are conducted on real world data with <a href=https://en.wikipedia.org/wiki/Journalist>journalists</a> in order to assess for the interest of both the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph representation</a> and link typing in a press reviewing task, showing the system to be of significant help for their work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4207/>Storyteller : Visual Analytics of Perspectives on Rich Text Interpretations<span class=acl-fixed-case>S</span>toryteller: Visual Analytics of Perspectives on Rich Text Interpretations</a></strong><br><a href=/people/m/maarten-van-meersbergen/>Maarten van Meersbergen</a>
|
<a href=/people/p/piek-vossen/>Piek Vossen</a>
|
<a href=/people/j/janneke-van-der-zwaan/>Janneke van der Zwaan</a>
|
<a href=/people/a/antske-fokkens/>Antske Fokkens</a>
|
<a href=/people/w/willem-robert-van-hage/>Willem van Hage</a>
|
<a href=/people/i/inger-leemans/>Inger Leemans</a>
|
<a href=/people/i/isa-maks/>Isa Maks</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4207><div class="card-body p-3 small">Complexity of <a href=https://en.wikipedia.org/wiki/Event_data>event data</a> in texts makes it difficult to assess its content, especially when considering larger collections in which different sources report on the same or similar situations. We present a system that makes it possible to visually analyze complex event and emotion data extracted from texts. We show that we can abstract from different data models for events and emotions to a single <a href=https://en.wikipedia.org/wiki/Data_model>data model</a> that can show the complex relations in four dimensions. The <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualization</a> has been applied to analyze 1) dynamic developments in how people both conceive and express emotions in theater plays and 2) how stories are told from the perspectyive of their sources based on rich event data extracted from news or biographies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4208.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4208 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4208 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4208/>Analyzing the Revision Logs of a Japanese Newspaper for Article Quality Assessment<span class=acl-fixed-case>J</span>apanese Newspaper for Article Quality Assessment</a></strong><br><a href=/people/h/hideaki-tamori/>Hideaki Tamori</a>
|
<a href=/people/y/yuta-hitomi/>Yuta Hitomi</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4208><div class="card-body p-3 small">We address the issue of the quality of journalism and analyze daily article revision logs from a <a href=https://en.wikipedia.org/wiki/List_of_newspapers_in_Japan>Japanese newspaper company</a>. The revision logs contain data that can help reveal the requirements of quality journalism such as the types and number of edit operations and aspects commonly focused in revision. This study also discusses potential applications such as <a href=https://en.wikipedia.org/wiki/Quality_assessment>quality assessment</a> and automatic article revision as our future research directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4209/>Improved Abusive Comment Moderation with User Embeddings</a></strong><br><a href=/people/j/john-pavlopoulos/>John Pavlopoulos</a>
|
<a href=/people/p/prodromos-malakasiotis/>Prodromos Malakasiotis</a>
|
<a href=/people/j/juli-bakagianni/>Juli Bakagianni</a>
|
<a href=/people/i/ion-androutsopoulos/>Ion Androutsopoulos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4209><div class="card-body p-3 small">Experimenting with a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of approximately 1.6 M user comments from a Greek news sports portal, we explore how a state of the art RNN-based moderation method can be improved by adding <a href=https://en.wikipedia.org/wiki/Graph_embedding>user embeddings</a>, <a href=https://en.wikipedia.org/wiki/Graph_embedding>user type embeddings</a>, user biases, or <a href=https://en.wikipedia.org/wiki/Graph_embedding>user type biases</a>. We observe improvements in all cases, with user embeddings leading to the biggest performance gains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4210.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4210 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4210 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4210/>Incongruent Headlines : Yet Another Way to Mislead Your Readers</a></strong><br><a href=/people/s/sophie-chesney/>Sophie Chesney</a>
|
<a href=/people/m/maria-liakata/>Maria Liakata</a>
|
<a href=/people/m/massimo-poesio/>Massimo Poesio</a>
|
<a href=/people/m/matthew-purver/>Matthew Purver</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4210><div class="card-body p-3 small">This paper discusses the problem of incongruent headlines : those which do not accurately represent the information contained in the article with which they occur. We emphasise that this phenomenon should be considered separately from recognised problematic headline types such as <a href=https://en.wikipedia.org/wiki/Clickbait>clickbait</a> and <a href=https://en.wikipedia.org/wiki/Sensationalism>sensationalism</a>, arguing that existing natural language processing (NLP) methods applied to these related concepts are not appropriate for the automatic detection of headline incongruence, as an analysis beyond stylistic traits is necessary. We therefore suggest a number of alternative <a href=https://en.wikipedia.org/wiki/Methodology>methodologies</a> that may be appropriate to the task at hand as a foundation for future work in this area. In addition, we provide an analysis of existing data sets which are related to this work, and motivate the need for a novel <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> in this domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4211.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4211 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4211 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4211/>Unsupervised Event Clustering and Aggregation from Newswire and Web Articles</a></strong><br><a href=/people/s/swen-ribeiro/>Swen Ribeiro</a>
|
<a href=/people/o/olivier-ferret/>Olivier Ferret</a>
|
<a href=/people/x/xavier-tannier/>Xavier Tannier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4211><div class="card-body p-3 small">In this paper, we present an unsupervised pipeline approach for clustering <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news articles</a> based on identified event instances in their content. We leverage press agency newswire and monolingual word alignment techniques to build meaningful and linguistically varied clusters of articles from the web in the perspective of a broader event type detection task. We validate our approach on a manually annotated corpus of Web articles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4212/>Semantic Storytelling, Cross-lingual Event Detection and other Semantic Services for a Newsroom Content Curation Dashboard</a></strong><br><a href=/people/j/julian-moreno-schneider/>Julian Moreno-Schneider</a>
|
<a href=/people/a/ankit-srivastava/>Ankit Srivastava</a>
|
<a href=/people/p/peter-bourgonje/>Peter Bourgonje</a>
|
<a href=/people/d/david-wabnitz/>David Wabnitz</a>
|
<a href=/people/g/georg-rehm/>Georg Rehm</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4212><div class="card-body p-3 small">We present a prototypical content curation dashboard, to be used in the newsroom, and several of its underlying semantic content analysis components (such as <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entity recognition</a>, entity linking, summarisation and temporal expression analysis). The idea is to enable journalists (a) to process incoming content (agency reports, twitter feeds, <a href=https://en.wikipedia.org/wiki/Report>reports</a>, blog posts, <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> etc.) and (b) to create new articles more easily and more efficiently. The prototype system also allows the automatic annotation of events in incoming content for the purpose of supporting journalists in identifying important, relevant or meaningful events and also to adapt the content currently in production accordingly in a semi-automatic way. One of our long-term goals is to support journalists building up entire storylines with automatic means. In the present <a href=https://en.wikipedia.org/wiki/Prototype>prototype</a> they are generated in a <a href=https://en.wikipedia.org/wiki/Front_and_back_ends>backend service</a> using <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clustering methods</a> that operate on the extracted events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4213/>Deception Detection in News Reports in the <a href=https://en.wikipedia.org/wiki/Russian_language>Russian Language</a> : Lexics and Discourse<span class=acl-fixed-case>R</span>ussian Language: Lexics and Discourse</a></strong><br><a href=/people/d/dina-pisarevskaya/>Dina Pisarevskaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4213><div class="card-body p-3 small">News verification and automated fact checking tend to be very important issues in our world. The research is initial. We collected a corpus for <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> (174 news reports, truthful and fake ones). We held two experiments, for both we applied SVMs algorithm (linear / rbf kernel) and <a href=https://en.wikipedia.org/wiki/Random_forest>Random Forest</a> to classify the news reports into 2 classes : truthful / deceptive. In the first experiment, we used 18 <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>markers</a> on lexics level, mostly frequencies of POS tags in texts. In the second experiment, on <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse level</a> we used frequencies of rhetorical relations types in texts. The classification task in the first experiment is solved better by SVMs (rbf kernel) (f-measure 0.65). The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on RST features shows best results with Random Forest Classifier (f-measure 0.54) and should be modified. In the next research, the combination of different deception detection markers for the <a href=https://en.wikipedia.org/wiki/Russian_language>Russian language</a> should be taken in order to make a better <a href=https://en.wikipedia.org/wiki/Predictive_modelling>predictive model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4214.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4214 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4214 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4214/>Fake news stance detection using stacked ensemble of classifiers</a></strong><br><a href=/people/j/james-thorne/>James Thorne</a>
|
<a href=/people/m/mingjie-chen/>Mingjie Chen</a>
|
<a href=/people/g/giorgos-myrianthous/>Giorgos Myrianthous</a>
|
<a href=/people/j/jiashu-pu/>Jiashu Pu</a>
|
<a href=/people/x/xiaoxuan-wang/>Xiaoxuan Wang</a>
|
<a href=/people/a/andreas-vlachos/>Andreas Vlachos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4214><div class="card-body p-3 small">Fake news has become a hotly debated topic in <a href=https://en.wikipedia.org/wiki/Journalism>journalism</a>. In this paper, we present our entry to the 2017 Fake News Challenge which models the detection of fake news as a stance classification task that finished in 11th place on the leader board. Our entry is an ensemble system of classifiers developed by students in the context of their coursework. We show how we used the stacking ensemble method for this purpose and obtained improvements in classification accuracy exceeding each of the individual models&#8217; performance on the development data. Finally, we discuss aspects of the experimental setup of the challenge.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4215.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4215 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4215 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4215/>From <a href=https://en.wikipedia.org/wiki/Clickbait>Clickbait</a> to Fake News Detection : An Approach based on Detecting the Stance of Headlines to Articles</a></strong><br><a href=/people/p/peter-bourgonje/>Peter Bourgonje</a>
|
<a href=/people/j/julian-moreno-schneider/>Julian Moreno Schneider</a>
|
<a href=/people/g/georg-rehm/>Georg Rehm</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4215><div class="card-body p-3 small">We present a <a href=https://en.wikipedia.org/wiki/System>system</a> for the detection of the stance of headlines with regard to their corresponding <a href=https://en.wikipedia.org/wiki/Article_(grammar)>article bodies</a>. The approach can be applied in <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a>, especially clickbait detection scenarios. The component is part of a larger platform for the curation of digital content ; we consider veracity and relevancy an increasingly important part of curating online information. We want to contribute to the debate on how to deal with <a href=https://en.wikipedia.org/wiki/Fake_news>fake news</a> and related online phenomena with technological means, by providing means to separate related from unrelated headlines and further classifying the related headlines. On a publicly available <a href=https://en.wikipedia.org/wiki/Data_set>data set</a> annotated for the stance of headlines with regard to their corresponding <a href=https://en.wikipedia.org/wiki/Article_(publishing)>article bodies</a>, we achieve a <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>(weighted) accuracy score</a> of 89.59.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4216/>‘Fighting’ or ‘Conflict’? An Approach to Revealing Concepts of Terms in Political Discourse</a></strong><br><a href=/people/l/linyuan-tang/>Linyuan Tang</a>
|
<a href=/people/k/kyo-kageura/>Kyo Kageura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4216><div class="card-body p-3 small">Previous work on the epistemology of <a href=https://en.wikipedia.org/wiki/Fact-checking>fact-checking</a> indicated the dilemma between the needs of binary answers for the public and ambiguity of political discussion. Determining concepts represented by terms in <a href=https://en.wikipedia.org/wiki/Discourse_analysis>political discourse</a> can be considered as a Word-Sense Disambiguation (WSD) task. The analysis of political discourse, however, requires identifying precise concepts of terms from relatively small data. This work attempts to provide a basic <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for revealing concepts of terms in <a href=https://en.wikipedia.org/wiki/Discourse_analysis>political discourse</a> with explicit contextual information. The framework consists of three parts : 1) extracting important terms, 2) generating concordance for each term with stipulative definitions and explanations, and 3) agglomerating similar information of the term by <a href=https://en.wikipedia.org/wiki/Hierarchical_clustering>hierarchical clustering</a>. Utterances made by Prime Minister Abe Shinzo in the Diet of Japan are used to examine our <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a>. Importantly, we revealed the conceptual inconsistency of the term Sonritsu-kiki-jitai. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> was proved to work, but only for a small number of terms due to lack of explicit contextual information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4217/>A News Chain Evaluation Methodology along with a Lattice-based Approach for News Chain Construction</a></strong><br><a href=/people/m/mustafa-toprak/>Mustafa Toprak</a>
|
<a href=/people/o/ozer-ozkahraman/>Özer Özkahraman</a>
|
<a href=/people/s/selma-tekir/>Selma Tekir</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4217><div class="card-body p-3 small">Chain construction is an important requirement for understanding news and establishing the context. A news chain can be defined as a coherent set of articles that explains an event or a story. There&#8217;s a lack of well-established methods in this area. In this work, we propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to evaluate the goodness of a given news chain and implement a concept lattice-based news chain construction method by Hossain et al.. The methodology part is vital as it directly affects the growth of research in this area. Our proposed methodology consists of collected news chains from different studies and two goodness metrics, minedge and <a href=https://en.wikipedia.org/wiki/Statistical_dispersion>dispersion coefficient</a> respectively. We assess the utility of the lattice-based news chain construction method by our proposed <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4218 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4218/>Using New York Times Picks to Identify Constructive Comments<span class=acl-fixed-case>N</span>ew <span class=acl-fixed-case>Y</span>ork <span class=acl-fixed-case>T</span>imes Picks to Identify Constructive Comments</a></strong><br><a href=/people/v/varada-kolhatkar/>Varada Kolhatkar</a>
|
<a href=/people/m/maite-taboada/>Maite Taboada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4218><div class="card-body p-3 small">We examine the extent to which we are able to automatically identify constructive online comments. We build several classifiers using New York Times Picks as positive examples and non-constructive thread comments from the Yahoo News Annotated Comments Corpus as negative examples of constructive online comments. We evaluate these <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> on a crowd-annotated corpus containing 1,121 comments. Our best <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> achieves a top F1 score of 0.84.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4219 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4219/>An <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP Analysis</a> of Exaggerated Claims in Science News<span class=acl-fixed-case>NLP</span> Analysis of Exaggerated Claims in Science News</a></strong><br><a href=/people/y/yingya-li/>Yingya Li</a>
|
<a href=/people/j/jieke-zhang/>Jieke Zhang</a>
|
<a href=/people/b/bei-yu/>Bei Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4219><div class="card-body p-3 small">The discrepancy between science and media has been affecting the effectiveness of <a href=https://en.wikipedia.org/wiki/Science_communication>science communication</a>. Original findings from <a href=https://en.wikipedia.org/wiki/Scientific_literature>science publications</a> may be distorted with altered claim strength when reported to the public, causing misinformation spread. This study conducts an NLP analysis of exaggerated claims in <a href=https://en.wikipedia.org/wiki/Science_journalism>science news</a>, and then constructed prediction models for identifying claim strength levels in <a href=https://en.wikipedia.org/wiki/Science_journalism>science reporting</a>. The results demonstrate different writing styles journal articles and news / press releases use for reporting scientific findings. Preliminary prediction models reached promising result with room for further improvement.</div></div></div><hr><div id=w17-43><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-43.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-43/>Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4300/>Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing</a></strong><br><a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/m/ming-wei-chang/>Ming-Wei Chang</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a>
|
<a href=/people/a/alexander-m-rush/>Alexander M. Rush</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4301/>Dependency Parsing with Dilated Iterated Graph CNNs<span class=acl-fixed-case>CNN</span>s</a></strong><br><a href=/people/e/emma-strubell/>Emma Strubell</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4301><div class="card-body p-3 small">Dependency parses are an effective way to inject linguistic knowledge into many downstream tasks, and many practitioners wish to efficiently parse sentences at scale. Recent advances in <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU hardware</a> have enabled <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to achieve significant gains over the previous best models, these models still fail to leverage GPUs&#8217; capability for <a href=https://en.wikipedia.org/wiki/Massively_parallel>massive parallelism</a> due to their requirement of sequential processing of the sentence. In response, we propose Dilated Iterated Graph Convolutional Neural Networks (DIG-CNNs) for graph-based dependency parsing, a graph convolutional architecture that allows for efficient end-to-end GPU parsing. In experiments on the English Penn TreeBank benchmark, we show that DIG-CNNs perform on par with some of the best <a href=https://en.wikipedia.org/wiki/Parsing>neural network parsers</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-4302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-4302/>Entity Identification as Multitasking</a></strong><br><a href=/people/k/karl-stratos/>Karl Stratos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4302><div class="card-body p-3 small">Standard approaches in <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity identification</a> hard-code boundary detection and type prediction into labels and perform <a href=https://en.wikipedia.org/wiki/Viterbi_algorithm>Viterbi</a>. This has two disadvantages : 1. the <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>runtime complexity</a> grows quadratically in the number of types, and 2. there is no natural segment-level representation. In this paper, we propose a neural architecture that addresses these disadvantages. We frame the problem as <a href=https://en.wikipedia.org/wiki/Computer_multitasking>multitasking</a>, separating boundary detection and type prediction but optimizing them jointly. Despite its simplicity, this <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> performs competitively with fully structured models such as BiLSTM-CRFs while scaling linearly in the number of types. Furthermore, by construction, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> induces type-disambiguating embeddings of predicted mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4303.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-4303/>Towards Neural Machine Translation with Latent Tree Attention</a></strong><br><a href=/people/j/james-bradbury/>James Bradbury</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4303><div class="card-body p-3 small">Building <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that take advantage of the hierarchical structure of language without a priori annotation is a longstanding goal in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. We introduce such a model for the task of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, pairing a recurrent neural network grammar encoder with a novel attentional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target. When trained on character-level datasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4304/>Structured Prediction via Learning to Search under Bandit Feedback</a></strong><br><a href=/people/a/amr-sharaf/>Amr Sharaf</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4304><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> under online bandit feedback. The <a href=https://en.wikipedia.org/wiki/Learning>learner</a> repeatedly predicts a sequence of actions, generating a structured output. It then observes <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> for that <a href=https://en.wikipedia.org/wiki/Output_(economics)>output</a> and no others. We consider two cases : a pure bandit setting in which it only observes a loss, and more fine-grained feedback in which it observes a loss for every action. We find that the fine-grained feedback is necessary for strong empirical performance, because it allows for a robust variance-reduction strategy. We empirically compare a number of different <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> and exploration methods and show the efficacy of BLS on sequence labeling and dependency parsing tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4305/>Syntax Aware LSTM model for <a href=https://en.wikipedia.org/wiki/Semantic_Role_Labeling>Semantic Role Labeling</a><span class=acl-fixed-case>LSTM</span> model for Semantic Role Labeling</a></strong><br><a href=/people/f/feng-qian/>Feng Qian</a>
|
<a href=/people/l/lei-sha/>Lei Sha</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/l/lu-chen-liu/>Lu-chen Liu</a>
|
<a href=/people/m/ming-zhang/>Ming Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4305><div class="card-body p-3 small">In Semantic Role Labeling (SRL) task, the tree structured dependency relation is rich in syntax information, but it is not well handled by existing models. In this paper, we propose Syntax Aware Long Short Time Memory (SA-LSTM). The structure of SA-LSTM changes according to dependency structure of each sentence, so that SA-LSTM can model the whole tree structure of dependency relation in an architecture engineering way. Experiments demonstrate that on Chinese Proposition Bank (CPB) 1.0, SA-LSTM improves <a href=https://en.wikipedia.org/wiki/F-number>F1</a> by 2.06 % than ordinary bi-LSTM with feature engineered dependency relation information, and gives state-of-the-art <a href=https://en.wikipedia.org/wiki/F-number>F1</a> of 79.92 %. On English CoNLL 2005 dataset, SA-LSTM brings improvement (2.1 %) to bi-LSTM model and also brings slight improvement (0.3 %) when added to the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4307/>Boosting Information Extraction Systems with Character-level Neural Networks and Free Noisy Supervision</a></strong><br><a href=/people/p/philipp-meerkamp/>Philipp Meerkamp</a>
|
<a href=/people/z/zhengyi-zhou/>Zhengyi Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4307><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> to boost the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of existing <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction systems</a>. This is achieved by augmenting the existing <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>, which may be constraint-based or hybrid statistical, with a character-level neural network. Our architecture combines the ability of constraint-based or hybrid extraction systems to easily incorporate <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> with the ability of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> to leverage large amounts of data to learn complex features. The <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> is trained using a measure of consistency between extracted data and existing <a href=https://en.wikipedia.org/wiki/Database>databases</a> as a form of cheap, noisy supervision. Our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> does not require large scale manual annotation or a <a href=https://en.wikipedia.org/wiki/Code_refactoring>system rewrite</a>. It has led to large precision improvements over an existing, highly-tuned production information extraction system used at <a href=https://en.wikipedia.org/wiki/Bloomberg_L.P.>Bloomberg LP</a> for financial language text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-4308" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-4308/>Piecewise Latent Variables for Neural Variational Text Processing</a></strong><br><a href=/people/i/iulian-vlad-serban/>Iulian Vlad Serban</a>
|
<a href=/people/a/alexander-ororbia-ii/>Alexander Ororbia II</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a>
|
<a href=/people/a/aaron-courville/>Aaron Courville</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4308><div class="card-body p-3 small">Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> will learn to represent rich, multi-modal latent factors in real-world data, such as <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language text</a>. However, current models often assume simplistic priors on the latent variables-such as the uni-modal Gaussian distribution-which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This <a href=https://en.wikipedia.org/wiki/Probability_distribution>distribution</a> has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> for <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>.</div></div></div><hr><div id=w17-44><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-44.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-44/>Proceedings of the 3rd Workshop on Noisy User-generated Text</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4400/>Proceedings of the 3rd Workshop on Noisy User-generated Text</a></strong><br><a href=/people/l/leon-derczynski/>Leon Derczynski</a>
|
<a href=/people/w/wei-xu/>Wei Xu</a>
|
<a href=/people/a/alan-ritter/>Alan Ritter</a>
|
<a href=/people/t/timothy-baldwin/>Tim Baldwin</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4401/>Boundary-based MWE segmentation with text partitioning<span class=acl-fixed-case>MWE</span> segmentation with text partitioning</a></strong><br><a href=/people/j/jake-williams/>Jake Williams</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4401><div class="card-body p-3 small">This submission describes the development of a fine-grained, text-chunking algorithm for the task of comprehensive MWE segmentation. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> notably focuses on the identification of colloquial and idiomatic language. The submission also includes a thorough model evaluation in the context of two recent shared tasks, spanning 19 different languages and many text domains, including noisy, user-generated text. Evaluations exhibit the presented model as the best overall for purposes of MWE segmentation, and <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source software</a> is released with the submission (although links are withheld for purposes of anonymity). Additionally, the authors acknowledge the existence of a pre-print document on arxiv.org, which should be avoided to maintain anonymity in review.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4402/>Towards the Understanding of Gaming Audiences by Modeling Twitch Emotes</a></strong><br><a href=/people/f/francesco-barbieri/>Francesco Barbieri</a>
|
<a href=/people/l/luis-espinosa-anke/>Luis Espinosa-Anke</a>
|
<a href=/people/m/miguel-ballesteros/>Miguel Ballesteros</a>
|
<a href=/people/j/juan-soler-company/>Juan Soler-Company</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4402><div class="card-body p-3 small">Videogame streaming platforms have become a paramount example of noisy user-generated text. These are websites where <a href=https://en.wikipedia.org/wiki/Video_game>gaming</a> is broadcasted, and allows interaction with viewers via integrated chatrooms. Probably the best known <a href=https://en.wikipedia.org/wiki/Computing_platform>platform</a> of this kind is <a href=https://en.wikipedia.org/wiki/Twitch.tv>Twitch</a>, which has more than 100 million monthly viewers. Despite these numbers, and unlike other <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> featuring short messages (e.g. Twitter), <a href=https://en.wikipedia.org/wiki/Twitch.tv>Twitch</a> has not received much attention from the Natural Language Processing community. In this paper we aim at bridging this gap by proposing two important tasks specific to the Twitch platform, namely (1) Emote prediction ; and (2) Trolling detection. In our experiments, we evaluate three models : a BOW baseline, a logistic supervised classifiers based on word embeddings, and a bidirectional long short-term memory recurrent neural network (LSTM). Our results show that the LSTM model outperforms the other two models, where explicit features with proven effectiveness for similar tasks were encoded.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-4404" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-4404/>To normalize, or not to normalize : The impact of <a href=https://en.wikipedia.org/wiki/Normalization_(image_processing)>normalization</a> on Part-of-Speech tagging</a></strong><br><a href=/people/r/rob-van-der-goot/>Rob van der Goot</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4404><div class="card-body p-3 small">Does <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> help Part-of-Speech (POS) tagging accuracy on noisy, non-canonical data? To the best of our knowledge, little is known on the actual impact of <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> in a real-world scenario, where gold error detection is not available. We investigate the effect of automatic normalization on POS tagging of tweets. We also compare <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> to strategies that leverage large amounts of unlabeled data kept in its raw form. Our results show that <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalization</a> helps, but does not add consistently beyond just word embedding layer initialization. The latter approach yields a tagging model that is competitive with a Twitter state-of-the-art <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagger</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4405/>Constructing an Alias List for Named Entities during an Event</a></strong><br><a href=/people/a/anietie-andy/>Anietie Andy</a>
|
<a href=/people/m/mark-dredze/>Mark Dredze</a>
|
<a href=/people/m/mugizi-rwebangira/>Mugizi Rwebangira</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4405><div class="card-body p-3 small">In certain fields, real-time knowledge from events can help in making informed decisions. In order to extract pertinent real-time knowledge related to an event, it is important to identify the <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a> and their corresponding <a href=https://en.wikipedia.org/wiki/Pseudonym>aliases</a> related to the event. The problem of identifying aliases of named entities that spike has remained unexplored. In this paper, we introduce an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>, EntitySpike, that identifies entities that spike in popularity in tweets from a given time period, and constructs an alias list for these spiked entities. EntitySpike uses a temporal heuristic to identify named entities with similar context that occur in the same time period (within minutes) during an event. Each entity is encoded as a vector using this temporal heuristic. We show how these entity-vectors can be used to create a named entity alias list. We evaluated our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> on a dataset of temporally ordered tweets from a single event, the 2013 <a href=https://en.wikipedia.org/wiki/55th_Annual_Grammy_Awards>Grammy Awards show</a>. We carried out various experiments on tweets that were published in the same time period and show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> identifies most entity name aliases and outperforms a competitive baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4406/>Incorporating <a href=https://en.wikipedia.org/wiki/Metadata>Metadata</a> into Content-Based User Embeddings</a></strong><br><a href=/people/l/linzi-xing/>Linzi Xing</a>
|
<a href=/people/m/michael-paul/>Michael J. Paul</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4406><div class="card-body p-3 small">Low-dimensional vector representations of social media users can benefit applications like <a href=https://en.wikipedia.org/wiki/Recommender_system>recommendation systems</a> and user attribute inference. Recent work has shown that user embeddings can be improved by combining different types of <a href=https://en.wikipedia.org/wiki/Information>information</a>, such as text and network data. We propose a data augmentation method that allows novel feature types to be used within off-the-shelf embedding models. Experimenting with the task of <a href=https://en.wikipedia.org/wiki/Recommender_system>friend recommendation</a> on a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 5,019 <a href=https://en.wikipedia.org/wiki/Twitter>Twitter users</a>, we show that our approach can lead to substantial performance gains with the simple addition of network and geographic features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4407/>Simple Queries as Distant Labels for Predicting Gender on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/c/chris-emmery/>Chris Emmery</a>
|
<a href=/people/g/grzegorz-chrupala/>Grzegorz Chrupała</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4407><div class="card-body p-3 small">The majority of research on extracting missing user attributes from <a href=https://en.wikipedia.org/wiki/User_profile>social media profiles</a> use costly hand-annotated labels for <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a>. Distantly supervised methods exist, although these generally rely on knowledge gathered using external sources. This paper demonstrates the effectiveness of gathering distant labels for self-reported gender on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> using simple queries. We confirm the reliability of this query heuristic by comparing with <a href=https://en.wikipedia.org/wiki/Annotation>manual annotation</a>. Moreover, using these labels for distant supervision, we demonstrate competitive model performance on the same data as <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> trained on manual annotations. As such, we offer a cheap, extensible, and fast alternative that can be employed beyond the task of <a href=https://en.wikipedia.org/wiki/Gender>gender classification</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4408/>A <a href=https://en.wikipedia.org/wiki/Dataset>Dataset</a> and Classifier for Recognizing Social Media English<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/s/su-lin-blodgett/>Su Lin Blodgett</a>
|
<a href=/people/j/johnny-wei/>Johnny Wei</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4408><div class="card-body p-3 small">While <a href=https://en.wikipedia.org/wiki/Language_identification>language identification</a> works well on standard texts, it performs much worse on social media language, in particular dialectal languageeven for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. First, to support work on English language identification, we contribute a new dataset of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> annotated for English versus non-English, with attention to <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a>, <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a>, and automatic generation issues. It is randomly sampled from all public messages, avoiding biases towards pre-existing <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>language classifiers</a>. Second, we find that a demographic language modelwhich identifies messages with language similar to that used by several <a href=https://en.wikipedia.org/wiki/Race_and_ethnicity_in_the_United_States>U.S. ethnic populations</a> on Twittercan be used to improve English language identification performance when combined with a traditional supervised language identifier. It increases <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> with almost no loss of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a>, including, surprisingly, for <a href=https://en.wikipedia.org/wiki/English_language>English messages</a> written by non-U.S. authors. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and identifier ensemble are available online.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4409/>Evaluating hypotheses in <a href=https://en.wikipedia.org/wiki/Geolocation>geolocation</a> on a very large sample of Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/b/bahar-salehi/>Bahar Salehi</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4409><div class="card-body p-3 small">Recent work in <a href=https://en.wikipedia.org/wiki/Geolocation>geolocation</a> has made several hypotheses about what <a href=https://en.wikipedia.org/wiki/Marker_(linguistics)>linguistic markers</a> are relevant to detect where people write from. In this paper, we examine six hypotheses against a corpus consisting of all geo-tagged tweets from the US, or whose geo-tags could be inferred, in a 19 % sample of Twitter history. Our experiments lend support to all six hypotheses, including that spelling variants and <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> are strong predictors of <a href=https://en.wikipedia.org/wiki/Location>location</a>. We also study what kinds of common nouns are predictive of location after controlling for named entities such as <a href=https://en.wikipedia.org/wiki/Dolphin>dolphins</a> or <a href=https://en.wikipedia.org/wiki/Shark>sharks</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4410 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4410/>The Effect of <a href=https://en.wikipedia.org/wiki/Error_rate>Error Rate</a> in Artificially Generated Data for Automatic Preposition and Determiner Correction</a></strong><br><a href=/people/f/fraser-bowen/>Fraser Bowen</a>
|
<a href=/people/j/jon-dehdari/>Jon Dehdari</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4410><div class="card-body p-3 small">In this research we investigate the impact of mismatches in the density and type of error between training and test data on a <a href=https://en.wikipedia.org/wiki/Nervous_system>neural system</a> correcting preposition and determiner errors. We use synthetically produced training data to control error density and type, and real error data for testing. Our results show it is possible to combine error types, although <a href=https://en.wikipedia.org/wiki/Preposition_and_postposition>prepositions</a> and <a href=https://en.wikipedia.org/wiki/Determiner>determiners</a> behave differently in terms of how much error should be artificially introduced into the training data in order to get the best results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4411.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4411 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4411 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4411/>An Entity Resolution Approach to Isolate Instances of Human Trafficking Online</a></strong><br><a href=/people/c/chirag-nagpal/>Chirag Nagpal</a>
|
<a href=/people/k/kyle-miller/>Kyle Miller</a>
|
<a href=/people/b/benedikt-boecking/>Benedikt Boecking</a>
|
<a href=/people/a/artur-dubrawski/>Artur Dubrawski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4411><div class="card-body p-3 small">Human trafficking is a challenging law enforcement problem, and traces of victims of such activity manifest as &#8216;escort advertisements&#8217; on various online forums. Given the large, heterogeneous and noisy structure of this <a href=https://en.wikipedia.org/wiki/Data>data</a>, building <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> to predict instances of <a href=https://en.wikipedia.org/wiki/Smuggling>trafficking</a> is a convoluted task. In this paper we propose an entity resolution pipeline using a notion of proxy labels, in order to extract clusters from this data with prior history of human trafficking activity. We apply this <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> to 5 M records from <a href=https://en.wikipedia.org/wiki/Backpage>backpage.com</a> and report on the performance of this approach, challenges in terms of <a href=https://en.wikipedia.org/wiki/Scalability>scalability</a>, and some significant domain specific characteristics of our resolved entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4412.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4412 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4412 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4412/>Noisy Uyghur Text Normalization<span class=acl-fixed-case>U</span>yghur Text Normalization</a></strong><br><a href=/people/o/osman-tursun/>Osman Tursun</a>
|
<a href=/people/r/ruket-cakici/>Ruket Cakici</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4412><div class="card-body p-3 small">Uyghur is the second largest and most actively used social media language in China. However, a non-negligible part of <a href=https://en.wikipedia.org/wiki/Uyghur_language>Uyghur text</a> appearing in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is unsystematically written with the <a href=https://en.wikipedia.org/wiki/Latin_alphabet>Latin alphabet</a>, and it continues to increase in size. Uyghur text in this format is incomprehensible and ambiguous even to native Uyghur speakers. In addition, Uyghur texts in this form lack the potential for any kind of advancement for the <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP tasks</a> related to the <a href=https://en.wikipedia.org/wiki/Uyghur_language>Uyghur language</a>. Restoring and preventing noisy Uyghur text written with unsystematic Latin alphabets will be essential to the protection of Uyghur language and improving the accuracy of Uyghur NLP tasks. To this purpose, in this work we propose and compare the noisy channel model and the neural encoder-decoder model as <a href=https://en.wikipedia.org/wiki/Normalization_(statistics)>normalizing methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4413/>Crowdsourcing Multiple Choice Science Questions</a></strong><br><a href=/people/j/johannes-welbl/>Johannes Welbl</a>
|
<a href=/people/n/nelson-f-liu/>Nelson F. Liu</a>
|
<a href=/people/m/matt-gardner/>Matt Gardner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4413><div class="card-body p-3 small">We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, <a href=https://en.wikipedia.org/wiki/Relevance>relevance</a> or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this <a href=https://en.wikipedia.org/wiki/Methodology>method</a> we have assembled SciQ, a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 13.7 K multiple choice science exam questions. We demonstrate that the method produces in-domain questions by providing an analysis of this new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and by showing that humans can not distinguish the crowdsourced questions from original questions. When using SciQ as additional training data to existing questions, we observe accuracy improvements on real science exams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4414 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4414/>A Text Normalisation System for Non-Standard English Words<span class=acl-fixed-case>E</span>nglish Words</a></strong><br><a href=/people/e/emma-flint/>Emma Flint</a>
|
<a href=/people/e/elliot-ford/>Elliot Ford</a>
|
<a href=/people/o/olivia-thomas/>Olivia Thomas</a>
|
<a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4414><div class="card-body p-3 small">This paper investigates the problem of text normalisation ; specifically, the normalisation of non-standard words (NSWs) in <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Non-standard words can be defined as those word tokens which do not have a dictionary entry, and can not be pronounced using the usual letter-to-phoneme conversion rules ; e.g. lbs, 99.3 %, # EMNLP2017. NSWs pose a challenge to the proper functioning of text-to-speech technology, and the solution is to spell them out in such a way that they can be pronounced appropriately. We describe our four-stage normalisation system made up of components for <a href=https://en.wikipedia.org/wiki/Detection>detection</a>, <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>classification</a>, division and expansion of NSWs. Performance is favourabe compared to previous work in the field (Sproat et al. 2001, Normalization of non-standard words), as well as state-of-the-art text-to-speech software. Further, we update Sproat et al.&#8217;s NSW taxonomy, and create a more customisable system where users are able to input their own abbreviations and specify into which variety of English (currently available : British or American) they wish to normalise.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4415/>Huntsville, hospitals, and hockey teams : Names can reveal your location</a></strong><br><a href=/people/b/bahar-salehi/>Bahar Salehi</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4415><div class="card-body p-3 small">Geolocation is the task of identifying a social media user&#8217;s primary location, and in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, there is a growing literature on to what extent automated analysis of social media posts can help. However, not all content features are equally revealing of a user&#8217;s location. In this paper, we evaluate nine name entity (NE) types. Using various metrics, we find that GEO-LOC, FACILITY and SPORT-TEAM are more informative for <a href=https://en.wikipedia.org/wiki/Geolocation>geolocation</a> than other NE types. Using these types, we improve geolocation accuracy and reduce <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>distance error</a> over various famous text-based methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4416/>Improving Document Clustering by Removing Unnatural Language</a></strong><br><a href=/people/m/myungha-jang/>Myungha Jang</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a>
|
<a href=/people/j/james-allan/>James Allan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4416><div class="card-body p-3 small">Technical documents contain a fair amount of unnatural language, such as tables, <a href=https://en.wikipedia.org/wiki/Formula>formulas</a>, and <a href=https://en.wikipedia.org/wiki/Pseudo-code>pseudo-code</a>. Unnatural language can bean important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>, and evaluates the impact of un-natural language detection on NLP tasks such as <a href=https://en.wikipedia.org/wiki/Document_clustering>document clustering</a>. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First, we create a new annotated corpus by collecting slides and papers in various for-mats, <a href=https://en.wikipedia.org/wiki/PDF>PPT</a>, <a href=https://en.wikipedia.org/wiki/PDF>PDF</a>, and <a href=https://en.wikipedia.org/wiki/HTML>HTML</a>, where unnatural language components are annotated into four categories. We then explore features available from <a href=https://en.wikipedia.org/wiki/Plain_text>plain text</a> to build a <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical model</a> that can handle any format as long as it is converted into plain text. Our experiments show that re-moving unnatural language components gives an absolute improvement in document cluster-ing by up to 15 %. Our corpus and tool are publicly available</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4417 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4417/>Lithium NLP : A System for Rich Information Extraction from Noisy User Generated Text on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a><span class=acl-fixed-case>NLP</span>: A System for Rich Information Extraction from Noisy User Generated Text on Social Media</a></strong><br><a href=/people/p/preeti-bhargava/>Preeti Bhargava</a>
|
<a href=/people/n/nemanja-spasojevic/>Nemanja Spasojevic</a>
|
<a href=/people/g/guoning-hu/>Guoning Hu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4417><div class="card-body p-3 small">In this paper, we describe the Lithium Natural Language Processing (NLP) system-a resource-constrained, high-throughput and language-agnostic system for information extraction from noisy user generated text on social media. Lithium NLP extracts a rich set of information including entities, topics, <a href=https://en.wikipedia.org/wiki/Hashtag>hashtags</a> and <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> from text. We discuss several real world applications of the <a href=https://en.wikipedia.org/wiki/System>system</a> currently incorporated in Lithium products. We also compare our <a href=https://en.wikipedia.org/wiki/System>system</a> with existing commercial and academic NLP systems in terms of performance, information extracted and languages supported. We show that Lithium NLP is at par with and in some cases, outperforms state-of-the-art commercial NLP systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4418/>Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition<span class=acl-fixed-case>WNUT</span>2017 Shared Task on Novel and Emerging Entity Recognition</a></strong><br><a href=/people/l/leon-derczynski/>Leon Derczynski</a>
|
<a href=/people/e/eric-nichols/>Eric Nichols</a>
|
<a href=/people/m/marieke-van-erp/>Marieke van Erp</a>
|
<a href=/people/n/nut-limsopatham/>Nut Limsopatham</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4418><div class="card-body p-3 small">This shared <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarization), but recall on them is a real problem in noisy text-even among annotators. This drop tends to be due to novel entities and <a href=https://en.wikipedia.org/wiki/Surface_(topology)>surface forms</a>. Take for example the tweet so.. kktny in 30 mins? ! even human experts find the entity &#8216;kktny&#8217; hard to detect and resolve. The goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these <a href=https://en.wikipedia.org/wiki/Non-physical_entity>entities</a>. The task as described in this paper evaluated the ability of participating entries to detect and classify novel and emerging <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>named entities</a> in noisy text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4419 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-4419" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-4419/>A Multi-task Approach for <a href=https://en.wikipedia.org/wiki/Named-entity_recognition>Named Entity Recognition</a> in Social Media Data</a></strong><br><a href=/people/g/gustavo-aguilar/>Gustavo Aguilar</a>
|
<a href=/people/s/suraj-maharjan/>Suraj Maharjan</a>
|
<a href=/people/a/adrian-pastor-lopez-monroy/>Adrian Pastor López-Monroy</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4419><div class="card-body p-3 small">Named Entity Recognition for <a href=https://en.wikipedia.org/wiki/Social_media>social media data</a> is challenging because of its inherent noisiness. In addition to improper grammatical structures, it contains <a href=https://en.wikipedia.org/wiki/Orthographic_error>spelling inconsistencies</a> and numerous <a href=https://en.wikipedia.org/wiki/Abbreviation>informal abbreviations</a>. We propose a novel multi-task approach by employing a more general secondary task of Named Entity (NE) segmentation together with the primary task of fine-grained NE categorization. The multi-task neural network architecture learns higher order feature representations from word and character sequences along with basic Part-of-Speech tags and gazetteer information. This <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> acts as a <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extractor</a> to feed a Conditional Random Fields classifier. We were able to obtain the first position in the 3rd Workshop on Noisy User-generated Text (WNUT-2017) with a 41.86 % entity F1-score and a 40.24 % surface F1-score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4421/>Multi-channel BiLSTM-CRF Model for Emerging Named Entity Recognition in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a><span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-<span class=acl-fixed-case>CRF</span> Model for Emerging Named Entity Recognition in Social Media</a></strong><br><a href=/people/b/bill-yuchen-lin/>Bill Y. Lin</a>
|
<a href=/people/f/frank-f-xu/>Frank Xu</a>
|
<a href=/people/z/zhiyi-luo/>Zhiyi Luo</a>
|
<a href=/people/k/kenny-zhu/>Kenny Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4421><div class="card-body p-3 small">In this paper, we present our multi-channel neural architecture for recognizing emerging named entity in social media messages, which we applied in the Novel and Emerging Named Entity Recognition shared task at the EMNLP 2017 Workshop on Noisy User-generated Text (W-NUT). We propose a novel approach, which incorporates comprehensive word representations with multi-channel information and Conditional Random Fields (CRF) into a traditional Bidirectional Long Short-Term Memory (BiLSTM) neural network without using any additional hand-craft features such as gazetteers. In comparison with other <a href=https://en.wikipedia.org/wiki/System>systems</a> participating in the shared task, our <a href=https://en.wikipedia.org/wiki/System>system</a> won the 2nd place.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4422/>Transfer Learning and Sentence Level Features for Named Entity Recognition on Tweets</a></strong><br><a href=/people/p/pius-von-daniken/>Pius von Däniken</a>
|
<a href=/people/m/mark-cieliebak/>Mark Cieliebak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4422><div class="card-body p-3 small">We present our system for the WNUT 2017 Named Entity Recognition challenge on Twitter data. We describe two modifications of a basic neural network architecture for sequence tagging. First, we show how we exploit additional labeled data, where the Named Entity tags differ from the target task. Then, we propose a way to incorporate <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level features</a>. Our system uses both methods and ranked second for entity level annotations, achieving an F1-score of 40.78, and <a href=https://en.wikipedia.org/wiki/Second>second</a> for surface form annotations, achieving an F1-score of 39.33.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4424.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4424 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4424 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4424/>A Feature-based Ensemble Approach to Recognition of Emerging and Rare Named Entities</a></strong><br><a href=/people/u/utpal-kumar-sikdar/>Utpal Kumar Sikdar</a>
|
<a href=/people/b/bjorn-gamback/>Björn Gambäck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4424><div class="card-body p-3 small">Detecting previously unseen named entities in text is a challenging task. The paper describes how three initial classifier models were built using Conditional Random Fields (CRFs), Support Vector Machines (SVMs) and a Long Short-Term Memory (LSTM) recurrent neural network. The outputs of these three <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> were then used as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to train another CRF classifier working as an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a>. 5-fold cross-validation based on training and development data for the emerging and rare named entity recognition shared task showed <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a>, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> of 66.87 %, 46.75 % and 54.97 %, respectively. For surface form evaluation, the CRF ensemble-based system achieved precision, <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> and F1 scores of 65.18 %, 45.20 % and 53.30 %. When applied to unseen test data, the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> reached 47.92 % <a href=https://en.wikipedia.org/wiki/Precision_(statistics)>precision</a>, 31.97 % recall and 38.55 % <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a> for entity level evaluation, with the corresponding surface form evaluation values of 44.91 %, 30.47 % and 36.31 %.</div></div></div><hr><div id=w17-45><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-45.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-45/>Proceedings of the Workshop on New Frontiers in Summarization</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4500/>Proceedings of the Workshop on New Frontiers in Summarization</a></strong><br><a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4501/>Video Highlights Detection and Summarization with Lag-Calibration based on Concept-Emotion Mapping of Crowdsourced Time-Sync Comments</a></strong><br><a href=/people/q/qing-ping/>Qing Ping</a>
|
<a href=/people/c/chaomei-chen/>Chaomei Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4501><div class="card-body p-3 small">With the prevalence of <a href=https://en.wikipedia.org/wiki/Online_video_platform>video sharing</a>, there are increasing demands for automatic video digestion such as highlight detection. Recently, <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> with crowdsourced time-sync video comments have emerged worldwide, providing a good opportunity for highlight detection. However, this task is non-trivial : (1) time-sync comments often lag behind their corresponding shot ; (2) time-sync comments are semantically sparse and noisy ; (3) to determine which shots are highlights is highly subjective. The present paper aims to tackle these challenges by proposing a framework that (1) uses concept-mapped lexical-chains for lag-calibration ; (2) models video highlights based on comment intensity and combination of <a href=https://en.wikipedia.org/wiki/Emotion>emotion</a> and concept concentration of each shot ; (3) summarize each detected highlight using improved SumBasic with emotion and concept mapping. Experiments on large real-world datasets show that our highlight detection method and summarization method both outperform other benchmarks with considerable margins.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4502/>Multimedia Summary Generation from Online Conversations : Current Approaches and Future Directions</a></strong><br><a href=/people/e/enamul-hoque/>Enamul Hoque</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4502><div class="card-body p-3 small">With the proliferation of Web-based social media, asynchronous conversations have become very common for supporting <a href=https://en.wikipedia.org/wiki/Online_communication>online communication</a> and <a href=https://en.wikipedia.org/wiki/Collaboration>collaboration</a>. Yet the increasing volume and complexity of conversational data often make it very difficult to get insights about the discussions. We consider combining textual summary with visual representation of conversational data as a promising way of supporting the user in exploring conversations. In this paper, we report our current work on developing visual interfaces that present multimedia summary combining text and visualization for online conversations and how our solutions have been tailored for a variety of domain problems. We then discuss the key challenges and opportunities for future work in this research space.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4504/>Towards Improving Abstractive Summarization via Entailment Generation</a></strong><br><a href=/people/r/ramakanth-pasunuru/>Ramakanth Pasunuru</a>
|
<a href=/people/h/han-guo/>Han Guo</a>
|
<a href=/people/m/mohit-bansal/>Mohit Bansal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4504><div class="card-body p-3 small">Abstractive summarization, the task of rewriting and compressing a document into a short summary, has achieved considerable success with neural sequence-to-sequence models. However, these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> can still benefit from stronger <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference skills</a>, since a correct summary is logically entailed by the input document, i.e., it should not contain any contradictory or unrelated information. We incorporate such knowledge into an abstractive summarization model via <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>, where we share its decoder parameters with those of an entailment generation model. We achieve promising initial improvements based on multiple metrics and datasets (including a test-only setting). The domain mismatch between the entailment (captions) and summarization (news) datasets suggests that the model is learning some domain-agnostic inference skills.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4505/>Coarse-to-Fine Attention Models for Document Summarization</a></strong><br><a href=/people/j/jeffrey-ling/>Jeffrey Ling</a>
|
<a href=/people/a/alexander-m-rush/>Alexander Rush</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4505><div class="card-body p-3 small">Sequence-to-sequence models with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> have been successful for a variety of NLP problems, but their speed does not scale well for tasks with long source sequences such as <a href=https://en.wikipedia.org/wiki/Document_summarization>document summarization</a>. We propose a novel coarse-to-fine attention model that hierarchically reads a document, using coarse attention to select top-level chunks of text and fine attention to read the words of the chosen chunks. While the computation for training standard attention models scales linearly with source sequence length, our method scales with the number of top-level chunks and can handle much longer sequences. Empirically, we find that while coarse-to-fine attention models lag behind state-of-the-art baselines, our method achieves the desired behavior of sparsely attending to subsets of the document for generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4506/>Automatic Community Creation for Abstractive Spoken Conversations Summarization</a></strong><br><a href=/people/k/karan-singla/>Karan Singla</a>
|
<a href=/people/e/evgeny-stepanov/>Evgeny Stepanov</a>
|
<a href=/people/a/ali-orkan-bayer/>Ali Orkan Bayer</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/g/giuseppe-riccardi/>Giuseppe Riccardi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4506><div class="card-body p-3 small">Summarization of spoken conversations is a challenging task, since it requires deep understanding of dialogs. Abstractive summarization techniques rely on linking the summary sentences to sets of original conversation sentences, i.e. communities. Unfortunately, such linking information is rarely available or requires trained annotators. We propose and experiment automatic community creation using <a href=https://en.wikipedia.org/wiki/Cosine_similarity>cosine similarity</a> on different levels of representation : raw text, WordNet SynSet IDs, and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We show that the abstractive summarization systems with automatic communities significantly outperform previously published results on both English and Italian corpora.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-4507" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-4507/>Combining <a href=https://en.wikipedia.org/wiki/Graph_degeneracy>Graph Degeneracy</a> and Submodularity for Unsupervised Extractive Summarization</a></strong><br><a href=/people/a/antoine-tixier/>Antoine Tixier</a>
|
<a href=/people/p/polykarpos-meladianos/>Polykarpos Meladianos</a>
|
<a href=/people/m/michalis-vazirgiannis/>Michalis Vazirgiannis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4507><div class="card-body p-3 small">We present a fully unsupervised, extractive text summarization system that leverages a submodularity framework introduced by past research. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> allows summaries to be generated in a greedy way while preserving near-optimal performance guarantees. Our main contribution is the novel coverage reward term of the <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> optimized by the <a href=https://en.wikipedia.org/wiki/Greedy_algorithm>greedy algorithm</a>. This component builds on the graph-of-words representation of text and the k-core decomposition algorithm to assign meaningful scores to words. We evaluate our approach on the AMI and ICSI meeting speech corpora, and on the DUC2001 news corpus. We reach state-of-the-art performance on all <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. Results indicate that our method is particularly well-suited to the meeting domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4508/>TL;DR : Mining Reddit to Learn Automatic Summarization<span class=acl-fixed-case>TL</span>;<span class=acl-fixed-case>DR</span>: Mining <span class=acl-fixed-case>R</span>eddit to Learn Automatic Summarization</a></strong><br><a href=/people/m/michael-volske/>Michael Völske</a>
|
<a href=/people/m/martin-potthast/>Martin Potthast</a>
|
<a href=/people/s/shahbaz-syed/>Shahbaz Syed</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4508><div class="card-body p-3 small">Recent advances in <a href=https://en.wikipedia.org/wiki/Automatic_text_summarization>automatic text summarization</a> have used deep neural networks to generate high-quality abstractive summaries, but the performance of these models strongly depends on large amounts of suitable training data. We propose a new method for mining <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> for author-provided summaries, taking advantage of the common practice of appending a TL;DR to long posts. A case study using a large Reddit crawl yields the Webis-TLDR-17 dataset, complementing existing corpora primarily from the <a href=https://en.wikipedia.org/wiki/News_media>news genre</a>. Our technique is likely applicable to other <a href=https://en.wikipedia.org/wiki/Social_media>social media sites</a> and <a href=https://en.wikipedia.org/wiki/Web_crawler>general web crawls</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4509.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-4509/>Topic Model Stability for Hierarchical Summarization</a></strong><br><a href=/people/j/john-miller/>John Miller</a>
|
<a href=/people/k/kathleen-f-mccoy/>Kathleen McCoy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4509><div class="card-body p-3 small">We envisioned responsive generic hierarchical text summarization with summaries organized by section and paragraph based on hierarchical structure topic models. But we had to be sure that <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> were stable for the sampled corpora. To that end we developed a methodology for aligning multiple hierarchical structure topic models run over the same corpus under similar conditions, calculating a representative centroid model, and reporting stability of the centroid model. We ran stability experiments for standard corpora and a development corpus of Global Warming articles. We found flat and hierarchical structures of two levels plus the root offer stable centroid models, but hierarchical structures of three levels plus the <a href=https://en.wikipedia.org/wiki/Zero_of_a_function>root</a> did n&#8217;t seem stable enough for use in hierarchical summarization.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4510.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4510 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4510 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4510/>Learning to Score System Summaries for Better Content Selection Evaluation.</a></strong><br><a href=/people/m/maxime-peyrard/>Maxime Peyrard</a>
|
<a href=/people/t/teresa-botschen/>Teresa Botschen</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4510><div class="card-body p-3 small">The evaluation of summaries is a challenging but crucial task of the summarization field. In this work, we propose to learn an automatic scoring metric based on the human judgements available as part of classical summarization datasets like TAC-2008 and TAC-2009. Any existing automatic scoring metrics can be included as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> learns the combination exhibiting the best correlation with human judgments. The <a href=https://en.wikipedia.org/wiki/Reliability_(statistics)>reliability</a> of the new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> is tested in a further manual evaluation where we ask humans to evaluate summaries covering the whole scoring spectrum of the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a>. We release the trained <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> as an open-source tool.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4513/>A Pilot Study of Domain Adaptation Effect for Neural Abstractive Summarization</a></strong><br><a href=/people/x/xinyu-hua/>Xinyu Hua</a>
|
<a href=/people/l/lu-wang/>Lu Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4513><div class="card-body p-3 small">We study the problem of <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for neural abstractive summarization. We make initial efforts in investigating what information can be transferred to a new domain. Experimental results on <a href=https://en.wikipedia.org/wiki/Article_(publishing)>news stories</a> and <a href=https://en.wikipedia.org/wiki/Opinion_piece>opinion articles</a> indicate that neural summarization model benefits from <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training</a> based on extractive summaries. We also find that the combination of in-domain and out-of-domain setup yields better summaries when in-domain data is insufficient. Further analysis shows that, the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is capable to select salient content even trained on out-of-domain data, but requires in-domain data to capture the style for a target domain.</div></div></div><hr><div id=w17-46><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-46.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-46/>Proceedings of the Workshop on Speech-Centric Natural Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4600/>Proceedings of the Workshop on Speech-Centric Natural Language Processing</a></strong><br><a href=/people/n/nicholas-ruiz/>Nicholas Ruiz</a>
|
<a href=/people/s/srinivas-bangalore/>Srinivas Bangalore</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4603.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-4603/>Analyzing Human and Machine Performance In Resolving Ambiguous Spoken Sentences</a></strong><br><a href=/people/h/hussein-ghaly/>Hussein Ghaly</a>
|
<a href=/people/m/michael-mandel/>Michael Mandel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4603><div class="card-body p-3 small">Written sentences can be more ambiguous than <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>spoken sentences</a>. We investigate this difference for two different types of <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguity</a> : prepositional phrase (PP) attachment and sentences where the addition of commas changes the meaning. We recorded a native English speaker saying several of each type of sentence both with and without disambiguating contextual information. These <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentences</a> were then presented either as text or audio and either with or without context to subjects who were asked to select the proper interpretation of the sentence. Results suggest that comma-ambiguous sentences are easier to disambiguate than PP-attachment-ambiguous sentences, possibly due to the presence of clear prosodic boundaries, namely silent pauses. Subject performance for sentences with PP-attachment ambiguity without context was 52 % for text only while it was 72.4 % for <a href=https://en.wikipedia.org/wiki/Sound_recording_and_reproduction>audio only</a>, suggesting that <a href=https://en.wikipedia.org/wiki/Sound_recording_and_reproduction>audio</a> has more disambiguating information than <a href=https://en.wikipedia.org/wiki/Written_language>text</a>. Using an analysis of acoustic features of two PP-attachment sentences, a simple classifier was implemented to resolve the PP-attachment ambiguity being early or late closure with a mean accuracy of 80 %.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4604.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4604 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4604 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4604/>Parsing transcripts of speech</a></strong><br><a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/m/michael-mccarthy/>Michael McCarthy</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4604><div class="card-body p-3 small">We present an analysis of <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> performance on speech data, comparing word type and token frequency distributions with written data, and evaluating parse accuracy by length of input string. We find that <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> performance tends to deteriorate with increasing length of string, more so for spoken than for written texts. We train an alternative parsing model with added <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech data</a> and demonstrate improvements in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on speech-units, with no deterioration in performance on <a href=https://en.wikipedia.org/wiki/Written_language>written text</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4606.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4606 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4606 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-4606" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-4606/>End-to-End Information Extraction without Token-Level Supervision</a></strong><br><a href=/people/r/rasmus-berg-palm/>Rasmus Berg Palm</a>
|
<a href=/people/d/dirk-hovy/>Dirk Hovy</a>
|
<a href=/people/f/florian-laws/>Florian Laws</a>
|
<a href=/people/o/ole-winther/>Ole Winther</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4606><div class="card-body p-3 small">Most state-of-the-art information extraction approaches rely on token-level labels to find the areas of interest in text. Unfortunately, these labels are time-consuming and costly to create, and consequently, not available for many real-life IE tasks. To make matters worse, token-level labels are usually not the desired output, but just an intermediary step. End-to-end (E2E) models, which take raw text as input and produce the desired output directly, need not depend on token-level labels. We propose an E2E model based on pointer networks, which can be trained directly on pairs of raw input and output text. We evaluate our model on the ATIS data set, MIT restaurant corpus and the MIT movie corpus and compare to neural baselines that do use token-level labels. We achieve competitive results, within a few percentage points of the baselines, showing the feasibility of E2E information extraction without the need for token-level labels. This opens up new possibilities, as for many tasks currently addressed by human extractors, raw input and output data are available, but not token-level labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4607.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4607 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4607 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4607/>Spoken Term Discovery for <a href=https://en.wikipedia.org/wiki/Language_documentation>Language Documentation</a> using Translations</a></strong><br><a href=/people/a/antonios-anastasopoulos/>Antonios Anastasopoulos</a>
|
<a href=/people/s/sameer-bansal/>Sameer Bansal</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a>
|
<a href=/people/a/adam-lopez/>Adam Lopez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4607><div class="card-body p-3 small">Vast amounts of speech data collected for <a href=https://en.wikipedia.org/wiki/Language_documentation>language documentation</a> and research remain untranscribed and unsearchable, but often a small amount of speech may have text translations available. We present a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for partially labeling additional speech with translations in this <a href=https://en.wikipedia.org/wiki/Scenario>scenario</a>. We modify an unsupervised speech-to-translation alignment model and obtain prototype speech segments that match the translation words, which are in turn used to discover terms in the unlabelled data. We evaluate our method on a Spanish-English speech translation corpus and on two corpora of endangered languages, <a href=https://en.wikipedia.org/wiki/Arapaho_language>Arapaho</a> and Ainu, demonstrating its appropriateness and applicability in an actual very-low-resource scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4608.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4608 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4608 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4608/>Amharic-English Speech Translation in Tourism Domain<span class=acl-fixed-case>A</span>mharic-<span class=acl-fixed-case>E</span>nglish Speech Translation in Tourism Domain</a></strong><br><a href=/people/m/michael-melese/>Michael Melese</a>
|
<a href=/people/l/laurent-besacier/>Laurent Besacier</a>
|
<a href=/people/m/million-meshesha/>Million Meshesha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4608><div class="card-body p-3 small">This paper describes speech translation from Amharic-to-English, particularly Automatic Speech Recognition (ASR) with post-editing feature and Amharic-English Statistical Machine Translation (SMT). ASR experiment is conducted using morpheme language model (LM) and phoneme acoustic model(AM). Likewise, SMT conducted using word and morpheme as unit. Morpheme based translation shows a 6.29 BLEU score at a 76.4 % of recognition accuracy while word based translation shows a 12.83 BLEU score using 77.4 % word recognition accuracy. Further, after post-edit on Amharic ASR using <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpus based n-gram</a>, the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>word recognition accuracy</a> increased by 1.42 %. Since post-edit approach reduces <a href=https://en.wikipedia.org/wiki/Propagation_of_uncertainty>error propagation</a>, the word based translation accuracy improved by 0.25 (1.95 %) BLEU score. We are now working towards further improving propagated errors through different <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> at each unit of speech translation cascading component.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4609.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4609 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4609 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4609/>Speech- and Text-driven Features for Automated Scoring of English Speaking Tasks<span class=acl-fixed-case>E</span>nglish Speaking Tasks</a></strong><br><a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/a/aoife-cahill/>Aoife Cahill</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4609><div class="card-body p-3 small">We consider the automatic scoring of a <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> for which both the content of the response as well its spoken fluency are important. We combine <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> from a text-only content scoring system originally designed for written responses with several categories of acoustic features. Although adding any single category of acoustic features to the text-only system on its own does not significantly improve performance, adding all acoustic features together does yield a small but significant improvement. These results are consistent for responses to open-ended questions and to questions focused on some given source material.</div></div></div><hr><div id=w17-47><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-47.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-47/>Proceedings of the Second Conference on Machine Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4700/>Proceedings of the Second Conference on Machine Translation</a></strong><br><a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/c/christian-buck/>Christian Buck</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a></span></p></div><hr><div id=w17-48><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-48.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-48/>Proceedings of the Third Workshop on Discourse in Machine Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4800/>Proceedings of the Third Workshop on Discourse in Machine Translation</a></strong><br><a href=/people/b/bonnie-webber/>Bonnie Webber</a>
|
<a href=/people/a/andrei-popescu-belis/>Andrei Popescu-Belis</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4801.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-4801/>Findings of the 2017 DiscoMT Shared Task on Cross-lingual Pronoun Prediction<span class=acl-fixed-case>D</span>isco<span class=acl-fixed-case>MT</span> Shared Task on Cross-lingual Pronoun Prediction</a></strong><br><a href=/people/s/sharid-loaiciga/>Sharid Loáiciga</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/m/mauro-cettolo/>Mauro Cettolo</a>
|
<a href=/people/y/yannick-versley/>Yannick Versley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4801><div class="card-body p-3 small">We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a <a href=https://en.wikipedia.org/wiki/Pronoun>target-language pronoun</a> given a <a href=https://en.wikipedia.org/wiki/Pronoun>source-language pronoun</a> in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the target-language lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of classes, using any type of information that can be extracted from the entire document. We offered four subtasks, each for a different language pair and translation direction : <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-to-French</a>, <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-to-German</a>, German-to-English, and <a href=https://en.wikipedia.org/wiki/Spanish_as_a_second_or_foreign_language>Spanish-to-English</a>. Five teams participated in the shared task, making submissions for all language pairs. The evaluation results show that most participating teams outperformed two strong n-gram-based language model-based baseline systems by a sizable margin.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4803/>Using a Graph-based Coherence Model in Document-Level Machine Translation</a></strong><br><a href=/people/l/leo-born/>Leo Born</a>
|
<a href=/people/m/mohsen-mesgar/>Mohsen Mesgar</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4803><div class="card-body p-3 small">Although <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>coherence</a> is an important aspect of any text generation system, it has received little attention in the context of machine translation (MT) so far. We hypothesize that the quality of document-level translation can be improved if MT models take into account the semantic relations among sentences during <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. We integrate the graph-based coherence model proposed by Mesgar and Strube, (2016) with Docent (Hardmeier et al., 2012, Hardmeier, 2014) a document-level machine translation system. The application of this graph-based coherence modeling approach is novel in the context of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We evaluate the coherence model and its effects on the quality of the <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. The result of our experiments shows that our <a href=https://en.wikipedia.org/wiki/Coherence_(physics)>coherence model</a> slightly improves the quality of translation in terms of the average Meteor score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4804.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4804 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4804 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4804/>Treatment of Markup in <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>Statistical Machine Translation</a></a></strong><br><a href=/people/m/mathias-muller/>Mathias Müller</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4804><div class="card-body p-3 small">We present work on handling <a href=https://en.wikipedia.org/wiki/XML>XML markup</a> in Statistical Machine Translation (SMT). The methods we propose can be used to effectively preserve <a href=https://en.wikipedia.org/wiki/Markup_language>markup</a> (for instance inline formatting or structure) and to place <a href=https://en.wikipedia.org/wiki/Markup_language>markup</a> correctly in a machine-translated segment. We evaluate our approaches with parallel data that naturally contains <a href=https://en.wikipedia.org/wiki/Markup_language>markup</a> or where <a href=https://en.wikipedia.org/wiki/Markup_language>markup</a> was inserted to create synthetic examples. In our experiments, hybrid reinsertion has proven the most accurate method to handle <a href=https://en.wikipedia.org/wiki/Markup_language>markup</a>, while alignment masking and alignment reinsertion should be regarded as viable alternatives. We provide implementations of all the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a> described and they are freely available as an open-source framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4805/>A BiLSTM-based System for Cross-lingual Pronoun Prediction<span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span>-based System for Cross-lingual Pronoun Prediction</a></strong><br><a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/s/sharid-loaiciga/>Sharid Loáiciga</a>
|
<a href=/people/f/fabienne-cap/>Fabienne Cap</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4805><div class="card-body p-3 small">We describe the Uppsala system for the 2017 DiscoMT shared task on cross-lingual pronoun prediction. The <a href=https://en.wikipedia.org/wiki/System>system</a> is based on a lower layer of BiLSTMs reading the source and target sentences respectively. Classification is based on the BiLSTM representation of the source and target positions for the <a href=https://en.wikipedia.org/wiki/Pronoun>pronouns</a>. In addition we enrich our system with dependency representations from an <a href=https://en.wikipedia.org/wiki/Parsing>external parser</a> and <a href=https://en.wikipedia.org/wiki/Character_encoding>character representations</a> of the source sentence. We show that these additions perform well for <a href=https://en.wikipedia.org/wiki/German_language>German</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> as source languages. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is competitive and is in first or second place for all language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4806/>Neural Machine Translation for Cross-Lingual Pronoun Prediction</a></strong><br><a href=/people/s/sebastien-jean/>Sebastien Jean</a>
|
<a href=/people/s/stanislas-lauly/>Stanislas Lauly</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a>
|
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4806><div class="card-body p-3 small">In this paper we present our systems for the DiscoMT 2017 cross-lingual pronoun prediction shared task. For all four language pairs, we trained a standard attention-based neural machine translation system as well as three variants that incorporate information from the preceding source sentence. We show that our <a href=https://en.wikipedia.org/wiki/System>systems</a>, which are not specifically designed for pronoun prediction and may be used to generate complete sentence translations, generally achieve competitive results on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4807.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4807 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4807 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4807/>Predicting Pronouns with a Convolutional Network and an <a href=https://en.wikipedia.org/wiki/N-gram_model>N-gram Model</a></a></strong><br><a href=/people/c/christian-hardmeier/>Christian Hardmeier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4807><div class="card-body p-3 small">This paper describes the UU-Hardmeier system submitted to the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The system is an ensemble of <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> combined with a source-aware n-gram language model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4808.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4808 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4808 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4808/>Cross-Lingual Pronoun Prediction with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Recurrent Neural Networks</a> v2.0</a></strong><br><a href=/people/j/juhani-luotolahti/>Juhani Luotolahti</a>
|
<a href=/people/j/jenna-kanerva/>Jenna Kanerva</a>
|
<a href=/people/f/filip-ginter/>Filip Ginter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4808><div class="card-body p-3 small">In this paper we present our <a href=https://en.wikipedia.org/wiki/System>system</a> in the DiscoMT 2017 Shared Task on Crosslingual Pronoun Prediction. Our entry builds on our last year&#8217;s success, our system based on deep recurrent neural networks outperformed all the other systems with a clear margin. This year we investigate whether different pre-trained word embeddings can be used to improve the neural systems, and whether the recently published Gated Convolutions outperform the Gated Recurrent Units used last year.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4809 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4809/>Combining the output of two coreference resolution systems for two source languages to improve annotation projection</a></strong><br><a href=/people/y/yulia-grishina/>Yulia Grishina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4809><div class="card-body p-3 small">Although parallel coreference corpora can to a high degree support the development of SMT systems, there are no large-scale parallel datasets available due to the complexity of the annotation task and the variability in annotation schemes. In this study, we exploit an annotation projection method to combine the output of two coreference resolution systems for two different source languages (English, German) in order to create an annotated corpus for a third language (Russian). We show that our technique is superior to projecting annotations from a single source language, and we provide an in-depth analysis of the projected annotations in order to assess the perspectives of our approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4810.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4810 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4810 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4810/>Discovery of Discourse-Related Language Contrasts through Alignment Discrepancies in English-German Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>G</span>erman Translation</a></strong><br><a href=/people/e/ekaterina-lapshinova-koltunski/>Ekaterina Lapshinova-Koltunski</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4810><div class="card-body p-3 small">In this paper, we analyse alignment discrepancies for discourse structures in English-German parallel data sentence pairs, in which discourse structures in target or source texts have no alignment in the corresponding parallel sentences. The discourse-related structures are designed in form of linguistic patterns based on the information delivered by automatic part-of-speech and dependency annotation. In addition to alignment errors (existing structures left unaligned), these alignment discrepancies can be caused by language contrasts or through the phenomena of explicitation and implicitation in the translation process. We propose a new approach including new type of resources for corpus-based language contrast analysis and apply it to study and classify the contrasts found in our English-German parallel corpus. As unaligned discourse structures may also result in the loss of discourse information in the MT training data, we hope to deliver information in support of discourse-aware machine translation (MT).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4811.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4811 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4811 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4811/>Neural Machine Translation with Extended Context</a></strong><br><a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/y/yves-scherrer/>Yves Scherrer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4811><div class="card-body p-3 small">We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> at least in some selected cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4812.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4812 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4812 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4812/>Translating Implicit Discourse Connectives Based on Cross-lingual Annotation and Alignment</a></strong><br><a href=/people/h/hongzheng-li/>Hongzheng Li</a>
|
<a href=/people/p/philippe-langlais/>Philippe Langlais</a>
|
<a href=/people/y/yaohong-jin/>Yaohong Jin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4812><div class="card-body p-3 small">Implicit discourse connectives and relations are distributed more widely in Chinese texts, when translating into <a href=https://en.wikipedia.org/wiki/English_language>English</a>, such connectives are usually translated explicitly. Towards Chinese-English MT, in this paper we describe cross-lingual annotation and alignment of dis-course connectives in a parallel corpus, describing related surveys and findings. We then conduct some evaluation experiments to testify the <a href=https://en.wikipedia.org/wiki/Translation>translation of implicit connectives</a> and whether representing implicit connectives explicitly in source language can improve the final <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance significantly. Preliminary results show it has little improvement by just inserting explicit connectives for implicit relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4814.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4814 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4814 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4814/>On Integrating Discourse in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a></a></strong><br><a href=/people/k/karin-sim-smith/>Karin Sim Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4814><div class="card-body p-3 small">As the quality of Machine Translation (MT) improves, research on improving discourse in automatic translations becomes more viable. This has resulted in an increase in the amount of work on <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> in MT. However many of the existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> have yet to integrate these insights. Part of this is due to the evaluation methodology, based as it is largely on matching to a single reference. At a time when MT is increasingly being used in a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> for other tasks, the semantic element of the translation process needs to be properly integrated into the task. Moreover, in order to take MT to another level, it will need to judge output not based on a single reference translation, but based on notions of <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a> and of adequacy ideally with reference to the source text.</div></div></div><hr><div id=w17-49><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-49.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-49/>Proceedings of the Workshop on Stylistic Variation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4900/>Proceedings of the Workshop on Stylistic Variation</a></strong><br><a href=/people/j/julian-brooke/>Julian Brooke</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a>
|
<a href=/people/m/moshe-koppel/>Moshe Koppel</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4901.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4901 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4901 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4901/>From Shakespeare to <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> : What are Language Styles all about?<span class=acl-fixed-case>T</span>witter: What are Language Styles all about?</a></strong><br><a href=/people/w/wei-xu/>Wei Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4901><div class="card-body p-3 small">As <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> research is growing and largely driven by the availability of data, we expanded research from news and small-scale dialog corpora to web and social media. User-generated data and <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> opened the door for investigating <a href=https://en.wikipedia.org/wiki/Human_language>human language</a> of various styles with more statistical power and real-world applications. In this position / survey paper, I will review and discuss seven language styles that I believe to be important and interesting to study : influential work in the past, challenges at the present, and potential impact for the future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4902.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-4902" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-4902/>Shakespearizing Modern Language Using Copy-Enriched Sequence to Sequence Models</a></strong><br><a href=/people/h/harsh-jhamtani/>Harsh Jhamtani</a>
|
<a href=/people/v/varun-gangal/>Varun Gangal</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4902><div class="card-body p-3 small">Variations in <a href=https://en.wikipedia.org/wiki/Writing_style>writing styles</a> are commonly used to adapt the content to a specific context, audience, or purpose. However, applying stylistic variations is still by and large a manual process, and there have been little efforts towards automating it. In this paper we explore automated methods to transform text from <a href=https://en.wikipedia.org/wiki/Modern_English>modern English</a> to <a href=https://en.wikipedia.org/wiki/Old_English>Shakespearean English</a> using an end to end trainable neural model with pointers to enable copy action. To tackle limited amount of parallel data, we pre-train embeddings of words by leveraging external dictionaries mapping Shakespearean words to modern English words as well as additional text. Our methods are able to get a BLEU score of 31 +, an improvement of 6 points above the strongest baseline. We publicly release our code to foster further research in this area.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4903/>Discovering Stylistic Variations in Distributional Vector Space Models via Lexical Paraphrases</a></strong><br><a href=/people/x/xing-niu/>Xing Niu</a>
|
<a href=/people/m/marine-carpuat/>Marine Carpuat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4903><div class="card-body p-3 small">Detecting and analyzing stylistic variation in <a href=https://en.wikipedia.org/wiki/Language>language</a> is relevant to diverse Natural Language Processing applications. In this work, we investigate whether salient dimensions of style variations are embedded in standard distributional vector spaces of word meaning. We hypothesizes that distances between embeddings of lexical paraphrases can help isolate <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>style</a> from meaning variations and help identify latent style dimensions. We conduct a qualitative analysis of latent style dimensions, and show the effectiveness of identified style subspaces on a lexical formality prediction task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4904.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4904 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4904 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4904/>Harvesting Creative Templates for Generating Stylistically Varied Restaurant Reviews</a></strong><br><a href=/people/s/shereen-oraby/>Shereen Oraby</a>
|
<a href=/people/s/sheideh-homayon/>Sheideh Homayon</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4904><div class="card-body p-3 small">Many of the creative and figurative elements that make language exciting are lost in <a href=https://en.wikipedia.org/wiki/Translation>translation</a> in current natural language generation engines. In this paper, we explore a method to harvest templates from positive and negative reviews in the restaurant domain, with the goal of vastly expanding the types of stylistic variation available to the <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generator</a>. We learn hyperbolic adjective patterns that are representative of the strongly-valenced expressive language commonly used in either positive or negative reviews. We then identify and delexicalize entities, and use <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> to extract generation templates from review sentences. We evaluate the learned templates against more traditional review templates, using subjective measures of convincingness, <a href=https://en.wikipedia.org/wiki/Interest_(emotion)>interestingness</a>, and <a href=https://en.wikipedia.org/wiki/Naturalness_(philosophy)>naturalness</a>. Our results show that the learned <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>templates</a> score highly on these measures. Finally, we analyze the linguistic categories that characterize the learned positive and negative templates. We plan to use the learned <a href=https://en.wikipedia.org/wiki/Template_(word_processing)>templates</a> to improve the conversational style of dialogue systems in the restaurant domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4906/>Deep Learning : Detecting Metaphoricity in Adjective-Noun Pairs</a></strong><br><a href=/people/y/yuri-bizzoni/>Yuri Bizzoni</a>
|
<a href=/people/s/stergios-chatzikyriakidis/>Stergios Chatzikyriakidis</a>
|
<a href=/people/m/mehdi-ghanimifard/>Mehdi Ghanimifard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4906><div class="card-body p-3 small">Metaphor is one of the most studied and widespread figures of speech and an essential element of individual style. In this paper we look at metaphor identification in Adjective-Noun pairs. We show that using a single <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> combined with pre-trained vector embeddings can outperform the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state of the art</a> in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. In specific, the approach presented in this paper is based on two ideas : a) <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> via using pre-trained vectors representing adjective noun pairs, and b) a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> as a model of composition that predicts a metaphoricity score as output. We present several different <a href=https://en.wikipedia.org/wiki/Systems_architecture>architectures</a> for our <a href=https://en.wikipedia.org/wiki/System>system</a> and evaluate their performances. Variations on dataset size and on the kinds of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are also investigated. We show considerable improvement over the previous <a href=https://en.wikipedia.org/wiki/Statistical_inference>approaches</a> both in terms of <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and w.r.t the size of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>annotated training data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4907/>Authorship Attribution with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> and POS-Eliding<span class=acl-fixed-case>POS</span>-Eliding</a></strong><br><a href=/people/j/julian-hitschler/>Julian Hitschler</a>
|
<a href=/people/e/esther-van-den-berg/>Esther van den Berg</a>
|
<a href=/people/i/ines-rehbein/>Ines Rehbein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4907><div class="card-body p-3 small">We use a <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural network</a> to perform authorship identification on a very homogeneous dataset of scientific publications. In order to investigate the effect of domain biases, we obscure words below a certain frequency threshold, retaining only their POS-tags. This <a href=https://en.wikipedia.org/wiki/Procedure_(term)>procedure</a> improves test performance due to better <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> on unseen data. Using our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, we are able to predict the authors of scientific publications in the same discipline at levels well above chance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4908/>Topic and audience effects on distinctively Scottish vocabulary usage in Twitter data<span class=acl-fixed-case>S</span>cottish vocabulary usage in <span class=acl-fixed-case>T</span>witter data</a></strong><br><a href=/people/p/philippa-shoemark/>Philippa Shoemark</a>
|
<a href=/people/j/james-kirby/>James Kirby</a>
|
<a href=/people/s/sharon-goldwater/>Sharon Goldwater</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4908><div class="card-body p-3 small">Sociolinguistic research suggests that speakers modulate their language style in response to their audience. Similar effects have recently been claimed to occur in the informal written context of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>, with users choosing less region-specific and non-standard vocabulary when addressing larger audiences. However, these studies have not carefully controlled for the possible confound of topic : that is, tweets addressed to a broad audience might also tend towards topics that engender a more <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>formal style</a>. In addition, it is not clear to what extent previous results generalize to different samples of users. Using mixed-effects models, we show that audience and topic have independent effects on the rate of distinctively Scottish usage in two demographically distinct Twitter user samples. However, not all effects are consistent between the two groups, underscoring the importance of replicating studies on distinct user samples before drawing strong conclusions from social media data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4909/>Differences in type-token ratio and part-of-speech frequencies in male and female Russian written texts<span class=acl-fixed-case>R</span>ussian written texts</a></strong><br><a href=/people/t/tatiana-litvinova/>Tatiana Litvinova</a>
|
<a href=/people/p/pavel-seredin/>Pavel Seredin</a>
|
<a href=/people/o/olga-litvinova/>Olga Litvinova</a>
|
<a href=/people/o/olga-zagorovskaya/>Olga Zagorovskaya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4909><div class="card-body p-3 small">The differences in the frequencies of some parts of speech (POS), particularly <a href=https://en.wikipedia.org/wiki/Function_word>function words</a>, and <a href=https://en.wikipedia.org/wiki/Lexical_diversity>lexical diversity</a> in male and female speech have been pointed out in a number of papers. The <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> using exclusively context-independent parameters have proved to be highly effective. However, there are still issues that have to be addressed as a lot of studies are performed for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and the genre and topic of texts is sometimes neglected. The aim of this paper is to investigate the association between context-independent parameters of Russian written texts and the gender of their authors and to design predictive re-gression models. A number of correlations were found. The obtained data is in good agreement with the results obtained for other languages. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> based on 5 parameters with the highest <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlation coefficients</a> was designed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4910.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4910 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4910 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4910/>Modeling Communicative Purpose with Functional Style : Corpus and Features for German Genre and Register Analysis<span class=acl-fixed-case>G</span>erman Genre and Register Analysis</a></strong><br><a href=/people/t/thomas-haider/>Thomas Haider</a>
|
<a href=/people/a/alexis-palmer/>Alexis Palmer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4910><div class="card-body p-3 small">While there is wide acknowledgement in NLP of the utility of document characterization by genre, it is quite difficult to determine a definitive set of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> or even a comprehensive list of <a href=https://en.wikipedia.org/wiki/Genre>genres</a>. This paper addresses both issues. First, with <a href=https://en.wikipedia.org/wiki/Prototype_semantics>prototype semantics</a>, we develop a hierarchical taxonomy of discourse functions. We implement the <a href=https://en.wikipedia.org/wiki/Taxonomy_(biology)>taxonomy</a> by developing a new text genre corpus of contemporary German to perform a text based comparative register analysis. Second, we extract a host of style features, both deep and shallow, aiming beyond linguistically motivated features at situational correlates in texts. The feature sets are used for supervised text genre classification, on which our models achieve high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>. The combination of the corpus typology and feature sets allows us to characterize types of communicative purpose in a comparative setup, by qualitative interpretation of style feature loadings of a regularized discriminant analysis. Finally, to determine the dependence of genre on topics (which are arguably the distinguishing factor of sub-genre), we compare and combine our style models with Latent Dirichlet Allocation features across different corpus settings with unstable topics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4911/>Stylistic Variation in Television Dialogue for Natural Language Generation</a></strong><br><a href=/people/g/grace-lin/>Grace Lin</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4911><div class="card-body p-3 small">Conversation is a critical component of <a href=https://en.wikipedia.org/wiki/Storytelling>storytelling</a>, where key information is often revealed by what / how a character says it. We focus on the issue of character voice and build stylistic models with <a href=https://en.wikipedia.org/wiki/Linguistic_description>linguistic features</a> related to <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language generation decisions</a>. Using a dialogue corpus of the television series, The <a href=https://en.wikipedia.org/wiki/The_Big_Bang_Theory>Big Bang Theory</a>, we apply <a href=https://en.wikipedia.org/wiki/Content_analysis>content analysis</a> to extract relevant linguistic features to build character-based stylistic models, and we test the model-fit through an user perceptual experiment with Amazon&#8217;s Mechanical Turk. The results are encouraging in that human subjects tend to perceive the generated utterances as being more similar to the character they are modeled on, than to another random character.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4912/>Controlling Linguistic Style Aspects in Neural Language Generation</a></strong><br><a href=/people/j/jessica-ficler/>Jessica Ficler</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4912><div class="card-body p-3 small">Most work on neural natural language generation (NNLG) focus on controlling the content of the generated text. We experiment with controling several stylistic aspects of the generated text, in addition to its content. The method is based on conditioned RNN language model, where the desired content as well as the stylistic parameters serve as conditioning contexts. We demonstrate the approach on the movie reviews domain and show that it is successful in generating coherent sentences corresponding to the required <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>linguistic style</a> and content.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4913.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4913 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4913 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4913/>Approximating Style by N-gram-based Annotation</a></strong><br><a href=/people/m/melanie-andresen/>Melanie Andresen</a>
|
<a href=/people/h/heike-zinsmeister/>Heike Zinsmeister</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4913><div class="card-body p-3 small">The concept of <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>style</a> is much debated in theoretical as well as empirical terms. From an empirical perspective, the key question is how to operationalize <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>style</a> and thus make it accessible for <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a> and <a href=https://en.wikipedia.org/wiki/Quantification_(science)>quantification</a>. In <a href=https://en.wikipedia.org/wiki/Authorship_attribution>authorship attribution</a>, many different approaches have successfully resolved this issue at the cost of <a href=https://en.wikipedia.org/wiki/Interpretability>linguistic interpretability</a> : The resulting <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> may be able to distinguish one <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language variety</a> from the other, but do not give us much information on their distinctive linguistic properties. We approach the issue of interpreting stylistic features by extracting linear and syntactic n-grams that are distinctive for a <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>language variety</a>. We present a study that exemplifies this process by a comparison of the German academic languages of linguistics and <a href=https://en.wikipedia.org/wiki/German_literature>literary studies</a>. Overall, our findings show that distinctive <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> can be related to linguistic categories. The results suggest that the <a href=https://en.wikipedia.org/wiki/Style_(manner_of_address)>style</a> of German literary studies is characterized by nominal structures and the style of linguistics by <a href=https://en.wikipedia.org/wiki/Linguistic_description>verbal ones</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4914.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4914 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4914 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4914/>Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution</a></strong><br><a href=/people/e/enrique-manjavacas/>Enrique Manjavacas</a>
|
<a href=/people/j/jeroen-de-gussem/>Jeroen De Gussem</a>
|
<a href=/people/w/walter-daelemans/>Walter Daelemans</a>
|
<a href=/people/m/mike-kestemont/>Mike Kestemont</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4914><div class="card-body p-3 small">Recent applications of neural language models have led to an increased interest in the <a href=https://en.wikipedia.org/wiki/Natural-language_generation>automatic generation of natural language</a>. However impressive, the evaluation of neurally generated text has so far remained rather informal and anecdotal. Here, we present an attempt at the systematic assessment of one aspect of the quality of neurally generated text. We focus on a specific aspect of neural language generation : its ability to reproduce authorial writing styles. Using established <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship attribution</a>, we empirically assess the <a href=https://en.wikipedia.org/wiki/Style_(visual_arts)>stylistic qualities</a> of neurally generated text. In comparison to conventional <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>, neural models generate fuzzier text, that is relatively harder to attribute correctly. Nevertheless, our results also suggest that neurally generated text offers more valuable perspectives for the augmentation of training data.</div></div></div><hr><div id=w17-50><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-50.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-50/>Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5000/>Proceedings of the 12th Workshop on Innovative Use of <span class=acl-fixed-case>NLP</span> for Building Educational Applications</a></strong><br><a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/j/jill-burstein/>Jill Burstein</a>
|
<a href=/people/c/claudia-leacock/>Claudia Leacock</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5001 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5001/>Question Difficulty How to Estimate Without Norming, How to Use for Automated Grading</a></strong><br><a href=/people/u/ulrike-pado/>Ulrike Padó</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5001><div class="card-body p-3 small">Question difficulty estimates guide test creation, but are too costly for small-scale testing. We empirically verify that <a href=https://en.wikipedia.org/wiki/Bloom&#8217;s_taxonomy>Bloom&#8217;s Taxonomy</a>, a standard tool for difficulty estimation during question creation, reliably predicts question difficulty observed after testing in a short-answer corpus. We also find that difficulty is mirrored in the amount of variation in student answers, which can be computed before <a href=https://en.wikipedia.org/wiki/Grading_in_education>grading</a>. We show that question difficulty and its approximations are useful for automated grading, allowing us to identify the optimal feature set for <a href=https://en.wikipedia.org/wiki/Grading_in_education>grading</a> each question even in an unseen-question setting.<i>automated grading</i>, allowing us to identify the optimal feature set for grading each question even in an unseen-question setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5002/>Combining CNNs and Pattern Matching for Question Interpretation in a Virtual Patient Dialogue System<span class=acl-fixed-case>CNN</span>s and Pattern Matching for Question Interpretation in a Virtual Patient Dialogue System</a></strong><br><a href=/people/l/lifeng-jin/>Lifeng Jin</a>
|
<a href=/people/m/michael-white/>Michael White</a>
|
<a href=/people/e/evan-jaffe/>Evan Jaffe</a>
|
<a href=/people/l/laura-zimmerman/>Laura Zimmerman</a>
|
<a href=/people/d/douglas-danforth/>Douglas Danforth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5002><div class="card-body p-3 small">For <a href=https://en.wikipedia.org/wiki/Medical_school>medical students</a>, virtual patient dialogue systems can provide useful training opportunities without the cost of employing actors to portray standardized patients. This work utilizes word- and character-based convolutional neural networks (CNNs) for question identification in a virtual patient dialogue system, outperforming a strong word- and character-based logistic regression baseline. While the CNNs perform well given sufficient training data, the best <a href=https://en.wikipedia.org/wiki/System>system</a> performance is ultimately achieved by combining CNNs with a hand-crafted pattern matching system that is robust to label sparsity, providing a 10 % boost in <a href=https://en.wikipedia.org/wiki/System>system accuracy</a> and an error reduction of 47 % as compared to the pattern-matching system alone.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5003 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5003/>Continuous fluency tracking and the challenges of varying text complexity</a></strong><br><a href=/people/b/beata-beigman-klebanov/>Beata Beigman Klebanov</a>
|
<a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/j/john-sabatini/>John Sabatini</a>
|
<a href=/people/t/tenaha-oreilly/>Tenaha O’Reilly</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5003><div class="card-body p-3 small">This paper is a preliminary report on using text complexity measurement in the service of a new <a href=https://en.wikipedia.org/wiki/Educational_software>educational application</a>. We describe a reading intervention where a child takes turns reading a book aloud with a virtual reading partner. Our ultimate goal is to provide meaningful feedback to the parent or the teacher by continuously tracking the child&#8217;s improvement in <a href=https://en.wikipedia.org/wiki/Literacy>reading fluency</a>. We show that this would not be a simple endeavor, due to an intricate relationship between text complexity from the point of view of <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension</a> and <a href=https://en.wikipedia.org/wiki/Reading_rate>reading rate</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5004 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5004/>Auxiliary Objectives for Neural Error Detection Models</a></strong><br><a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/h/helen-yannakoudakis/>Helen Yannakoudakis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5004><div class="card-body p-3 small">We investigate the utility of different auxiliary objectives and training strategies within a neural sequence labeling approach to <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection</a> in learner writing. Auxiliary costs provide the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with additional linguistic information, allowing it to learn general-purpose compositional features that can then be exploited for other objectives. Our experiments show that a joint learning approach trained with parallel labels on in-domain data improves performance over the previous best <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection system</a>. While the resulting <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has the same number of parameters, the additional objectives allow <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to be optimised more efficiently and achieve better performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5005/>Linked Data for Language-Learning Applications</a></strong><br><a href=/people/r/robyn-loughnane/>Robyn Loughnane</a>
|
<a href=/people/k/kate-mccurdy/>Kate McCurdy</a>
|
<a href=/people/p/peter-kolb/>Peter Kolb</a>
|
<a href=/people/s/stefan-selent/>Stefan Selent</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5005><div class="card-body p-3 small">The use of <a href=https://en.wikipedia.org/wiki/Linked_data>linked data</a> within language-learning applications is an open research question. A research prototype is presented that applies linked-data principles to store linguistic annotation generated from language-learning content using a variety of NLP tools. The result is a <a href=https://en.wikipedia.org/wiki/Database>database</a> that links learning content, <a href=https://en.wikipedia.org/wiki/Annotation>linguistic annotation</a> and <a href=https://en.wikipedia.org/wiki/Open-source_software>open-source resources</a>, on top of which a diverse range of tools for language-learning applications can be built.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5006 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5006/>Predicting Specificity in Classroom Discussion</a></strong><br><a href=/people/l/luca-lugini/>Luca Lugini</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5006><div class="card-body p-3 small">High quality classroom discussion is important to student development, enhancing abilities to express claims, reason about other students&#8217; claims, and retain information for longer periods of time. Previous small-scale studies have shown that one indicator of classroom discussion quality is <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>specificity</a>. In this paper we tackle the problem of predicting specificity for classroom discussions. We propose several <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> and feature sets capable of outperforming the state of the art in specificity prediction. Additionally, we provide a set of meaningful, interpretable features that can be used to analyze classroom discussions at a pedagogical level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5007 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5007/>A Report on the 2017 Native Language Identification Shared Task</a></strong><br><a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/k/keelan-evanini/>Keelan Evanini</a>
|
<a href=/people/a/aoife-cahill/>Aoife Cahill</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a>
|
<a href=/people/r/robert-pugh/>Robert Pugh</a>
|
<a href=/people/c/christopher-hamill/>Christopher Hamill</a>
|
<a href=/people/d/diane-napolitano/>Diane Napolitano</a>
|
<a href=/people/y/yao-qian/>Yao Qian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5007><div class="card-body p-3 small">Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language. It is typically framed as a classification task where the set of <a href=https://en.wikipedia.org/wiki/L1_(protein)>L1s</a> is known a priori. Two previous shared tasks on NLI have been organized where the aim was to identify the L1 of learners of <a href=https://en.wikipedia.org/wiki/English_language>English</a> based on essays (2013) and spoken responses (2016) they provided during a standardized assessment of academic English proficiency. The 2017 shared task combines the inputs from the two prior tasks for the first time. There are three tracks : <a href=https://en.wikipedia.org/wiki/Natural_language_understanding>NLI</a> on the essay only, <a href=https://en.wikipedia.org/wiki/Natural_language_understanding>NLI</a> on the spoken response only (based on a transcription of the response and i-vector acoustic features), and <a href=https://en.wikipedia.org/wiki/Natural_language_understanding>NLI</a> using both responses. We believe this makes for a more interesting <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a> while building on the methods and results from the previous two <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared tasks</a>. In this paper, we report the results of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a>. A total of 19 teams competed across the three different sub-tasks. The fusion track showed that combining the written and spoken responses provides a large boost in <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>prediction accuracy</a>. Multiple classifier systems (e.g. ensembles and meta-classifiers) were the most effective in all tasks, with most based on traditional <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> (e.g. SVMs) with lexical / syntactic features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5008 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5008/>Evaluation of Automatically Generated Pronoun Reference Questions</a></strong><br><a href=/people/a/arief-yudha-satria/>Arief Yudha Satria</a>
|
<a href=/people/t/takenobu-tokunaga/>Takenobu Tokunaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5008><div class="card-body p-3 small">This study provides a detailed analysis of evaluation of English pronoun reference questions which are created automatically by machine. Pronoun reference questions are multiple choice questions that ask test takers to choose an antecedent of a target pronoun in a reading passage from four options. The <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> was performed from two perspectives : the perspective of English teachers and that of English learners. Item analysis suggests that machine-generated questions achieve comparable quality with <a href=https://en.wikipedia.org/wiki/Questionnaire>human-made questions</a>. Correlation analysis revealed a strong correlation between the scores of machine-generated questions and that of human-made questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5010 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5010/>Collecting fluency corrections for spoken learner English<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/a/andrew-caines/>Andrew Caines</a>
|
<a href=/people/e/emma-flint/>Emma Flint</a>
|
<a href=/people/p/paula-buttery/>Paula Buttery</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5010><div class="card-body p-3 small">We present crowdsourced collection of <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error annotations</a> for <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcriptions</a> of spoken learner English. Our emphasis in <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> is on fluency corrections, a more complete correction than has traditionally been aimed for in grammatical error correction research (GEC). Fluency corrections require improvements to the text, taking discourse and utterance level semantics into account : the result is a more naturalistic, holistic version of the original. We propose that this shifted emphasis be reflected in a new name for the task : &#8216;holistic error correction&#8217; (HEC). We analyse crowdworker behaviour in <a href=https://en.wikipedia.org/wiki/Higher_Education_Commission_(Pakistan)>HEC</a> and conclude that the method is useful with certain amendments for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5012 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5012/>An Investigation into the Pedagogical Features of Documents</a></strong><br><a href=/people/e/emily-sheng/>Emily Sheng</a>
|
<a href=/people/p/prem-natarajan/>Prem Natarajan</a>
|
<a href=/people/j/jonathan-gordon/>Jonathan Gordon</a>
|
<a href=/people/g/gully-burns/>Gully Burns</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5012><div class="card-body p-3 small">Characterizing the content of a technical document in terms of its learning utility can be useful for applications related to <a href=https://en.wikipedia.org/wiki/Education>education</a>, such as generating reading lists from large collections of documents. We refer to this learning utility as the pedagogical value of the document to the learner. While pedagogical value is an important concept that has been studied extensively within the education domain, there has been little work exploring it from a computational, i.e., natural language processing (NLP), perspective. To allow a computational exploration of this concept, we introduce the notion of pedagogical roles of documents (e.g., <a href=https://en.wikipedia.org/wiki/Tutorial>Tutorial</a> and Survey) as an intermediary component for the study of pedagogical value. Given the lack of available corpora for our exploration, we create the first annotated corpus of pedagogical roles and use it to test baseline techniques for automatic prediction of such <a href=https://en.wikipedia.org/wiki/Role>roles</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5013 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5013/>Combining Multiple Corpora for Readability Assessment for People with Cognitive Disabilities</a></strong><br><a href=/people/v/victoria-yaneva/>Victoria Yaneva</a>
|
<a href=/people/c/constantin-orasan/>Constantin Orăsan</a>
|
<a href=/people/r/richard-evans/>Richard Evans</a>
|
<a href=/people/o/omid-rohanian/>Omid Rohanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5013><div class="card-body p-3 small">Given the lack of large user-evaluated corpora in disability-related NLP research (e.g. text simplification or readability assessment for people with cognitive disabilities), the question of choosing suitable training data for NLP models is not straightforward. The use of large generic corpora may be problematic because such <a href=https://en.wikipedia.org/wiki/Data>data</a> may not reflect the needs of the target population. The use of the available user-evaluated corpora may be problematic because these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are not large enough to be used as training data. In this paper we explore a third approach, in which a large generic corpus is combined with a smaller population-specific corpus to train a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> which is evaluated using two sets of unseen user-evaluated data. One of these <a href=https://en.wikipedia.org/wiki/Set_(mathematics)>sets</a>, the ASD Comprehension corpus, is developed for the purposes of this study and made freely available. We explore the effects of the size and type of the training data used on the performance of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>, and the effects of the type of the unseen test datasets on the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5014 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-5014.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-5014/>Automatic Extraction of High-Quality Example Sentences for Word Learning Using a <a href=https://en.wikipedia.org/wiki/Determinantal_point_process>Determinantal Point Process</a></a></strong><br><a href=/people/a/arseny-tolmachev/>Arseny Tolmachev</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5014><div class="card-body p-3 small">Flashcard systems are effective tools for learning words but have their limitations in teaching word usage. To overcome this problem, we propose a novel flashcard system that shows a new example sentence on each repetition. This <a href=https://en.wikipedia.org/wiki/Extension_(semantics)>extension</a> requires high-quality example sentences, automatically extracted from a huge corpus. To do this, we use a <a href=https://en.wikipedia.org/wiki/Determinantal_point_process>Determinantal Point Process</a> which scales well to large data and allows to naturally represent sentence similarity and quality as <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>. Our human evaluation experiment on <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese language</a> indicates that the proposed method successfully extracted high-quality example sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5016 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5016/>An Error-Oriented Approach to Word Embedding Pre-Training</a></strong><br><a href=/people/y/youmna-farag/>Youmna Farag</a>
|
<a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5016><div class="card-body p-3 small">We propose a novel word embedding pre-training approach that exploits writing errors in learners&#8217; scripts. We compare our method to previous models that tune the embeddings based on script scores and the discrimination between correct and corrupt word contexts in addition to the generic commonly-used embeddings pre-trained on large corpora. The comparison is achieved by using the aforementioned models to bootstrap a <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> that learns to predict a holistic score for <a href=https://en.wikipedia.org/wiki/Scripting_language>scripts</a>. Furthermore, we investigate augmenting our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error corrections</a> and monitor the impact on performance. Our results show that our error-oriented approach outperforms other comparable ones which is further demonstrated when training on more data. Additionally, extending the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> with corrections provides further performance gains when data sparsity is an issue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5017 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5017/>Investigating neural architectures for short answer scoring</a></strong><br><a href=/people/b/brian-riordan/>Brian Riordan</a>
|
<a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/a/aoife-cahill/>Aoife Cahill</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a>
|
<a href=/people/c/chungmin-lee/>Chong Min Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5017><div class="card-body p-3 small">Neural approaches to <a href=https://en.wikipedia.org/wiki/Automated_essay_scoring>automated essay scoring</a> have recently shown state-of-the-art performance. The automated essay scoring task typically involves a broad notion of writing quality that encompasses <a href=https://en.wikipedia.org/wiki/Content_(media)>content</a>, <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a>, <a href=https://en.wikipedia.org/wiki/Organization>organization</a>, and <a href=https://en.wikipedia.org/wiki/Convention_(norm)>conventions</a>. This differs from the short answer content scoring task, which focuses on content accuracy. The inputs to neural essay scoring models ngrams and embeddings are arguably well-suited to evaluate content in short answer scoring tasks. We investigate how several basic neural approaches similar to those used for automated essay scoring perform on short answer scoring. We show that neural architectures can outperform a strong non-neural baseline, but performance and optimal parameter settings vary across the more diverse types of prompts typical of short answer scoring.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5018 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5018/>Human and Automated CEFR-based Grading of Short Answers<span class=acl-fixed-case>CEFR</span>-based Grading of Short Answers</a></strong><br><a href=/people/a/anais-tack/>Anaïs Tack</a>
|
<a href=/people/t/thomas-francois/>Thomas François</a>
|
<a href=/people/s/sophie-roekhaut/>Sophie Roekhaut</a>
|
<a href=/people/c/cedrick-fairon/>Cédrick Fairon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5018><div class="card-body p-3 small">This paper is concerned with the task of automatically assessing the written proficiency level of non-native (L2) learners of <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Drawing on previous research on automated L2 writing assessment following the Common European Framework of Reference for Languages (CEFR), we investigate the possibilities and difficulties of deriving the CEFR level from short answers to open-ended questions, which has not yet been subjected to numerous studies up to date. The object of our study is twofold : to examine the intricacy involved with both human and automated CEFR-based grading of short answers. On the one hand, we describe the compilation of a learner corpus of short answers graded with CEFR levels by three certified Cambridge examiners. We mainly observe that, although the shortness of the answers is reported as undermining a clear-cut evaluation, the length of the answer does not necessarily correlate with inter-examiner disagreement. On the other hand, we explore the development of a soft-voting system for the automated CEFR-based grading of short answers and draw tentative conclusions about its use in a computer-assisted testing (CAT) setting.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5019 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5019/>GEC into the future : Where are we going and how do we get there?<span class=acl-fixed-case>GEC</span> into the future: Where are we going and how do we get there?</a></strong><br><a href=/people/k/keisuke-sakaguchi/>Keisuke Sakaguchi</a>
|
<a href=/people/c/courtney-napoles/>Courtney Napoles</a>
|
<a href=/people/j/joel-tetreault/>Joel Tetreault</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5019><div class="card-body p-3 small">The field of grammatical error correction (GEC) has made tremendous bounds in the last ten years, but new questions and obstacles are revealing themselves. In this position paper, we discuss the issues that need to be addressed and provide recommendations for the field to continue to make progress, and propose a new shared task. We invite suggestions and critiques from the audience to make the new shared task a community-driven venture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5020 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5020/>Detecting Off-topic Responses to Visual Prompts</a></strong><br><a href=/people/m/marek-rei/>Marek Rei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5020><div class="card-body p-3 small">Automated methods for essay scoring have made great progress in recent years, achieving accuracies very close to human annotators. However, a known weakness of such automated scorers is not taking into account the semantic relevance of the submitted text. While there is existing work on detecting answer relevance given a textual prompt, very little previous research has been done to incorporate visual writing prompts. We propose a neural architecture and several extensions for detecting off-topic responses to visual prompts and evaluate it on a dataset of texts written by <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learners</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5021 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5021/>Combining Textual and Speech Features in the NLI Task Using State-of-the-Art Machine Learning Techniques<span class=acl-fixed-case>NLI</span> Task Using State-of-the-Art Machine Learning Techniques</a></strong><br><a href=/people/p/pavel-ircing/>Pavel Ircing</a>
|
<a href=/people/j/jan-svec/>Jan Švec</a>
|
<a href=/people/z/zbynek-zajic/>Zbyněk Zajíc</a>
|
<a href=/people/b/barbora-hladka/>Barbora Hladká</a>
|
<a href=/people/m/martin-holub/>Martin Holub</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5021><div class="card-body p-3 small">We summarize the involvement of our CEMI team in the NLI Shared Task 2017, which deals with both textual and speech input data. We submitted the results achieved by using three different system architectures ; each of them combines multiple <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning models</a> trained on various <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature sets</a>. As expected, better results are achieved with the <a href=https://en.wikipedia.org/wiki/System>systems</a> that use both the <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual data</a> and the <a href=https://en.wikipedia.org/wiki/Speech>spoken responses</a>. Combining the input data of two different modalities led to a rather dramatic improvement in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance. Our best performing method is based on a set of feed-forward neural networks whose hidden-layer outputs are combined together using a softmax layer. We achieved a macro-averaged F1 score of 0.9257 on the evaluation (unseen) test set and our team placed first in the main task together with other three teams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5022 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-5022.Attachment.pdf data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-5022/>Native Language Identification Using a Mixture of Character and Word N-grams</a></strong><br><a href=/people/e/elham-mohammadi/>Elham Mohammadi</a>
|
<a href=/people/h/hadi-veisi/>Hadi Veisi</a>
|
<a href=/people/h/hessam-amini/>Hessam Amini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5022><div class="card-body p-3 small">Native language identification (NLI) is the task of determining an author&#8217;s native language, based on a piece of his / her writing in a second language. In recent years, NLI has received much attention due to its challenging nature and its applications in <a href=https://en.wikipedia.org/wiki/Language_pedagogy>language pedagogy</a> and <a href=https://en.wikipedia.org/wiki/Forensic_linguistics>forensic linguistics</a>. We participated in the NLI2017 shared task under the name UT-DSP. In our effort to implement a method for <a href=https://en.wikipedia.org/wiki/Native-language_identification>native language identification</a>, we made use of a fusion of character and word N-grams, and achieved an optimal <a href=https://en.wikipedia.org/wiki/F-number>F1-Score</a> of 77.64 %, using both essay and speech transcription datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5024 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5024/>Can string kernels pass the test of time in Native Language Identification?</a></strong><br><a href=/people/r/radu-tudor-ionescu/>Radu Tudor Ionescu</a>
|
<a href=/people/m/marius-popescu/>Marius Popescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5024><div class="card-body p-3 small">We describe a machine learning approach for the 2017 shared task on Native Language Identification (NLI). The proposed approach combines several <a href=https://en.wikipedia.org/wiki/Kernel_(operating_system)>kernels</a> using <a href=https://en.wikipedia.org/wiki/Multiple_kernel_learning>multiple kernel learning</a>. While most of our kernels are based on character p-grams (also known as n-grams) extracted from essays or speech transcripts, we also use a kernel based on i-vectors, a low-dimensional representation of audio recordings, provided by the shared task organizers. For the learning stage, we choose Kernel Discriminant Analysis (KDA) over Kernel Ridge Regression (KRR), because the former <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> obtains better results than the latter one on the development set. In our previous work, we have used a similar machine learning approach to achieve state-of-the-art NLI results. The goal of this paper is to demonstrate that our shallow and simple approach based on string kernels (with minor improvements) can pass the test of time and reach state-of-the-art performance in the 2017 NLI shared task, despite the recent advances in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. We participated in all three tracks, in which the competitors were allowed to use only the essays (essay track), only the speech transcripts (speech track), or both (fusion track). Using only the data provided by the organizers for training our <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, we have reached a macro F1 score of 86.95 % in the closed essay track, a macro F1 score of 87.55 % in the closed speech track, and a macro F1 score of 93.19 % in the closed fusion track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5025 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5025/>Neural Networks and Spelling Features for Native Language Identification</a></strong><br><a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/g/gintare-grigonyte/>Gintarė Grigonytė</a>
|
<a href=/people/r/robert-ostling/>Robert Östling</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5025><div class="card-body p-3 small">We present the RUG-SU team&#8217;s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a> using word representations, a deep residual network using word and character features, and a system based on a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>. Our best <a href=https://en.wikipedia.org/wiki/System>system</a> is an ensemble of <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>, reaching an F1 score of 0.8323. Although our <a href=https://en.wikipedia.org/wiki/System>system</a> is not the highest ranking one, we do outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> by far.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5026 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5026/>A study of N-gram and Embedding Representations for Native Language Identification</a></strong><br><a href=/people/s/sowmya-vajjala/>Sowmya Vajjala</a>
|
<a href=/people/s/sagnik-banerjee/>Sagnik Banerjee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5026><div class="card-body p-3 small">We report on our experiments with N-gram and embedding based feature representations for Native Language Identification (NLI) as a part of the NLI Shared Task 2017 (team name : NLI-ISU). Our best performing system on the test set for written essays had a macro F1 of 0.8264 and was based on word uni, bi and trigram features. We explored <a href=https://en.wikipedia.org/wiki/N-gram>n-grams</a> covering word, character, POS and word-POS mixed representations for this task. For embedding based feature representations, we employed both word and document embeddings. We had a relatively poor performance with all embedding representations compared to n-grams, which could be because of the fact that embeddings capture semantic similarities whereas L1 differences are more stylistic in nature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5029 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5029/>Structured Generation of Technical Reading Lists</a></strong><br><a href=/people/j/jonathan-gordon/>Jonathan Gordon</a>
|
<a href=/people/s/stephen-aguilar/>Stephen Aguilar</a>
|
<a href=/people/e/emily-sheng/>Emily Sheng</a>
|
<a href=/people/g/gully-burns/>Gully Burns</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5029><div class="card-body p-3 small">Learners need to find suitable documents to read and prioritize them in an appropriate order. We present a method of automatically generating reading lists, selecting documents based on their pedagogical value to the learner and ordering them using the structure of concepts in the domain. Resulting reading lists related to <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational linguistics</a> were evaluated by advanced learners and judged to be near the quality of those generated by domain experts. We provide an open-source implementation of our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> to enable future work on reading list generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5030 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5030/>Effects of Lexical Properties on Viewing Time per Word in Autistic and Neurotypical Readers</a></strong><br><a href=/people/s/sanja-stajner/>Sanja Štajner</a>
|
<a href=/people/v/victoria-yaneva/>Victoria Yaneva</a>
|
<a href=/people/r/ruslan-mitkov/>Ruslan Mitkov</a>
|
<a href=/people/s/simone-paolo-ponzetto/>Simone Paolo Ponzetto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5030><div class="card-body p-3 small">Eye tracking studies from the past few decades have shaped the way we think of word complexity and <a href=https://en.wikipedia.org/wiki/Cognitive_load>cognitive load</a> : words that are long, rare and ambiguous are more difficult to read. However, online processing techniques have been scarcely applied to investigating the reading difficulties of people with autism and what vocabulary is challenging for them. We present parallel gaze data obtained from adult readers with autism and a control group of neurotypical readers and show that the former required higher cognitive effort to comprehend the texts as evidenced by three gaze-based measures. We divide all words into four classes based on their viewing times for both groups and investigate the relationship between longer viewing times and word length, word frequency, and four cognitively-based measures (word concreteness, familiarity, age of acquisition and imagability).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5031 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5031/>Transparent text quality assessment with <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a></a></strong><br><a href=/people/r/robert-ostling/>Robert Östling</a>
|
<a href=/people/g/gintare-grigonyte/>Gintare Grigonyte</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5031><div class="card-body p-3 small">We present a very simple model for text quality assessment based on a <a href=https://en.wikipedia.org/wiki/Deep_convolutional_neural_network>deep convolutional neural network</a>, where the only supervision required is one corpus of user-generated text of varying quality, and one contrasting text corpus of consistently high quality. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is able to provide local quality assessments in different parts of a text, which allows visual feedback about where potentially problematic parts of the text are located, as well as a way to evaluate which textual features are captured by our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We evaluate our method on two corpora : a large corpus of manually graded student essays and a longitudinal corpus of language learner written production, and find that the text quality metric learned by our model is a fairly strong predictor of both essay grade and learner proficiency level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5032 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5032/>Artificial Error Generation with Machine Translation and Syntactic Patterns</a></strong><br><a href=/people/m/marek-rei/>Marek Rei</a>
|
<a href=/people/m/mariano-felice/>Mariano Felice</a>
|
<a href=/people/z/zheng-yuan/>Zheng Yuan</a>
|
<a href=/people/t/ted-briscoe/>Ted Briscoe</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5032><div class="card-body p-3 small">Shortage of available training data is holding back progress in the area of automated error detection. This paper investigates two alternative methods for artificially generating writing errors, in order to create additional resources. We propose treating error generation as a machine translation task, where grammatically correct text is translated to contain errors. In addition, we explore a system for extracting textual patterns from an annotated corpus, which can then be used to insert <a href=https://en.wikipedia.org/wiki/Error_(linguistics)>errors</a> into grammatically correct sentences. Our experiments show that the inclusion of <a href=https://en.wikipedia.org/wiki/Errors-in-variables_models>artificially generated errors</a> significantly improves <a href=https://en.wikipedia.org/wiki/Error_detection_and_correction>error detection accuracy</a> on both FCE and CoNLL 2014 datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5033 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5033/>Modelling semantic acquisition in second language learning</a></strong><br><a href=/people/e/ekaterina-kochmar/>Ekaterina Kochmar</a>
|
<a href=/people/e/ekaterina-shutova/>Ekaterina Shutova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5033><div class="card-body p-3 small">Using methods of <a href=https://en.wikipedia.org/wiki/Statistical_inference>statistical analysis</a>, we investigate how semantic knowledge is acquired in <a href=https://en.wikipedia.org/wiki/English_language>English</a> as a second language and evaluate the pace of development across a number of predicate types and content word combinations, as well as across the levels of language proficiency and native languages. Our exploratory study helps identify the most problematic areas for <a href=https://en.wikipedia.org/wiki/Language_acquisition>language learners</a> with different backgrounds and at different stages of learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5034 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-5034.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-5034/>Multiple Choice Question Generation Utilizing An Ontology</a></strong><br><a href=/people/k/katherine-stasaski/>Katherine Stasaski</a>
|
<a href=/people/m/marti-a-hearst/>Marti A. Hearst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5034><div class="card-body p-3 small">Ontologies provide a structured representation of concepts and the relationships which connect them. This work investigates how a pre-existing educational Biology ontology can be used to generate useful practice questions for students by using the connectivity structure in a novel way. It also introduces a novel way to generate multiple-choice distractors from the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a>, and compares this to a baseline of using embedding representations of nodes. An assessment by an experienced science teacher shows a significant advantage over a baseline when using the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> for distractor generation. A subsequent study with three science teachers on the results of a modified question generation algorithm finds significant improvements. An in-depth analysis of the teachers&#8217; comments yields useful insights for any researcher working on automated question generation for <a href=https://en.wikipedia.org/wiki/Educational_technology>educational applications</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5035 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5035/>Simplifying metaphorical language for young readers : A corpus study on news text</a></strong><br><a href=/people/m/magdalena-wolska/>Magdalena Wolska</a>
|
<a href=/people/y/yulia-clausen/>Yulia Clausen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5035><div class="card-body p-3 small">The paper presents first results of an ongoing project on <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> focusing on <a href=https://en.wikipedia.org/wiki/Metaphor>linguistic metaphors</a>. Based on an analysis of a parallel corpus of news text professionally simplified for different grade levels, we identify six types of simplification choices falling into two broad categories : preserving metaphors or dropping them. An annotation study on almost 300 source sentences with <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> (grade level 12) and their simplified counterparts (grade 4) is conducted. The results show that most <a href=https://en.wikipedia.org/wiki/Metaphor>metaphors</a> are preserved and when they are dropped, the semantic content tends to be preserved rather than dropped, however, it is reworded without metaphorical language. In general, some of the expected tendencies in <a href=https://en.wikipedia.org/wiki/Complexity_reduction>complexity reduction</a>, measured with psycholinguistic variables linked to metaphor comprehension, are observed, suggesting good prospect for machine learning-based metaphor simplification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5037 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5037/>Connecting the Dots : Towards Human-Level Grammatical Error Correction</a></strong><br><a href=/people/s/shamil-chollampatt/>Shamil Chollampatt</a>
|
<a href=/people/h/hwee-tou-ng/>Hwee Tou Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5037><div class="card-body p-3 small">We build a grammatical error correction (GEC) system primarily based on the state-of-the-art statistical machine translation (SMT) approach, using task-specific features and tuning, and further enhance it with the modeling power of neural network joint models. The SMT-based system is weak in generalizing beyond patterns seen during training and lacks granularity below the word level. To address this issue, we incorporate a character-level SMT component targeting the misspelled words that the original SMT-based system fails to correct. Our final <a href=https://en.wikipedia.org/wiki/System>system</a> achieves 53.14 % F 0.5 score on the benchmark CoNLL-2014 test set, an improvement of 3.62 % F 0.5 over the best previous published score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5038 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5038/>Question Generation for <a href=https://en.wikipedia.org/wiki/Language_acquisition>Language Learning</a> : From ensuring texts are read to supporting learning</a></strong><br><a href=/people/m/maria-chinkina/>Maria Chinkina</a>
|
<a href=/people/d/detmar-meurers/>Detmar Meurers</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5038><div class="card-body p-3 small">In Foreign Language Teaching and Learning (FLTL), questions are systematically used to assess the learner&#8217;s understanding of a text. Computational linguistic approaches have been developed to generate such <a href=https://en.wikipedia.org/wiki/Question>questions</a> automatically given a text (e.g., Heilman, 2011). In this paper, we want to broaden the perspective on the different functions questions can play in FLTL and discuss how automatic question generation can support the different uses. Complementing the focus on meaning and comprehension, we want to highlight the fact that questions can also be used to make learners notice form aspects of the <a href=https://en.wikipedia.org/wiki/Linguistic_system>linguistic system</a> and their interpretation. Automatically generating questions that target linguistic forms and grammatical categories in a text in essence supports incidental focus-on-form (Loewen, 2005) in a meaning-focused reading task. We discuss two types of questions serving this purpose, how they can be generated automatically ; and we report on a crowd-sourcing evaluation comparing automatically generated to manually written questions targeting particle verbs, a challenging linguistic form for learners of <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5039 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5039" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5039/>Systematically Adapting <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> for Grammatical Error Correction</a></strong><br><a href=/people/c/courtney-napoles/>Courtney Napoles</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5039><div class="card-body p-3 small">n this work we adapt machine translation (MT) to grammatical error correction, identifying how components of the statistical MT pipeline can be modified for this task and analyzing how each modification impacts system performance. We evaluate the contribution of each of these components with standard evaluation metrics and automatically characterize the morphological and lexical transformations made in system output. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> rivals the current state of the art using a fraction of the training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5040 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5040/>Fine-grained essay scoring of a complex writing task for native speakers</a></strong><br><a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/d/dirk-scholten-akoun/>Dirk Scholten-Akoun</a>
|
<a href=/people/y/yuning-ding/>Yuning Ding</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5040><div class="card-body p-3 small">Automatic essay scoring is nowadays successfully used even in high-stakes tests, but this is mainly limited to holistic scoring of learner essays. We present a new dataset of <a href=https://en.wikipedia.org/wiki/Essay>essays</a> written by highly proficient German native speakers that is scored using a fine-grained rubric with the goal to provide detailed feedback. Our experiments with two state-of-the-art scoring systems (a neural and a SVM-based one) show a large drop in performance compared to existing datasets. This demonstrates the need for such <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that allow to guide research on more elaborate essay scoring methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5042 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5042/>CIC-FBK Approach to Native Language Identification<span class=acl-fixed-case>CIC</span>-<span class=acl-fixed-case>FBK</span> Approach to Native Language Identification</a></strong><br><a href=/people/i/ilia-markov/>Ilia Markov</a>
|
<a href=/people/l/lingzhen-chen/>Lingzhen Chen</a>
|
<a href=/people/c/carlo-strapparava/>Carlo Strapparava</a>
|
<a href=/people/g/grigori-sidorov/>Grigori Sidorov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5042><div class="card-body p-3 small">We present the CIC-FBK system, which took part in the Native Language Identification (NLI) Shared Task 2017. Our approach combines <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> commonly used in previous NLI research, i.e., word n-grams, lemma n-grams, part-of-speech n-grams, and function words, with recently introduced character n-grams from misspelled words, and <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> that are novel in this task, such as typed character n-grams, and syntactic n-grams of words and of syntactic relation tags. We use log-entropy weighting scheme and perform <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> using the Support Vector Machines (SVM) algorithm. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieved 0.8808 macro-averaged F1-score and shared the 1st rank in the NLI Shared Task 2017 scoring.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5043 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5043/>The Power of Character N-grams in Native Language Identification</a></strong><br><a href=/people/a/artur-kulmizev/>Artur Kulmizev</a>
|
<a href=/people/b/bo-blankers/>Bo Blankers</a>
|
<a href=/people/j/johannes-bjerva/>Johannes Bjerva</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a>
|
<a href=/people/g/gertjan-van-noord/>Gertjan van Noord</a>
|
<a href=/people/b/barbara-plank/>Barbara Plank</a>
|
<a href=/people/m/martijn-wieling/>Martijn Wieling</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5043><div class="card-body p-3 small">In this paper, we explore the performance of a linear SVM trained on language independent character features for the NLI Shared Task 2017. Our basic <a href=https://en.wikipedia.org/wiki/System>system</a> (GRONINGEN) achieves the best performance (87.56 F1-score) on the evaluation set using only 1-9 character n-grams as <a href=https://en.wikipedia.org/wiki/Feature_(computer_vision)>features</a>. We compare this against several ensemble and meta-classifiers in order to examine how the <a href=https://en.wikipedia.org/wiki/Linear_system>linear system</a> fares when combined with other, especially non-linear classifiers. Special emphasis is placed on the topic bias that exists by virtue of the assessment essay prompt distribution.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5044 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5044/>Classifier Stacking for Native Language Identification</a></strong><br><a href=/people/w/wen-li/>Wen Li</a>
|
<a href=/people/l/liang-zou/>Liang Zou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5044><div class="card-body p-3 small">This paper reports our contribution (team WLZ) to the NLI Shared Task 2017 (essay track). We first extract lexical and syntactic features from the essays, perform feature weighting and selection, and train linear support vector machine (SVM) classifiers each on an individual feature type. The output of base classifiers, as probabilities for each class, are then fed into a <a href=https://en.wikipedia.org/wiki/Multilayer_perceptron>multilayer perceptron</a> to predict the native language of the author. We also report the performance of each feature type, as well as the best <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> of a type. Our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 86.55 %, which is among the best performing <a href=https://en.wikipedia.org/wiki/System>systems</a> of this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>shared task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5045 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5045/>Native Language Identification on Text and Speech</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/a/alina-maria-ciobanu/>Alina Maria Ciobanu</a>
|
<a href=/people/l/liviu-p-dinu/>Liviu P. Dinu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5045><div class="card-body p-3 small">This paper presents an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble system</a> combining the output of multiple SVM classifiers to native language identification (NLI). The system was submitted to the NLI Shared Task 2017 fusion track which featured students essays and spoken responses in form of <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>audio transcriptions</a> and iVectors by non-native English speakers of eleven native languages. Our system competed in the challenge under the team name ZCD and was based on an ensemble of SVM classifiers trained on <a href=https://en.wikipedia.org/wiki/Character_(computing)>character n-grams</a> achieving 83.58 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and ranking 3rd in the shared task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5047 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5047/>A deep-learning based native-language classification by using a <a href=https://en.wikipedia.org/wiki/Latent_semantic_analysis>latent semantic analysis</a> for the NLI Shared Task 2017<span class=acl-fixed-case>NLI</span> Shared Task 2017</a></strong><br><a href=/people/y/yoo-rhee-oh/>Yoo Rhee Oh</a>
|
<a href=/people/h/hyung-bae-jeon/>Hyung-Bae Jeon</a>
|
<a href=/people/h/hwa-jeon-song/>Hwa Jeon Song</a>
|
<a href=/people/y/yun-kyung-lee/>Yun-Kyung Lee</a>
|
<a href=/people/j/jeon-gue-park/>Jeon-Gue Park</a>
|
<a href=/people/y/yun-keun-lee/>Yun-Keun Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5047><div class="card-body p-3 small">This paper proposes a deep-learning based native-language identification (NLI) using a latent semantic analysis (LSA) as a participant (ETRI-SLP) of the NLI Shared Task 2017 where the NLI Shared Task 2017 aims to detect the native language of an essay or speech response of a standardized assessment of English proficiency for academic purposes. To this end, we use the six unit forms of a text data such as character 4/5/6-grams and word 1/2/3-grams. For each unit form of text data, we convert it into a count-based vector, extract a 2000-rank LSA feature, and perform a linear discriminant analysis (LDA) based dimension reduction. From the count-based vector or the LSA-LDA feature, we also obtain the output prediction values of a support vector machine (SVM) based classifier, the output prediction values of a deep neural network (DNN) based classifier, and the bottleneck values of a DNN based classifier. In order to incorporate the various kinds of text-based features and a speech-based i-vector feature, we design two DNN based ensemble classifiers for late fusion and early fusion, respectively. From the NLI experiments, the F1 (macro) scores are obtained as 0.8601, 0.8664, and 0.9220 for the essay track, the speech track, and the fusion track, respectively. The proposed method has comparable performance to the top-ranked teams for the speech and fusion tracks, although it has slightly lower performance for the essay track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5048 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5048/>Fusion of Simple Models for Native Language Identification</a></strong><br><a href=/people/f/fabio-kepler/>Fabio Kepler</a>
|
<a href=/people/r/ramon-fernandez-astudillo/>Ramon F. Astudillo</a>
|
<a href=/people/a/alberto-abad/>Alberto Abad</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5048><div class="card-body p-3 small">In this paper we describe the approaches we explored for the 2017 Native Language Identification shared task. We focused on simple word and sub-word units avoiding heavy use of hand-crafted features. Following recent trends, we explored linear and neural networks models to attempt to compensate for the lack of rich feature use. Initial efforts yielded <a href=https://en.wikipedia.org/wiki/F-number>f1-scores</a> of 82.39 % and 83.77 % in the development and test sets of the <a href=https://en.wikipedia.org/wiki/Fusion_power>fusion track</a>, and were officially submitted to the task as team L2F. After the task was closed, we carried on further experiments and relied on a late fusion strategy for combining our simple proposed approaches with modifications of the baselines provided by the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. As expected, the i-vectors based sub-system dominates the performance of the system combinations, and results in the major contributor to our achieved scores. Our best combined <a href=https://en.wikipedia.org/wiki/System>system</a> achieves 90.1 % and 90.2 % <a href=https://en.wikipedia.org/wiki/F-number>f1-score</a> in the development and test sets of the <a href=https://en.wikipedia.org/wiki/Fusion_energy>fusion track</a>, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5050 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5050/>Using Gaze to Predict Text Readability</a></strong><br><a href=/people/a/ana-valeria-gonzalez-garduno/>Ana Valeria González-Garduño</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5050><div class="card-body p-3 small">We show that text readability prediction improves significantly from hard parameter sharing with models predicting first pass duration, total fixation duration and regression duration. Specifically, we induce multi-task Multilayer Perceptrons and Logistic Regression models over sentence representations that capture various <a href=https://en.wikipedia.org/wiki/Aggregate_statistics>aggregate statistics</a>, from two different text readability corpora for English, as well as the Dundee eye-tracking corpus. Our approach leads to significant improvements over Single task learning and over previous <a href=https://en.wikipedia.org/wiki/System>systems</a>. In addition, our improvements are consistent across <a href=https://en.wikipedia.org/wiki/Sample_size_determination>train sample sizes</a>, making our approach especially applicable to <a href=https://en.wikipedia.org/wiki/Sample_size_determination>small datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5051 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5051/>Annotating Orthographic Target Hypotheses in a German L1 Learner Corpus<span class=acl-fixed-case>G</span>erman <span class=acl-fixed-case>L</span>1 Learner Corpus</a></strong><br><a href=/people/r/ronja-laarmann-quante/>Ronja Laarmann-Quante</a>
|
<a href=/people/k/katrin-ortmann/>Katrin Ortmann</a>
|
<a href=/people/a/anna-ehlert/>Anna Ehlert</a>
|
<a href=/people/m/maurice-vogel/>Maurice Vogel</a>
|
<a href=/people/s/stefanie-dipper/>Stefanie Dipper</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5051><div class="card-body p-3 small">NLP applications for learners often rely on annotated learner corpora. Thereby, it is important that the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> are both meaningful for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, and consistent and reliable. We present a new longitudinal L1 learner corpus for <a href=https://en.wikipedia.org/wiki/German_language>German</a> (handwritten texts collected in grade 24), which is transcribed and annotated with a target hypothesis that strictly only corrects orthographic errors, and is thereby tailored to research and tool development for orthographic issues in <a href=https://en.wikipedia.org/wiki/Primary_school>primary school</a>. While for most <a href=https://en.wikipedia.org/wiki/Corpus_linguistics>corpora</a>, <a href=https://en.wikipedia.org/wiki/Transcription_(biology)>transcription</a> and target hypothesis are not evaluated, we conducted a detailed inter-annotator agreement study for both tasks. Although we achieved high agreement, our discussion of cases of disagreement shows that even with detailed guidelines, annotators differ here and there for different reasons, which should also be considered when working with transcriptions and target hypotheses of other corpora, especially if no explicit guidelines for their construction are known.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5052 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5052/>A Large Scale Quantitative Exploration of Modeling Strategies for Content Scoring</a></strong><br><a href=/people/n/nitin-madnani/>Nitin Madnani</a>
|
<a href=/people/a/anastassia-loukina/>Anastassia Loukina</a>
|
<a href=/people/a/aoife-cahill/>Aoife Cahill</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5052><div class="card-body p-3 small">We explore various supervised learning strategies for automated scoring of content knowledge for a large corpus of 130 different content-based questions spanning four subject areas (Science, Math, English Language Arts, and Social Studies) and containing over 230,000 responses scored by human raters. Based on our analyses, we provide specific recommendations for content scoring. These are based on patterns observed across multiple questions and assessments and are, therefore, likely to generalize to other scenarios and prove useful to the community as automated content scoring becomes more popular in schools and classrooms.</div></div></div><hr><div id=w17-51><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-51.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-51/>Proceedings of the 4th Workshop on Argument Mining</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5100/>Proceedings of the 4th Workshop on Argument Mining</a></strong><br><a href=/people/i/ivan-habernal/>Ivan Habernal</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a>
|
<a href=/people/k/kevin-d-ashley/>Kevin Ashley</a>
|
<a href=/people/c/claire-cardie/>Claire Cardie</a>
|
<a href=/people/n/nancy-green/>Nancy Green</a>
|
<a href=/people/d/diane-litman/>Diane Litman</a>
|
<a href=/people/g/georgios-petasis/>Georgios Petasis</a>
|
<a href=/people/c/chris-reed/>Chris Reed</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a>
|
<a href=/people/v/vern-walker/>Vern Walker</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5101 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5101/>200K+ Crowdsourced Political Arguments for a New Chilean Constitution<span class=acl-fixed-case>K</span>+ Crowdsourced Political Arguments for a New <span class=acl-fixed-case>C</span>hilean Constitution</a></strong><br><a href=/people/c/constanza-fierro/>Constanza Fierro</a>
|
<a href=/people/c/claudio-fuentes/>Claudio Fuentes</a>
|
<a href=/people/j/jorge-perez/>Jorge Pérez</a>
|
<a href=/people/m/mauricio-quezada/>Mauricio Quezada</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5101><div class="card-body p-3 small">In this paper we present the dataset of 200,000 + political arguments produced in the local phase of the 2016 Chilean constitutional process. We describe the human processing of this <a href=https://en.wikipedia.org/wiki/Data>data</a> by the government officials, and the manual tagging of arguments performed by members of our research group. Afterwards we focus on classification tasks that mimic the human processes, comparing <a href=https://en.wikipedia.org/wiki/Linear_map>linear methods</a> with neural network architectures. The experiments show that some of the <a href=https://en.wikipedia.org/wiki/Manual_labour>manual tasks</a> are suitable for <a href=https://en.wikipedia.org/wiki/Automation>automatization</a>. In particular, the best methods achieve a 90 % top-5 accuracy in a multi-class classification of arguments, and 65 % macro-averaged F1-score for tagging arguments according to a three-part argumentation model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5102 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5102/>Analyzing the Semantic Types of Claims and Premises in an Online Persuasive Forum</a></strong><br><a href=/people/c/christopher-hidey/>Christopher Hidey</a>
|
<a href=/people/e/elena-musi/>Elena Musi</a>
|
<a href=/people/a/alyssa-hwang/>Alyssa Hwang</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a>
|
<a href=/people/k/kathleen-mckeown/>Kathy McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5102><div class="card-body p-3 small">Argumentative text has been analyzed both theoretically and computationally in terms of argumentative structure that consists of argument components (e.g., claims, premises) and their argumentative relations (e.g., support, attack). Less emphasis has been placed on analyzing the semantic types of <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>argument components</a>. We propose a two-tiered annotation scheme to label claims and premises and their semantic types in an online persuasive forum, Change My View, with the long-term goal of understanding what makes a message persuasive. Premises are annotated with the three types of persuasive modes : <a href=https://en.wikipedia.org/wiki/Ethos>ethos</a>, <a href=https://en.wikipedia.org/wiki/Logos>logos</a>, <a href=https://en.wikipedia.org/wiki/Pathos>pathos</a>, while claims are labeled as interpretation, evaluation, agreement, or disagreement, the latter two designed to account for the dialogical nature of our corpus. We aim to answer three questions : 1) can humans reliably annotate the semantic types of argument components? 2) are types of premises / claims positioned in recurrent orders? and 3) are certain types of claims and/or premises more likely to appear in persuasive messages than in non-persuasive messages?</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5103 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5103/>Annotation of argument structure in Japanese legal documents<span class=acl-fixed-case>J</span>apanese legal documents</a></strong><br><a href=/people/h/hiroaki-yamada/>Hiroaki Yamada</a>
|
<a href=/people/s/simone-teufel/>Simone Teufel</a>
|
<a href=/people/t/takenobu-tokunaga/>Takenobu Tokunaga</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5103><div class="card-body p-3 small">We propose a method for the annotation of Japanese civil judgment documents, with the purpose of creating flexible summaries of these. The first step, described in the current paper, concerns content selection, i.e., the question of which material should be extracted initially for the summary. In particular, we utilize the hierarchical argument structure of the <a href=https://en.wikipedia.org/wiki/Judgment_(law)>judgment documents</a>. Our main contributions are a) the design of an annotation scheme that stresses the connection between legal points (called issue topics) and argument structure, b) an adaptation of rhetorical status to suit the Japanese legal system and c) the definition of a linked argument structure based on legal sub-arguments. In this paper, we report agreement between two annotators on several aspects of the overall task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5104 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5104/>Improving Claim Stance Classification with Lexical Knowledge Expansion and Context Utilization</a></strong><br><a href=/people/r/roy-bar-haim/>Roy Bar-Haim</a>
|
<a href=/people/l/lilach-edelstein/>Lilach Edelstein</a>
|
<a href=/people/c/charles-jochim/>Charles Jochim</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5104><div class="card-body p-3 small">Stance classification is a core component in on-demand argument construction pipelines. Previous work on claim stance classification relied on background knowledge such as manually-composed sentiment lexicons. We show that both <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Coverage_(telecommunication)>coverage</a> can be significantly improved through automatic expansion of the initial lexicon. We also developed a set of contextual features that further improves the state-of-the-art for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5105 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5105/>Mining Argumentative Structure from Natural Language text using Automatically Generated Premise-Conclusion Topic Models</a></strong><br><a href=/people/j/john-lawrence/>John Lawrence</a>
|
<a href=/people/c/chris-reed/>Chris Reed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5105><div class="card-body p-3 small">This paper presents a <a href=https://en.wikipedia.org/wiki/Scientific_method>method</a> of extracting argumentative structure from <a href=https://en.wikipedia.org/wiki/Natural_language>natural language text</a>. The approach presented is based on the way in which we understand an argument being made, not just from the words said, but from existing <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual knowledge</a> and understanding of the broader issues. We leverage high-precision, low-recall techniques in order to automatically build a large corpus of inferential statements related to the text&#8217;s topic. These <a href=https://en.wikipedia.org/wiki/Statement_(logic)>statements</a> are then used to produce a <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a> representing the <a href=https://en.wikipedia.org/wiki/Inference>inferential relationship</a> between different aspects of the topic. From this <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>matrix</a>, we are able to determine connectedness and directionality of inference between statements in the original text. By following this approach, we obtain results that compare favourably to those of other similar techniques to classify premise-conclusion pairs (with results 22 points above baseline), but without the requirement of large volumes of annotated, domain specific data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5106.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5106 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5106 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5106/>Building an Argument Search Engine for the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a></a></strong><br><a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/m/martin-potthast/>Martin Potthast</a>
|
<a href=/people/k/khalid-al-khatib/>Khalid Al-Khatib</a>
|
<a href=/people/y/yamen-ajjour/>Yamen Ajjour</a>
|
<a href=/people/j/jana-puschmann/>Jana Puschmann</a>
|
<a href=/people/j/jiani-qu/>Jiani Qu</a>
|
<a href=/people/j/jonas-dorsch/>Jonas Dorsch</a>
|
<a href=/people/v/viorel-morari/>Viorel Morari</a>
|
<a href=/people/j/janek-bevendorff/>Janek Bevendorff</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5106><div class="card-body p-3 small">Computational argumentation is expected to play a critical role in the future of <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search</a>. To make this happen, many search-related questions must be revisited, such as how people query for arguments, how to mine arguments from the web, or how to rank them. In this paper, we develop an argument search framework for studying these and further questions. The framework allows for the composition of approaches to acquiring, mining, assessing, indexing, querying, retrieving, ranking, and presenting arguments while relying on standard infrastructure and interfaces. Based on the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>, we build a prototype search engine, called args, that relies on an initial, freely accessible index of nearly 300k arguments crawled from reliable <a href=https://en.wikipedia.org/wiki/Web_resource>web resources</a>. The <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> and the argument search engine are intended as an environment for collaborative research on computational argumentation and its practical evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5107.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5107 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5107 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5107/>Argument Relation Classification Using a Joint Inference Model</a></strong><br><a href=/people/y/yufang-hou/>Yufang Hou</a>
|
<a href=/people/c/charles-jochim/>Charles Jochim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5107><div class="card-body p-3 small">In this paper, we address the problem of argument relation classification where <a href=https://en.wikipedia.org/wiki/Argument_(linguistics)>argument units</a> are from different texts. We design a joint inference method for the task by modeling argument relation classification and <a href=https://en.wikipedia.org/wiki/Stance_(linguistics)>stance classification</a> jointly. We show that our joint model improves the results over several strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5108.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5108 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5108 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-5108.Attachment.txt data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-5108/>Projection of Argumentative Corpora from Source to Target Languages</a></strong><br><a href=/people/a/ahmet-aker/>Ahmet Aker</a>
|
<a href=/people/h/huangpan-zhang/>Huangpan Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5108><div class="card-body p-3 small">Argumentative corpora are costly to create and are available in only few languages with <a href=https://en.wikipedia.org/wiki/English_language>English</a> dominating the area. In this paper we release the first publicly available Mandarin argumentative corpus. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> is created by exploiting the idea of comparable corpora from <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>Statistical Machine Translation</a>. We use existing <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and manually map the claims and premises to comparable corpora in <a href=https://en.wikipedia.org/wiki/Mandarin_Chinese>Mandarin</a>. We also implement a simple solution to automate this approach with the view of creating argumentative corpora in other less-resourced languages. In this way we introduce a new task of multi-lingual argument mapping that can be evaluated using our English-Mandarin argumentative corpus. The preliminary results of our automatic argument mapper mirror the simplicity of our approach, but provide a baseline for further improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5109 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5109/>Manual Identification of Arguments with Implicit Conclusions Using Semantic Rules for Argument Mining</a></strong><br><a href=/people/n/nancy-green/>Nancy Green</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5109><div class="card-body p-3 small">This paper describes a pilot study to evaluate human analysts&#8217; ability to identify the <a href=https://en.wikipedia.org/wiki/Argumentation_theory>argumentation scheme</a> and premises of an argument having an implicit conclusion. In preparation for the study, argumentation scheme definitions were crafted for <a href=https://en.wikipedia.org/wiki/Scientific_literature>genetics research articles</a>. The schemes were defined in <a href=https://en.wikipedia.org/wiki/Semantics>semantic terms</a>, following a proposal to use semantic rules to mine arguments in that literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5110 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-5110.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-5110/>Unsupervised corpuswide claim detection</a></strong><br><a href=/people/r/ran-levy/>Ran Levy</a>
|
<a href=/people/s/shai-gretz/>Shai Gretz</a>
|
<a href=/people/b/benjamin-sznajder/>Benjamin Sznajder</a>
|
<a href=/people/s/shay-hummel/>Shay Hummel</a>
|
<a href=/people/r/ranit-aharonov/>Ranit Aharonov</a>
|
<a href=/people/n/noam-slonim/>Noam Slonim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5110><div class="card-body p-3 small">Automatic claim detection is a fundamental argument mining task that aims to automatically mine claims regarding a topic of consideration. Previous works on mining argumentative content have assumed that a set of relevant documents is given in advance. Here, we present a first corpus wide claim detection framework, that can be directly applied to massive corpora. Using simple and intuitive empirical observations, we derive a claim sentence query by which we are able to directly retrieve sentences in which the <a href=https://en.wikipedia.org/wiki/Prior_probability>prior probability</a> to include topic-relevant claims is greatly enhanced. Next, we employ simple <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a> to rank the sentences, leading to an unsupervised corpuswide claim detection system, with precision that outperforms previously reported results on the task of claim detection given relevant documents and labeled data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5111 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5111/>Using Question-Answering Techniques to Implement a Knowledge-Driven Argument Mining Approach</a></strong><br><a href=/people/p/patrick-saint-dizier/>Patrick Saint-Dizier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5111><div class="card-body p-3 small">This short paper presents a first implementation of a knowledge-driven argument mining approach. The major processing steps and <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>language resources</a> of the <a href=https://en.wikipedia.org/wiki/System>system</a> are surveyed. An indicative evaluation outlines challenges and improvement directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5112.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5112 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5112 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5112/>What works and what does not : Classifier and feature analysis for <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a></a></strong><br><a href=/people/a/ahmet-aker/>Ahmet Aker</a>
|
<a href=/people/a/alfred-sliwa/>Alfred Sliwa</a>
|
<a href=/people/y/yuan-ma/>Yuan Ma</a>
|
<a href=/people/r/ruishen-lui/>Ruishen Lui</a>
|
<a href=/people/n/niravkumar-borad/>Niravkumar Borad</a>
|
<a href=/people/s/seyedeh-ziyaei/>Seyedeh Ziyaei</a>
|
<a href=/people/m/mina-ghobadi/>Mina Ghobadi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5112><div class="card-body p-3 small">This paper offers a comparative analysis of the performance of different supervised machine learning methods and <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature sets</a> on argument mining tasks. Specifically, we address the tasks of extracting argumentative segments from texts and predicting the structure between those <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>segments</a>. Eight <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> and different combinations of six feature types reported in previous work are evaluated. The results indicate that overall best performing features are the structural ones. Although the performance of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> varies depending on the feature combinations and corpora used for training and testing, <a href=https://en.wikipedia.org/wiki/Random_forest>Random Forest</a> seems to be among the best performing <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a>. These results build a basis for further development of <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining techniques</a> and can guide an implementation of <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a> into different applications such as argument based search.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5113.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5113 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5113 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5113/>Unsupervised Detection of Argumentative Units though Topic Modeling Techniques</a></strong><br><a href=/people/a/alfio-ferrara/>Alfio Ferrara</a>
|
<a href=/people/s/stefano-montanelli/>Stefano Montanelli</a>
|
<a href=/people/g/georgios-petasis/>Georgios Petasis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5113><div class="card-body p-3 small">In this paper we present a new unsupervised approach, Attraction to Topics A2 T, for the detection of argumentative units, a sub-task of <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>. Motivated by the importance of topic identification in manual annotation, we examine whether <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a> can be used for performing unsupervised detection of argumentative sentences, and to what extend <a href=https://en.wikipedia.org/wiki/Topic_modeling>topic modeling</a> can be used to classify sentences as claims and premises. Preliminary evaluation results suggest that topic information can be successfully used for the detection of argumentative sentences, at least for corpora used for evaluation. Our approach has been evaluated on two English corpora, the first of which contains 90 persuasive essays, while the second is a collection of 340 documents from <a href=https://en.wikipedia.org/wiki/User-generated_content>user generated content</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5114 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5114/>Using Complex Argumentative Interactions to Reconstruct the Argumentative Structure of Large-Scale Debates</a></strong><br><a href=/people/j/john-lawrence/>John Lawrence</a>
|
<a href=/people/c/chris-reed/>Chris Reed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5114><div class="card-body p-3 small">In this paper we consider the insights that can be gained by considering large scale argument networks and the complex interactions between their constituent propositions. We investigate <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> for analysing properties of these <a href=https://en.wikipedia.org/wiki/Social_network>networks</a>, illustrating these using a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of arguments</a> taken from the 2016 US Presidential Debates. We present techniques for determining these <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> directly from <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language text</a> and show that there is a strong correlation between these automatically identified <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>features</a> and the argumentative structure contained within the text. Finally, we combine these <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> with argument mining techniques and show how the identification of argumentative relations can be improved by considering the larger context in which they occur.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5115.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5115 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5115 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5115" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5115/>Unit Segmentation of Argumentative Texts</a></strong><br><a href=/people/y/yamen-ajjour/>Yamen Ajjour</a>
|
<a href=/people/w/wei-fan-chen/>Wei-Fan Chen</a>
|
<a href=/people/j/johannes-kiesel/>Johannes Kiesel</a>
|
<a href=/people/h/henning-wachsmuth/>Henning Wachsmuth</a>
|
<a href=/people/b/benno-stein/>Benno Stein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5115><div class="card-body p-3 small">The segmentation of an argumentative text into argument units and their non-argumentative counterparts is the first step in identifying the argumentative structure of the text. Despite its importance for <a href=https://en.wikipedia.org/wiki/Argument_mining>argument mining</a>, unit segmentation has been approached only sporadically so far. This paper studies the major parameters of unit segmentation systematically. We explore the effectiveness of various <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>, when capturing words separately, along with their neighbors, or even along with the entire text. Each such context is reflected by one <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning model</a> that we evaluate within and across three domains of texts. Among the <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a>, our new deep learning approach capturing the entire text turns out best within all domains, with an F-score of up to 88.54. While structural features generalize best across domains, the <a href=https://en.wikipedia.org/wiki/Domain_transfer>domain transfer</a> remains hard, which points to major challenges of unit segmentation.</div></div></div><hr><div id=w17-52><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-52.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-52/>Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5200/>Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</a></strong><br><a href=/people/a/alexandra-balahur/>Alexandra Balahur</a>
|
<a href=/people/s/saif-mohammad/>Saif M. Mohammad</a>
|
<a href=/people/e/erik-van-der-goot/>Erik van der Goot</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5201.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5201 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5201 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5201/>Detecting Sarcasm Using Different Forms Of Incongruity</a></strong><br><a href=/people/a/aditya-joshi/>Aditya Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5201><div class="card-body p-3 small">Sarcasm is a form of <a href=https://en.wikipedia.org/wiki/Irony>verbal irony</a> that is intended to express contempt or ridicule. Often quoted as a challenge to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcasm</a> involves use of words of positive or no polarity to convey negative sentiment. Incongruity has been observed to be at the heart of sarcasm understanding in humans. Our work in sarcasm detection identifies different forms of incongruity and employs different machine learning techniques to capture them. This talk will describe the approach, datasets and challenges in sarcasm detection using different forms of incongruity. We identify two forms of <a href=https://en.wikipedia.org/wiki/Incongruity>incongruity</a> : <a href=https://en.wikipedia.org/wiki/Incongruity>incongruity</a> which can be understood based on the target text and common background knowledge, and <a href=https://en.wikipedia.org/wiki/Incongruity>incongruity</a> which can be understood based on the target text and additional, specific context. The former involves use of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment-based features</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, and <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a>. The latter involves creation of author&#8217;s historical context based on their historical data, and creation of conversational context for sarcasm detection of dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5203.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5203 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5203 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-5203.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-5203/>Annotation, Modelling and Analysis of Fine-Grained Emotions on a Stance and Sentiment Detection Corpus</a></strong><br><a href=/people/h/hendrik-schuff/>Hendrik Schuff</a>
|
<a href=/people/j/jeremy-barnes/>Jeremy Barnes</a>
|
<a href=/people/j/julian-mohme/>Julian Mohme</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5203><div class="card-body p-3 small">There is a rich variety of data sets for sentiment analysis (viz., polarity and subjectivity classification). For the more challenging task of detecting discrete emotions following the definitions of Ekman and Plutchik, however, there are much fewer data sets, and notably no resources for the social media domain. This paper contributes to closing this gap by extending the SemEval 2016 stance and sentiment datasetwith emotion annotation. We (a) analyse annotation reliability and annotation merging ; (b) investigate the relation between emotion annotation and the other annotation layers (stance, sentiment) ; (c) report modelling results as a baseline for future work.<i>SemEval 2016 stance and sentiment dataset</i>\n\nwith emotion annotation. We (a) analyse annotation reliability and annotation merging; (b) investigate the relation between emotion annotation and the other annotation layers (stance, sentiment); (c) report modelling results as a baseline for future work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5204.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5204 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5204 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5204/>Ranking Right-Wing Extremist Social Media Profiles by Similarity to Democratic and Extremist Groups</a></strong><br><a href=/people/m/matthias-hartung/>Matthias Hartung</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a>
|
<a href=/people/f/franziska-schmidtke/>Franziska Schmidtke</a>
|
<a href=/people/l/lars-vogel/>Lars Vogel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5204><div class="card-body p-3 small">Social media are used by an increasing number of political actors. A small subset of these is interested in pursuing extremist motives such as <a href=https://en.wikipedia.org/wiki/Mobilization>mobilization</a>, recruiting or radicalization activities. In order to counteract these trends, online providers and state institutions reinforce their monitoring efforts, mostly relying on manual workflows. We propose a machine learning approach to support manual attempts towards identifying right-wing extremist content in <a href=https://en.wikipedia.org/wiki/Twitter>German Twitter profiles</a>. Based on a fine-grained conceptualization of <a href=https://en.wikipedia.org/wiki/Far-right_politics>right-wing extremism</a>, we frame the task as ranking each individual profile on a continuum spanning different degrees of <a href=https://en.wikipedia.org/wiki/Far-right_politics>right-wing extremism</a>, based on a <a href=https://en.wikipedia.org/wiki/Nearest_neighbour_search>nearest neighbour approach</a>. A quantitative evaluation reveals that our <a href=https://en.wikipedia.org/wiki/Ranking_(statistics)>ranking model</a> yields robust performance (up to 0.81 F_1 score) when being used for predicting discrete class labels. At the same time, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> provides plausible continuous ranking scores for a small sample of borderline cases at the division of right-wing extremism and New Right political movements.<tex-math>_1</tex-math> score) when being used for predicting discrete class labels. At the same time, the model provides plausible continuous ranking scores for a small sample of borderline cases at the division of right-wing extremism and New Right political movements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5205.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5205 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5205 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5205/>WASSA-2017 Shared Task on Emotion Intensity<span class=acl-fixed-case>WASSA</span>-2017 Shared Task on Emotion Intensity</a></strong><br><a href=/people/s/saif-mohammad/>Saif Mohammad</a>
|
<a href=/people/f/felipe-bravo-marquez/>Felipe Bravo-Marquez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5205><div class="card-body p-3 small">We present the first shared task on detecting the intensity of emotion felt by the speaker of a tweet. We create the first datasets of <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> annotated for <a href=https://en.wikipedia.org/wiki/Anger>anger</a>, <a href=https://en.wikipedia.org/wiki/Fear>fear</a>, <a href=https://en.wikipedia.org/wiki/Joy>joy</a>, and sadness intensities using a technique called bestworst scaling (BWS). We show that the annotations lead to reliable fine-grained intensity scores (rankings of tweets by intensity). The <a href=https://en.wikipedia.org/wiki/Data>data</a> was partitioned into training, development, and test sets for the competition. Twenty-two teams participated in the shared task, with the best system obtaining a Pearson correlation of 0.747 with the gold intensity scores. We summarize the machine learning setups, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful for the task. The emotion intensity dataset and the shared task are helping improve our understanding of how we convey more or less intense emotions through language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5206.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5206 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5206 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5206/>IMS at EmoInt-2017 : Emotion Intensity Prediction with Affective Norms, Automatically Extended Resources and <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a><span class=acl-fixed-case>IMS</span> at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: Emotion Intensity Prediction with Affective Norms, Automatically Extended Resources and Deep Learning</a></strong><br><a href=/people/m/maximilian-koper/>Maximilian Köper</a>
|
<a href=/people/e/evgeny-kim/>Evgeny Kim</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5206><div class="card-body p-3 small">Our submission to the WASSA-2017 shared task on the prediction of emotion intensity in tweets is a supervised learning method with extended lexicons of affective norms. We combine three main information sources in a random forrest regressor, namely (1), manually created resources, (2) automatically extended lexicons, and (3) the output of a neural network (CNN-LSTM) for sentence regression. All three <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature sets</a> perform similarly well in isolation (.67 macro average Pearson correlation). The <a href=https://en.wikipedia.org/wiki/Combination>combination</a> achieves.72 on the official test set (ranked 2nd out of 22 participants). Our analysis reveals that performance is increased by providing cross-emotional intensity predictions. The automatic extension of lexicon features benefit from domain specific embeddings. Complementary ratings for affective norms increase the impact of <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon features</a>. Our resources (ratings for 1.6 million twitter specific words) and our <a href=https://en.wikipedia.org/wiki/Implementation>implementation</a> is publicly available at.<url>http://www.ims.uni-stuttgart.de/data/ims_emoint</url>.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5207.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5207 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5207 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5207/>Prayas at EmoInt 2017 : An Ensemble of Deep Neural Architectures for Emotion Intensity Prediction in Tweets<span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt 2017: An Ensemble of Deep Neural Architectures for Emotion Intensity Prediction in Tweets</a></strong><br><a href=/people/p/pranav-goel/>Pranav Goel</a>
|
<a href=/people/d/devang-kulshreshtha/>Devang Kulshreshtha</a>
|
<a href=/people/p/prayas-jain/>Prayas Jain</a>
|
<a href=/people/k/kaushal-kumar-shukla/>Kaushal Kumar Shukla</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5207><div class="card-body p-3 small">The paper describes the best performing system for EmoInt-a shared task to predict the intensity of emotions in tweets. Intensity is a real valued score, between 0 and 1. The <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> are classified as-anger, <a href=https://en.wikipedia.org/wiki/Fear>fear</a>, <a href=https://en.wikipedia.org/wiki/Joy>joy</a> and sadness. We apply three different deep neural network based models, which approach the problem from essentially different directions. Our final performance quantified by an average pearson correlation score of 74.7 and an average spearman correlation score of 73.5 is obtained using an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> of the three <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We outperform the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline model</a> of the shared task by 9.9 % and 9.4 % pearson and spearman correlation scores respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5209.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5209 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5209 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5209" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5209/>Towards Syntactic Iberian Polarity Classification<span class=acl-fixed-case>I</span>berian Polarity Classification</a></strong><br><a href=/people/d/david-vilares/>David Vilares</a>
|
<a href=/people/m/marcos-garcia/>Marcos Garcia</a>
|
<a href=/people/m/miguel-a-alonso/>Miguel A. Alonso</a>
|
<a href=/people/c/carlos-gomez-rodriguez/>Carlos Gómez-Rodríguez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5209><div class="card-body p-3 small">Lexicon-based methods using syntactic rules for polarity classification rely on <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> that are dependent on the language and on <a href=https://en.wikipedia.org/wiki/Treebank>treebank guidelines</a>. Thus, <a href=https://en.wikipedia.org/wiki/Rule_of_inference>rules</a> are also dependent and require adaptation, especially in multilingual scenarios. We tackle this challenge in the context of the <a href=https://en.wikipedia.org/wiki/Iberian_Peninsula>Iberian Peninsula</a>, releasing the first symbolic syntax-based Iberian system with rules shared across five official languages : <a href=https://en.wikipedia.org/wiki/Basque_language>Basque</a>, <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a>, <a href=https://en.wikipedia.org/wiki/Galician_language>Galician</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>. The model is made available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5212.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5212 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5212 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5212/>Forecasting Consumer Spending from Purchase Intentions Expressed on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/v/viktor-pekar/>Viktor Pekar</a>
|
<a href=/people/j/jane-binner/>Jane Binner</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5212><div class="card-body p-3 small">Consumer spending is an important macroeconomic indicator that is used by policy-makers to judge the health of an economy. In this paper we present a novel method for predicting future consumer spending from <a href=https://en.wikipedia.org/wiki/Social_media>social media data</a>. In contrast to previous work that largely relied on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>, the proposed method models <a href=https://en.wikipedia.org/wiki/Consumer_spending>consumer spending</a> from purchase intentions found on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Our experiments with time series analysis models and machine-learning regression models reveal utility of this data for making short-term forecasts of consumer spending : for three- and seven-day horizons, prediction variables derived from social media help to improve forecast accuracy by 11 % to 18 % for all the three models, in comparison to models that used only autoregressive predictors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5213.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5213 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5213 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5213" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5213/>Mining fine-grained opinions on closed captions of YouTube videos with an attention-RNN<span class=acl-fixed-case>Y</span>ou<span class=acl-fixed-case>T</span>ube videos with an attention-<span class=acl-fixed-case>RNN</span></a></strong><br><a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/j/jorge-balazs/>Jorge Balazs</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5213><div class="card-body p-3 small">Video reviews are the natural evolution of written product reviews. In this paper we target this phenomenon and introduce the first dataset created from closed captions of YouTube product review videos as well as a new attention-RNN model for <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> and joint aspect extraction and sentiment classification. Our model provides state-of-the-art performance on <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a> without requiring the usage of hand-crafted features on the SemEval ABSA corpus, while it outperforms the baseline on the joint task. In our dataset, the attention-RNN model outperforms the baseline for both tasks, but we observe important performance drops for all models in comparison to <a href=https://en.wikipedia.org/wiki/SemEval>SemEval</a>. These results, as well as further experiments on <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> for <a href=https://en.wikipedia.org/wiki/Aspect_extraction>aspect extraction</a>, suggest that differences between speech and written text, which have been discussed extensively in the literature, also extend to the domain of product reviews, where they are relevant for fine-grained opinion mining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5216.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5216 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5216 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5216/>Investigating Redundancy in Emoji Use : Study on a Twitter Based Corpus<span class=acl-fixed-case>T</span>witter Based Corpus</a></strong><br><a href=/people/g/giulia-donato/>Giulia Donato</a>
|
<a href=/people/p/patrizia-paggio/>Patrizia Paggio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5216><div class="card-body p-3 small">In this paper we present an annotated corpus created with the aim of analyzing the informative behaviour of <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a> an issue of importance for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. The corpus consists of 2475 tweets all containing at least one <a href=https://en.wikipedia.org/wiki/Emoji>emoji</a>, which has been annotated using one of the three possible classes : <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>Redundant</a>, Non Redundant, and Non Redundant + POS. We explain how the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> was collected, describe the annotation procedure and the <a href=https://en.wikipedia.org/wiki/Interface_(computing)>interface</a> developed for the task. We provide an analysis of the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>, considering also possible predictive features, discuss the problematic aspects of the <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>, and suggest future improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5217.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5217 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5217 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5217/>Modeling Temporal Progression of Emotional Status in Mental Health Forum : A Recurrent Neural Net Approach</a></strong><br><a href=/people/k/kishaloy-halder/>Kishaloy Halder</a>
|
<a href=/people/l/lahari-poddar/>Lahari Poddar</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5217><div class="card-body p-3 small">Patients turn to <a href=https://en.wikipedia.org/wiki/Online_health_communities>Online Health Communities</a> not only for information on specific conditions but also for <a href=https://en.wikipedia.org/wiki/Emotional_support>emotional support</a>. Previous research has indicated that the progression of emotional status can be studied through the linguistic patterns of an individual&#8217;s posts. We analyze a real-world dataset from the Mental Health section of HealthBoards.com. Estimated from the word usages in their posts, we find that the emotional progress across patients vary widely. We study the problem of predicting a patient&#8217;s emotional status in the future from her past posts and we propose a Recurrent Neural Network (RNN) based architecture to address it. We find that the future emotional status can be predicted with reasonable accuracy given her historical posts and participation features. Our evaluation results demonstrate the efficacy of our proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a>, by outperforming <a href=https://en.wikipedia.org/wiki/Software_architecture>state-of-the-art approaches</a> with over 0.13 reduction in <a href=https://en.wikipedia.org/wiki/Mean_absolute_error>Mean Absolute Error</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5218.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5218 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5218 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5218/>Towards an integrated pipeline for aspect-based sentiment analysis in various domains</a></strong><br><a href=/people/o/orphee-de-clercq/>Orphée De Clercq</a>
|
<a href=/people/e/els-lefever/>Els Lefever</a>
|
<a href=/people/g/gilles-jacobs/>Gilles Jacobs</a>
|
<a href=/people/t/tijl-carpels/>Tijl Carpels</a>
|
<a href=/people/v/veronique-hoste/>Véronique Hoste</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5218><div class="card-body p-3 small">This paper presents an integrated ABSA pipeline for <a href=https://en.wikipedia.org/wiki/Dutch_language>Dutch</a> that has been developed and tested on qualitative user feedback coming from three domains : <a href=https://en.wikipedia.org/wiki/Retail>retail</a>, <a href=https://en.wikipedia.org/wiki/Bank>banking</a> and <a href=https://en.wikipedia.org/wiki/Human_resources>human resources</a>. The two latter <a href=https://en.wikipedia.org/wiki/Domain_(software_engineering)>domains</a> provide <a href=https://en.wikipedia.org/wiki/Service-oriented_architecture>service-oriented data</a>, which has not been investigated before in ABSA. By performing in-domain and cross-domain experiments the validity of our approach was investigated. We show promising results for the three ABSA subtasks, aspect term extraction, aspect category classification and aspect polarity classification.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5219.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5219 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5219 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5219/>Building a SentiWordNet for Odia<span class=acl-fixed-case>S</span>enti<span class=acl-fixed-case>W</span>ord<span class=acl-fixed-case>N</span>et for <span class=acl-fixed-case>O</span>dia</a></strong><br><a href=/people/g/gaurav-mohanty/>Gaurav Mohanty</a>
|
<a href=/people/a/abishek-kannan/>Abishek Kannan</a>
|
<a href=/people/r/radhika-mamidi/>Radhika Mamidi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5219><div class="card-body p-3 small">As a discipline of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> is used to extract and analyze <a href=https://en.wikipedia.org/wiki/Subjectivity>subjective information</a> present in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language data</a>. The task of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> has acquired wide commercial uses including social media monitoring tasks, survey responses, review systems, etc. Languages like <a href=https://en.wikipedia.org/wiki/English_language>English</a> have several resources which aid in the task of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a>. SentiWordNet and Subjectivity WordList are examples of such tools and resources. With more data being available in native vernacular, language-specific SentiWordNet(s) have become essential. For resource poor languages, creating such SentiWordNet(s) is a difficult task to achieve. One solution is to use available resources in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and translate the final source lexicon to target lexicon via <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Machine translation systems for the English-Odia language pair have not yet been developed. In this paper, we discuss a method to create a SentiWordNet for <a href=https://en.wikipedia.org/wiki/Odia_language>Odia</a>, which is resource-poor, by only using resources which are currently available for <a href=https://en.wikipedia.org/wiki/Languages_of_India>Indian languages</a>. The lexicon created, would serve as a tool for Sentiment Analysis related task specific to Odia data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5220.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5220 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5220 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5220/>Lexicon Integrated CNN Models with Attention for Sentiment Analysis<span class=acl-fixed-case>CNN</span> Models with Attention for Sentiment Analysis</a></strong><br><a href=/people/b/bonggun-shin/>Bonggun Shin</a>
|
<a href=/people/t/timothy-lee/>Timothy Lee</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5220><div class="card-body p-3 small">With the advent of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a> are no longer fully utilized for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> although they still provide important features in the traditional setting. This paper introduces a novel approach to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> that integrates lexicon embeddings and an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> into <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a>. Our approach performs separate convolutions for word and lexicon embeddings and provides a global view of the document using <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. Our models are experimented on both the SemEval&#8217;16 Task 4 dataset and the Stanford Sentiment Treebank and show comparative or better results against the existing state-of-the-art systems. Our analysis shows that lexicon embeddings allow building high-performing models with much smaller word embeddings, and the attention mechanism effectively dims out noisy words for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5221.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5221 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5221 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5221" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5221/>Explaining Recurrent Neural Network Predictions in Sentiment Analysis</a></strong><br><a href=/people/l/leila-arras/>Leila Arras</a>
|
<a href=/people/g/gregoire-montavon/>Grégoire Montavon</a>
|
<a href=/people/k/klaus-robert-muller/>Klaus-Robert Müller</a>
|
<a href=/people/w/wojciech-samek/>Wojciech Samek</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5221><div class="card-body p-3 small">Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a>. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5222.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5222 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5222 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5222/>GradAscent at EmoInt-2017 : Character and Word Level Recurrent Neural Network Models for Tweet Emotion Intensity Detection<span class=acl-fixed-case>G</span>rad<span class=acl-fixed-case>A</span>scent at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: Character and Word Level Recurrent Neural Network Models for Tweet Emotion Intensity Detection</a></strong><br><a href=/people/e/egor-lakomkin/>Egor Lakomkin</a>
|
<a href=/people/c/chandrakant-bothe/>Chandrakant Bothe</a>
|
<a href=/people/s/stefan-wermter/>Stefan Wermter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5222><div class="card-body p-3 small">The WASSA 2017 EmoInt shared task has the goal to predict emotion intensity values of <a href=https://en.wikipedia.org/wiki/Twitter>tweet messages</a>. Given the text of a tweet and its emotion category (anger, joy, fear, and sadness), the participants were asked to build a system that assigns emotion intensity values. Emotion intensity estimation is a challenging problem given the short length of the tweets, the noisy structure of the text and the lack of annotated data. To solve this problem, we developed an ensemble of two neural models, processing input on the character. and word-level with a lexicon-driven system. The correlation scores across all four emotions are averaged to determine the bottom-line competition metric, and our system ranks place forth in full intensity range and third in 0.5-1 range of intensity among 23 systems at the time of writing (June 2017).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5223.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5223 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5223 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5223/>NUIG at EmoInt-2017 : BiLSTM and SVR Ensemble to Detect Emotion Intensity<span class=acl-fixed-case>NUIG</span> at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: <span class=acl-fixed-case>B</span>i<span class=acl-fixed-case>LSTM</span> and <span class=acl-fixed-case>SVR</span> Ensemble to Detect Emotion Intensity</a></strong><br><a href=/people/v/vladimir-andryushechkin/>Vladimir Andryushechkin</a>
|
<a href=/people/i/ian-wood/>Ian Wood</a>
|
<a href=/people/j/james-o-neill/>James O’ Neill</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5223><div class="card-body p-3 small">This paper describes the entry NUIG in the WASSA 2017 (8th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis) shared task on <a href=https://en.wikipedia.org/wiki/Emotion_recognition>emotion recognition</a>. The NUIG system used an SVR (SVM regression) and BLSTM ensemble, utilizing primarily n-grams (for SVR features) and tweet word embeddings (for BLSTM features). Experiments were carried out on several other candidate features, some of which were added to the SVR model. Parameter selection for the SVR model was run as a <a href=https://en.wikipedia.org/wiki/Grid_search>grid search</a> whilst parameters for the BLSTM model were selected through a non-exhaustive ad-hoc search.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5224.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5224 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5224 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5224/>Unsupervised Aspect Term Extraction with B-LSTM & CRF using Automatically Labelled Datasets<span class=acl-fixed-case>B</span>-<span class=acl-fixed-case>LSTM</span> & <span class=acl-fixed-case>CRF</span> using Automatically Labelled Datasets</a></strong><br><a href=/people/a/athanasios-giannakopoulos/>Athanasios Giannakopoulos</a>
|
<a href=/people/c/claudiu-musat/>Claudiu Musat</a>
|
<a href=/people/a/andreea-hossmann/>Andreea Hossmann</a>
|
<a href=/people/m/michael-baeriswyl/>Michael Baeriswyl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5224><div class="card-body p-3 small">Aspect Term Extraction (ATE) identifies opinionated aspect terms in texts and is one of the tasks in the SemEval Aspect Based Sentiment Analysis (ABSA) contest. The small amount of available datasets for supervised ATE and the costly human annotation for aspect term labelling give rise to the need for unsupervised ATE. In this paper, we introduce an <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> that achieves top-ranking performance for supervised ATE. Moreover, it can be used efficiently as feature extractor and <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> for <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised ATE</a>. Our second contribution is a method to automatically construct <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> for ATE. We train a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> on our automatically labelled datasets and evaluate it on the human annotated SemEval ABSA test sets. Compared to a strong rule-based baseline, we obtain a dramatically higher <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> and attain <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> values above 80 %. Our <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> beats the supervised ABSA baseline from <a href=https://en.wikipedia.org/wiki/SemEval>SemEval</a>, while preserving high precision scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5227.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5227 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5227 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5227/>YNU-HPCC at EmoInt-2017 : Using a CNN-LSTM Model for Sentiment Intensity Prediction<span class=acl-fixed-case>YNU</span>-<span class=acl-fixed-case>HPCC</span> at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: Using a <span class=acl-fixed-case>CNN</span>-<span class=acl-fixed-case>LSTM</span> Model for Sentiment Intensity Prediction</a></strong><br><a href=/people/y/you-zhang/>You Zhang</a>
|
<a href=/people/h/hang-yuan/>Hang Yuan</a>
|
<a href=/people/j/jin-wang/>Jin Wang</a>
|
<a href=/people/x/xuejie-zhang/>Xuejie Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5227><div class="card-body p-3 small">In this paper, we present a system that uses a convolutional neural network with long short-term memory (CNN-LSTM) model to complete the task. The CNN-LSTM model has two combined parts : CNN extracts local n-gram features within tweets and LSTM composes the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> to capture long-distance dependency across tweets. Additionally, we used other three models (CNN, LSTM, BiLSTM) as baseline algorithms. Our introduced <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> showed good performance in the experimental results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5228.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5228 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5228 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-5228.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5228" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5228/>Seernet at EmoInt-2017 : Tweet Emotion Intensity Estimator<span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: Tweet Emotion Intensity Estimator</a></strong><br><a href=/people/v/venkatesh-duppada/>Venkatesh Duppada</a>
|
<a href=/people/s/sushant-hiray/>Sushant Hiray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5228><div class="card-body p-3 small">The paper describes experiments on estimating emotion intensity in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a> using a generalized regressor system. The system combines various independent feature extractors, trains them on general regressors and finally combines the best performing models to create an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a>. The proposed <a href=https://en.wikipedia.org/wiki/System>system</a> stood 3rd out of 22 systems in leaderboard of WASSA-2017 Shared Task on Emotion Intensity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5230.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5230 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5230 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5230/>NSEmo at EmoInt-2017 : An Ensemble to Predict Emotion Intensity in Tweets<span class=acl-fixed-case>NSE</span>mo at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: An Ensemble to Predict Emotion Intensity in Tweets</a></strong><br><a href=/people/s/sreekanth-madisetty/>Sreekanth Madisetty</a>
|
<a href=/people/m/maunendra-sankar-desarkar/>Maunendra Sankar Desarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5230><div class="card-body p-3 small">In this paper, we describe a <a href=https://en.wikipedia.org/wiki/Methodology>method</a> to predict emotion intensity in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. Our approach is an ensemble of three <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression methods</a>. The first method uses content-based features (hashtags, <a href=https://en.wikipedia.org/wiki/Emoticon>emoticons</a>, elongated words, etc.). The second method considers <a href=https://en.wikipedia.org/wiki/N-gram>word n-grams</a> and character n-grams for training. The final method uses <a href=https://en.wikipedia.org/wiki/Lexicon>lexicons</a>, <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, <a href=https://en.wikipedia.org/wiki/N-gram>word n-grams</a>, character n-grams for training the model. An <a href=https://en.wikipedia.org/wiki/Ensemble_cast>ensemble</a> of these three <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> gives better performance than individual methods. We applied our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> on WASSA emotion dataset. Achieved results are as follows : <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>average Pearson correlation</a> is 0.706, <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>average Spearman correlation</a> is 0.696, <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>average Pearson correlation</a> for gold scores in range 0.5 to 1 is 0.539, and <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>average Spearman correlation</a> for gold scores in range 0.5 to 1 is 0.514.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5231.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5231 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5231 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5231/>Tecnolengua Lingmotif at EmoInt-2017 : A lexicon-based approach<span class=acl-fixed-case>T</span>ecnolengua <span class=acl-fixed-case>L</span>ingmotif at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: A lexicon-based approach</a></strong><br><a href=/people/a/antonio-moreno-ortiz/>Antonio Moreno-Ortiz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5231><div class="card-body p-3 small">In this paper we describe Tecnolengua Group&#8217;s participation in the shared task on emotion intensity at WASSA 2017. We used the Lingmotif tool and a new, complementary tool, Lingmotif Learn, which we developed for this occasion. We based our intensity predictions for the four test datasets entirely on Lingmotif&#8217;s TSS (text sentiment score) feature. We also developed <a href=https://en.wikipedia.org/wiki/Mechanism_design>mechanisms</a> for dealing with the <a href=https://en.wikipedia.org/wiki/Idiosyncrasy>idiosyncrasies</a> of <a href=https://en.wikipedia.org/wiki/Twitter>Twitter text</a>. Results were comparatively poor, but the experience meant a good opportunity for us to identify issues in our score calculation for short texts, a genre for which the Lingmotif tool was not originally designed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5232.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5232 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5232 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5232" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5232/>EmoAtt at EmoInt-2017 : Inner attention sentence embedding for Emotion Intensity<span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>A</span>tt at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: Inner attention sentence embedding for Emotion Intensity</a></strong><br><a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5232><div class="card-body p-3 small">In this paper we describe a deep learning system that has been designed and built for the WASSA 2017 Emotion Intensity Shared Task. We introduce a representation learning approach based on inner attention on top of an <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>RNN</a>. Results show that our model offers good capabilities and is able to successfully identify emotion-bearing words to predict intensity without leveraging on lexicons, obtaining the 13 t place among 22 shared task competitors.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5234.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5234 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5234 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5234/>DMGroup at EmoInt-2017 : Emotion Intensity Using Ensemble Method<span class=acl-fixed-case>DMG</span>roup at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: Emotion Intensity Using Ensemble Method</a></strong><br><a href=/people/s/song-jiang/>Song Jiang</a>
|
<a href=/people/x/xiaotian-han/>Xiaotian Han</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5234><div class="card-body p-3 small">In this paper, we present a novel ensemble learning architecture for emotion intensity analysis, particularly a novel framework of ensemble method. The ensemble method has two stages and each stage includes several single machine learning models. In stage1, we employ both linear and nonlinear regression models to obtain a more diverse emotion intensity representation. In stage2, we use two <a href=https://en.wikipedia.org/wiki/Regression_analysis>regression models</a> including <a href=https://en.wikipedia.org/wiki/Linear_regression>linear regression</a> and <a href=https://en.wikipedia.org/wiki/XGBoost>XGBoost</a>. The result of stage1 serves as the input of stage2, so the two different type models (linear and non-linear) in stage2 can describe the input in two opposite aspects. We also added a method for analyzing and splitting multi-words hashtags and appending them to the emotion intensity corpus before feeding it to our model. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves 0.571 <a href=https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>Pearson-measure</a> for the average of four emotions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5235.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5235 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5235 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5235/>UWat-Emote at EmoInt-2017 : Emotion Intensity Detection using Affect Clues, Sentiment Polarity and Word Embeddings<span class=acl-fixed-case>UW</span>at-Emote at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: Emotion Intensity Detection using Affect Clues, Sentiment Polarity and Word Embeddings</a></strong><br><a href=/people/v/vineet-john/>Vineet John</a>
|
<a href=/people/o/olga-vechtomova/>Olga Vechtomova</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5235><div class="card-body p-3 small">This paper describes the UWaterloo affect prediction system developed for EmoInt-2017. We delve into our feature selection approach for affect intensity, affect presence, sentiment intensity and sentiment presence lexica alongside pre-trained word embeddings, which are utilized to extract emotion intensity signals from tweets in an ensemble learning approach. The system employs emotion specific model training, and utilizes distinct <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for each of the emotion corpora in isolation. Our system utilizes gradient boosted regression as the primary learning technique to predict the final emotion intensities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5236.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5236 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5236 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5236/>LIPN-UAM at EmoInt-2017 : Combination of Lexicon-based features and Sentence-level Vector Representations for Emotion Intensity Determination<span class=acl-fixed-case>LIPN</span>-<span class=acl-fixed-case>UAM</span> at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017:Combination of Lexicon-based features and Sentence-level Vector Representations for Emotion Intensity Determination</a></strong><br><a href=/people/d/davide-buscaldi/>Davide Buscaldi</a>
|
<a href=/people/b/belem-priego-sanchez/>Belem Priego</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5236><div class="card-body p-3 small">This paper presents the combined LIPN-UAM participation in the WASSA 2017 Shared Task on Emotion Intensity. In particular, the paper provides some highlights on the Tweetaneuse system that was presented to the shared task. We combined lexicon-based features with sentence-level vector representations to implement a random forest regressor.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5237.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5237 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5237 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5237/>deepCybErNet at EmoInt-2017 : Deep Emotion Intensities in Tweets<span class=acl-fixed-case>C</span>yb<span class=acl-fixed-case>E</span>r<span class=acl-fixed-case>N</span>et at <span class=acl-fixed-case>E</span>mo<span class=acl-fixed-case>I</span>nt-2017: Deep Emotion Intensities in Tweets</a></strong><br><a href=/people/v/vinayakumar-r/>Vinayakumar R</a>
|
<a href=/people/p/premjith-b/>Premjith B</a>
|
<a href=/people/s/sachin-kumar-s/>Sachin Kumar S</a>
|
<a href=/people/s/soman-kp/>Soman KP</a>
|
<a href=/people/p/prabaharan-poornachandran/>Prabaharan Poornachandran</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5237><div class="card-body p-3 small">This working note presents the methodology used in deepCybErNet submission to the shared task on Emotion Intensities in Tweets (EmoInt) WASSA-2017. The goal of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is to predict a real valued score in the range [ 0-1 ] for a particular tweet with an <a href=https://en.wikipedia.org/wiki/Emotion>emotion type</a>. To do this, we used Bag-of-Words and embedding based on recurrent network architecture. We have developed two systems and experiments are conducted on the Emotion Intensity shared Task 1 data base at WASSA-2017. A system which uses <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> based on recurrent network architecture has achieved highest 5 fold cross-validation accuracy. This has used <a href=https://en.wikipedia.org/wiki/Embedding>embedding</a> with <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent network</a> to extract optimal features at tweet level and <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a> for <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a>. These methods are highly language independent and experimental results shows that the proposed methods are apt for predicting a real valued score in than range [ 0-1 ] for a given tweet with its emotion type.</div></div></div><hr><div id=w17-53><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-53.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-53/>Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5300/>Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for <span class=acl-fixed-case>NLP</span></a></strong><br><a href=/people/s/samuel-bowman/>Samuel Bowman</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/f/felix-hill/>Felix Hill</a>
|
<a href=/people/a/angeliki-lazaridou/>Angeliki Lazaridou</a>
|
<a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a>
|
<a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5301/>The RepEval 2017 Shared Task : Multi-Genre Natural Language Inference with Sentence Representations<span class=acl-fixed-case>R</span>ep<span class=acl-fixed-case>E</span>val 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations</a></strong><br><a href=/people/n/nikita-nangia/>Nikita Nangia</a>
|
<a href=/people/a/adina-williams/>Adina Williams</a>
|
<a href=/people/a/angeliki-lazaridou/>Angeliki Lazaridou</a>
|
<a href=/people/s/samuel-bowman/>Samuel Bowman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5301><div class="card-body p-3 small">This paper presents the results of the RepEval 2017 Shared Task, which evaluated neural network sentence representation learning models on the Multi-Genre Natural Language Inference corpus (MultiNLI) recently introduced by Williams et al. All of the five participating teams beat the bidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in Williams et al. The best single <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> used stacked BiLSTMs with residual connections to extract <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence features</a> and reached 74.5 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on the genre-matched test set. Surprisingly, the results of the competition were fairly consistent across the genre-matched and genre-mismatched test sets, and across subsets of the test data representing a variety of linguistic phenomena, suggesting that all of the submitted systems learned reasonably domain-independent representations for sentence meaning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5302/>Traversal-Free Word Vector Evaluation in Analogy Space</a></strong><br><a href=/people/x/xiaoyin-che/>Xiaoyin Che</a>
|
<a href=/people/n/nico-ring/>Nico Ring</a>
|
<a href=/people/w/willi-raschkowski/>Willi Raschkowski</a>
|
<a href=/people/h/haojin-yang/>Haojin Yang</a>
|
<a href=/people/c/christoph-meinel/>Christoph Meinel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5302><div class="card-body p-3 small">In this paper, we propose an alternative evaluating metric for word analogy questions (A to B is as C to D) in word vector evaluation. Different from the traditional method which predicts the fourth word by the given three, we measure the similarity directly on the relations of two pairs of given words, just as shifting the relation vectors into a new analogy space. Cosine and Euclidean distances are then calculated as measurements. Observation and experiments shows the proposed analogy space evaluation could offer a more comprehensive evaluating result on word vectors with word analogy questions. Meanwhile, <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> are remarkably reduced by avoiding traversing the vocabulary.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5303/>Hypothesis Testing based Intrinsic Evaluation of Word Embeddings</a></strong><br><a href=/people/n/nishant-gurnani/>Nishant Gurnani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5303><div class="card-body p-3 small">We introduce the cross-match test-an exact, distribution free, high-dimensional hypothesis test as an intrinsic evaluation metric for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We show that <a href=https://en.wikipedia.org/wiki/Cross-matching>cross-match</a> is an effective means of measuring the distributional similarity between different <a href=https://en.wikipedia.org/wiki/Vector_space>vector representations</a> and of evaluating the <a href=https://en.wikipedia.org/wiki/Statistical_significance>statistical significance</a> of different vector embedding models. Additionally, we find that <a href=https://en.wikipedia.org/wiki/Cross-matching>cross-match</a> can be used to provide a quantitative measure of linguistic similarity for selecting bridge languages for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. We demonstrate that the results of the <a href=https://en.wikipedia.org/wiki/Statistical_hypothesis_testing>hypothesis test</a> align with our expectations and note that the framework of two sample hypothesis testing is not limited to <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and can be extended to all <a href=https://en.wikipedia.org/wiki/Vector_space>vector representations</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5304/>Evaluation of word embeddings against <a href=https://en.wikipedia.org/wiki/Cognition>cognitive processes</a> : primed reaction times in lexical decision and naming tasks</a></strong><br><a href=/people/j/jeremy-auguste/>Jeremy Auguste</a>
|
<a href=/people/a/arnaud-rey/>Arnaud Rey</a>
|
<a href=/people/b/benoit-favre/>Benoit Favre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5304><div class="card-body p-3 small">This work presents a <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for word similarity evaluation grounded on cognitive sciences experimental data. Word pair similarities are compared to <a href=https://en.wikipedia.org/wiki/Mental_chronometry>reaction times</a> of subjects in large scale lexical decision and naming tasks under <a href=https://en.wikipedia.org/wiki/Semantic_priming>semantic priming</a>. Results show that GloVe embeddings lead to significantly higher correlation with experimental measurements than other controlled and off-the-shelf embeddings, and that the choice of a <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training corpus</a> is less important than that of the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a>. Comparison of rankings with other datasets shows that the cognitive phenomenon covers more aspects than simply <a href=https://en.wikipedia.org/wiki/Word_sense>word relatedness</a> or <a href=https://en.wikipedia.org/wiki/Similarity_(psychology)>similarity</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5305/>Playing with Embeddings : Evaluating embeddings for Robot Language Learning through MUD Games<span class=acl-fixed-case>MUD</span> Games</a></strong><br><a href=/people/a/anmol-gulati/>Anmol Gulati</a>
|
<a href=/people/k/kumar-krishna-agrawal/>Kumar Krishna Agrawal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5305><div class="card-body p-3 small">Acquiring language provides a ubiquitous mode of communication, across humans and robots. To this effect, distributional representations of words based on co-occurrence statistics, have provided significant advancements ranging across <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> to <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension</a>. In this paper, we study the suitability of using general purpose word-embeddings for language learning in robots. We propose using text-based games as a proxy to evaluating <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> on real robots. Based in a risk-reward setting, we review the effectiveness of the embeddings in navigating tasks in fantasy games, as an approximation to their performance on more complex scenarios, like language assisted robot navigation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5306/>Recognizing Textual Entailment in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> Using Word Embeddings<span class=acl-fixed-case>T</span>witter Using Word Embeddings</a></strong><br><a href=/people/o/octavia-maria-sulea/>Octavia-Maria Şulea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5306><div class="card-body p-3 small">In this paper, we investigate the application of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning techniques</a> and <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> to the task of Recognizing Textual Entailment (RTE) in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a>. We look at a manually labeled dataset consisting of user generated short texts posted on Twitter (tweets) and related to four recent media events (the <a href=https://en.wikipedia.org/wiki/Charlie_Hebdo_shooting>Charlie Hebdo shooting</a>, the Ottawa shooting, the Sydney Siege, and the German Wings crash) and test to what extent neural techniques and embeddings are able to distinguish between tweets that entail or contradict each other or that claim unrelated things. We obtain comparable results to the state of the art in a train-test setting, but we show that, due to the noisy aspect of the data, results plummet in an evaluation strategy crafted to better simulate a real-life train-test scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5309" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5309/>Character-level Intra Attention Network for Natural Language Inference</a></strong><br><a href=/people/h/han-yang/>Han Yang</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussà</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5309><div class="card-body p-3 small">Natural language inference (NLI) is a central problem in <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>language understanding</a>. End-to-end artificial neural networks have reached state-of-the-art performance in NLI field recently. In this paper, we propose Character-level Intra Attention Network (CIAN) for the NLI task. In our model, we use the character-level convolutional network to replace the standard word embedding layer, and we use the intra attention to capture the intra-sentence semantics. The proposed CIAN model provides improved results based on a newly published MNLI corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5310" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5310/>Refining <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>Raw Sentence Representations</a> for Textual Entailment Recognition via <a href=https://en.wikipedia.org/wiki/Attention>Attention</a></a></strong><br><a href=/people/j/jorge-balazs/>Jorge Balazs</a>
|
<a href=/people/e/edison-marrese-taylor/>Edison Marrese-Taylor</a>
|
<a href=/people/p/pablo-loyola/>Pablo Loyola</a>
|
<a href=/people/y/yutaka-matsuo/>Yutaka Matsuo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5310><div class="card-body p-3 small">In this paper we present the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> used by the team Rivercorners for the 2017 RepEval shared task. First, our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> separately encodes a pair of sentences into variable-length representations by using a bidirectional LSTM. Later, <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> creates fixed-length raw representations by means of simple aggregation functions, which are then refined using an <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a>. Finally it combines the refined <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> of both sentences into a single vector to be used for <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a>. With this model we obtained test accuracies of 72.057 % and 72.055 % in the matched and mismatched evaluation tracks respectively, outperforming the LSTM baseline, and obtaining performances similar to a model that relies on shared information between sentences (ESIM). When using an <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> both accuracies increased to 72.247 % and 72.827 % respectively.</div></div></div><hr><div id=w17-54><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-54.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-54/>Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5400/>Proceedings of the First Workshop on Building Linguistically Generalizable <span class=acl-fixed-case>NLP</span> Systems</a></strong><br><a href=/people/e/emily-m-bender/>Emily Bender</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a>
|
<a href=/people/a/allyson-ettinger/>Allyson Ettinger</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5401/>Towards Linguistically Generalizable NLP Systems : A Workshop and Shared Task<span class=acl-fixed-case>NLP</span> Systems: A Workshop and Shared Task</a></strong><br><a href=/people/a/allyson-ettinger/>Allyson Ettinger</a>
|
<a href=/people/s/sudha-rao/>Sudha Rao</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a>
|
<a href=/people/e/emily-m-bender/>Emily M. Bender</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5401><div class="card-body p-3 small">This paper presents a summary of the first Workshop on Building Linguistically Generalizable Natural Language Processing Systems, and the associated Build It Break It, The Language Edition shared task. The goal of this workshop was to bring together researchers in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> and <a href=https://en.wikipedia.org/wiki/Linguistics>linguistics</a> with a carefully designed shared task aimed at testing the generalizability of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a> beyond the distributions of their training data. We describe the motivation, setup, and participation of the shared task, provide discussion of some highlighted results, and discuss lessons learned.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5402/>Analysing Errors of Open Information Extraction Systems</a></strong><br><a href=/people/r/rudolf-schneider/>Rudolf Schneider</a>
|
<a href=/people/t/tom-oberhauser/>Tom Oberhauser</a>
|
<a href=/people/t/tobias-klatt/>Tobias Klatt</a>
|
<a href=/people/f/felix-a-gers/>Felix A. Gers</a>
|
<a href=/people/a/alexander-loser/>Alexander Löser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5402><div class="card-body p-3 small">We report results on benchmarking Open Information Extraction (OIE) systems using RelVis, a toolkit for benchmarking Open Information Extraction systems. Our comprehensive <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark</a> contains three data sets from the news domain and one data set from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a> with overall 4522 labeled sentences and 11243 binary or n-ary OIE relations. In our analysis on these data sets we compared the performance of four popular OIE systems, ClausIE, OpenIE 4.2, Stanford OpenIE and PredPatt. In addition, we evaluated the impact of five common error classes on a subset of 749 n-ary tuples. From our deep analysis we unreveal important research directions for a next generation on OIE systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5403 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5403" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5403/>Massively Multilingual Neural Grapheme-to-Phoneme Conversion</a></strong><br><a href=/people/b/ben-peters/>Ben Peters</a>
|
<a href=/people/j/jon-dehdari/>Jon Dehdari</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5403><div class="card-body p-3 small">Grapheme-to-phoneme conversion (g2p) is necessary for text-to-speech and automatic speech recognition systems. Most g2p systems are monolingual : they require language-specific data or handcrafting of rules. Such systems are difficult to extend to low resource languages, for which data and handcrafted rules are not available. As an alternative, we present a neural sequence-to-sequence approach to g2p which is trained on spellingpronunciation pairs in hundreds of languages. The system shares a single encoder and decoder across all languages, allowing it to utilize the intrinsic similarities between different <a href=https://en.wikipedia.org/wiki/Writing_system>writing systems</a>. We show an 11 % improvement in phoneme error rate over an approach based on adapting high-resource monolingual g2p models to low-resource languages. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is also much more compact relative to previous <a href=https://en.wikipedia.org/wiki/Scientific_modelling>approaches</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5404 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5404/>BIBI System Description : Building with CNNs and Breaking with <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Reinforcement Learning</a><span class=acl-fixed-case>BIBI</span> System Description: Building with <span class=acl-fixed-case>CNN</span>s and Breaking with Deep Reinforcement Learning</a></strong><br><a href=/people/y/yitong-li/>Yitong Li</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5404><div class="card-body p-3 small">This paper describes our submission to the sentiment analysis sub-task of Build It, Break It : The Language Edition (BIBI), on both the builder and breaker sides. As a builder, we use <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural nets</a>, trained on both phrase and sentence data. As a breaker, we use <a href=https://en.wikipedia.org/wiki/Q-learning>Q-learning</a> to learn minimal change pairs, and apply a token substitution method automatically. We analyse the results to gauge the <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5405/>Breaking NLP : Using <a href=https://en.wikipedia.org/wiki/Morphosyntax>Morphosyntax</a>, <a href=https://en.wikipedia.org/wiki/Semantics>Semantics</a>, <a href=https://en.wikipedia.org/wiki/Pragmatics>Pragmatics</a> and World Knowledge to Fool Sentiment Analysis Systems<span class=acl-fixed-case>NLP</span>: Using Morphosyntax, Semantics, Pragmatics and World Knowledge to Fool Sentiment Analysis Systems</a></strong><br><a href=/people/t/taylor-mahler/>Taylor Mahler</a>
|
<a href=/people/w/willy-cheung/>Willy Cheung</a>
|
<a href=/people/m/micha-elsner/>Micha Elsner</a>
|
<a href=/people/d/david-king/>David King</a>
|
<a href=/people/m/marie-catherine-de-marneffe/>Marie-Catherine de Marneffe</a>
|
<a href=/people/c/cory-shain/>Cory Shain</a>
|
<a href=/people/s/symon-stevens-guille/>Symon Stevens-Guille</a>
|
<a href=/people/m/michael-white/>Michael White</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5405><div class="card-body p-3 small">This paper describes our breaker submission to the 2017 EMNLP Build It Break It shared task on <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. In order to cause the builder systems to make incorrect predictions, we edited items in the blind test data according to linguistically interpretable strategies that allow us to assess the ease with which the builder systems learn various components of linguistic structure. On the whole, our submitted pairs break all systems at a high rate (72.6 %), indicating that <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> as an NLP task may still have a lot of ground to cover. Of the breaker strategies that we consider, we find our semantic and pragmatic manipulations to pose the most substantial difficulties for the builder systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5406/>An Adaptable Lexical Simplification Architecture for Major Ibero-Romance Languages<span class=acl-fixed-case>I</span>bero-<span class=acl-fixed-case>R</span>omance Languages</a></strong><br><a href=/people/d/daniel-ferres/>Daniel Ferrés</a>
|
<a href=/people/h/horacio-saggion/>Horacio Saggion</a>
|
<a href=/people/x/xavier-gomez-guinovart/>Xavier Gómez Guinovart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5406><div class="card-body p-3 small">Lexical Simplification is the task of reducing the lexical complexity of <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>textual documents</a> by replacing difficult words with easier to read (or understand) expressions while preserving the original meaning. The development of robust pipelined multilingual architectures able to adapt to new languages is of paramount importance in <a href=https://en.wikipedia.org/wiki/Lexical_simplification>lexical simplification</a>. This paper describes and evaluates a modular hybrid linguistic-statistical Lexical Simplifier that deals with the four major Ibero-Romance Languages : <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a>, <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a>, and <a href=https://en.wikipedia.org/wiki/Galician_language>Galician</a>. The architecture of the <a href=https://en.wikipedia.org/wiki/System>system</a> is the same for the four languages addressed, only the language resources used during <a href=https://en.wikipedia.org/wiki/Simplification>simplification</a> are language specific.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5407/>Cross-genre Document Retrieval : Matching between Conversational and Formal Writings</a></strong><br><a href=/people/t/tomasz-jurczyk/>Tomasz Jurczyk</a>
|
<a href=/people/j/jinho-d-choi/>Jinho D. Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5407><div class="card-body p-3 small">This paper challenges a cross-genre document retrieval task, where the queries are in <a href=https://en.wikipedia.org/wiki/Formal_writing>formal writing</a> and the target documents are in conversational writing. In this task, a query, is a sentence extracted from either a summary or a plot of an episode in a TV show, and the target document consists of transcripts from the corresponding episode. To establish a strong baseline, we employ the current state-of-the-art <a href=https://en.wikipedia.org/wiki/Web_search_engine>search engine</a> to perform <a href=https://en.wikipedia.org/wiki/Document_retrieval>document retrieval</a> on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> collected for this work. We then introduce a structure reranking approach to improve the initial ranking by utilizing syntactic and semantic structures generated by <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tools</a>. Our evaluation shows an improvement of more than 4 % when the structure reranking is applied, which is very promising.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5409" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5409/>Strawman : An Ensemble of Deep Bag-of-Ngrams for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a><span class=acl-fixed-case>S</span>trawman: An Ensemble of Deep Bag-of-Ngrams for Sentiment Analysis</a></strong><br><a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5409><div class="card-body p-3 small">This paper describes a builder entry, named strawman, to the sentence-level sentiment analysis task of the Build It, Break It shared task of the First Workshop on Building Linguistically Generalizable NLP Systems. The goal of a builder is to provide an automated sentiment analyzer that would serve as a target for breakers whose goal is to find pairs of minimally-differing sentences that break the analyzer.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5410 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5410/>Breaking Sentiment Analysis of Movie Reviews</a></strong><br><a href=/people/i/ieva-staliunaite/>Ieva Staliūnaitė</a>
|
<a href=/people/b/ben-bonfil/>Ben Bonfil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5410><div class="card-body p-3 small">The current paper covers several strategies we used to &#8216;break&#8217; predictions of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis systems</a> participating in the BLGNLP2017 workshop. Specifically, we identify difficulties of participating systems in understanding <a href=https://en.wikipedia.org/wiki/Modal_logic>modals</a>, subjective judgments, world-knowledge based references and certain differences in <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and perspective.</div></div></div><hr><div id=w17-55><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-55.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-55/>Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5500/>Proceedings of the 18th Annual <span class=acl-fixed-case>SIG</span>dial Meeting on Discourse and Dialogue</a></strong><br><a href=/people/k/kristiina-jokinen/>Kristiina Jokinen</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a>
|
<a href=/people/d/david-devault/>David DeVault</a>
|
<a href=/people/a/annie-louis/>Annie Louis</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5501.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5501 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5501 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5501" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5501/>Automatic Mapping of French Discourse Connectives to PDTB Discourse Relations<span class=acl-fixed-case>F</span>rench Discourse Connectives to <span class=acl-fixed-case>PDTB</span> Discourse Relations</a></strong><br><a href=/people/m/majid-laali/>Majid Laali</a>
|
<a href=/people/l/leila-kosseim/>Leila Kosseim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5501><div class="card-body p-3 small">In this paper, we present an approach to exploit phrase tables generated by <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation</a> in order to map French discourse connectives to <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>. Using this approach, we created DisCoRel, a lexicon of French discourse connectives and their PDTB relations. When evaluated against LEXCONN, DisCoRel achieves a recall of 0.81 and an Average Precision of 0.68 for the Concession and Condition relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5502.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5502 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5502 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5502/>Towards Full Text Shallow Discourse Relation Annotation : Experiments with Cross-Paragraph Implicit Relations in the PDTB<span class=acl-fixed-case>PDTB</span></a></strong><br><a href=/people/r/rashmi-prasad/>Rashmi Prasad</a>
|
<a href=/people/k/kate-forbes-riley/>Katherine Forbes Riley</a>
|
<a href=/people/a/alan-lee/>Alan Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5502><div class="card-body p-3 small">Full text discourse parsing relies on texts comprehensively annotated with <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse relations</a>. To this end, we address a significant gap in the inter-sentential discourse relations annotated in the Penn Discourse Treebank (PDTB), namely the class of cross-paragraph implicit relations, which account for 30 % of inter-sentential relations in the corpus. We present our annotation study to explore the <a href=https://en.wikipedia.org/wiki/Incidence_(epidemiology)>incidence rate</a> of adjacent vs. non-adjacent implicit relations in cross-paragraph contexts, and the relative degree of difficulty in annotating them. Our experiments show a high incidence of non-adjacent relations that are difficult to annotate reliably, suggesting the practicality of backing off from their annotation to reduce noise for corpus-based studies. Our resulting guidelines follow the PDTB adjacency constraint for implicits while employing an underspecified representation of non-adjacent implicits, and yield 62 % inter-annotator agreement on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5503.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5503 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5503 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5503/>User-initiated Sub-dialogues in State-of-the-art Dialogue Systems</a></strong><br><a href=/people/s/staffan-larsson/>Staffan Larsson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5503><div class="card-body p-3 small">We test state of the art <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> for their behaviour in response to user-initiated sub-dialogues, i.e. interactions where a system question is responded to with a question or request from the user, who thus initiates a sub-dialogue. We look at sub-dialogues both within a single app (where the sub-dialogue concerns another topic in the original domain) and across apps (where the sub-dialogue concerns a different domain). The overall conclusion of the tests is that none of the <a href=https://en.wikipedia.org/wiki/System>systems</a> can be said to deal appropriately with user-initiated sub-dialogues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5504.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5504 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5504 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5504/>A Multimodal Dialogue System for Medical Decision Support inside Virtual Reality</a></strong><br><a href=/people/a/alexander-prange/>Alexander Prange</a>
|
<a href=/people/m/margarita-chikobava/>Margarita Chikobava</a>
|
<a href=/people/p/peter-poller/>Peter Poller</a>
|
<a href=/people/m/michael-barz/>Michael Barz</a>
|
<a href=/people/d/daniel-sonntag/>Daniel Sonntag</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5504><div class="card-body p-3 small">We present a multimodal dialogue system that allows doctors to interact with a medical decision support system in <a href=https://en.wikipedia.org/wiki/Virtual_reality>virtual reality (VR)</a>. We integrate an interactive visualization of patient records and radiology image data, as well as therapy predictions. Therapy predictions are computed in real-time using a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5505.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5505 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5505 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5505/>Generative Encoder-Decoder Models for Task-Oriented Spoken Dialog Systems with Chatting Capability</a></strong><br><a href=/people/t/tiancheng-zhao/>Tiancheng Zhao</a>
|
<a href=/people/a/allen-lu/>Allen Lu</a>
|
<a href=/people/k/kyusong-lee/>Kyusong Lee</a>
|
<a href=/people/m/maxine-eskenazi/>Maxine Eskenazi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5505><div class="card-body p-3 small">Generative encoder-decoder models offer great promise in developing domain-general dialog systems. However, they have mainly been applied to open-domain conversations. This paper presents a practical and novel <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for building task-oriented dialog systems based on encoder-decoder models. This <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> enables encoder-decoder models to accomplish slot-value independent decision-making and interact with <a href=https://en.wikipedia.org/wiki/Database>external databases</a>. Moreover, this paper shows the flexibility of the proposed method by interleaving chatting capability with a slot-filling system for better out-of-domain recovery. The models were trained on both <a href=https://en.wikipedia.org/wiki/User_data>real-user data</a> from a bus information system and <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-human chat data</a>. Results show that the proposed <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves good performance in both offline evaluation metrics and in task success rate with <a href=https://en.wikipedia.org/wiki/User_(computing)>human users</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5506.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5506 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5506 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5506" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5506/>Key-Value Retrieval Networks for Task-Oriented Dialogue</a></strong><br><a href=/people/m/mihail-eric/>Mihail Eric</a>
|
<a href=/people/l/lakshmi-krishnan/>Lakshmi Krishnan</a>
|
<a href=/people/f/francois-charette/>Francois Charette</a>
|
<a href=/people/c/christopher-d-manning/>Christopher D. Manning</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5506><div class="card-body p-3 small">Neural task-oriented dialogue systems often struggle to smoothly interface with a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. In this work, we seek to address this problem by proposing a new neural dialogue agent that is able to effectively sustain grounded, multi-domain discourse through a novel key-value retrieval mechanism. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is end-to-end differentiable and does not need to explicitly model dialogue state or belief trackers. We also release a new dataset of 3,031 dialogues that are grounded through underlying knowledge bases and span three distinct tasks in the in-car personal assistant space : <a href=https://en.wikipedia.org/wiki/Calendaring_software>calendar scheduling</a>, <a href=https://en.wikipedia.org/wiki/Weather_forecasting>weather information retrieval</a>, and point-of-interest navigation. Our architecture is simultaneously trained on data from all domains and significantly outperforms a competitive rule-based system and other existing neural dialogue architectures on the provided domains according to both automatic and human evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5507.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5507 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5507 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5507/>Lexical Acquisition through Implicit Confirmations over Multiple Dialogues</a></strong><br><a href=/people/k/kohei-ono/>Kohei Ono</a>
|
<a href=/people/r/ryu-takeda/>Ryu Takeda</a>
|
<a href=/people/e/eric-nichols/>Eric Nichols</a>
|
<a href=/people/m/mikio-nakano/>Mikio Nakano</a>
|
<a href=/people/k/kazunori-komatani/>Kazunori Komatani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5507><div class="card-body p-3 small">We address the problem of acquiring the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological categories</a> of unknown terms through implicit confirmation in dialogues. We develop an approach that makes implicit confirmation requests with an unknown term&#8217;s predicted category. Our approach does not degrade user experience with repetitive explicit confirmations, but the <a href=https://en.wikipedia.org/wiki/System>system</a> has difficulty determining if information in the confirmation request can be correctly acquired. To overcome this challenge, we propose a method for determining whether or not the predicted category is correct, which is included in an implicit confirmation request. Our method exploits multiple user responses to implicit confirmation requests containing the same <a href=https://en.wikipedia.org/wiki/Ontological_category>ontological category</a>. Experimental results revealed that the proposed method exhibited a higher precision rate for determining the correctly predicted categories than when only single user responses were considered.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5508.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5508 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5508 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5508/>Utterance Intent Classification of a Spoken Dialogue System with Efficiently Untied Recursive Autoencoders</a></strong><br><a href=/people/t/tsuneo-kato/>Tsuneo Kato</a>
|
<a href=/people/a/atsushi-nagai/>Atsushi Nagai</a>
|
<a href=/people/n/naoki-noda/>Naoki Noda</a>
|
<a href=/people/r/ryosuke-sumitomo/>Ryosuke Sumitomo</a>
|
<a href=/people/j/jianming-wu/>Jianming Wu</a>
|
<a href=/people/s/seiichi-yamamoto/>Seiichi Yamamoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5508><div class="card-body p-3 small">Recursive autoencoders (RAEs) for compositionality of a vector space model were applied to utterance intent classification of a smartphone-based Japanese-language spoken dialogue system. Though the RAEs express a nonlinear operation on the vectors of child nodes, the <a href=https://en.wikipedia.org/wiki/Operation_(mathematics)>operation</a> is considered to be different intrinsically depending on types of <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>child nodes</a>. To relax the difference, a data-driven untying of autoencoders (AEs) is proposed. The experimental result of the utterance intent classification showed an improved accuracy with the proposed method compared with the basic tied RAE and untied RAE based on a manual rule.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5509.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5509 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5509 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5509/>Reward-Balancing for Statistical Spoken Dialogue Systems using Multi-objective Reinforcement Learning</a></strong><br><a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a>
|
<a href=/people/s/steve-young/>Steve Young</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5509><div class="card-body p-3 small">Reinforcement learning is widely used for dialogue policy optimization where the <a href=https://en.wikipedia.org/wiki/Reward_system>reward function</a> often consists of more than one <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>component</a>, e.g., the dialogue success and the dialogue length. In this work, we propose a structured method for finding a good balance between these components by searching for the optimal reward component weighting. To render this search feasible, we use multi-objective reinforcement learning to significantly reduce the number of training dialogues required. We apply our proposed method to find optimized component weights for six domains and compare them to a default baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5511.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5511 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5511 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5511/>Demonstration of interactive teaching for end-to-end dialog control with hybrid code networks</a></strong><br><a href=/people/j/jason-d-williams/>Jason D. Williams</a>
|
<a href=/people/l/lars-liden/>Lars Liden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5511><div class="card-body p-3 small">This is a demonstration of interactive teaching for practical end-to-end dialog systems driven by a <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network</a>. In this approach, a developer teaches the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> by interacting with the <a href=https://en.wikipedia.org/wiki/System>system</a> and providing on-the-spot corrections. Once a <a href=https://en.wikipedia.org/wiki/System>system</a> is deployed, a developer can also correct mistakes in logged dialogs. This demonstration shows both of these teaching methods applied to dialog systems in three domains : pizza ordering, <a href=https://en.wikipedia.org/wiki/Restaurant>restaurant information</a>, and <a href=https://en.wikipedia.org/wiki/Weather_forecasting>weather forecasts</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5512.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5512 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5512 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5512/>Sub-domain Modelling for <a href=https://en.wikipedia.org/wiki/Dialogue_management>Dialogue Management</a> with Hierarchical Reinforcement Learning</a></strong><br><a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/n/nikola-mrksic/>Nikola Mrkšić</a>
|
<a href=/people/t/tsung-hsien-wen/>Tsung-Hsien Wen</a>
|
<a href=/people/i/inigo-casanueva/>Iñigo Casanueva</a>
|
<a href=/people/l/lina-m-rojas-barahona/>Lina M. Rojas-Barahona</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5512><div class="card-body p-3 small">Human conversation is inherently complex, often spanning many different topics / domains. This makes <a href=https://en.wikipedia.org/wiki/Policy_learning>policy learning</a> for <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> very challenging. Standard flat reinforcement learning methods do not provide an efficient <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> for modelling such <a href=https://en.wikipedia.org/wiki/Dialogue>dialogues</a>. In this paper, we focus on the under-explored problem of multi-domain dialogue management. First, we propose a new <a href=https://en.wikipedia.org/wiki/Methodology>method</a> for hierarchical reinforcement learning using the option framework. Next, we show that the proposed <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a> learns faster and arrives at a better <a href=https://en.wikipedia.org/wiki/Policy>policy</a> than the existing flat ones do. Moreover, we show how pretrained policies can be adapted to more complex systems with an additional set of new actions. In doing that, we show that our approach has the potential to facilitate policy optimisation for more sophisticated multi-domain dialogue systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5513.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5513 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5513 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5513/>MACA : A Modular Architecture for Conversational Agents<span class=acl-fixed-case>MACA</span>: A Modular Architecture for Conversational Agents</a></strong><br><a href=/people/h/hoai-phuoc-truong/>Hoai Phuoc Truong</a>
|
<a href=/people/p/prasanna-parthasarathi/>Prasanna Parthasarathi</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5513><div class="card-body p-3 small">We propose a <a href=https://en.wikipedia.org/wiki/Software_architecture>software architecture</a> designed to ease the implementation of <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>. The Modular Architecture for Conversational Agents (MACA) uses a plug-n-play style that allows quick prototyping, thereby facilitating the development of new techniques and the reproduction of previous work. The <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> separates the domain of the conversation from the agent&#8217;s dialogue strategy, and as such can be easily extended to multiple domains. MACA provides tools to host dialogue agents on Amazon Mechanical Turk (mTurk) for data collection and allows processing of other sources of training data. The current version of the <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> already incorporates several domains and existing <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue strategies</a> from the recent literature.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5514.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5514 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5514 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5514/>Sequential Dialogue Context Modeling for Spoken Language Understanding</a></strong><br><a href=/people/a/ankur-bapna/>Ankur Bapna</a>
|
<a href=/people/g/gokhan-tur/>Gokhan Tür</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tür</a>
|
<a href=/people/l/larry-heck/>Larry Heck</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5514><div class="card-body p-3 small">Spoken Language Understanding (SLU) is a key component of goal oriented dialogue systems that would parse user utterances into semantic frame representations. Traditionally SLU does not utilize the dialogue history beyond the previous system turn and contextual ambiguities are resolved by the downstream components. In this paper, we explore novel approaches for modeling dialogue context in a recurrent neural network (RNN) based language understanding system. We propose the Sequential Dialogue Encoder Network, that allows <a href=https://en.wikipedia.org/wiki/Context_(language_use)>encoding context</a> from the dialogue history in <a href=https://en.wikipedia.org/wiki/Chronology>chronological order</a>. We compare the performance of our proposed architecture with two context models, one that uses just the previous turn context and another that encodes dialogue context in a memory network, but loses the order of utterances in the dialogue history. Experiments with a multi-domain dialogue dataset demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> results in reduced semantic frame error rates.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5515.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5515 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5515 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5515/>Redundancy Localization for the Conversationalization of Unstructured Responses</a></strong><br><a href=/people/s/sebastian-krause/>Sebastian Krause</a>
|
<a href=/people/m/mikhail-kozhevnikov/>Mikhail Kozhevnikov</a>
|
<a href=/people/e/eric-malmi/>Eric Malmi</a>
|
<a href=/people/d/daniele-pighin/>Daniele Pighin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5515><div class="card-body p-3 small">Conversational agents offer users a <a href=https://en.wikipedia.org/wiki/Natural-language_user_interface>natural-language interface</a> to accomplish tasks, entertain themselves, or access information. Informational dialogue is particularly challenging in that the agent has to hold a conversation on an open topic, and to achieve a reasonable coverage it generally needs to digest and present <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured information</a> from textual sources. Making responses based on such sources sound natural and fit appropriately into the conversation context is a topic of ongoing research, one of the key issues of which is preventing the agent&#8217;s responses from sounding repetitive. Targeting this issue, we propose a new task, known as redundancy localization, which aims to pinpoint semantic overlap between text passages. To help address it systematically, we formalize the task, prepare a public dataset with fine-grained redundancy labels, and propose a model utilizing a weak training signal defined over the results of a passage-retrieval system on web texts. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> demonstrates superior performance compared to a state-of-the-art entailment model and yields encouraging results when applied to a real-world dialogue.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5516.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5516 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5516 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5516/>Attentive listening system with backchanneling, response generation and flexible turn-taking</a></strong><br><a href=/people/d/divesh-lala/>Divesh Lala</a>
|
<a href=/people/p/pierrick-milhorat/>Pierrick Milhorat</a>
|
<a href=/people/k/koji-inoue/>Koji Inoue</a>
|
<a href=/people/m/masanari-ishida/>Masanari Ishida</a>
|
<a href=/people/k/katsuya-takanashi/>Katsuya Takanashi</a>
|
<a href=/people/t/tatsuya-kawahara/>Tatsuya Kawahara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5516><div class="card-body p-3 small">Attentive listening systems are designed to let people, especially senior people, keep talking to maintain <a href=https://en.wikipedia.org/wiki/Communication>communication ability</a> and <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a>. This paper addresses key components of an attentive listening system which encourages users to talk smoothly. First, we introduce continuous prediction of end-of-utterances and generation of backchannels, rather than generating backchannels after end-point detection of utterances. This improves subjective evaluations of backchannels. Second, we propose an effective statement response mechanism which detects focus words and responds in the form of a question or partial repeat. This can be applied to any statement. Moreover, a flexible turn-taking mechanism is designed which uses backchannels or <a href=https://en.wikipedia.org/wiki/Filler_(materials)>fillers</a> when the turn-switch is ambiguous. These techniques are integrated into a <a href=https://en.wikipedia.org/wiki/Humanoid_robot>humanoid robot</a> to conduct attentive listening. We test the feasibility of the <a href=https://en.wikipedia.org/wiki/System>system</a> in a pilot experiment and show that it can produce coherent dialogues during conversation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5517.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5517 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5517 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5517/>Natural Language Input for In-Car Spoken Dialog Systems : How Natural is Natural?</a></strong><br><a href=/people/p/patricia-braunger/>Patricia Braunger</a>
|
<a href=/people/w/wolfgang-maier/>Wolfgang Maier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5517><div class="card-body p-3 small">Recent spoken dialog systems are moving away from <a href=https://en.wikipedia.org/wiki/Command_and_control>command and control</a> towards a more intuitive and natural style of interaction. In order to choose an appropriate system design which allows the <a href=https://en.wikipedia.org/wiki/System>system</a> to deal with naturally spoken user input, a definition of what exactly constitutes naturalness in <a href=https://en.wikipedia.org/wiki/Input_(computer_science)>user input</a> is important. In this paper, we examine how different user groups naturally speak to an automotive spoken dialog system (SDS). We conduct a <a href=https://en.wikipedia.org/wiki/User_study>user study</a> in which we collect freely spoken user utterances for a wide range of use cases in <a href=https://en.wikipedia.org/wiki/German_language>German</a>. By means of a comparative study of the utterances from the study with interpersonal utterances, we provide criteria what constitutes naturalness in the user input of an state-of-the-art automotive SDS.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5518.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5518 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5518 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5518/>Sample-efficient Actor-Critic Reinforcement Learning with Supervised Data for Dialogue Management</a></strong><br><a href=/people/p/pei-hao-su/>Pei-Hao Su</a>
|
<a href=/people/p/pawel-budzianowski/>Paweł Budzianowski</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a>
|
<a href=/people/m/milica-gasic/>Milica Gašić</a>
|
<a href=/people/s/steve-young/>Steve Young</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5518><div class="card-body p-3 small">Deep reinforcement learning (RL) methods have significant potential for dialogue policy optimisation. However, <a href=https://en.wikipedia.org/wiki/They_(2017_film)>they</a> suffer from a poor performance in the early stages of learning. This is especially problematic for <a href=https://en.wikipedia.org/wiki/Educational_technology>on-line learning</a> with real users. Two <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>approaches</a> are introduced to tackle this problem. Firstly, to speed up the learning process, two sample-efficient neural networks algorithms : trust region actor-critic with experience replay (TRACER) and episodic natural actor-critic with experience replay (eNACER) are presented. For TRACER, the <a href=https://en.wikipedia.org/wiki/Trust_region>trust region</a> helps to control the learning step size and avoid catastrophic model changes. For eNACER, the natural gradient identifies the steepest ascent direction in policy space to speed up the <a href=https://en.wikipedia.org/wiki/Convergence_of_random_variables>convergence</a>. Both models employ off-policy learning with experience replay to improve sample-efficiency. Secondly, to mitigate the cold start issue, a corpus of demonstration data is utilised to pre-train the models prior to on-line reinforcement learning. Combining these two approaches, we demonstrate a practical approach to learn deep RL-based dialogue policies and demonstrate their effectiveness in a task-oriented information seeking domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5519.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5519 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5519 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5519/>A surprisingly effective out-of-the-box char2char model on the E2E NLG Challenge dataset<span class=acl-fixed-case>E</span>2<span class=acl-fixed-case>E</span> <span class=acl-fixed-case>NLG</span> Challenge dataset</a></strong><br><a href=/people/s/shubham-agarwal/>Shubham Agarwal</a>
|
<a href=/people/m/marc-dymetman/>Marc Dymetman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5519><div class="card-body p-3 small">We train a char2char model on the E2E NLG Challenge data, by exploiting out-of-the-box the recently released tfseq2seq framework, using some of the standard options offered by this tool. With minimal effort, and in particular without delexicalization, <a href=https://en.wikipedia.org/wiki/Lexicalization>tokenization</a> or lowercasing, the obtained raw predictions, according to a small scale human evaluation, are excellent on the linguistic side and quite reasonable on the adequacy side, the primary downside being the possible omissions of semantic material. However, in a significant number of cases (more than 70 %), a perfect solution can be found in the top-20 predictions, indicating promising directions for solving the remaining issues.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5520.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5520 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5520 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5520/>Interaction Quality Estimation Using Long Short-Term Memories</a></strong><br><a href=/people/n/niklas-rach/>Niklas Rach</a>
|
<a href=/people/w/wolfgang-minker/>Wolfgang Minker</a>
|
<a href=/people/s/stefan-ultes/>Stefan Ultes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5520><div class="card-body p-3 small">For estimating the Interaction Quality (IQ) in Spoken Dialogue Systems (SDS), the dialogue history is of significant importance. Previous works included this information manually in the form of precomputed temporal features into the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification process</a>. Here, we employ a deep learning architecture based on Long Short-Term Memories (LSTM) to extract this information automatically from the data, thus estimating <a href=https://en.wikipedia.org/wiki/Intelligence_quotient>IQ</a> solely by using current exchange features. We show that it is thereby possible to achieve competitive results as in a scenario where manually optimized temporal features have been included.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5522.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5522 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5522 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5522" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5522/>Evaluating Natural Language Understanding Services for Conversational Question Answering Systems</a></strong><br><a href=/people/d/daniel-braun/>Daniel Braun</a>
|
<a href=/people/a/adrian-hernandez-mendez/>Adrian Hernandez Mendez</a>
|
<a href=/people/f/florian-matthes/>Florian Matthes</a>
|
<a href=/people/m/manfred-langen/>Manfred Langen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5522><div class="card-body p-3 small">Conversational interfaces recently gained a lot of attention. One of the reasons for the current hype is the fact that <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> (one particularly popular form of conversational interfaces) nowadays can be created without any programming knowledge, thanks to different toolkits and so-called Natural Language Understanding (NLU) services. While these NLU services are already widely used in both, industry and science, so far, they have not been analysed systematically. In this paper, we present a method to evaluate the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance of NLU services. Moreover, we present two new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, one consisting of annotated questions and one consisting of annotated questions with the corresponding answers. Based on these <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a>, we conduct an evaluation of some of the most popular NLU services. Thereby we want to enable both, researchers and companies to make more educated decisions about which service they should use.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5523.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5523 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5523 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5523" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5523/>The Role of Conversation Context for Sarcasm Detection in Online Interactions</a></strong><br><a href=/people/d/debanjan-ghosh/>Debanjan Ghosh</a>
|
<a href=/people/a/alexander-richard-fabbri/>Alexander Richard Fabbri</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5523><div class="card-body p-3 small">Computational models for sarcasm detection have often relied on the content of utterances in isolation. However, speaker&#8217;s sarcastic intent is not always obvious without additional context. Focusing on social media discussions, we investigate two issues : (1) does modeling of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversation context</a> help in sarcasm detection and (2) can we understand what part of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>conversation context</a> triggered the <a href=https://en.wikipedia.org/wiki/Sarcasm>sarcastic reply</a>. To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the sarcastic response. We show that the conditional LSTM network (Rocktschel et al. 2015) and LSTM networks with sentence level attention on context and response outperform the LSTM model that reads only the response. To address the second issue, we present a qualitative analysis of <a href=https://en.wikipedia.org/wiki/Attention>attention weights</a> produced by the LSTM models with <a href=https://en.wikipedia.org/wiki/Attention>attention</a> and discuss the results compared with human performance on the task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5524.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5524 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5524 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5524/>VOILA : An Optimised Dialogue System for Interactively Learning Visually-Grounded Word Meanings (Demonstration System)<span class=acl-fixed-case>VOILA</span>: An Optimised Dialogue System for Interactively Learning Visually-Grounded Word Meanings (Demonstration System)</a></strong><br><a href=/people/y/yanchao-yu/>Yanchao Yu</a>
|
<a href=/people/a/arash-eshghi/>Arash Eshghi</a>
|
<a href=/people/o/oliver-lemon/>Oliver Lemon</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5524><div class="card-body p-3 small">We present VOILA : an optimised, multi-modal dialogue agent for interactive learning of visually grounded word meanings from a human user. VOILA is : (1) able to learn new visual categories interactively from users from scratch ; (2) trained on real human-human dialogues in the same domain, and so is able to conduct natural spontaneous dialogue ; (3) optimised to find the most effective trade-off between the accuracy of the visual categories it learns and the cost it incurs to users. VOILA is deployed on Furhat, a human-like, multi-modal robot head with back-projection of the face, and a graphical virtual character.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5525.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5525 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5525 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5525/>The E2E Dataset : New Challenges For End-to-End Generation<span class=acl-fixed-case>E</span>2<span class=acl-fixed-case>E</span> Dataset: New Challenges For End-to-End Generation</a></strong><br><a href=/people/j/jekaterina-novikova/>Jekaterina Novikova</a>
|
<a href=/people/o/ondrej-dusek/>Ondřej Dušek</a>
|
<a href=/people/v/verena-rieser/>Verena Rieser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5525><div class="card-body p-3 small">This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges : (1) its human reference texts show more <a href=https://en.wikipedia.org/wiki/Lexicon>lexical richness</a> and <a href=https://en.wikipedia.org/wiki/Syntax>syntactic variation</a>, including <a href=https://en.wikipedia.org/wiki/Discourse>discourse phenomena</a> ; (2) generating from this set requires content selection. As such, learning from this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> promises more natural, varied and less template-like system utterances. We also establish a <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline</a> on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, which illustrates some of the difficulties associated with this <a href=https://en.wikipedia.org/wiki/Data>data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5526.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5526 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5526 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5526/>Frames : a corpus for adding <a href=https://en.wikipedia.org/wiki/Memory>memory</a> to goal-oriented dialogue systems<span class=acl-fixed-case>F</span>rames: a corpus for adding memory to goal-oriented dialogue systems</a></strong><br><a href=/people/l/layla-el-asri/>Layla El Asri</a>
|
<a href=/people/h/hannes-schulz/>Hannes Schulz</a>
|
<a href=/people/s/shikhar-kr-sarma/>Shikhar Sharma</a>
|
<a href=/people/j/jeremie-zumer/>Jeremie Zumer</a>
|
<a href=/people/j/justin-harris/>Justin Harris</a>
|
<a href=/people/e/emery-fine/>Emery Fine</a>
|
<a href=/people/r/rahul-mehrotra/>Rahul Mehrotra</a>
|
<a href=/people/k/kaheer-suleman/>Kaheer Suleman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5526><div class="card-body p-3 small">This paper proposes a new dataset, Frames, composed of 1369 human-human dialogues with an average of 15 turns per dialogue. This <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> contains goal-oriented dialogues between users who are given some constraints to book a trip and assistants who search a database to find appropriate trips. The users exhibit complex decision-making behaviour which involve comparing trips, exploring different options, and selecting among the trips that were discussed during the dialogue. To drive research on dialogue systems towards handling such <a href=https://en.wikipedia.org/wiki/Behavior>behaviour</a>, we have annotated and released the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> and we propose in this paper a task called frame tracking. This <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> consists of keeping track of different <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>semantic frames</a> throughout each dialogue. We propose a rule-based baseline and analyse the frame tracking task through this <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5527.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5527 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5527 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5527/>Towards a General, Continuous Model of Turn-taking in Spoken Dialogue using LSTM Recurrent Neural Networks<span class=acl-fixed-case>LSTM</span> Recurrent Neural Networks</a></strong><br><a href=/people/g/gabriel-skantze/>Gabriel Skantze</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5527><div class="card-body p-3 small">Previous models of turn-taking have mostly been trained for specific turn-taking decisions, such as discriminating between turn shifts and turn retention in pauses. In this paper, we present a predictive, continuous model of turn-taking using Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN). The <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is trained on <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-human dialogue data</a> to predict upcoming <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech activity</a> in a future time window. We show how this general <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be applied to two different <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>tasks</a> that <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> was not specifically trained for. First, to predict whether a turn-shift will occur or not in pauses, where the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves a better performance than human observers, and better than results achieved with more traditional models. Second, to make a prediction at speech onset whether the utterance will be a short backchannel or a longer utterance. Finally, we show how the hidden layer in the <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> can be used as a <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>feature vector</a> for turn-taking decisions in a human-robot interaction scenario.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5528.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5528 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5528 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5528/>Neural-based Natural Language Generation in Dialogue using RNN Encoder-Decoder with Semantic Aggregation<span class=acl-fixed-case>RNN</span> Encoder-Decoder with Semantic Aggregation</a></strong><br><a href=/people/v/van-khanh-tran/>Van-Khanh Tran</a>
|
<a href=/people/m/minh-le-nguyen/>Le-Minh Nguyen</a>
|
<a href=/people/s/satoshi-tojo/>Satoshi Tojo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5528><div class="card-body p-3 small">Natural language generation (NLG) is an important component in spoken dialogue systems. This paper presents a model called Encoder-Aggregator-Decoder which is an extension of an Recurrent Neural Network based Encoder-Decoder architecture. The proposed Semantic Aggregator consists of two components : an Aligner and a Refiner. The Aligner is a conventional <a href=https://en.wikipedia.org/wiki/Attention>attention</a> calculated over the encoded input information, while the Refiner is another <a href=https://en.wikipedia.org/wiki/Attention>attention or gating mechanism</a> stacked over the attentive Aligner in order to further select and aggregate the semantic elements. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> can be jointly trained both sentence planning and surface realization to produce <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language utterances</a>. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was extensively assessed on four different NLG domains, in which the experimental results showed that the proposed generator consistently outperforms the previous methods on all the NLG domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5529.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5529 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5529 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5529/>Beyond On-hold Messages : Conversational Time-buying in Task-oriented Dialogue</a></strong><br><a href=/people/m/m-soledad-lopez-gambino/>Soledad López Gambino</a>
|
<a href=/people/s/sina-zarriess/>Sina Zarrieß</a>
|
<a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5529><div class="card-body p-3 small">A common convention in <a href=https://en.wikipedia.org/wiki/Graphical_user_interface>graphical user interfaces</a> is to indicate a <a href=https://en.wikipedia.org/wiki/Wait_state>wait state</a>, for example while a program is preparing a response, through a changed cursor state or a <a href=https://en.wikipedia.org/wiki/Progress_bar>progress bar</a>. What should the analogue be in a spoken conversational system? To address this question, we set up an experiment in which a human information provider (IP) was given their information only in a delayed and incremental manner, which systematically created situations where the IP had the turn but could not provide task-related information. Our data analysis shows that 1) IPs bridge the gap until they can provide information by re-purposing a whole variety of task- and grounding-related communicative actions (e.g. echoing the user&#8217;s request, <a href=https://en.wikipedia.org/wiki/Signaling_(telecommunications)>signaling understanding</a>, asserting partially relevant information), rather than being silent or explicitly asking for time (e.g. please wait), and that 2) IPs combined these actions productively to ensure an ongoing conversation. These results, we argue, indicate that natural conversational interfaces should also be able to manage their time flexibly using a variety of conversational resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5530.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5530 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5530 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5530/>Neural-based Context Representation Learning for Dialog Act Classification</a></strong><br><a href=/people/d/daniel-ortega/>Daniel Ortega</a>
|
<a href=/people/n/ngoc-thang-vu/>Ngoc Thang Vu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5530><div class="card-body p-3 small">We explore context representation learning methods in neural-based models for dialog act classification. We propose and compare extensively different methods which combine <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural network architectures</a> and attention mechanisms (AMs) at different context levels. Our experimental results on two benchmark datasets show consistent improvements compared to the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> without contextual information and reveal that the most suitable AM in the <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> depends on the nature of the dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5531.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5531 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5531 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5531/>Predicting Success in Goal-Driven Human-Human Dialogues</a></strong><br><a href=/people/m/michael-noseworthy/>Michael Noseworthy</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5531><div class="card-body p-3 small">In goal-driven dialogue systems, success is often defined based on a structured definition of the goal. This requires that the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> be constrained to handle a specific class of goals and that there be a mechanism to measure success with respect to that goal. However, in many human-human dialogues the diversity of goals makes it infeasible to define success in such a way. To address this scenario, we consider the task of automatically predicting success in goal-driven human-human dialogues using only the information communicated between participants in the form of text. We build a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> from <a href=https://en.wikipedia.org/wiki/Stackoverflow>stackoverflow.com</a> which consists of exchanges between two users in the technical domain where ground-truth success labels are available. We then propose a turn-based hierarchical neural network model that can be used to predict success without requiring a structured goal definition. We show this model outperforms <a href=https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making>rule-based heuristics</a> and other baselines as it is able to detect patterns over the course of a dialogue and capture notions such as <a href=https://en.wikipedia.org/wiki/Gratitude>gratitude</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5532.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5532 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5532 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5532/>Generating and Evaluating Summaries for Partial Email Threads : Conversational Bayesian Surprise and Silver Standards<span class=acl-fixed-case>B</span>ayesian Surprise and Silver Standards</a></strong><br><a href=/people/j/jordon-johnson/>Jordon Johnson</a>
|
<a href=/people/v/vaden-masrani/>Vaden Masrani</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/r/raymond-ng/>Raymond Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5532><div class="card-body p-3 small">We define and motivate the problem of summarizing partial email threads. This problem introduces the challenge of generating reference summaries for partial threads when human annotation is only available for the threads as a whole, particularly when the human-selected sentences are not uniformly distributed within the threads. We propose an oracular algorithm for generating these reference summaries with arbitrary length, and we are making the resulting <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> publicly available. In addition, we apply a recent <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised method</a> based on Bayesian Surprise that incorporates background knowledge into partial thread summarization, extend it with conversational features, and modify the mechanism by which it handles redundancy. Experiments with our method indicate improved performance over the baseline for shorter partial threads ; and our results suggest that the potential benefits of background knowledge to partial thread summarization should be further investigated with larger datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5533.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5533 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5533 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5533/>Enabling robust and fluid spoken dialogue with cognitively impaired users</a></strong><br><a href=/people/r/ramin-yaghoubzadeh/>Ramin Yaghoubzadeh</a>
|
<a href=/people/s/stefan-kopp/>Stefan Kopp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5533><div class="card-body p-3 small">We present the flexdiam dialogue management architecture, which was developed in a series of projects dedicated to tailoring spoken interaction to the needs of users with cognitive impairments in an everyday assistive domain, using a multimodal front-end. This hybrid DM architecture affords incremental processing of uncertain input, a flexible, mixed-initiative information grounding process that can be adapted to users&#8217; cognitive capacities and interactive idiosyncrasies, and generic mechanisms that foster transitions in the joint discourse state that are understandable and controllable by those users, in order to effect a robust interaction for users with varying capacities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5534.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5534 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5534 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5534/>Adversarial evaluation for open-domain dialogue generation</a></strong><br><a href=/people/e/elia-bruni/>Elia Bruni</a>
|
<a href=/people/r/raquel-fernandez/>Raquel Fernández</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5534><div class="card-body p-3 small">We investigate the potential of adversarial evaluation methods for open-domain dialogue generation systems, comparing the performance of a discriminative agent to that of humans on the same task. Our results show that the task is hard, both for automated models and humans, but that a discriminative agent can learn patterns that lead to above-chance performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5535.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5535 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5535 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5535/>Exploring Joint Neural Model for Sentence Level Discourse Parsing and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a></a></strong><br><a href=/people/b/bita-nejat/>Bita Nejat</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/r/raymond-ng/>Raymond Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5535><div class="card-body p-3 small">Discourse Parsing and <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a> are two fundamental tasks in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a> that have been shown to be mutually beneficial. In this work, we design and compare two Neural Based models for jointly learning both <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. In the proposed approach, we first create a <a href=https://en.wikipedia.org/wiki/Vector_graphics>vector representation</a> for all the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>text segments</a> in the input sentence. Next, we apply three different Recursive Neural Net models : one for discourse structure prediction, one for discourse relation prediction and one for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a>. Finally, we combine these Neural Nets in two different joint models : <a href=https://en.wikipedia.org/wiki/Computer_multitasking>Multi-tasking</a> and Pre-training. Our results on two standard corpora indicate that both methods result in improvements in each task but <a href=https://en.wikipedia.org/wiki/Multi-tasking>Multi-tasking</a> has a bigger impact than Pre-training. Specifically for Discourse Parsing, we see improvements in the prediction of the set of contrastive relations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5538.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5538 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5538 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5538/>Finding Structure in <a href=https://en.wikipedia.org/wiki/Figurative_language>Figurative Language</a> : Metaphor Detection with Topic-based Frames</a></strong><br><a href=/people/h/hyeju-jang/>Hyeju Jang</a>
|
<a href=/people/k/keith-maki/>Keith Maki</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a>
|
<a href=/people/c/carolyn-rose/>Carolyn Rosé</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5538><div class="card-body p-3 small">In this paper, we present a novel and highly effective method for <a href=https://en.wikipedia.org/wiki/Inductive_reasoning>induction</a> and application of metaphor frame templates as a step toward detecting metaphor in extended discourse. We infer implicit facets of a given metaphor frame using a semi-supervised bootstrapping approach on an unlabeled corpus. Our model applies this frame facet information to metaphor detection, and achieves the state-of-the-art performance on a social media dataset when building upon other proven <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> in a nonlinear machine learning model. In addition, we illustrate the mechanism through which the frame and topic information enable the more accurate metaphor detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5539.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5539 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5539 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5539/>Using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning</a> to Model Incrementality in a Fast-Paced Dialogue Game</a></strong><br><a href=/people/r/ramesh-manuvinakurike/>Ramesh Manuvinakurike</a>
|
<a href=/people/d/david-devault/>David DeVault</a>
|
<a href=/people/k/kallirroi-georgila/>Kallirroi Georgila</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5539><div class="card-body p-3 small">We apply <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>Reinforcement Learning (RL)</a> to the problem of incremental dialogue policy learning in the context of a fast-paced dialogue game. We compare the <a href=https://en.wikipedia.org/wiki/Policy>policy</a> learned by RL with a high-performance baseline policy which has been shown to perform very efficiently (nearly as well as humans) in this dialogue game. The RL policy outperforms the baseline policy in offline simulations (based on real user data). We provide a detailed comparison of the RL policy and the baseline policy, including information about how much effort and time it took to develop each one of them. We also highlight the cases where the RL policy performs better, and show that understanding the RL policy can provide valuable insights which can inform the creation of an even better rule-based policy.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5540.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5540 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5540 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-5540.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-5540/>Inferring Narrative Causality between Event Pairs in Films</a></strong><br><a href=/people/z/zhichao-hu/>Zhichao Hu</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5540><div class="card-body p-3 small">To understand <a href=https://en.wikipedia.org/wiki/Narrative>narrative</a>, humans draw inferences about the underlying relations between <a href=https://en.wikipedia.org/wiki/Narrative>narrative events</a>. Cognitive theories of narrative understanding define these inferences as four different types of <a href=https://en.wikipedia.org/wiki/Causality>causality</a>, that include pairs of events A, B where A physically causes B (X drop, X break), to pairs of events where A causes emotional state B (Y saw X, Y felt fear). Previous work on learning narrative relations from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a> has either focused on strict physical causality, or has been vague about what relation is being learned. This paper learns pairs of causal events from a corpus of film scene descriptions which are action rich and tend to be told in chronological order. We show that event pairs induced using our methods are of high quality and are judged to have a stronger <a href=https://en.wikipedia.org/wiki/Causality>causal relation</a> than event pairs from Rel-Grams.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5542.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5542 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5542 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5542/>Information Navigation System with Discovering User Interests</a></strong><br><a href=/people/k/koichiro-yoshino/>Koichiro Yoshino</a>
|
<a href=/people/y/yu-suzuki/>Yu Suzuki</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5542><div class="card-body p-3 small">We demonstrate an information navigation system for sightseeing domains that has a <a href=https://en.wikipedia.org/wiki/User_interface>dialogue interface</a> for discovering user interests for <a href=https://en.wikipedia.org/wiki/Tourism>tourist activities</a>. The system discovers interests of a user with focus detection on user utterances, and proactively presents related information to the discovered user interest. A partially observable Markov decision process (POMDP)-based dialogue manager, which is extended with user focus states, controls the behavior of the system to provide information with several dialogue acts for providing information. We transferred the belief-update function and the policy of the manager from other <a href=https://en.wikipedia.org/wiki/System>system</a> trained on a different domain to show the generality of defined dialogue acts for our information navigation system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5543.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5543 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5543 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-5543.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-5543/>Modelling Protagonist Goals and Desires in First-Person Narrative</a></strong><br><a href=/people/e/elahe-rahimtoroghi/>Elahe Rahimtoroghi</a>
|
<a href=/people/j/jiaqi-wu/>Jiaqi Wu</a>
|
<a href=/people/r/ruimin-wang/>Ruimin Wang</a>
|
<a href=/people/p/pranav-anand/>Pranav Anand</a>
|
<a href=/people/m/marilyn-walker/>Marilyn Walker</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5543><div class="card-body p-3 small">Many genres of natural language text are narratively structured, a testament to our predilection for organizing our experiences as narratives. There is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes. However, to date, there has been limited work on <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> for this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. We introduce a new dataset, DesireDB, which includes gold-standard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context. We report experiments on tracking desire fulfillment using different methods, and show that LSTM Skip-Thought model achieves <a href=https://en.wikipedia.org/wiki/F-measure>F-measure</a> of 0.7 on our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5544.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5544 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5544 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5544/>SHIHbot : A Facebook chatbot for Sexual Health Information on HIV / AIDS<span class=acl-fixed-case>SHIH</span>bot: A <span class=acl-fixed-case>F</span>acebook chatbot for Sexual Health Information on <span class=acl-fixed-case>HIV</span>/<span class=acl-fixed-case>AIDS</span></a></strong><br><a href=/people/j/jacqueline-brixey/>Jacqueline Brixey</a>
|
<a href=/people/r/rens-hoegen/>Rens Hoegen</a>
|
<a href=/people/w/wei-lan/>Wei Lan</a>
|
<a href=/people/j/joshua-rusow/>Joshua Rusow</a>
|
<a href=/people/k/karan-singla/>Karan Singla</a>
|
<a href=/people/x/xusen-yin/>Xusen Yin</a>
|
<a href=/people/r/ron-artstein/>Ron Artstein</a>
|
<a href=/people/a/anton-leuski/>Anton Leuski</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5544><div class="card-body p-3 small">We present the implementation of an autonomous chatbot, SHIHbot, deployed on <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a>, which answers a wide variety of sexual health questions on HIV / AIDS. The chatbot&#8217;s response database is com-piled from professional medical and public health resources in order to provide reliable information to users. The system&#8217;s backend is NPCEditor, a response selection platform trained on linked questions and answers ; to our knowledge this is the first retrieval-based chatbot deployed on a large public social network.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5545.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5545 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5545 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5545/>How Would You Say It? Eliciting Lexically Diverse Dialogue for Supervised Semantic Parsing</a></strong><br><a href=/people/a/abhilasha-ravichander/>Abhilasha Ravichander</a>
|
<a href=/people/t/thomas-manzini/>Thomas Manzini</a>
|
<a href=/people/m/matthias-grabmair/>Matthias Grabmair</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/j/jonathan-francis/>Jonathan Francis</a>
|
<a href=/people/e/eric-nyberg/>Eric Nyberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5545><div class="card-body p-3 small">Building dialogue interfaces for real-world scenarios often entails training semantic parsers starting from zero examples. How can we build <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> that better capture the variety of ways users might phrase their queries, and what queries are actually realistic? Wang et al. (2015) proposed a method to build semantic parsing datasets by generating canonical utterances using a <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> and having crowdworkers paraphrase them into natural wording. A limitation of this approach is that it induces bias towards using similar language as the canonical utterances. In this work, we present a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> that elicits meaningful and lexically diverse queries from users for semantic parsing tasks. Starting from a seed lexicon and a <a href=https://en.wikipedia.org/wiki/Generative_grammar>generative grammar</a>, we pair logical forms with mixed text-image representations and ask crowdworkers to paraphrase and confirm the plausibility of the queries that they generated. We use this method to build a semantic parsing dataset from scratch for a dialog agent in a smart-home simulation. We find evidence that this dataset, which we have named SmartHome, is demonstrably more lexically diverse and difficult to parse than existing domain-specific semantic parsing datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5547.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5547 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5547 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5547/>A data-driven model of explanations for a <a href=https://en.wikipedia.org/wiki/Chatbot>chatbot</a> that helps to practice conversation in a foreign language</a></strong><br><a href=/people/s/sviatlana-hohn/>Sviatlana Höhn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5547><div class="card-body p-3 small">This article describes a <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> of other-initiated self-repair for a <a href=https://en.wikipedia.org/wiki/Chatbot>chatbot</a> that helps to practice conversation in a foreign language. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> was developed using a <a href=https://en.wikipedia.org/wiki/Instant_messaging>corpus of instant messaging conversations</a> between <a href=https://en.wikipedia.org/wiki/German_language>German native and non-native speakers</a>. Conversation Analysis helped to create <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> from a small number of examples. The <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> has been validated in an AIML-based chatbot. Unlike typical retrieval-based dialogue systems, the explanations are generated at run-time from a linguistic database.</div></div></div><hr><div id=w17-56><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-56.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-56/>Proceedings of the First Workshop on Curation and Applications of Parallel and Comparable Corpora</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5600/>Proceedings of the First Workshop on Curation and Applications of Parallel and Comparable Corpora</a></strong><br><a href=/people/h/haithem-afli/>Haithem Afli</a>
|
<a href=/people/c/chao-hong-liu/>Chao-Hong Liu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5601.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5601 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5601 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5601/>Building a Better Bitext for Structurally Different Languages through Self-training</a></strong><br><a href=/people/j/jungyeul-park/>Jungyeul Park</a>
|
<a href=/people/l/loic-dugast/>Loïc Dugast</a>
|
<a href=/people/j/jeen-pyo-hong/>Jeen-Pyo Hong</a>
|
<a href=/people/c/chang-uk-shin/>Chang-Uk Shin</a>
|
<a href=/people/j/jeong-won-cha/>Jeong-Won Cha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5601><div class="card-body p-3 small">We propose a novel method to bootstrap the construction of parallel corpora for new pairs of structurally different languages. We do so by combining the use of a <a href=https://en.wikipedia.org/wiki/Pivot_language>pivot language</a> and <a href=https://en.wikipedia.org/wiki/Self-help>self-training</a>. A <a href=https://en.wikipedia.org/wiki/Pivot_language>pivot language</a> enables the use of existing translation models to bootstrap the <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a> and a self-training procedure enables to achieve better <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a>, both at the document and sentence level. We also propose several evaluation methods for the resulting <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5603.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5603 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5603 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5603/>Learning Phrase Embeddings from <a href=https://en.wikipedia.org/wiki/Paraphrase>Paraphrases</a> with GRUs<span class=acl-fixed-case>GRU</span>s</a></strong><br><a href=/people/z/zhihao-zhou/>Zhihao Zhou</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5603><div class="card-body p-3 small">Learning phrase representations has been widely explored in many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing tasks</a> (e.g., <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>Sentiment Analysis</a>, <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>) and has shown promising improvements. Previous studies either learn non-compositional phrase representations with general word embedding learning techniques or learn compositional phrase representations based on syntactic structures, which either require huge amounts of human annotations or can not be easily generalized to all phrases. In this work, we propose to take advantage of large-scaled paraphrase database and present a pairwise-GRU framework to generate compositional phrase representations. Our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> can be re-used to generate <a href=https://en.wikipedia.org/wiki/Representation_(systemics)>representations</a> for any phrases. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> achieves state-of-the-art results on several phrase similarity tasks.</div></div></div><hr><div id=w17-57><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-57.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-57/>Proceedings of the 4th Workshop on Asian Translation (WAT2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5700.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5700/>Proceedings of the 4th Workshop on <span class=acl-fixed-case>A</span>sian Translation (<span class=acl-fixed-case>WAT</span>2017)</a></strong><br><a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/i/isao-goto/>Isao Goto</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5702.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5702 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5702 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5702/>Controlling Target Features in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> via Prefix Constraints</a></strong><br><a href=/people/s/shunsuke-takeno/>Shunsuke Takeno</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a>
|
<a href=/people/k/kazuhide-yamamoto/>Kazuhide Yamamoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5702><div class="card-body p-3 small">We propose prefix constraints, a novel method to enforce constraints on target sentences in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. It places a sequence of special tokens at the beginning of target sentence (target prefix), while side constraints places a special token at the end of source sentence (source suffix). Prefix constraints can be predicted from source sentence jointly with target sentence, while side constraints (Sennrich et al., 2016) must be provided by the user or predicted by some other methods. In both <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>methods</a>, special tokens are designed to encode arbitrary features on target-side or metatextual information. We show that prefix constraints are more flexible than side constraints and can be used to control the behavior of neural machine translation, in terms of output length, bidirectional decoding, domain adaptation, and unaligned target word generation.<i>prefix constraints</i>, a novel method to enforce constraints on\n target sentences in neural machine translation. It places a sequence of\n special tokens at the beginning of target sentence (target prefix), while\n side constraints places a special token at the end of source sentence\n (source suffix). Prefix constraints can be predicted from source sentence\n jointly with target sentence, while side constraints (Sennrich et al., 2016) must be provided by\n the user or predicted by some other methods. In both methods, special\n tokens are designed to encode arbitrary features on target-side or\n metatextual information. We show that prefix constraints are more flexible\n than side constraints and can be used to control the behavior of neural\n machine translation, in terms of output length, bidirectional decoding,\n domain adaptation, and unaligned target word generation.\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5703.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5703 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5703 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5703/>Improving Japanese-to-English Neural Machine Translation by Paraphrasing the Target Language<span class=acl-fixed-case>J</span>apanese-to-<span class=acl-fixed-case>E</span>nglish Neural Machine Translation by Paraphrasing the Target Language</a></strong><br><a href=/people/y/yuuki-sekizawa/>Yuuki Sekizawa</a>
|
<a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5703><div class="card-body p-3 small">Neural machine translation (NMT) produces sentences that are more fluent than those produced by <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation (SMT)</a>. However, NMT has a very high <a href=https://en.wikipedia.org/wiki/Computational_cost>computational cost</a> because of the high dimensionality of the output layer. Generally, NMT restricts the size of vocabulary, which results in infrequent words being treated as out-of-vocabulary (OOV) and degrades the performance of the <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. In evaluation, we achieved a statistically significant BLEU score improvement of 0.55-0.77 over the baselines including the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art method</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5707.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5707 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5707 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5707/>XMU Neural Machine Translation Systems for WAT 2017<span class=acl-fixed-case>XMU</span> Neural Machine Translation Systems for <span class=acl-fixed-case>WAT</span> 2017</a></strong><br><a href=/people/b/boli-wang/>Boli Wang</a>
|
<a href=/people/z/zhixing-tan/>Zhixing Tan</a>
|
<a href=/people/j/jinming-hu/>Jinming Hu</a>
|
<a href=/people/y/yidong-chen/>Yidong Chen</a>
|
<a href=/people/x/xiaodong-shi/>Xiaodong Shi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5707><div class="card-body p-3 small">This paper describes the Neural Machine Translation systems of Xiamen University for the shared translation tasks of WAT 2017. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> are based on the Encoder-Decoder framework with <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. We participated in three subtasks. We experimented subword segmentation, <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>synthetic training data</a> and model ensembling. Experiments show that all these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can give substantial improvements.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5708.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5708 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5708 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5708" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5708/>A Bag of Useful Tricks for Practical <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> : Embedding Layer Initialization and Large Batch Size</a></strong><br><a href=/people/m/masato-neishi/>Masato Neishi</a>
|
<a href=/people/j/jin-sakuma/>Jin Sakuma</a>
|
<a href=/people/s/satoshi-tohda/>Satoshi Tohda</a>
|
<a href=/people/s/shonosuke-ishiwatari/>Shonosuke Ishiwatari</a>
|
<a href=/people/n/naoki-yoshinaga/>Naoki Yoshinaga</a>
|
<a href=/people/m/masashi-toyoda/>Masashi Toyoda</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5708><div class="card-body p-3 small">In this paper, we describe the team UT-IIS&#8217;s system and results for the WAT 2017 translation tasks. We further investigated several tricks including a novel technique for initializing embedding layers using only the <a href=https://en.wikipedia.org/wiki/Parallel_corpus>parallel corpus</a>, which increased the BLEU score by 1.28, found a practical large batch size of 256, and gained insights regarding <a href=https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)>hyperparameter settings</a>. Ultimately, our <a href=https://en.wikipedia.org/wiki/System>system</a> obtained a better result than the state-of-the-art <a href=https://en.wikipedia.org/wiki/System>system</a> of WAT 2016. Our code is available on.<url>https://github.com/nem6ishi/wat17</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5709.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5709 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5709 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5709/>Patent NMT integrated with Large Vocabulary Phrase Translation by SMT at WAT 2017<span class=acl-fixed-case>NMT</span> integrated with Large Vocabulary Phrase Translation by <span class=acl-fixed-case>SMT</span> at <span class=acl-fixed-case>WAT</span> 2017</a></strong><br><a href=/people/z/zi-long/>Zi Long</a>
|
<a href=/people/r/ryuichiro-kimura/>Ryuichiro Kimura</a>
|
<a href=/people/t/takehito-utsuro/>Takehito Utsuro</a>
|
<a href=/people/t/tomoharu-mitsuhashi/>Tomoharu Mitsuhashi</a>
|
<a href=/people/m/mikio-yamamoto/>Mikio Yamamoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5709><div class="card-body p-3 small">Neural machine translation (NMT) can not handle a larger vocabulary because the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training complexity</a> and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many <a href=https://en.wikipedia.org/wiki/Jargon>technical terms</a> that are observed infrequently. Long et al. (2017) proposed to select phrases that contain out-of-vocabulary words using the statistical approach of branching entropy. The selected phrases are then replaced with tokens during training and post-translated by the phrase translation table of SMT. In this paper, we apply the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> proposed by Long et al. (2017) to the WAT 2017 Japanese-Chinese and Japanese-English patent datasets. Evaluation on Japanese-to-Chinese, Chinese-to-Japanese, Japanese-to-English and English-to-Japanese patent sentence translation proved the effectiveness of phrases selected with branching entropy, where the NMT model of Long et al. (2017) achieves a substantial improvement over a baseline NMT model without the technique proposed by Long et al.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5712.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5712 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5712 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5712/>A Simple and Strong Baseline : NAIST-NICT Neural Machine Translation System for WAT2017 English-Japanese Translation Task<span class=acl-fixed-case>NAIST</span>-<span class=acl-fixed-case>NICT</span> Neural Machine Translation System for <span class=acl-fixed-case>WAT</span>2017 <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>J</span>apanese Translation Task</a></strong><br><a href=/people/y/yusuke-oda/>Yusuke Oda</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5712><div class="card-body p-3 small">This paper describes the details about the NAIST-NICT machine translation system for WAT2017 English-Japanese Scientific Paper Translation Task. The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our <a href=https://en.wikipedia.org/wiki/System>system</a> achieves higher translation accuracy than any systems submitted previous campaigns despite simple <a href=https://en.wikipedia.org/wiki/Conceptual_model>model architecture</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5713.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5713 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5713 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5713/>Comparison of <a href=https://en.wikipedia.org/wiki/Simultaneous_multithreading>SMT</a> and <a href=https://en.wikipedia.org/wiki/Simultaneous_multithreading>NMT</a> trained with large Patent Corpora : Japio at WAT2017<span class=acl-fixed-case>SMT</span> and <span class=acl-fixed-case>NMT</span> trained with large Patent Corpora: <span class=acl-fixed-case>J</span>apio at <span class=acl-fixed-case>WAT</span>2017</a></strong><br><a href=/people/s/satoshi-kinoshita/>Satoshi Kinoshita</a>
|
<a href=/people/t/tadaaki-oshio/>Tadaaki Oshio</a>
|
<a href=/people/t/tomoharu-mitsuhashi/>Tomoharu Mitsuhashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5713><div class="card-body p-3 small">Japio participates in patent subtasks (JPC-EJ / JE / CJ / KJ) with phrase-based statistical machine translation (SMT) and neural machine translation (NMT) systems which are trained with its own patent corpora in addition to the subtask corpora provided by organizers of WAT2017. In EJ and CJ subtasks, SMT and NMT systems whose sizes of training corpora are about 50 million and 10 million sentence pairs respectively achieved comparable scores for automatic evaluations, but NMT systems were superior to SMT systems for both official and in-house human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5714.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5714 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5714 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-5714" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-5714/>Kyoto University Participation to WAT 2017<span class=acl-fixed-case>K</span>yoto <span class=acl-fixed-case>U</span>niversity Participation to <span class=acl-fixed-case>WAT</span> 2017</a></strong><br><a href=/people/f/fabien-cromieres/>Fabien Cromieres</a>
|
<a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5714><div class="card-body p-3 small">We describe here our approaches and results on the WAT 2017 shared translation tasks. Following our good results with <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> in the previous shared task, we continue this approach this year, with incremental improvements in <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and training methods. We focused on the ASPEC dataset and could improve the state-of-the-art results for Chinese-to-Japanese and Japanese-to-Chinese translations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5715.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5715 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5715 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5715/>CUNI NMT System for WAT 2017 Translation Tasks<span class=acl-fixed-case>CUNI</span> <span class=acl-fixed-case>NMT</span> System for <span class=acl-fixed-case>WAT</span> 2017 Translation Tasks</a></strong><br><a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/d/dusan-varis/>Dušan Variš</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5715><div class="card-body p-3 small">The paper presents this year&#8217;s CUNI submissions to the WAT 2017 Translation Task focusing on the Japanese-English translation, namely Scientific papers subtask, Patents subtask and Newswire subtask. We compare two neural network architectures, the standard sequence-to-sequence with attention (Seq2Seq) and an architecture using convolutional sentence encoder (FBConv2Seq), both implemented in the NMT framework Neural Monkey that we currently participate in developing. We also compare various types of preprocessing of the source <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>Japanese sentences</a> and their impact on the overall results. Furthermore, we include the results of our experiments with out-of-domain data obtained by combining the corpora provided for each subtask.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5716.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5716 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5716 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5716/>Tokyo Metropolitan University Neural Machine Translation System for WAT 2017<span class=acl-fixed-case>T</span>okyo Metropolitan University Neural Machine Translation System for <span class=acl-fixed-case>WAT</span> 2017</a></strong><br><a href=/people/y/yukio-matsumura/>Yukio Matsumura</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5716><div class="card-body p-3 small">In this paper, we describe our neural machine translation (NMT) system, which is based on the attention-based NMT and uses long short-term memories (LSTM) as RNN. We implemented <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> and ensemble decoding in the NMT system. The <a href=https://en.wikipedia.org/wiki/System>system</a> was tested on the 4th Workshop on Asian Translation (WAT 2017) shared tasks. In our experiments, we participated in the scientific paper subtasks and attempted Japanese-English, English-Japanese, and Japanese-Chinese translation tasks. The experimental results showed that implementation of <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a> and ensemble decoding can effectively improve the translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5717.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5717 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5717 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5717/>Comparing Recurrent and Convolutional Architectures for English-Hindi Neural Machine Translation<span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>H</span>indi Neural Machine Translation</a></strong><br><a href=/people/s/sandhya-singh/>Sandhya Singh</a>
|
<a href=/people/r/ritesh-panjwani/>Ritesh Panjwani</a>
|
<a href=/people/a/anoop-kunchukuttan/>Anoop Kunchukuttan</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharyya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5717><div class="card-body p-3 small">In this paper, we empirically compare the two encoder-decoder neural machine translation architectures : convolutional sequence to sequence model (ConvS2S) and recurrent sequence to sequence model (RNNS2S) for English-Hindi language pair as part of IIT Bombay&#8217;s submission to WAT2017 shared task. We report the results for both English-Hindi and Hindi-English direction of language pair.</div></div></div><hr><div id=w17-58><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-58.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-58/>Proceedings of the International Workshop on Digital Disease Detection using Social Media 2017 (DDDSM-2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5800/>Proceedings of the International Workshop on Digital Disease Detection using Social Media 2017 (<span class=acl-fixed-case>DDDSM</span>-2017)</a></strong><br><a href=/people/j/jitendra-jonnagaddala/>Jitendra Jonnagaddala</a>
|
<a href=/people/h/hong-jie-dai/>Hong-Jie Dai</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5801.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5801 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5801 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5801/>Automatic detection of stance towards vaccination in online discussion forums</a></strong><br><a href=/people/m/maria-skeppstedt/>Maria Skeppstedt</a>
|
<a href=/people/a/andreas-kerren/>Andreas Kerren</a>
|
<a href=/people/m/manfred-stede/>Manfred Stede</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5801><div class="card-body p-3 small">A classifier for automatic detection of stance towards vaccination in <a href=https://en.wikipedia.org/wiki/Internet_forum>online forums</a> was trained and evaluated. Debate posts from six discussion threads on the British parental website Mumsnet were manually annotated for stance &#8216;against&#8217; or &#8216;for&#8217; vaccination, or as &#8216;undecided&#8217;. A <a href=https://en.wikipedia.org/wiki/Support_vector_machine>support vector machine</a>, trained to detect the three classes, achieved a macro F-score of 0.44, while a macro F-score of 0.62 was obtained by the same type of classifier on the binary classification task of distinguishing stance &#8216;against&#8217; vaccination from stance &#8216;for&#8217; vaccination. These results show that vaccine stance detection in <a href=https://en.wikipedia.org/wiki/Internet_forum>online forums</a> is a difficult task, at least for the type of <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> investigated and for the relatively small training corpus that was used. Future work will therefore include an expansion of the training data and an evaluation of other types of <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifiers</a> and <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5802.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5802 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5802 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5802/>Analysing the Causes of <a href=https://en.wikipedia.org/wiki/Depression_(mood)>Depressed Mood</a> from Depression Vulnerable Individuals</a></strong><br><a href=/people/n/noor-fazilla-abd-yusof/>Noor Fazilla Abd Yusof</a>
|
<a href=/people/c/chenghua-lin/>Chenghua Lin</a>
|
<a href=/people/f/frank-guerin/>Frank Guerin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5802><div class="card-body p-3 small">We develop a <a href=https://en.wikipedia.org/wiki/Computational_model>computational model</a> to discover the potential causes of depression by analysing the topics in a <a href=https://en.wikipedia.org/wiki/User-generated_content>usergenerated text</a>. We show the most prominent causes, and how these <a href=https://en.wikipedia.org/wiki/Causality>causes</a> evolve over time. Also, we highlight the differences in causes between students with low and high neuroticism. Our studies demonstrate that the topics reveal valuable clues about the causes contributing to <a href=https://en.wikipedia.org/wiki/Depression_(mood)>depressed mood</a>. Identifying causes can have a significant impact on improving the quality of depression care ; thereby providing greater insights into a patient&#8217;s state for pertinent treatment recommendations. Hence, this study significantly expands the ability to discover the potential factors that trigger depression, making it possible to increase the efficiency of depression treatment.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5803.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5803 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5803 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5803/>Multivariate Linear Regression of Symptoms-related Tweets for Infectious Gastroenteritis Scale Estimation</a></strong><br><a href=/people/r/ryo-takeuchi/>Ryo Takeuchi</a>
|
<a href=/people/h/hayate-iso/>Hayate Iso</a>
|
<a href=/people/k/kaoru-ito/>Kaoru Ito</a>
|
<a href=/people/s/shoko-wakamiya/>Shoko Wakamiya</a>
|
<a href=/people/e/eiji-aramaki/>Eiji Aramaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5803><div class="card-body p-3 small">To date, various Twitter-based event detection systems have been proposed. Most of their targets, however, share common characteristics. They are seasonal or global events such as <a href=https://en.wikipedia.org/wiki/Earthquake>earthquakes</a> and <a href=https://en.wikipedia.org/wiki/Influenza_pandemic>flu pandemics</a>. In contrast, this study targets unseasonal and local disease events. Our system investigates the frequencies of disease-related words such as <a href=https://en.wikipedia.org/wiki/Nausea>nausea</a>, chill, and <a href=https://en.wikipedia.org/wiki/Diarrhea>diarrhea</a> and estimates the number of patients using regression of these word frequencies. Experiments conducted using Japanese 47 areas from January 2017 to April 2017 revealed that the detection of small and unseasonal event is extremely difficult (overall performance : 0.13). However, we found that the event scale and the detection performance show high correlation in the specified cases (in the phase of patient increasing or decreasing). The results also suggest that when 150 and more patients appear in a high population area, we can expect that our social sensors detect this <a href=https://en.wikipedia.org/wiki/Outbreak>outbreak</a>. Based on these results, we can infer that social sensors can reliably detect unseasonal and local disease events under certain conditions, just as they can for seasonal or global events.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5805.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5805 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5805 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5805/>Using a Recurrent Neural Network Model for Classification of Tweets Conveyed Influenza-related Information</a></strong><br><a href=/people/c/chen-kai-wang/>Chen-Kai Wang</a>
|
<a href=/people/o/onkar-singh/>Onkar Singh</a>
|
<a href=/people/z/zhao-li-tang/>Zhao-Li Tang</a>
|
<a href=/people/h/hong-jie-dai/>Hong-Jie Dai</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5805><div class="card-body p-3 small">Traditional disease surveillance systems depend on outpatient reporting and virological test results released by hospitals. These <a href=https://en.wikipedia.org/wiki/Data>data</a> have valid and accurate information about emerging outbreaks but it&#8217;s often not timely. In recent years the exponential growth of users getting connected to <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> provides immense knowledge about <a href=https://en.wikipedia.org/wiki/Epidemic>epidemics</a> by sharing related information. Social media can now flag more immediate concerns related to out-breaks in real time. In this paper we apply the long short-term memory recurrent neural net-work (RNN) architecture to classify tweets conveyed influenza-related information and compare its performance with baseline algorithms including support vector machine (SVM), <a href=https://en.wikipedia.org/wiki/Decision_tree_learning>decision tree</a>, <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>naive Bayes</a>, simple logistics, and <a href=https://en.wikipedia.org/wiki/Naive_Bayes_classifier>naive Bayes multinomial</a>. The developed RNN model achieved an <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of 0.845 on the MedWeb task test set, which outperforms the <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of SVM without applying the synthetic minority oversampling technique by 0.08. The <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> of the RNN model is within 1 % of the highest score achieved by SVM with oversampling technique.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5806.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5806 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5806 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5806/>ZikaHack 2016 : A digital disease detection competition<span class=acl-fixed-case>Z</span>ika<span class=acl-fixed-case>H</span>ack 2016: A digital disease detection competition</a></strong><br><a href=/people/d/dillon-c-adam/>Dillon C Adam</a>
|
<a href=/people/j/jitendra-jonnagaddala/>Jitendra Jonnagaddala</a>
|
<a href=/people/d/daniel-han-chen/>Daniel Han-Chen</a>
|
<a href=/people/s/sean-batongbacal/>Sean Batongbacal</a>
|
<a href=/people/l/luan-almeida/>Luan Almeida</a>
|
<a href=/people/j/jing-z-zhu/>Jing Z Zhu</a>
|
<a href=/people/j/jenny-j-yang/>Jenny J Yang</a>
|
<a href=/people/j/jumail-m-mundekkat/>Jumail M Mundekkat</a>
|
<a href=/people/s/steven-badman/>Steven Badman</a>
|
<a href=/people/a/abrar-chughtai/>Abrar Chughtai</a>
|
<a href=/people/c/c-raina-macintyre/>C Raina MacIntyre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5806><div class="card-body p-3 small">Effective response to infectious diseases outbreaks relies on the rapid and early detection of those outbreaks. Invalidated, yet timely and openly available digital information can be used for the early detection of outbreaks. Public health surveillance authorities can exploit these early warnings to plan and co-ordinate rapid surveillance and emergency response programs. In 2016, a digital disease detection competition named ZikaHack was launched. The objective of the competition was for multidisciplinary teams to design, develop and demonstrate innovative digital disease detection solutions to retrospectively detect the 2015-16 Brazilian Zika virus outbreak earlier than traditional surveillance methods. In this paper, an overview of the ZikaHack competition is provided. The challenges and lessons learned in organizing this <a href=https://en.wikipedia.org/wiki/Competition>competition</a> are also discussed for use by other researchers interested in organizing similar <a href=https://en.wikipedia.org/wiki/Competition>competitions</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5809.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5809 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5809 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5809/>Chemical-Induced Disease Detection Using Invariance-based Pattern Learning Model</a></strong><br><a href=/people/n/neha-warikoo/>Neha Warikoo</a>
|
<a href=/people/y/yung-chun-chang/>Yung-Chun Chang</a>
|
<a href=/people/w/wen-lian-hsu/>Wen-Lian Hsu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5809><div class="card-body p-3 small">In this work, we introduce a novel feature engineering approach named algebraic invariance to identify discriminative patterns for learning relation pair features for the chemical-disease relation (CDR) task of BioCreative V. Our method exploits the existing structural similarity of the key concepts of relation descriptions from the CDR corpus to generate robust linguistic patterns for SVM tree kernel-based learning. Preprocessing of the training data classifies the entity pairs as either related or unrelated to build instance types for both inter-sentential and intra-sentential scenarios. An <a href=https://en.wikipedia.org/wiki/Invariant_(mathematics)>invariant function</a> is proposed to process and optimally cluster similar patterns for both positive and negative instances. The learning model for CDR pairs is based on the SVM tree kernel approach, which generates feature trees and vectors and is modeled on suitable invariance based patterns, bringing brevity, precision and context to the identifier features. Results demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> outperformed other compared approaches, achieved a high <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall rate</a> of 85.08 %, and averaged an F1-score of 54.34 % without the use of any additional <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge bases</a>.</div></div></div><hr><div id=w17-59><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-59.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-59/>Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5900/>Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications (<span class=acl-fixed-case>NLPTEA</span> 2017)</a></strong><br><a href=/people/y/yuen-hsien-tseng/>Yuen-Hsien Tseng</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a>
|
<a href=/people/l/lung-hao-lee/>Lung-Hao Lee</a>
|
<a href=/people/l/liang-chih-yu/>Liang-Chih Yu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5902.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5902 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5902 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5902/>Understanding Non-Native Writings : Can a <a href=https://en.wikipedia.org/wiki/Parsis>Parser</a> Help?</a></strong><br><a href=/people/j/jirka-hana/>Jirka Hana</a>
|
<a href=/people/b/barbora-hladka/>Barbora Hladká</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5902><div class="card-body p-3 small">We present a pilot study on parsing non-native texts written by learners of <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a>. We performed experiments that have shown that at least high-level syntactic functions, like subject, predicate, and object, can be assigned based on a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> trained on standard <a href=https://en.wikipedia.org/wiki/First_language>native language</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5903.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5903 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5903 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5903/>Carrier Sentence Selection for Fill-in-the-blank Items<span class=acl-fixed-case>C</span>arrier Sentence Selection for Fill-in-the-blank Items</a></strong><br><a href=/people/s/shu-jiang/>Shu Jiang</a>
|
<a href=/people/j/john-s-y-lee/>John Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5903><div class="card-body p-3 small">Fill-in-the-blank items are a common form of exercise in computer-assisted language learning systems. To automatically generate an effective item, the system must be able to select a high-quality carrier sentence that illustrates the usage of the target word. Previous approaches for carrier sentence selection have considered <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence length</a>, vocabulary difficulty, the position of the target word and the presence of <a href=https://en.wikipedia.org/wiki/Finite_verb>finite verbs</a>. This paper investigates the utility of word co-occurrence statistics and <a href=https://en.wikipedia.org/wiki/Lexical_similarity>lexical similarity</a> as selection criteria. In an evaluation on generating fill-in-the-blank items for learning <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a foreign language, we show that these two criteria can improve carrier sentence quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5906.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5906 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5906 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5906/>Chinese Spelling Check based on N-gram and String Matching Algorithm<span class=acl-fixed-case>C</span>hinese Spelling Check based on N-gram and String Matching Algorithm</a></strong><br><a href=/people/j/jui-feng-yeh/>Jui-Feng Yeh</a>
|
<a href=/people/l/li-ting-chang/>Li-Ting Chang</a>
|
<a href=/people/c/chan-yi-liu/>Chan-Yi Liu</a>
|
<a href=/people/t/tsung-wei-hsu/>Tsung-Wei Hsu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5906><div class="card-body p-3 small">This paper presents a Chinese spelling check approach based on <a href=https://en.wikipedia.org/wiki/Language_model>language models</a> combined with string match algorithm to treat the problems resulted from the influence caused by Cantonese mother tone. N-grams first used to detecting the probability of sentence constructed by the writers, a string matching algorithm called Knuth-Morris-Pratt (KMP) Algorithm is used to detect and correct the error. According to the experimental results, the proposed approach can detect the error and provide the corresponding correction.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5907.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5907 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5907 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5907/>N-gram Model for Chinese Grammatical Error Diagnosis<span class=acl-fixed-case>C</span>hinese Grammatical Error Diagnosis</a></strong><br><a href=/people/j/jianbo-zhao/>Jianbo Zhao</a>
|
<a href=/people/h/hao-liu/>Hao Liu</a>
|
<a href=/people/z/zuyi-bao/>Zuyi Bao</a>
|
<a href=/people/x/xiaopeng-bai/>Xiaopeng Bai</a>
|
<a href=/people/s/si-li/>Si Li</a>
|
<a href=/people/z/zhiqing-lin/>Zhiqing Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5907><div class="card-body p-3 small">Detection and correction of Chinese grammatical errors have been two of major challenges for Chinese automatic grammatical error diagnosis. This paper presents an <a href=https://en.wikipedia.org/wiki/N-gram_model>N-gram model</a> for automatic detection and correction of Chinese grammatical errors in NLPTEA 2017 task. The experiment results show that the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> is good at correction of <a href=https://en.wikipedia.org/wiki/Chinese_grammar>Chinese grammatical errors</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5908.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5908 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5908 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5908/>The Influence of Spelling Errors on Content Scoring Performance</a></strong><br><a href=/people/a/andrea-horbach/>Andrea Horbach</a>
|
<a href=/people/y/yuning-ding/>Yuning Ding</a>
|
<a href=/people/t/torsten-zesch/>Torsten Zesch</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5908><div class="card-body p-3 small">Spelling errors occur frequently in educational settings, but their influence on <a href=https://en.wikipedia.org/wiki/Score_(game)>automatic scoring</a> is largely unknown. We therefore investigate the influence of spelling errors on content scoring performance using the example of the ASAP corpus. We conduct an annotation study on the nature of spelling errors in the ASAP dataset and utilize these finding in machine learning experiments that measure the influence of spelling errors on automatic content scoring. Our main finding is that scoring methods using both token and character n-gram features are robust against spelling errors up to the error frequency in ASAP.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5909.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5909 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5909 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5909/>Analyzing the Impact of Spelling Errors on POS-Tagging and Chunking in Learner English<span class=acl-fixed-case>POS</span>-Tagging and Chunking in Learner <span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/t/tomoya-mizumoto/>Tomoya Mizumoto</a>
|
<a href=/people/r/ryo-nagata/>Ryo Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5909><div class="card-body p-3 small">Part-of-speech (POS) tagging and chunking have been used in tasks targeting learner English ; however, to the best our knowledge, few studies have evaluated their performance and no studies have revealed the causes of POS-tagging / chunking errors in detail. Therefore, we investigate performance and analyze the causes of failure. We focus on spelling errors that occur frequently in learner English. We demonstrate that spelling errors reduced POS-tagging performance by 0.23 % owing to spelling errors, and that a <a href=https://en.wikipedia.org/wiki/Spell_checker>spell checker</a> is not necessary for POS-tagging / chunking of learner English.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5910.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5910 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5910 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5910/>Complex Word Identification : Challenges in Data Annotation and System Performance</a></strong><br><a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5910><div class="card-body p-3 small">This paper revisits the problem of complex word identification (CWI) following up the SemEval CWI shared task. We use <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble classifiers</a> to investigate how well <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational methods</a> can discriminate between complex and non-complex words. Furthermore, we analyze the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance to understand what makes lexical complexity challenging. Our findings show that most systems performed poorly on the SemEval CWI dataset, and one of the reasons for that is the way in which human annotation was performed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5911.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5911 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5911 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5911/>Suggesting Sentences for <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>ESL</a> using Kernel Embeddings<span class=acl-fixed-case>ESL</span> using Kernel Embeddings</a></strong><br><a href=/people/k/kent-shioda/>Kent Shioda</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/r/rue-ikeya/>Rue Ikeya</a>
|
<a href=/people/d/daichi-mochihashi/>Daichi Mochihashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5911><div class="card-body p-3 small">Sentence retrieval is an important NLP application for <a href=https://en.wikipedia.org/wiki/English_language>English</a> as a Second Language (ESL) learners. ESL learners are familiar with <a href=https://en.wikipedia.org/wiki/Web_search_engine>web search engines</a>, but generic web search results may not be adequate for composing documents in a specific domain. However, if we build our own <a href=https://en.wikipedia.org/wiki/Web_search_engine>search system</a> specialized to a domain, it may be subject to the data sparseness problem. Recently proposed <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a> partially addresses the data sparseness problem, but fails to extract sentences relevant to queries owing to the modeling of the latent intent of the query. Thus, we propose a method of retrieving example sentences using kernel embeddings and N-gram windows. This method implicitly models latent intent of query and sentences, and alleviates the problem of noisy alignment. Our results show that our method achieved higher precision in sentence retrieval for <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>ESL</a> in the domain of a university press release corpus, as compared to a previous unsupervised method used for a semantic textual similarity task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-5912.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-5912 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-5912 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-5912/>Event Timeline Generation from History Textbooks</a></strong><br><a href=/people/h/harsimran-bedi/>Harsimran Bedi</a>
|
<a href=/people/s/sangameshwar-patil/>Sangameshwar Patil</a>
|
<a href=/people/s/swapnil-hingmire/>Swapnil Hingmire</a>
|
<a href=/people/g/girish-palshikar/>Girish Palshikar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-5912><div class="card-body p-3 small">Event timeline serves as the basic structure of history, and it is used as a disposition of key phenomena in studying history as a subject in secondary school. In order to enable a student to understand a historical phenomenon as a series of connected events, we present a system for automatic event timeline generation from <a href=https://en.wikipedia.org/wiki/Textbook>history textbooks</a>. Additionally, we propose Message Sequence Chart (MSC) and time-map based visualization techniques to visualize an event timeline. We also identify key computational challenges in developing <a href=https://en.wikipedia.org/wiki/Natural-language_user_interface>natural language processing based applications</a> for <a href=https://en.wikipedia.org/wiki/Textbook>history textbooks</a>.</div></div></div><hr><div id=w17-60><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-60.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-60/>Proceedings of the 9th SIGHAN Workshop on Chinese Language Processing</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6000/>Proceedings of the 9th <span class=acl-fixed-case>SIGHAN</span> Workshop on <span class=acl-fixed-case>C</span>hinese Language Processing</a></strong><br><a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/z/zhifang-sui/>Zhifang Sui</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6001 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6001/>Group Linguistic Bias Aware Neural Response Generation</a></strong><br><a href=/people/j/jianan-wang/>Jianan Wang</a>
|
<a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/f/fang-li/>Fang Li</a>
|
<a href=/people/z/zhen-xu/>Zhen Xu</a>
|
<a href=/people/z/zhuoran-wang/>Zhuoran Wang</a>
|
<a href=/people/b/baoxun-wang/>Baoxun Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6001><div class="card-body p-3 small">For practical chatbots, one of the essential factor for improving <a href=https://en.wikipedia.org/wiki/User_experience>user experience</a> is the capability of customizing the talking style of the agents, that is, to make <a href=https://en.wikipedia.org/wiki/Chatbot>chatbots</a> provide responses meeting users&#8217; preference on language styles, topics, etc. To address this issue, this paper proposes to incorporate linguistic biases, which implicitly involved in the conversation corpora generated by human groups in the Social Network Services (SNS), into the encoder-decoder based response generator. By attaching a specially designed neural component to dynamically control the impact of linguistic biases in response generation, a Group Linguistic Bias Aware Neural Response Generation (GLBA-NRG) model is eventually presented. The experimental results on the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> from the Chinese SNS show that the proposed <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> outperforms the current response generating models by producing both meaningful and vivid responses with customized styles.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6002 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6002/>Neural Regularized Domain Adaptation for Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/z/zuyi-bao/>Zuyi Bao</a>
|
<a href=/people/s/si-li/>Si Li</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a>
|
<a href=/people/s/sheng-gao/>Sheng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6002><div class="card-body p-3 small">For Chinese word segmentation, the large-scale annotated corpora mainly focus on <a href=https://en.wikipedia.org/wiki/News_agency>newswire</a> and only a handful of annotated data is available in other domains such as <a href=https://en.wikipedia.org/wiki/Patent>patents</a> and <a href=https://en.wikipedia.org/wiki/Literature>literature</a>. Considering the limited amount of annotated target domain data, it is a challenge for segmenters to learn domain-specific information while avoid getting over-fitted at the same time. In this paper, we propose a neural regularized domain adaptation method for <a href=https://en.wikipedia.org/wiki/Chinese_word_segmentation>Chinese word segmentation</a>. The teacher networks trained in source domain are employed to regularize the training process of the student network by preserving the <a href=https://en.wikipedia.org/wiki/General_knowledge>general knowledge</a>. In the experiments, our neural regularized domain adaptation method achieves a better performance comparing to previous methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6005 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6005/>Learning from Parenthetical Sentences for Term Translation in Machine Translation</a></strong><br><a href=/people/g/guoping-huang/>Guoping Huang</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/y/yu-zhou/>Yu Zhou</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6005><div class="card-body p-3 small">Terms extensively exist in specific domains, and term translation plays a critical role in domain-specific machine translation (MT) tasks. However, it&#8217;s a challenging task to translate them correctly for the huge number of pre-existing terms and the endless new terms. To achieve better term translation quality, it is necessary to inject external term knowledge into the underlying MT system. Fortunately, there are plenty of term translation knowledge in parenthetical sentences on the Internet. In this paper, we propose a simple, straightforward and effective <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> to improve term translation by learning from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>parenthetical sentences</a>. This framework includes : (1) a focused web crawler ; (2) a parenthetical sentence filter, acquiring parenthetical sentences including bilingual term pairs ; (3) a term translation knowledge extractor, extracting bilingual term translation candidates ; (4) a probability learner, generating the term translation table for MT decoders. The extensive experiments demonstrate that our proposed <a href=https://en.wikipedia.org/wiki/Conceptual_framework>framework</a> significantly improves the translation quality of terms and sentences.</div></div></div><hr><div id=w17-62><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-62.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-62/>Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6200.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6200/>Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms</a></strong><br><a href=/people/m/marco-kuhlmann/>Marco Kuhlmann</a>
|
<a href=/people/t/tatjana-scheffler/>Tatjana Scheffler</a></span></p></div><hr><div id=w17-63><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-63.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-63/>Proceedings of the 15th International Conference on Parsing Technologies</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6300/>Proceedings of the 15th International Conference on Parsing Technologies</a></strong><br><a href=/people/y/yusuke-miyao/>Yusuke Miyao</a>
|
<a href=/people/k/kenji-sagae/>Kenji Sagae</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6301/>Automatically Acquired Lexical Knowledge Improves Japanese Joint Morphological and Dependency Analysis<span class=acl-fixed-case>J</span>apanese Joint Morphological and Dependency Analysis</a></strong><br><a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/y/yuta-hayashibe/>Yuta Hayashibe</a>
|
<a href=/people/h/hajime-morita/>Hajime Morita</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6301><div class="card-body p-3 small">This paper presents a joint model for morphological and dependency analysis based on automatically acquired lexical knowledge. This model takes advantage of rich lexical knowledge to simultaneously resolve <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a>, POS, and dependency ambiguities. In our experiments on <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, we show the effectiveness of our joint model over conventional pipeline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6302/>Dependency Language Models for Transition-based Dependency Parsing</a></strong><br><a href=/people/j/juntao-yu/>Juntao Yu</a>
|
<a href=/people/b/bernd-bohnet/>Bernd Bohnet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6302><div class="card-body p-3 small">In this paper, we present an approach to improve the accuracy of a strong transition-based dependency parser by exploiting dependency language models that are extracted from a large parsed corpus. We integrated a small number of <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> based on the dependency language models into the <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>. To demonstrate the effectiveness of the proposed approach, we evaluate our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> on standard English and Chinese data where the base parser could achieve competitive accuracy scores. Our enhanced <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieved state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese data</a> and competitive results on <a href=https://en.wikipedia.org/wiki/English_language>English data</a>. We gained a large absolute improvement of one point (UAS) on <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> and 0.5 points for <a href=https://en.wikipedia.org/wiki/English_language>English</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6303/>Lexicalized vs. Delexicalized Parsing in Low-Resource Scenarios</a></strong><br><a href=/people/a/agnieszka-falenska/>Agnieszka Falenska</a>
|
<a href=/people/o/ozlem-cetinoglu/>Özlem Çetinoğlu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6303><div class="card-body p-3 small">We present a systematic analysis of <a href=https://en.wikipedia.org/wiki/Lexicalization>lexicalized vs. delexicalized parsing</a> in low-resource scenarios, and propose a <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> to choose one method over another under certain conditions. We create a set of simulation experiments on 41 languages and apply our findings to 9 low-resource languages. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> chooses the best approach in 8 out of 9 cases.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6305/>Prepositional Phrase Attachment over Word Embedding Products</a></strong><br><a href=/people/p/pranava-swaroop-madhyastha/>Pranava Swaroop Madhyastha</a>
|
<a href=/people/x/xavier-carreras/>Xavier Carreras</a>
|
<a href=/people/a/ariadna-quattoni/>Ariadna Quattoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6305><div class="card-body p-3 small">We present a low-rank multi-linear model for the task of solving prepositional phrase attachment ambiguity (PP task). Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> exploits tensor products of word embeddings, capturing all possible conjunctions of latent embeddings. Our results on a wide range of datasets and task settings show that <a href=https://en.wikipedia.org/wiki/Tensor_product>tensor products</a> are the best compositional operation and that a relatively simple multi-linear model that uses only word embeddings of lexical features can outperform more complex non-linear architectures that exploit the same information. Our proposed model gives the current best reported performance on an out-of-domain evaluation and performs competively on out-of-domain dependency parsing datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6306.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6306 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6306 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6306/>L1-L2 Parallel Dependency Treebank as Learner Corpus<span class=acl-fixed-case>L</span>1-<span class=acl-fixed-case>L</span>2 Parallel Dependency Treebank as Learner Corpus</a></strong><br><a href=/people/j/john-s-y-lee/>John Lee</a>
|
<a href=/people/k/keying-li/>Keying Li</a>
|
<a href=/people/h/herman-leung/>Herman Leung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6306><div class="card-body p-3 small">This opinion paper proposes the use of parallel treebank as learner corpus. We show how an L1-L2 parallel treebank i.e., <a href=https://en.wikipedia.org/wiki/Parse_tree>parse trees</a> of non-native sentences, aligned to the <a href=https://en.wikipedia.org/wiki/Parse_tree>parse trees</a> of their target hypotheses can facilitate retrieval of sentences with specific learner errors. We argue for its benefits, in terms of corpus re-use and interoperability, over a conventional learner corpus annotated with error tags. As a proof of concept, we conduct a case study on word-order errors made by learners of <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> as a foreign language. We report <a href=https://en.wikipedia.org/wiki/Precision_(computer_science)>precision</a> and <a href=https://en.wikipedia.org/wiki/Recall_(memory)>recall</a> in retrieving a range of word-order error categories from L1-L2 tree pairs annotated in the Universal Dependency framework.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6307/>Splitting Complex English Sentences<span class=acl-fixed-case>E</span>nglish Sentences</a></strong><br><a href=/people/j/john-s-y-lee/>John Lee</a>
|
<a href=/people/j/j-buddhika-k-pathirage-don/>J. Buddhika K. Pathirage Don</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6307><div class="card-body p-3 small">This paper applies parsing technology to the task of syntactic simplification of English sentences, focusing on the identification of text spans that can be removed from a complex sentence. We report the most comprehensive evaluation to-date on this task, using a dataset of sentences that exhibit simplification based on coordination, subordination, punctuation / parataxis, adjectival clauses, participial phrases, and appositive phrases. We train a <a href=https://en.wikipedia.org/wiki/Decision_tree>decision tree</a> with features derived from text span length, POS tags and dependency relations, and show that it significantly outperforms a parser-only baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6308/>Hierarchical Word Structure-based Parsing : A Feasibility Study on UD-style Dependency Parsing in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a><span class=acl-fixed-case>UD</span>-style Dependency Parsing in <span class=acl-fixed-case>J</span>apanese</a></strong><br><a href=/people/t/takaaki-tanaka/>Takaaki Tanaka</a>
|
<a href=/people/k/katsuhiko-hayashi/>Katsuhiko Hayashi</a>
|
<a href=/people/m/masaaki-nagata/>Masaaki Nagata</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6308><div class="card-body p-3 small">In applying word-based dependency parsing such as Universal Dependencies (UD) to <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>, the uncertainty of <a href=https://en.wikipedia.org/wiki/Word_segmentation>word segmentation</a> emerges for defining a word unit of the dependencies. We introduce the following hierarchical word structures to dependency parsing in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> : morphological units (a short unit word, SUW) and syntactic units (a long unit word, LUW). An SUW can be used to segment a sentence consistently, while it is too short to represent syntactic construction. An LUW is a unit including functional multiwords and LUW-based analysis facilitates the capturing of syntactic structure and makes <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> results more precise than SUW-based analysis. This paper describes the results of a feasibility study on the ability and the effectiveness of parsing methods based on hierarchical word structure (LUW chunking+parsing) in comparison to single layer word structure (SUW parsing). We also show joint analysis of LUW-chunking and dependency parsing improves the performance of identifying predicate-argument structures, while there is not much difference between overall results of them. not much difference between overall results of them.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6309.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6309 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6309 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6309/>Leveraging Newswire Treebanks for Parsing Conversational Data with Argument Scrambling</a></strong><br><a href=/people/r/riyaz-ahmad-bhat/>Riyaz A. Bhat</a>
|
<a href=/people/i/irshad-bhat/>Irshad Bhat</a>
|
<a href=/people/d/dipti-misra-sharma/>Dipti Sharma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6309><div class="card-body p-3 small">We investigate the problem of parsing conversational data of morphologically-rich languages such as <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> where argument scrambling occurs frequently. We evaluate a state-of-the-art non-linear transition-based parsing system on a new dataset containing 506 dependency trees for sentences from Bollywood (Hindi) movie scripts and Twitter posts of Hindi monolingual speakers. We show that a <a href=https://en.wikipedia.org/wiki/Dependency_grammar>dependency parser</a> trained on a newswire treebank is strongly biased towards the <a href=https://en.wikipedia.org/wiki/Canonical_form>canonical structures</a> and degrades when applied to conversational data. Inspired by <a href=https://en.wikipedia.org/wiki/Transformational_grammar>Transformational Generative Grammar</a> (Chomsky, 1965), we mitigate the <a href=https://en.wikipedia.org/wiki/Sampling_bias>sampling bias</a> by generating all theoretically possible alternative word orders of a clause from the existing (kernel) structures in the <a href=https://en.wikipedia.org/wiki/Treebank>treebank</a>. Training our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> on canonical and transformed structures improves performance on conversational data by around 9 % LAS over the baseline newswire parser.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6310.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6310 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6310 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-6310" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-6310/>Using <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> to improve multilingual partial parsers</a></strong><br><a href=/people/a/anders-sogaard/>Anders Søgaard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6310><div class="card-body p-3 small">Syntactic annotation is costly and not available for the vast majority of the world&#8217;s languages. We show that sometimes we can do away with less labeled data by exploiting more readily available forms of <a href=https://en.wikipedia.org/wiki/Markup_language>mark-up</a>. Specifically, we revisit an idea from Valentin Spitkovsky&#8217;s work (2010), namely that <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> typically bracket syntactic constituents or chunks. We strengthen his results by showing that not only can <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> help in low resource scenarios, exemplified here by Quechua, but learning from <a href=https://en.wikipedia.org/wiki/Hyperlink>hyperlinks</a> can also improve state-of-the-art NLP models for English newswire. We also present out-of-domain evaluation on English Ontonotes 4.0.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6311.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6311 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6311 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6311/>Correcting prepositional phrase attachments using multimodal corpora</a></strong><br><a href=/people/s/sebastien-delecraz/>Sebastien Delecraz</a>
|
<a href=/people/a/alexis-nasr/>Alexis Nasr</a>
|
<a href=/people/f/frederic-bechet/>Frederic Bechet</a>
|
<a href=/people/b/benoit-favre/>Benoit Favre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6311><div class="card-body p-3 small">PP-attachments are an important source of errors in parsing <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. We propose in this article to use data coming from a multimodal corpus, combining textual, visual and conceptual information, as well as a correction strategy, to propose alternative attachments in the output of a <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6312.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6312 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6312 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6312/>Exploiting Structure in <a href=https://en.wikipedia.org/wiki/Parsing>Parsing</a> to 1-Endpoint-Crossing Graphs</a></strong><br><a href=/people/r/robin-kurtz/>Robin Kurtz</a>
|
<a href=/people/m/marco-kuhlmann/>Marco Kuhlmann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6312><div class="card-body p-3 small">Deep dependency parsing can be cast as the search for maximum acyclic subgraphs in weighted digraphs. Because this search problem is intractable in the general case, we consider its restriction to the class of 1-endpoint-crossing (1ec) graphs, which has high coverage on standard data sets. Our main contribution is a characterization of 1ec graphs as a subclass of the <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> with pagenumber at most 3. Building on this we show how to extend an existing <a href=https://en.wikipedia.org/wiki/Parsing>parsing algorithm</a> for 1-endpoint-crossing trees to the full class. While the <a href=https://en.wikipedia.org/wiki/Time_complexity>runtime complexity</a> of the extended <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is polynomial in the length of the input sentence, it features a large constant, which poses a challenge for practical implementations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6313.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6313 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6313 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6313/>Effective Online Reordering with Arc-Eager Transitions</a></strong><br><a href=/people/r/ryosuke-kohita/>Ryosuke Kohita</a>
|
<a href=/people/h/hiroshi-noji/>Hiroshi Noji</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6313><div class="card-body p-3 small">We present a new <a href=https://en.wikipedia.org/wiki/Transition_system>transition system</a> with word reordering for unrestricted non-projective dependency parsing. Our system is based on decomposed arc-eager rather than arc-standard, which allows more flexible ambiguity resolution between a local projective and non-local crossing attachment. In our experiment on Universal Dependencies 2.0, we find our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> outperforms the ordinary swap-based parser particularly on languages with a large amount of non-projectivity.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6314.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6314 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6314 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-6314.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-6314" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-6314/>Arc-Hybrid Non-Projective Dependency Parsing with a Static-Dynamic Oracle</a></strong><br><a href=/people/m/miryam-de-lhoneux/>Miryam de Lhoneux</a>
|
<a href=/people/s/sara-stymne/>Sara Stymne</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6314><div class="card-body p-3 small">In this paper, we extend the arc-hybrid system for transition-based parsing with a swap transition that enables reordering of the words and construction of non-projective trees. Although this extension breaks the arc-decomposability of the transition system, we show how the existing dynamic oracle for this <a href=https://en.wikipedia.org/wiki/System>system</a> can be modified and combined with a static oracle only for the swap transition. Experiments on 5 languages show that the new <a href=https://en.wikipedia.org/wiki/System>system</a> gives competitive accuracy and is significantly better than a <a href=https://en.wikipedia.org/wiki/System>system</a> trained with a purely static oracle.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6315.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6315 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6315 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-6315" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-6315/>Encoder-Decoder Shift-Reduce Syntactic Parsing</a></strong><br><a href=/people/j/jiangming-liu/>Jiangming Liu</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6315><div class="card-body p-3 small">Encoder-decoder neural networks have been used for many <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP tasks</a>, such as <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a>. They have also been applied to constituent parsing by using bracketed tree structures as a target language, translating input sentences into syntactic trees. A more commonly used method to linearize syntactic trees is the shift-reduce system, which uses a sequence of transition-actions to build <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>trees</a>. We empirically investigate the effectiveness of applying the encoder-decoder network to transition-based parsing. On standard benchmarks, our system gives comparable results to the stack LSTM parser for dependency parsing, and significantly better results compared to the aforementioned <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> for constituent parsing, which uses bracketed tree formats.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6318.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-6318 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-6318 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6318/>Evaluating LSTM models for grammatical function labelling<span class=acl-fixed-case>LSTM</span> models for grammatical function labelling</a></strong><br><a href=/people/b/bich-ngoc-do/>Bich-Ngoc Do</a>
|
<a href=/people/i/ines-rehbein/>Ines Rehbein</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-6318><div class="card-body p-3 small">To improve grammatical function labelling for <a href=https://en.wikipedia.org/wiki/German_language>German</a>, we augment the labelling component of a neural dependency parser with a decision history. We present different ways to encode the history, using different LSTM architectures, and show that our models yield significant improvements, resulting in a LAS for <a href=https://en.wikipedia.org/wiki/German_language>German</a> that is close to the best result from the SPMRL 2014 shared task (without the reranker).</div></div></div><hr><div id=w17-65><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-65.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-65/>Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6500/>Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017)</a></strong><br><a href=/people/s/simonetta-montemagni/>Simonetta Montemagni</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p></div><hr><div id=w17-66><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-66.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-66/>Proceedings of the 11th Brazilian Symposium in Information and Human Language Technology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6600/>Proceedings of the 11th <span class=acl-fixed-case>B</span>razilian Symposium in Information and Human Language Technology</a></strong><br><a href=/people/g/gustavo-paetzold/>Gustavo Henrique Paetzold</a>
|
<a href=/people/v/vladia-pinheiro/>Vládia Pinheiro</a></span></p></div><hr><div id=w17-68><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-68/>IWCS 2017 - 12th International Conference on Computational Semantics - Long papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6800.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6800/><span class=acl-fixed-case>IWCS</span> 2017 - 12th International Conference on Computational Semantics - Long papers</a></strong><br><a href=/people/c/claire-gardent/>Claire Gardent</a>
|
<a href=/people/c/christian-retore/>Christian Retoré</a></span></p></div><hr><div id=w17-69><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-69/>IWCS 2017 — 12th International Conference on Computational Semantics — Short papers</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-6900.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-6900/><span class=acl-fixed-case>IWCS</span> 2017 — 12th International Conference on Computational Semantics — Short papers</a></strong><br><a href=/people/c/claire-gardent/>Claire Gardent</a>
|
<a href=/people/c/christian-retore/>Christian Retoré</a></span></p></div><hr><div id=w17-70><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-70/>Proceedings of Language, Ontology, Terminology and Knowledge Structures Workshop (LOTKS 2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-7000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-7000/>Proceedings of Language, Ontology, Terminology and Knowledge Structures Workshop (<span class=acl-fixed-case>LOTKS</span> 2017)</a></strong><br><a href=/people/f/francesca-frontini/>Francesca Frontini</a>
|
<a href=/people/l/larisa-grcic-simeunovic/>Larisa Grčić Simeunović</a>
|
<a href=/people/s/spela-vintar/>Špela Vintar</a>
|
<a href=/people/f/fahad-khan/>Anas Fahad Khan</a>
|
<a href=/people/a/artemis-parvizi/>Artemis Parvisi</a></span></p></div><hr><div id=w17-71><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-71/>Proceedings of the IWCS workshop on Foundations of Situated and Multimodal Communication</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-7100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-7100/>Proceedings of the <span class=acl-fixed-case>IWCS</span> workshop on Foundations of Situated and Multimodal Communication</a></strong><br><a href=/people/n/nicholas-asher/>Nicholas Asher</a>
|
<a href=/people/j/julie-hunter/>Julie Hunter</a>
|
<a href=/people/a/alex-lascarides/>Alex Lascarides</a></span></p></div><hr><div id=w17-72><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-72/>Proceedings of the Computing Natural Language Inference Workshop</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-7200/>Proceedings of the Computing Natural Language Inference Workshop</a></strong><br></span></p></div><hr><div id=w17-73><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-73/>Proceedings of the 2nd Workshop on Semantic Deep Learning (SemDeep-2)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-7300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-7300/>Proceedings of the 2nd Workshop on Semantic Deep Learning (<span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>D</span>eep-2)</a></strong><br><a href=/people/d/dagmar-gromann/>Dagmar Gromann</a>
|
<a href=/people/t/thierry-declerck/>Thierry Declerck</a>
|
<a href=/people/g/georg-heigl/>Georg Heigl</a></span></p></div><hr><div id=w17-74><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-74/>Proceedings of the 13th Joint ISO-ACL Workshop on Interoperable Semantic Annotation (ISA-13)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-7400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-7400/>Proceedings of the 13th Joint <span class=acl-fixed-case>ISO</span>-<span class=acl-fixed-case>ACL</span> Workshop on Interoperable Semantic Annotation (<span class=acl-fixed-case>ISA</span>-13)</a></strong><br></span></p></div><hr><div id=w17-75><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-75/>Proceedings of the 14th International Conference on Natural Language Processing (ICON-2017)</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-7500.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-7500/>Proceedings of the 14th International Conference on Natural Language Processing (<span class=acl-fixed-case>ICON</span>-2017)</a></strong><br><a href=/people/s/sivaji-bandyopadhyay/>Sivaji Bandyopadhyay</a></span></p></div><hr><div id=w17-76><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-76.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/W17-76/>Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-7600.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-7600/>Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories</a></strong><br><a href=/people/j/jan-hajic/>Jan Hajič</a></span></p></div><hr><div id=w17-77><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-77/>Proceedings of the 1st Workshop on Natural Language Processing and Information Retrieval associated with RANLP 2017</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-7700/>Proceedings of the 1st Workshop on Natural Language Processing and Information Retrieval associated with <span class=acl-fixed-case>RANLP</span> 2017</a></strong><br><a href=/people/m/mireille-makary/>Mireille Makary</a>
|
<a href=/people/m/michael-oakes/>Michael Oakes</a></span></p></div><hr><div id=w17-78><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-78/>Proceedings of the Workshop Knowledge Resources for the Socio-Economic Sciences and Humanities associated with RANLP 2017</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-7800/>Proceedings of the Workshop Knowledge Resources for the Socio-Economic Sciences and Humanities associated with <span class=acl-fixed-case>RANLP</span> 2017</a></strong><br><a href=/people/k/kalliopi-zervanou/>Kalliopi Zervanou</a>
|
<a href=/people/p/petya-osenova/>Petya Osenova</a>
|
<a href=/people/e/eveline-wandl-vogt/>Eveline Wandl-Vogt</a>
|
<a href=/people/d/dan-cristea/>Dan Cristea</a></span></p></div><hr><div id=w17-79><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-79/>Proceedings of the Workshop Human-Informed Translation and Interpreting Technology</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-7900/>Proceedings of the Workshop Human-Informed Translation and Interpreting Technology</a></strong><br><a href=/people/i/irina-temnikova/>Irina Temnikova</a>
|
<a href=/people/c/constantin-orasan/>Constantin Orasan</a>
|
<a href=/people/g/gloria-corpas-pastor/>Gloria Corpas Pastor</a>
|
<a href=/people/s/stephan-vogel/>Stephan Vogel</a></span></p></div><hr><div id=w17-80><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-80/>Proceedings of the Biomedical NLP Workshop associated with RANLP 2017</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-8000/>Proceedings of the Biomedical <span class=acl-fixed-case>NLP</span> Workshop associated with <span class=acl-fixed-case>RANLP</span> 2017</a></strong><br><a href=/people/s/svetla-boytcheva/>Svetla Boytcheva</a>
|
<a href=/people/k/k-bretonnel-cohen/>Kevin Bretonnel Cohen</a>
|
<a href=/people/g/guergana-savova/>Guergana Savova</a>
|
<a href=/people/g/galia-angelova/>Galia Angelova</a></span></p></div><hr><div id=w17-81><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"></span>
<a class=align-middle href=/volumes/W17-81/>Proceedings of the First Workshop on Language technology for Digital Humanities in Central and (South-)Eastern Europe</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-8100/>Proceedings of the First Workshop on Language technology for Digital Humanities in Central and (South-)Eastern <span class=acl-fixed-case>E</span>urope</a></strong><br><a href=/people/a/anca-dinu/>Anca Dinu</a>
|
<a href=/people/p/petya-osenova/>Petya Osenova</a>
|
<a href=/people/c/cristina-vertan/>Cristina Vertan</a></span></p></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>