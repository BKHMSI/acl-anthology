<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>International Conference on Spoken Language Translation (2020) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>International Conference on Spoken Language Translation (2020)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2020iwslt-1>Proceedings of the 17th International Conference on Spoken Language Translation</a>
<span class="badge badge-info align-middle ml-1">14&nbsp;papers</span></li></ul></div></div><div id=2020iwslt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2020.iwslt-1/>Proceedings of the 17th International Conference on Spoken Language Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.0/>Proceedings of the 17th International Conference on Spoken Language Translation</a></strong><br><a href=/people/m/marcello-federico/>Marcello Federico</a>
|
<a href=/people/a/alex-waibel/>Alex Waibel</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a>
|
<a href=/people/h/hermann-ney/>Hermann Ney</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a>
|
<a href=/people/s/sebastian-stuker/>Sebastian Stüker</a>
|
<a href=/people/d/dekai-wu/>Dekai Wu</a>
|
<a href=/people/j/joseph-mariani/>Joseph Mariani</a>
|
<a href=/people/f/francois-yvon/>Francois Yvon</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.9/>SRPOL’s System for the IWSLT 2020 End-to-End Speech Translation Task<span class=acl-fixed-case>SRPOL</span>’s System for the <span class=acl-fixed-case>IWSLT</span> 2020 End-to-End Speech Translation Task</a></strong><br><a href=/people/t/tomasz-potapczyk/>Tomasz Potapczyk</a>
|
<a href=/people/p/pawel-przybysz/>Pawel Przybysz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--9><div class="card-body p-3 small">We took part in the offline End-to-End English to German TED lectures translation task. We based our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> on our last year&#8217;s submission. We used a slightly altered Transformer architecture with ResNet-like convolutional layer preparing the audio input to Transformer encoder. To improve the model&#8217;s quality of translation we introduced two regularization techniques and trained on machine translated Librispeech corpus in addition to iwslt-corpus, TEDLIUM2 andMust_C corpora. Our best <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> scored almost 3 BLEU higher than last year&#8217;s model. To segment 2020 test set we used exactly the same procedure as last year.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929617 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.10/>The University of Helsinki Submission to the IWSLT2020 Offline SpeechTranslation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>H</span>elsinki Submission to the <span class=acl-fixed-case>IWSLT</span>2020 Offline <span class=acl-fixed-case>S</span>peech<span class=acl-fixed-case>T</span>ranslation Task</a></strong><br><a href=/people/r/raul-vazquez/>Raúl Vázquez</a>
|
<a href=/people/m/mikko-aulamo/>Mikko Aulamo</a>
|
<a href=/people/u/umut-sulubacak/>Umut Sulubacak</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--10><div class="card-body p-3 small">This paper describes the University of Helsinki Language Technology group&#8217;s participation in the IWSLT 2020 offline speech translation task, addressing the translation of English audio into German text. In line with this year&#8217;s task objective, we train both cascade and end-to-end systems for spoken language translation. We opt for an end-to-end multitasking architecture with shared internal representations and a cascade approach that follows a standard procedure consisting of ASR, correction, and MT stages. We also describe the experiments that served as a basis for the submitted <a href=https://en.wikipedia.org/wiki/System>systems</a>. Our experiments reveal that multitasking training with shared internal representations is not only possible but allows for <a href=https://en.wikipedia.org/wiki/Knowledge_transfer>knowledge-transfer</a> across modalities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929615 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.11/>The AFRL IWSLT 2020 Systems : Work-From-Home Edition<span class=acl-fixed-case>AFRL</span> <span class=acl-fixed-case>IWSLT</span> 2020 Systems: Work-From-Home Edition</a></strong><br><a href=/people/b/brian-ore/>Brian Ore</a>
|
<a href=/people/e/eric-hansen/>Eric Hansen</a>
|
<a href=/people/t/tim-anderson/>Tim Anderson</a>
|
<a href=/people/j/jeremy-gwinnup/>Jeremy Gwinnup</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--11><div class="card-body p-3 small">This report summarizes the Air Force Research Laboratory (AFRL) submission to the offline spoken language translation (SLT) task as part of the IWSLT 2020 evaluation campaign. As in previous years, we chose to adopt the cascade approach of using separate systems to perform <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech activity detection</a>, <a href=https://en.wikipedia.org/wiki/Speech_recognition>automatic speech recognition</a>, <a href=https://en.wikipedia.org/wiki/Sentence_segmentation>sentence segmentation</a>, and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. All systems were neural based, including a fully-connected neural network for speech activity detection, a Kaldi factorized time delay neural network with recurrent neural network (RNN) language model rescoring for speech recognition, a bidirectional RNN with attention mechanism for sentence segmentation, and transformer networks trained with OpenNMT and Marian for machine translation. Our primary submission yielded BLEU scores of 21.28 on tst2019 and 23.33 on tst2020.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929611 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.13/>OPPO’s Machine Translation System for the IWSLT 2020 Open Domain Translation Task<span class=acl-fixed-case>OPPO</span>’s Machine Translation System for the <span class=acl-fixed-case>IWSLT</span> 2020 Open Domain Translation Task</a></strong><br><a href=/people/q/qian-zhang/>Qian Zhang</a>
|
<a href=/people/x/xiaopu-li/>Xiaopu Li</a>
|
<a href=/people/d/dawei-dang/>Dawei Dang</a>
|
<a href=/people/t/tingxun-shi/>Tingxun Shi</a>
|
<a href=/people/d/di-ai/>Di Ai</a>
|
<a href=/people/z/zhengshan-xue/>Zhengshan Xue</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--13><div class="card-body p-3 small">In this paper, we demonstrate our <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation system</a> applied for the Chinese-Japanese bidirectional translation task (aka. open domain translation task) for the IWSLT 2020. Our model is based on Transformer (Vaswani et al., 2017), with the help of many popular, widely proved effective data preprocessing and augmentation methods. Experiments show that these <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> can improve the <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline model</a> steadily and significantly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929590 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.iwslt-1.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.14/>Character Mapping and Ad-hoc Adaptation : Edinburgh’s IWSLT 2020 Open Domain Translation System<span class=acl-fixed-case>E</span>dinburgh’s <span class=acl-fixed-case>IWSLT</span> 2020 Open Domain Translation System</a></strong><br><a href=/people/p/pinzhen-chen/>Pinzhen Chen</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/u/ulrich-germann/>Ulrich Germann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--14><div class="card-body p-3 small">This paper describes the University of Edinburgh&#8217;s neural machine translation systems submitted to the IWSLT 2020 open domain JapaneseChinese translation task. On top of commonplace techniques like <a href=https://en.wikipedia.org/wiki/Lexical_analysis>tokenisation</a> and corpus cleaning, we explore character mapping and unsupervised decoding-time adaptation. Our techniques focus on leveraging the provided data, and we show the positive impact of each technique through the gradual improvement of BLEU.<tex-math>\\leftrightarrow</tex-math>Chinese translation task. On top of commonplace techniques like tokenisation and corpus cleaning, we explore character mapping and unsupervised decoding-time adaptation. Our techniques focus on leveraging the provided data, and we show the positive impact of each technique through the gradual improvement of BLEU.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.15.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--15 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.15 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929589 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.15/>CASIA’s System for IWSLT 2020 Open Domain Translation<span class=acl-fixed-case>CASIA</span>’s System for <span class=acl-fixed-case>IWSLT</span> 2020 Open Domain Translation</a></strong><br><a href=/people/q/qian-wang/>Qian Wang</a>
|
<a href=/people/y/yuchen-liu/>Yuchen Liu</a>
|
<a href=/people/c/cong-ma/>Cong Ma</a>
|
<a href=/people/y/yu-lu/>Yu Lu</a>
|
<a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/l/long-zhou/>Long Zhou</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--15><div class="card-body p-3 small">This paper describes the CASIA&#8217;s system for the IWSLT 2020 open domain translation task. This year we participate in both ChineseJapanese and JapaneseChinese translation tasks. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is neural machine translation system based on Transformer model. We augment the training data with knowledge distillation and back translation to improve the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on <a href=https://en.wikipedia.org/wiki/Software_development_process>development data</a> with different model settings and different <a href=https://en.wikipedia.org/wiki/Data_processing>data processing techniques</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929592 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.16/>Deep Blue Sonics’ Submission to IWSLT 2020 Open Domain Translation Task<span class=acl-fixed-case>IWSLT</span> 2020 Open Domain Translation Task</a></strong><br><a href=/people/e/enmin-su/>Enmin Su</a>
|
<a href=/people/y/yi-ren/>Yi Ren</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--16><div class="card-body p-3 small">We present in this report our submission to IWSLT 2020 Open Domain Translation Task. We built a data pre-processing pipeline to efficiently handle large noisy web-crawled corpora, which boosts the BLEU score of a widely used transformer model in this translation task. To tackle the open-domain nature of this task, back- translation is applied to further improve the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.19/>ISTIC’s Neural Machine Translation System for IWSLT’2020<span class=acl-fixed-case>ISTIC</span>’s Neural Machine Translation System for <span class=acl-fixed-case>IWSLT</span>’2020</a></strong><br><a href=/people/j/jiaze-wei/>Jiaze Wei</a>
|
<a href=/people/w/wenbin-liu/>Wenbin Liu</a>
|
<a href=/people/z/zhenfeng-wu/>Zhenfeng Wu</a>
|
<a href=/people/y/you-pan/>You Pan</a>
|
<a href=/people/y/yanqing-he/>Yanqing He</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--19><div class="card-body p-3 small">This paper introduces technical details of machine translation system of Institute of Scientific and Technical Information of China (ISTIC) for the 17th International Conference on Spoken Language Translation (IWSLT 2020). ISTIC participated in both translation tasks of the Open Domain Translation track : Japanese-to-Chinese MT task and Chinese-to-Japanese MT task. The paper mainly elaborates on the <a href=https://en.wikipedia.org/wiki/Model-driven_architecture>model framework</a>, <a href=https://en.wikipedia.org/wiki/Data_preprocessing>data preprocessing methods</a> and decoding strategies adopted in our <a href=https://en.wikipedia.org/wiki/System>system</a>. In addition, the <a href=https://en.wikipedia.org/wiki/System>system</a> performance on the development set are given under different settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929616 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.23/>The HW-TSC Video Speech Translation System at IWSLT 2020<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span> Video Speech Translation System at <span class=acl-fixed-case>IWSLT</span> 2020</a></strong><br><a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/yao-deng/>Yao Deng</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/n/ning-xie/>Ning Xie</a>
|
<a href=/people/x/xiaochun-li/>Xiaochun Li</a>
|
<a href=/people/j/jiaxian-guo/>Jiaxian Guo</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--23><div class="card-body p-3 small">The paper presents details of our <a href=https://en.wikipedia.org/wiki/System>system</a> in the IWSLT Video Speech Translation evaluation. The <a href=https://en.wikipedia.org/wiki/System>system</a> works in a cascade form, which contains three <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a> : 1) A proprietary ASR system. 2) A disfluency correction system aims to remove interregnums or other disfluent expressions with a fine-tuned BERT and a series of rule-based algorithms. 3) An NMT System based on the Transformer and trained with massive publicly available corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.24/>CUNI Neural ASR with Phoneme-Level Intermediate Step for ~ Non-Native ~ SLT at IWSLT 2020<span class=acl-fixed-case>CUNI</span> Neural <span class=acl-fixed-case>ASR</span> with Phoneme-Level Intermediate Step for~<span class=acl-fixed-case>N</span>on-<span class=acl-fixed-case>N</span>ative~<span class=acl-fixed-case>SLT</span> at <span class=acl-fixed-case>IWSLT</span> 2020</a></strong><br><a href=/people/p/peter-polak/>Peter Polák</a>
|
<a href=/people/s/sangeet-sagar/>Sangeet Sagar</a>
|
<a href=/people/d/dominik-machacek/>Dominik Macháček</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--24><div class="card-body p-3 small">In this paper, we present our submission to the Non-Native Speech Translation Task for IWSLT 2020. Our main contribution is a proposed speech recognition pipeline that consists of an <a href=https://en.wikipedia.org/wiki/Acoustic_model>acoustic model</a> and a phoneme-to-grapheme model. As an <a href=https://en.wikipedia.org/wiki/Intermediate_representation>intermediate representation</a>, we utilize <a href=https://en.wikipedia.org/wiki/Phoneme>phonemes</a>. We demonstrate that the proposed <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> surpasses commercially used automatic speech recognition (ASR) and submit it into the ASR track. We complement this ASR with off-the-shelf MT systems to take part also in the speech translation track.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929595 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.25/>ELITR Non-Native Speech Translation at IWSLT 2020<span class=acl-fixed-case>ELITR</span> Non-Native Speech Translation at <span class=acl-fixed-case>IWSLT</span> 2020</a></strong><br><a href=/people/d/dominik-machacek/>Dominik Macháček</a>
|
<a href=/people/j/jonas-kratochvil/>Jonáš Kratochvíl</a>
|
<a href=/people/s/sangeet-sagar/>Sangeet Sagar</a>
|
<a href=/people/m/matus-zilinec/>Matúš Žilinec</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a>
|
<a href=/people/t/thai-son-nguyen/>Thai-Son Nguyen</a>
|
<a href=/people/f/felix-schneider/>Felix Schneider</a>
|
<a href=/people/p/philip-williams/>Philip Williams</a>
|
<a href=/people/y/yuekun-yao/>Yuekun Yao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--25><div class="card-body p-3 small">This paper is an ELITR system submission for the non-native speech translation task at IWSLT 2020. We describe systems for offline ASR, real-time ASR, and our cascaded approach to offline SLT and real-time SLT. We select our primary candidates from a pool of pre-existing systems, develop a new end-to-end general ASR system, and a hybrid ASR trained on non-native speech. The provided small validation set prevents us from carrying out a complex validation, but we submit all the unselected candidates for contrastive evaluation on the test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.29.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--29 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.29 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929608 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.29/>Neural Simultaneous Speech Translation Using Alignment-Based Chunking</a></strong><br><a href=/people/p/patrick-wilken/>Patrick Wilken</a>
|
<a href=/people/t/tamer-alkhouli/>Tamer Alkhouli</a>
|
<a href=/people/e/evgeny-matusov/>Evgeny Matusov</a>
|
<a href=/people/p/pavel-golik/>Pavel Golik</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--29><div class="card-body p-3 small">In simultaneous machine translation, the objective is to determine when to produce a partial translation given a continuous stream of source words, with a trade-off between <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>latency</a> and <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality</a>. We propose a neural machine translation (NMT) model that makes dynamic decisions when to continue feeding on input or generate output words. The model is composed of two main <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> : one to dynamically decide on ending a source chunk, and another that translates the consumed chunk. We train the components jointly and in a manner consistent with the <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference conditions</a>. To generate chunked training data, we propose a method that utilizes <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a> while also preserving enough context. We compare models with bidirectional and unidirectional encoders of different depths, both on real speech and text input. Our results on the IWSLT 2020 English-to-German task outperform a wait-k baseline by 2.6 to 3.7 % BLEU absolute.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.iwslt-1.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--iwslt-1--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.iwslt-1.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=http://slideslive.com/38929594 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.iwslt-1.33/>Efficient Automatic Punctuation Restoration Using Bidirectional Transformers with Robust Inference</a></strong><br><a href=/people/m/maury-courtland/>Maury Courtland</a>
|
<a href=/people/a/adam-faulkner/>Adam Faulkner</a>
|
<a href=/people/g/gayle-mcelvain/>Gayle McElvain</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--iwslt-1--33><div class="card-body p-3 small">Though people rarely speak in complete sentences, <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a> confers many benefits to the readers of <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcribed speech</a>. Unfortunately, most ASR systems do not produce punctuated output. To address this, we propose a <a href=https://en.wikipedia.org/wiki/Solution>solution</a> for automatic punctuation that is both cost efficient and easy to train. Our <a href=https://en.wikipedia.org/wiki/Solution>solution</a> benefits from the recent trend in fine-tuning transformer-based language models. We also modify the typical framing of this task by predicting <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a> for sequences rather than individual tokens, which makes for more efficient <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a>. Finally, we find that aggregating predictions across multiple context windows improves accuracy even further. Our best model achieves a new state of the art on benchmark data (TED Talks) with a combined F1 of 83.9, representing a 48.7 % relative improvement (15.3 absolute) over the previous state of the art.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>