<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Workshop on Statistical Machine Translation (2021) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Workshop on Statistical Machine Translation (2021)</h2><hr><div class="card bg-light mb-2 mb-lg-4"><div class=card-body><h4 class=card-title>Contents</h4><ul class=list-pl-responsive><li><a class=align-middle href=#2021wmt-1>Proceedings of the Sixth Conference on Machine Translation</a>
<span class="badge badge-info align-middle ml-1">52&nbsp;papers</span></li></ul></div></div><div id=2021wmt-1><small><a href=# class=text-muted><i class="fas fa-arrow-up"></i> up</a></small><h4 class="d-sm-flex pb-2 border-bottom"><span class="d-block mr-2 list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.pdf data-toggle=tooltip data-placement=top title="Open full proceedings volume as PDF">pdf&nbsp;(full)</a><br class="d-none d-sm-inline-block"></span><a class=align-middle href=/volumes/2021.wmt-1/>Proceedings of the Sixth Conference on Machine Translation</a></h4><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.0/>Proceedings of the Sixth Conference on Machine Translation</a></strong><br><a href=/people/l/loic-barrault/>Loic Barrault</a>
|
<a href=/people/o/ondrej-bojar/>Ondrej Bojar</a>
|
<a href=/people/f/fethi-bougares/>Fethi Bougares</a>
|
<a href=/people/r/rajen-chatterjee/>Rajen Chatterjee</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussa</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/m/markus-freitag/>Markus Freitag</a>
|
<a href=/people/y/yvette-graham/>Yvette Graham</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a>
|
<a href=/people/p/paco-guzman/>Paco Guzman</a>
|
<a href=/people/b/barry-haddow/>Barry Haddow</a>
|
<a href=/people/m/matthias-huck/>Matthias Huck</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/t/tom-kocmi/>Tom Kocmi</a>
|
<a href=/people/a/andre-f-t-martins/>Andre Martins</a>
|
<a href=/people/m/makoto-morishita/>Makoto Morishita</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.3.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--3 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.3 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.3/>GTCOM Neural Machine Translation Systems for WMT21<span class=acl-fixed-case>GTCOM</span> Neural Machine Translation Systems for <span class=acl-fixed-case>WMT</span>21</a></strong><br><a href=/people/c/chao-bei/>Chao Bei</a>
|
<a href=/people/h/hao-zong/>Hao Zong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--3><div class="card-body p-3 small">This paper describes the Global Tone Communication Co., Ltd.&#8217;s submission of the WMT21 shared news translation task. We participate in six directions : <a href=https://en.wikipedia.org/wiki/English_language>English</a> to / from <a href=https://en.wikipedia.org/wiki/Hausa_language>Hausa</a>, <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> to / from Bengali and <a href=https://en.wikipedia.org/wiki/Zulu_language>Zulu</a> to / from <a href=https://en.wikipedia.org/wiki/Xhosa_language>Xhosa</a>. Our submitted systems are unconstrained and focus on multilingual translation odel, backtranslation and forward-translation. We also apply rules and language model to filter monolingual, parallel sentences and synthetic sentences.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.6/>The TALP-UPC Participation in WMT21 News Translation Task : an mBART-based NMT Approach<span class=acl-fixed-case>TALP</span>-<span class=acl-fixed-case>UPC</span> Participation in <span class=acl-fixed-case>WMT</span>21 News Translation Task: an m<span class=acl-fixed-case>BART</span>-based <span class=acl-fixed-case>NMT</span> Approach</a></strong><br><a href=/people/c/carlos-escolano/>Carlos Escolano</a>
|
<a href=/people/i/ioannis-tsiamas/>Ioannis Tsiamas</a>
|
<a href=/people/c/christine-basta/>Christine Basta</a>
|
<a href=/people/j/javier-ferrando/>Javier Ferrando</a>
|
<a href=/people/m/marta-r-costa-jussa/>Marta R. Costa-jussa</a>
|
<a href=/people/j/jose-a-r-fonollosa/>José A. R. Fonollosa</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--6><div class="card-body p-3 small">This paper describes the submission to the WMT 2021 news translation shared task by the UPC Machine Translation group. The goal of the task is to translate German to French (De-Fr) and French to German (Fr-De). Our submission focuses on fine-tuning a pre-trained model to take advantage of <a href=https://en.wikipedia.org/wiki/Monolingualism>monolingual data</a>. We fine-tune mBART50 using the filtered data, and additionally, we train a Transformer model on the same <a href=https://en.wikipedia.org/wiki/Data>data</a> from scratch. In the experiments, we show that fine-tuning mBART50 results in 31.69 <a href=https://en.wikipedia.org/wiki/British_thermal_unit>BLEU</a> for De-Fr and 23.63 <a href=https://en.wikipedia.org/wiki/British_thermal_unit>BLEU</a> for Fr-De, which increases 2.71 and 1.90 BLEU accordingly, as compared to the model we train from scratch. Our final submission is an ensemble of these two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, further increasing 0.3 <a href=https://en.wikipedia.org/wiki/Bijection>BLEU</a> for Fr-De.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.9/>Mieind’s WMT 2021 Submission<span class=acl-fixed-case>WMT</span> 2021 Submission</a></strong><br><a href=/people/h/haukur-barri-simonarson/>Haukur Barri Símonarson</a>
|
<a href=/people/v/vesteinn-snaebjarnarson/>Vésteinn Snæbjarnarson</a>
|
<a href=/people/p/petur-orri-ragnarson/>Pétur Orri Ragnarson</a>
|
<a href=/people/h/haukur-jonsson/>Haukur Jónsson</a>
|
<a href=/people/v/vilhjalmur-thorsteinsson/>Vilhjalmur Thorsteinsson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--9><div class="card-body p-3 small">We present Mieind&#8217;s submission for the EnglishIcelandic and IcelandicEnglish subsets of the 2021 WMT news translation task. Transformer-base models are trained for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> on <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> to generate backtranslations teratively. A pretrained mBART-25 model is then adapted for <a href=https://en.wikipedia.org/wiki/Translation>translation</a> using parallel data as well as the last backtranslation iteration. This adapted pretrained model is then used to re-generate backtranslations, and the training of the adapted <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is continued.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.10/>Allegro.eu Submission to WMT21 News Translation Task<span class=acl-fixed-case>WMT</span>21 News Translation Task</a></strong><br><a href=/people/m/mikolaj-koszowski/>Mikołaj Koszowski</a>
|
<a href=/people/k/karol-grzegorczyk/>Karol Grzegorczyk</a>
|
<a href=/people/t/tsimur-hadeliya/>Tsimur Hadeliya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--10><div class="card-body p-3 small">We submitted two uni-directional models, one for EnglishIcelandic direction and other for IcelandicEnglish direction. Our news translation system is based on the transformer-big architecture, it makes use of corpora filtering, <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> and forward translation applied to parallel and monolingual data alike</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.11/>Illinois Japanese English News Translation for WMT 2021<span class=acl-fixed-case>I</span>llinois <span class=acl-fixed-case>J</span>apanese <span class=tex-math>↔</span> <span class=acl-fixed-case>E</span>nglish <span class=acl-fixed-case>N</span>ews <span class=acl-fixed-case>T</span>ranslation for <span class=acl-fixed-case>WMT</span> 2021</a></strong><br><a href=/people/g/giang-le/>Giang Le</a>
|
<a href=/people/s/shinka-mori/>Shinka Mori</a>
|
<a href=/people/l/lane-schwartz/>Lane Schwartz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--11><div class="card-body p-3 small">This system paper describes an end-to-end NMT pipeline for the Japanese English news translation task as submitted to WMT 2021, where we explore the efficacy of techniques such as tokenizing with language-independent and language-dependent tokenizers, normalizing by orthographic conversion, creating a politeness-and-formality-aware model by implementing a tagger, back-translation, model ensembling, and n-best reranking. We use parallel corpora provided by WMT 2021 organizers for training, and development and test data from WMT 2020 for evaluation of different experiment models. The preprocessed corpora are trained with a Transformer neural network model. We found that combining various techniques described herein, such as language-independent BPE tokenization, incorporating politeness and formality tags, model ensembling, n-best reranking, and back-translation produced the best translation models relative to other experiment systems.<tex-math>\\leftrightarrow</tex-math> English news translation task as submitted to WMT 2021, where we explore the efficacy of techniques such as tokenizing with language-independent and language-dependent tokenizers, normalizing by orthographic conversion, creating a politeness-and-formality-aware model by implementing a tagger, back-translation, model ensembling, and n-best reranking. We use parallel corpora provided by WMT 2021 organizers for training, and development and test data from WMT 2020 for evaluation of different experiment models. The preprocessed corpora are trained with a Transformer neural network model. We found that combining various techniques described herein, such as language-independent BPE tokenization, incorporating politeness and formality tags, model ensembling, n-best reranking, and back-translation produced the best translation models relative to other experiment systems.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.13/>The Fujitsu DMATH Submissions for WMT21 News Translation and Biomedical Translation Tasks<span class=acl-fixed-case>DMATH</span> Submissions for <span class=acl-fixed-case>WMT</span>21 News Translation and Biomedical Translation Tasks</a></strong><br><a href=/people/a/ander-martinez/>Ander Martinez</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--13><div class="card-body p-3 small">This paper describes the Fujitsu DMATH systems used for WMT 2021 News Translation and Biomedical Translation tasks. We focused on low-resource pairs, using a simple <a href=https://en.wikipedia.org/wiki/System>system</a>. We conducted experiments on <a href=https://en.wikipedia.org/wiki/Hausa_language>English-Hausa</a>, <a href=https://en.wikipedia.org/wiki/Xhosa_language>Xhosa-Zulu</a> and <a href=https://en.wikipedia.org/wiki/Basque_language>English-Basque</a>, and submitted the results for XhosaZulu in the News Translation Task, and EnglishBasque in the Biomedical Translation Task, abstract and terminology translation subtasks. Our system combines BPE dropout, sub-subword features and back-translation with a Transformer (base) model, achieving good results on the evaluation sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.16.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--16 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.16 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.16/>The University of Edinburgh’s Bengali-Hindi Submissions to the WMT21 News Translation Task<span class=acl-fixed-case>U</span>niversity of <span class=acl-fixed-case>E</span>dinburgh’s <span class=acl-fixed-case>B</span>engali-<span class=acl-fixed-case>H</span>indi Submissions to the <span class=acl-fixed-case>WMT</span>21 News Translation Task</a></strong><br><a href=/people/p/proyag-pal/>Proyag Pal</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/p/pinzhen-chen/>Pinzhen Chen</a>
|
<a href=/people/s/sukanta-sen/>Sukanta Sen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--16><div class="card-body p-3 small">We describe the University of Edinburgh&#8217;s BengaliHindi constrained systems submitted to the WMT21 News Translation task. We submitted ensembles of Transformer models built with large-scale back-translation and fine-tuned on subsets of training data retrieved based on similarity to the target domain.<tex-math>\\leftrightarrow</tex-math>Hindi constrained systems submitted to the WMT21 News Translation task. We submitted ensembles of Transformer models built with large-scale back-translation and fine-tuned on subsets of training data retrieved based on similarity to the target domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.17/>The Volctrans GLAT System : Non-autoregressive Translation Meets WMT21<span class=acl-fixed-case>GLAT</span> System: Non-autoregressive Translation Meets <span class=acl-fixed-case>WMT</span>21</a></strong><br><a href=/people/l/lihua-qian/>Lihua Qian</a>
|
<a href=/people/y/yi-zhou/>Yi Zhou</a>
|
<a href=/people/z/zaixiang-zheng/>Zaixiang Zheng</a>
|
<a href=/people/y/yaoming-zhu/>Yaoming Zhu</a>
|
<a href=/people/z/zehui-lin/>Zehui Lin</a>
|
<a href=/people/j/jiangtao-feng/>Jiangtao Feng</a>
|
<a href=/people/s/shanbo-cheng/>Shanbo Cheng</a>
|
<a href=/people/l/lei-li/>Lei Li</a>
|
<a href=/people/m/mingxuan-wang/>Mingxuan Wang</a>
|
<a href=/people/h/hao-zhou/>Hao Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--17><div class="card-body p-3 small">This paper describes the Volctrans&#8217; submission to the WMT21 news translation shared task for German-English translation. We build a parallel (i.e., non-autoregressive) translation system using the Glancing Transformer, which enables fast and accurate parallel decoding in contrast to the currently prevailing <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a>. To the best of our knowledge, this is the first parallel translation system that can be scaled to such a practical scenario like WMT competition. More importantly, our parallel translation system achieves the best BLEU score (35.0) on German-English translation task, outperforming all strong <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive counterparts</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.20/>Tencent Translation System for the WMT21 News Translation Task<span class=acl-fixed-case>WMT</span>21 News Translation Task</a></strong><br><a href=/people/l/longyue-wang/>Longyue Wang</a>
|
<a href=/people/m/mu-li/>Mu Li</a>
|
<a href=/people/f/fangxu-liu/>Fangxu Liu</a>
|
<a href=/people/s/shuming-shi/>Shuming Shi</a>
|
<a href=/people/z/zhaopeng-tu/>Zhaopeng Tu</a>
|
<a href=/people/x/xing-wang/>Xing Wang</a>
|
<a href=/people/s/shuangzhi-wu/>Shuangzhi Wu</a>
|
<a href=/people/j/jiali-zeng/>Jiali Zeng</a>
|
<a href=/people/w/wen-zhang/>Wen Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--20><div class="card-body p-3 small">This paper describes Tencent Translation systems for the WMT21 shared task. We participate in the news translation task on three language pairs : Chinese-English, English-Chinese and German-English. Our <a href=https://en.wikipedia.org/wiki/System>systems</a> are built on various Transformer models with novel techniques adapted from our recent research work. First, we combine different data augmentation methods including <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, forward-translation and right-to-left training to enlarge the training data. We also apply language coverage bias, data rejuvenation and uncertainty-based sampling approaches to select content-relevant and high-quality data from large parallel and monolingual corpora. Expect for in-domain fine-tuning, we also propose a fine-grained one model one domain approach to model characteristics of different news genres at fine-tuning and decoding stages. Besides, we use greed-based ensemble algorithm and transductive ensemble method to further boost our systems. Based on our success in the last WMT, we continuously employed advanced techniques such as large batch training, data selection and data filtering. Finally, our constrained Chinese-English system achieves 33.4 case-sensitive BLEU score, which is the highest among all submissions. The German-English system is ranked at second place accordingly.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.21/>HW-TSC’s Participation in the WMT 2021 News Translation Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2021 News Translation Shared Task</a></strong><br><a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--21><div class="card-body p-3 small">This paper presents the submission of Huawei Translate Services Center (HW-TSC) to the WMT 2021 News Translation Shared Task. We participate in 7 language pairs, including Zh / En, De / En, Ja / En, Ha / En, Is / En, Hi / Bn, and Xh / Zu in both directions under the constrained condition. We use Transformer architecture and obtain the best performance via multiple variants with larger parameter sizes. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual datasets. Several commonly used strategies are used to train our models, such as <a href=https://en.wikipedia.org/wiki/Back_translation>Back Translation</a>, Forward Translation, Multilingual Translation, Ensemble Knowledge Distillation, etc. Our submission obtains competitive results in the final evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.24.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--24 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.24 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.24/>Small Model and In-Domain Data Are All You Need</a></strong><br><a href=/people/h/hui-zeng/>Hui Zeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--24><div class="card-body p-3 small">I participated in the WMT shared news translation task and focus on one high resource language pair : <a href=https://en.wikipedia.org/wiki/English_language>English</a> and Chinese (two directions, Chinese to English and English to Chinese). The submitted systems (ZengHuiMT) focus on <a href=https://en.wikipedia.org/wiki/Data_cleansing>data cleaning</a>, data selection, <a href=https://en.wikipedia.org/wiki/Back_translation>back translation</a> and model ensemble. The techniques I used for data filtering and selection include filtering by rules, <a href=https://en.wikipedia.org/wiki/Language_model>language model</a> and <a href=https://en.wikipedia.org/wiki/Word_alignment>word alignment</a>. I used a base translation model trained on initial corpus to obtain the target versions of the WMT21 test sets, then I used language models to find out the monolingual data that is most similar to the target version of test set, such monolingual data was then used to do back translation. On the test set, my best submitted systems achieve 35.9 and 32.2 <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> for English to Chinese and Chinese to English directions respectively, which are quite high for a small model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.25/>The Mininglamp Machine Translation System for WMT21<span class=acl-fixed-case>WMT</span>21</a></strong><br><a href=/people/s/shiyu-zhao/>Shiyu Zhao</a>
|
<a href=/people/x/xiaopu-li/>Xiaopu Li</a>
|
<a href=/people/m/minghui-wu/>Minghui Wu</a>
|
<a href=/people/j/jie-hao/>Jie Hao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--25><div class="card-body p-3 small">This paper describes Mininglamp neural machine translation systems of the WMT2021 news translation tasks. We have participated in eight directions translation tasks for news text including <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> to / from English, Hausa to / from English, <a href=https://en.wikipedia.org/wiki/German_language>German</a> to / from English and <a href=https://en.wikipedia.org/wiki/French_language>French</a> to / from German. Our fundamental system was based on Transformer architecture, with wider or smaller construction for different news translation tasks. We mainly utilized the method of <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, knowledge distillation and <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a> to boost single model, while the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> was used to combine single models. Our final submission has ranked first for the English to / from Hausa task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.27.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--27 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.27 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.27/>Improving Similar Language Translation With <a href=https://en.wikipedia.org/wiki/Transfer_of_learning>Transfer Learning</a></a></strong><br><a href=/people/i/ife-adebara/>Ife Adebara</a>
|
<a href=/people/m/muhammad-abdul-mageed/>Muhammad Abdul-Mageed</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--27><div class="card-body p-3 small">We investigate <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> based on pre-trained neural machine translation models to translate between (low-resource) similar languages. This work is part of our contribution to the WMT 2021 Similar Languages Translation Shared Task where we submitted models for different language pairs, including French-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for Catalan-Spanish (82.79 BLEU)and Portuguese-Spanish (87.11 BLEU) rank top 1 in the official shared task evaluation, and we are the only team to submit <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> for the <a href=https://en.wikipedia.org/wiki/Bambara_language>French-Bambara pairs</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.28.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--28 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.28 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.28/>T4 T Solution : WMT21 Similar Language Task for the Spanish-Catalan and Spanish-Portuguese Language Pair<span class=acl-fixed-case>T</span>4<span class=acl-fixed-case>T</span> Solution: <span class=acl-fixed-case>WMT</span>21 Similar Language Task for the <span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>C</span>atalan and <span class=acl-fixed-case>S</span>panish-<span class=acl-fixed-case>P</span>ortuguese Language Pair</a></strong><br><a href=/people/m/miguel-canals/>Miguel Canals</a>
|
<a href=/people/m/marc-raventos-tato/>Marc Raventós Tato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--28><div class="card-body p-3 small">The main idea of this <a href=https://en.wikipedia.org/wiki/Solution>solution</a> has been to focus on corpus cleaning and preparation and after that, use an out of box solution (OpenNMT) with its default published transformer model. To prepare the corpus, we have used set of standard tools (as Moses scripts or python packages), but also, among other python scripts, a python custom tokenizer with the ability to replace numbers for variables, solve the upper / lower case issue of the vocabulary and provide good segmentation for most of the punctuation. We also have started a line to clean corpus based on statistical probability estimation of source-target corpus, with unclear results. Also, we have run some tests with syllabical word segmentation, again with unclear results, so at the end, after word sentence tokenization we have used BPE SentencePiece for subword units to feed OpenNMT.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.31/>Similar Language Translation for <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a>, <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> Using Marian NMT<span class=acl-fixed-case>C</span>atalan, <span class=acl-fixed-case>P</span>ortuguese and <span class=acl-fixed-case>S</span>panish Using <span class=acl-fixed-case>M</span>arian <span class=acl-fixed-case>NMT</span></a></strong><br><a href=/people/r/reinhard-rapp/>Reinhard Rapp</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--31><div class="card-body p-3 small">This paper describes the SEBAMAT contribution to the 2021 WMT Similar Language Translation shared task. Using the Marian neural machine translation toolkit, translation systems based on Google&#8217;s transformer architecture were built in both directions of CatalanSpanish and PortugueseSpanish. The systems were trained in two contrastive parameter settings (different vocabulary sizes for byte pair encoding) using only the parallel but not the comparable corpora provided by the shared task organizers. According to their official evaluation results, the SEBAMAT system turned out to be competitive with rankings among the top teams and BLEU scores between 38 and 47 for the language pairs involving <a href=https://en.wikipedia.org/wiki/Portuguese_language>Portuguese</a> and between 76 and 80 for the language pairs involving <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.35/>Adapting <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> for Automatic Post-Editing</a></strong><br><a href=/people/a/abhishek-sharma/>Abhishek Sharma</a>
|
<a href=/people/p/prabhakar-gupta/>Prabhakar Gupta</a>
|
<a href=/people/a/anil-nelakanti/>Anil Nelakanti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--35><div class="card-body p-3 small">Automatic post-editing (APE) models are usedto correct machine translation (MT) system outputs by learning from human post-editing patterns. We present the system used in our submission to the WMT&#8217;21 Automatic Post-Editing (APE) English-German (En-De) shared task. We leverage the state-of-the-art MT system (Ng et al., 2019) for this task. For further improvements, we adapt the MT model to the task domain by using WikiMatrix (Schwenket al., 2021) followed by fine-tuning with additional APE samples from previous editions of the <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared task</a> (WMT-16,17,18) and ensembling the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. Our systems beat the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline</a> on TER scores on the WMT&#8217;21 test set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.37.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--37 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.37 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.37/>HW-TSC’s Participation in the WMT 2021 Triangular MT Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2021 Triangular <span class=acl-fixed-case>MT</span> Shared Task</a></strong><br><a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--37><div class="card-body p-3 small">This paper presents the submission of Huawei Translation Service Center (HW-TSC) to WMT 2021 Triangular MT Shared Task. We participate in the Russian-to-Chinese task under the constrained condition. We use Transformer architecture and obtain the best performance via a variant with larger parameter sizes. We perform detailed data pre-processing and filtering on the provided large-scale bilingual data. Several strategies are used to train our models, such as Multilingual Translation, Back Translation, Forward Translation, Data Denoising, Average Checkpoint, Ensemble, Fine-tuning, etc. Our <a href=https://en.wikipedia.org/wiki/System>system</a> obtains 32.5 <a href=https://en.wikipedia.org/wiki/British_undergraduate_degree_classification>BLEU</a> on the <a href=https://en.wikipedia.org/wiki/British_undergraduate_degree_classification>dev set</a> and 27.7 BLEU on the <a href=https://en.wikipedia.org/wiki/British_undergraduate_degree_classification>test set</a>, the highest score among all submissions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.43.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--43 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.43 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.43" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.43/>Transfer Learning with Shallow Decoders : BSC at WMT2021’s Multilingual Low-Resource Translation for Indo-European Languages Shared Task<span class=acl-fixed-case>BSC</span> at <span class=acl-fixed-case>WMT</span>2021’s Multilingual Low-Resource Translation for <span class=acl-fixed-case>I</span>ndo-<span class=acl-fixed-case>E</span>uropean Languages Shared Task</a></strong><br><a href=/people/k/ksenia-kharitonova/>Ksenia Kharitonova</a>
|
<a href=/people/o/ona-de-gibert-bonet/>Ona de Gibert Bonet</a>
|
<a href=/people/j/jordi-armengol-estape/>Jordi Armengol-Estapé</a>
|
<a href=/people/m/mar-rodriguez-i-alvarez/>Mar Rodriguez i Alvarez</a>
|
<a href=/people/m/maite-melero/>Maite Melero</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--43><div class="card-body p-3 small">This paper describes the participation of the BSC team in the WMT2021&#8217;s Multilingual Low-Resource Translation for Indo-European Languages Shared Task. The system aims to solve the Subtask 2 : Wikipedia cultural heritage articles, which involves translation in four <a href=https://en.wikipedia.org/wiki/Romance_languages>Romance languages</a> : <a href=https://en.wikipedia.org/wiki/Catalan_language>Catalan</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, <a href=https://en.wikipedia.org/wiki/Occitan_language>Occitan</a> and <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a>. The submitted <a href=https://en.wikipedia.org/wiki/System>system</a> is a multilingual semi-supervised machine translation model. It is based on a pre-trained language model, namely XLM-RoBERTa, that is later fine-tuned with parallel data obtained mostly from <a href=https://en.wikipedia.org/wiki/OPUS>OPUS</a>. Unlike other works, we only use XLM to initialize the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> and randomly initialize a shallow decoder. The reported results are robust and perform well for all tested languages.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.50.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--50 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.50 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.50" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.50/>Back-translation for Large-Scale Multilingual Machine Translation</a></strong><br><a href=/people/b/baohao-liao/>Baohao Liao</a>
|
<a href=/people/s/shahram-khadivi/>Shahram Khadivi</a>
|
<a href=/people/s/sanjika-hewavitharana/>Sanjika Hewavitharana</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--50><div class="card-body p-3 small">This paper illustrates our approach to the shared task on large-scale multilingual machine translation in the sixth conference on machine translation (WMT-21). In this work, we aim to build a single multilingual translation system with a hypothesis that a universal cross-language representation leads to better multilingual translation performance. We extend the exploration of different back-translation methods from bilingual translation to multilingual translation. Better performance is obtained by the constrained sampling method, which is different from the finding of the bilingual translation. Besides, we also explore the effect of <a href=https://en.wikipedia.org/wiki/Vocabulary>vocabularies</a> and the amount of <a href=https://en.wikipedia.org/wiki/Synthetic_data>synthetic data</a>. Surprisingly, the smaller size of vocabularies perform better, and the extensive monolingual English data offers a modest improvement. We submitted to both the small tasks and achieve the second place.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.51.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--51 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.51 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.51/>Maastricht University’s Large-Scale Multilingual Machine Translation System for WMT 2021<span class=acl-fixed-case>WMT</span> 2021</a></strong><br><a href=/people/d/danni-liu/>Danni Liu</a>
|
<a href=/people/j/jan-niehues/>Jan Niehues</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--51><div class="card-body p-3 small">We present our development of the multilingual machine translation system for the large-scale multilingual machine translation task at WMT 2021. Starting form the provided <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a>, we investigated several techniques to improve the translation quality on the target subset of languages. We were able to significantly improve the translation quality by adapting the <a href=https://en.wikipedia.org/wiki/System>system</a> towards the target subset of languages and by generating synthetic data using the initial <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. Techniques successfully applied in zero-shot multilingual machine translation (e.g. similarity regularizer) only had a minor effect on the final translation performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.54.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--54 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.54 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.54/>Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task<span class=acl-fixed-case>M</span>icrosoft for <span class=acl-fixed-case>WMT</span>21 Shared Task</a></strong><br><a href=/people/j/jian-yang/>Jian Yang</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/h/haoyang-huang/>Haoyang Huang</a>
|
<a href=/people/d/dongdong-zhang/>Dongdong Zhang</a>
|
<a href=/people/l/li-dong/>Li Dong</a>
|
<a href=/people/s/shaohan-huang/>Shaohan Huang</a>
|
<a href=/people/a/alexandre-muzio/>Alexandre Muzio</a>
|
<a href=/people/s/saksham-singhal/>Saksham Singhal</a>
|
<a href=/people/h/hany-hassan-awadalla/>Hany Hassan</a>
|
<a href=/people/x/xia-song/>Xia Song</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--54><div class="card-body p-3 small">This report describes Microsoft&#8217;s <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a> for the WMT21 shared task on large-scale multilingual machine translation. We participated in all three evaluation tracks including Large Track and two Small Tracks where the former one is unconstrained and the latter two are fully constrained. Our model submissions to the shared task were initialized with DeltaLM, a generic pre-trained multilingual encoder-decoder model, and fine-tuned correspondingly with the vast collected parallel data and allowed data sources according to track settings, together with applying progressive learning and iterative back-translation approaches to further improve the performance. Our final submissions ranked first on three tracks in terms of the automatic evaluation metric.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.55.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--55 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.55 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.55/>HW-TSC’s Participation in the WMT 2021 Large-Scale Multilingual Translation Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation in the <span class=acl-fixed-case>WMT</span> 2021 Large-Scale Multilingual Translation Task</a></strong><br><a href=/people/z/zhengzhe-yu/>Zhengzhe Yu</a>
|
<a href=/people/d/daimeng-wei/>Daimeng Wei</a>
|
<a href=/people/z/zongyao-li/>Zongyao Li</a>
|
<a href=/people/h/hengchao-shang/>Hengchao Shang</a>
|
<a href=/people/x/xiaoyu-chen/>Xiaoyu Chen</a>
|
<a href=/people/z/zhanglin-wu/>Zhanglin Wu</a>
|
<a href=/people/j/jiaxin-guo/>Jiaxin Guo</a>
|
<a href=/people/m/minghan-wang/>Minghan Wang</a>
|
<a href=/people/l/lizhi-lei/>Lizhi Lei</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/y/ying-qin/>Ying Qin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--55><div class="card-body p-3 small">This paper presents the submission of Huawei Translation Services Center (HW-TSC) to the WMT 2021 Large-Scale Multilingual Translation Task. We participate in Samll Track # 2, including 6 languages : <a href=https://en.wikipedia.org/wiki/Javanese_language>Javanese (Jv)</a>, <a href=https://en.wikipedia.org/wiki/Indonesian_language>Indonesian (I d)</a>, <a href=https://en.wikipedia.org/wiki/Malay_language>Malay (Ms)</a>, <a href=https://en.wikipedia.org/wiki/Tagalog_language>Tagalog (Tl)</a>, <a href=https://en.wikipedia.org/wiki/Tamil_language>Tamil (Ta)</a> and <a href=https://en.wikipedia.org/wiki/English_language>English (En)</a> with 30 directions under the constrained condition. We use Transformer architecture and obtain the best performance via multiple variants with larger parameter sizes. We train a single multilingual model to translate all the 30 directions. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual datasets. Several commonly used strategies are used to train our models, such as <a href=https://en.wikipedia.org/wiki/Back_translation>Back Translation</a>, Forward Translation, Ensemble Knowledge Distillation, Adapter Fine-tuning. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> obtains competitive results in the end.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.58" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.58/>Just Ask ! Evaluating <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> by Asking and Answering Questions</a></strong><br><a href=/people/m/mateusz-krubinski/>Mateusz Krubiński</a>
|
<a href=/people/e/erfan-ghadery/>Erfan Ghadery</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>
|
<a href=/people/p/pavel-pecina/>Pavel Pecina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--58><div class="card-body p-3 small">In this paper, we show that automatically-generated questions and answers can be used to evaluate the quality of Machine Translation (MT) systems. Building on recent work on the evaluation of abstractive text summarization, we propose a new <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metric</a> for system-level MT evaluation, compare it with other state-of-the-art solutions, and show its robustness by conducting experiments for various MT directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.60.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--60 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.60 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.60" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.60/>Evaluating Multiway Multilingual NMT in the <a href=https://en.wikipedia.org/wiki/Turkic_languages>Turkic Languages</a><span class=acl-fixed-case>NMT</span> in the <span class=acl-fixed-case>T</span>urkic Languages</a></strong><br><a href=/people/j/jamshidbek-mirzakhalov/>Jamshidbek Mirzakhalov</a>
|
<a href=/people/a/anoop-babu/>Anoop Babu</a>
|
<a href=/people/a/aigiz-kunafin/>Aigiz Kunafin</a>
|
<a href=/people/a/ahsan-wahab/>Ahsan Wahab</a>
|
<a href=/people/b/bekhzodbek-moydinboyev/>Bekhzodbek Moydinboyev</a>
|
<a href=/people/s/sardana-ivanova/>Sardana Ivanova</a>
|
<a href=/people/m/mokhiyakhon-uzokova/>Mokhiyakhon Uzokova</a>
|
<a href=/people/s/shaxnoza-pulatova/>Shaxnoza Pulatova</a>
|
<a href=/people/d/duygu-ataman/>Duygu Ataman</a>
|
<a href=/people/j/julia-kreutzer/>Julia Kreutzer</a>
|
<a href=/people/f/francis-tyers/>Francis Tyers</a>
|
<a href=/people/o/orhan-firat/>Orhan Firat</a>
|
<a href=/people/j/john-licato/>John Licato</a>
|
<a href=/people/s/sriram-chellappan/>Sriram Chellappan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--60><div class="card-body p-3 small">Despite the increasing number of large and comprehensive machine translation (MT) systems, evaluation of these methods in various languages has been restrained by the lack of high-quality parallel corpora as well as engagement with the people that speak these languages. In this study, we present an evaluation of state-of-the-art approaches to training and evaluating MT systems in 22 languages from the <a href=https://en.wikipedia.org/wiki/Turkic_languages>Turkic language family</a>, most of which being extremely under-explored. First, we adopt the TIL Corpus with a few key improvements to the training and the evaluation sets. Then, we train 26 bilingual baselines as well as a multi-way neural MT (MNMT) model using the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> and perform an extensive analysis using automatic metrics as well as human evaluations. We find that the MNMT model outperforms almost all bilingual baselines in the out-of-domain test sets and finetuning the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on a downstream task of a single pair also results in a huge performance boost in both low- and high-resource scenarios. Our attentive analysis of evaluation criteria for MT models in <a href=https://en.wikipedia.org/wiki/Turkic_languages>Turkic languages</a> also points to the necessity for further research in this direction. We release the corpus splits, test sets as well as <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> to the public.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.63/>DELA Corpus-A Document-Level Corpus Annotated with Context-Related Issues<span class=acl-fixed-case>DELA</span> Corpus - A Document-Level Corpus Annotated with Context-Related Issues</a></strong><br><a href=/people/s/sheila-castilho/>Sheila Castilho</a>
|
<a href=/people/j/joao-lucas-cavalheiro-camargo/>João Lucas Cavalheiro Camargo</a>
|
<a href=/people/m/miguel-menezes/>Miguel Menezes</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--63><div class="card-body p-3 small">Recently, the Machine Translation (MT) community has become more interested in document-level evaluation especially in light of reactions to claims of human parity, since examining the quality at the level of the document rather than at the sentence level allows for the assessment of suprasentential context, providing a more reliable evaluation. This paper presents a document-level corpus annotated in English with context-aware issues that arise when translating from <a href=https://en.wikipedia.org/wiki/English_language>English</a> into <a href=https://en.wikipedia.org/wiki/Brazilian_Portuguese>Brazilian Portuguese</a>, namely ellipsis, gender, lexical ambiguity, number, reference, and terminology, with six different domains. The <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> can be used as a challenge test set for evaluation and as a training / testing corpus for MT as well as for deep linguistic analysis of context issues. To the best of our knowledge, this is the first corpus of its kind.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.66.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--66 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.66 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.66/>Improving <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> of Rare and Unseen Word Senses</a></strong><br><a href=/people/v/viktor-hangya/>Viktor Hangya</a>
|
<a href=/people/q/qianchu-liu/>Qianchu Liu</a>
|
<a href=/people/d/dario-stojanovski/>Dario Stojanovski</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--66><div class="card-body p-3 small">The performance of NMT systems has improved drastically in the past few years but the translation of multi-sense words still poses a challenge. Since word senses are not represented uniformly in the <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> used for training, there is an excessive use of the most frequent sense in MT output. In this work, we propose CmBT (Contextually-mined Back-Translation), an approach for improving multi-sense word translation leveraging pre-trained cross-lingual contextual word representations (CCWRs). Because of their contextual sensitivity and their large <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>pre-training data</a>, CCWRs can easily capture <a href=https://en.wikipedia.org/wiki/Word_sense>word senses</a> that are missing or very rare in <a href=https://en.wikipedia.org/wiki/Parallel_text>parallel corpora</a> used to train MT. Specifically, CmBT applies bilingual lexicon induction on CCWRs to mine sense-specific target sentences from a monolingual dataset, and then back-translates these sentences to generate a pseudo parallel corpus as additional training data for an MT system. We test the translation quality of ambiguous words on the MuCoW test suite, which was built to test the <a href=https://en.wikipedia.org/wiki/Word-sense_disambiguation>word sense disambiguation</a> effectiveness of MT systems. We show that our <a href=https://en.wikipedia.org/wiki/System>system</a> improves on the translation of difficult unseen and low frequency word senses.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.71/>Findings of the WMT 2021 Shared Task on Quality Estimation<span class=acl-fixed-case>WMT</span> 2021 Shared Task on Quality Estimation</a></strong><br><a href=/people/l/lucia-specia/>Lucia Specia</a>
|
<a href=/people/f/frederic-blain/>Frédéric Blain</a>
|
<a href=/people/m/marina-fomicheva/>Marina Fomicheva</a>
|
<a href=/people/c/chrysoula-zerva/>Chrysoula Zerva</a>
|
<a href=/people/z/zhenhao-li/>Zhenhao Li</a>
|
<a href=/people/v/vishrav-chaudhary/>Vishrav Chaudhary</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--71><div class="card-body p-3 small">We report the results of the WMT 2021 shared task on Quality Estimation, where the challenge is to predict the quality of the output of neural machine translation systems at the word and sentence levels. This edition focused on two main novel additions : (i) <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> for unseen languages, i.e. zero-shot settings, and (ii) prediction of sentences with catastrophic errors. In addition, new <a href=https://en.wikipedia.org/wiki/Data_(computing)>data</a> was released for a number of languages, especially post-edited data. Participating teams from 19 institutions submitted altogether 1263 <a href=https://en.wikipedia.org/wiki/System>systems</a> to different task variants and language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.74.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--74 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.74 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.74/>Efficient <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> with Model Pruning and Quantization</a></strong><br><a href=/people/m/maximiliana-behnke/>Maximiliana Behnke</a>
|
<a href=/people/n/nikolay-bogoychev/>Nikolay Bogoychev</a>
|
<a href=/people/a/alham-fikri-aji/>Alham Fikri Aji</a>
|
<a href=/people/k/kenneth-heafield/>Kenneth Heafield</a>
|
<a href=/people/g/graeme-nail/>Graeme Nail</a>
|
<a href=/people/q/qianqian-zhu/>Qianqian Zhu</a>
|
<a href=/people/s/svetlana-tchistiakova/>Svetlana Tchistiakova</a>
|
<a href=/people/j/jelmer-van-der-linde/>Jelmer van der Linde</a>
|
<a href=/people/p/pinzhen-chen/>Pinzhen Chen</a>
|
<a href=/people/s/sidharth-kashyap/>Sidharth Kashyap</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--74><div class="card-body p-3 small">We participated in all tracks of the WMT 2021 efficient machine translation task : <a href=https://en.wikipedia.org/wiki/Single-core>single-core CPU</a>, <a href=https://en.wikipedia.org/wiki/Multi-core_processor>multi-core CPU</a>, and <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU hardware</a> with throughput and latency conditions. Our submissions combine several efficiency strategies : knowledge distillation, a simpler simple recurrent unit (SSRU) decoder with one or two layers, lexical shortlists, smaller numerical formats, and pruning. For the CPU track, we used <a href=https://en.wikipedia.org/wiki/Quantization_(signal_processing)>quantized 8-bit models</a>. For the GPU track, we experimented with <a href=https://en.wikipedia.org/wiki/FP16>FP16</a> and 8-bit integers in tensorcores. Some of our submissions optimize for size via 4-bit log quantization and omitting a <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical shortlist</a>. We have extended pruning to more parts of the network, emphasizing component- and block-level pruning that actually improves speed unlike coefficient-wise pruning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.78.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--78 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.78 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.78/>Lingua Custodia’s Participation at the WMT 2021 Machine Translation Using Terminologies Shared Task<span class=acl-fixed-case>WMT</span> 2021 Machine Translation Using Terminologies Shared Task</a></strong><br><a href=/people/m/melissa-ailem/>Melissa Ailem</a>
|
<a href=/people/j/jingshu-liu/>Jingshu Liu</a>
|
<a href=/people/r/raheel-qader/>Raheel Qader</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--78><div class="card-body p-3 small">This paper describes Lingua Custodia&#8217;s submission to the WMT21 shared task on <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> using <a href=https://en.wikipedia.org/wiki/Terminology>terminologies</a>. We consider three directions, namely <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/French_language>French</a>, <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>, and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. We rely on a Transformer-based architecture as a building block, and we explore a method which introduces two main changes to the standard procedure to handle terminologies. The first one consists in augmenting the training data in such a way as to encourage the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to learn a copy behavior when it encounters terminology constraint terms. The second change is constraint token masking, whose purpose is to ease copy behavior learning and to improve model generalization. Empirical results show that our method satisfies most terminology constraints while maintaining high translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.79.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--79 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.79 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.79/>Kakao Enterprise’s WMT21 Machine Translation Using Terminologies Task Submission<span class=acl-fixed-case>WMT</span>21 Machine Translation Using Terminologies Task Submission</a></strong><br><a href=/people/y/yunju-bak/>Yunju Bak</a>
|
<a href=/people/j/jimin-sun/>Jimin Sun</a>
|
<a href=/people/j/jay-kim/>Jay Kim</a>
|
<a href=/people/s/sungwon-lyu/>Sungwon Lyu</a>
|
<a href=/people/c/changmin-lee/>Changmin Lee</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--79><div class="card-body p-3 small">This paper describes Kakao Enterprise&#8217;s submission to the WMT21 shared Machine Translation using Terminologies task. We integrate terminology constraints by pre-training with target lemma annotations and fine-tuning with exact target annotations utilizing the given terminology dataset. This approach yields a model that achieves outstanding results in terms of both translation quality and term consistency, ranking first based on COMET in the EnFr language direction. Furthermore, we explore various methods such as <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a>, explicitly training <a href=https://en.wikipedia.org/wiki/Terminology>terminologies</a> as additional parallel data, and in-domain data selection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.80/>The SPECTRANS System Description for the WMT21 Terminology Task<span class=acl-fixed-case>SPECTRANS</span> System Description for the <span class=acl-fixed-case>WMT</span>21 Terminology Task</a></strong><br><a href=/people/n/nicolas-ballier/>Nicolas Ballier</a>
|
<a href=/people/d/dahn-cho/>Dahn Cho</a>
|
<a href=/people/b/bilal-faye/>Bilal Faye</a>
|
<a href=/people/z/zong-you-ke/>Zong-You Ke</a>
|
<a href=/people/h/hanna-martikainen/>Hanna Martikainen</a>
|
<a href=/people/m/mojca-pecman/>Mojca Pecman</a>
|
<a href=/people/g/guillaume-wisniewski/>Guillaume Wisniewski</a>
|
<a href=/people/j/jean-baptiste-yunes/>Jean-Baptiste Yunès</a>
|
<a href=/people/l/lichao-zhu/>Lichao Zhu</a>
|
<a href=/people/m/maria-zimina-poirot/>Maria Zimina-Poirot</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--80><div class="card-body p-3 small">This paper discusses the WMT 2021 terminology shared task from a meta perspective. We present the results of our experiments using the terminology dataset and the OpenNMT (Klein et al., 2017) and JoeyNMT (Kreutzer et al., 2019) toolkits for the language direction English to French. Our experiment 1 compares the predictions of the two <a href=https://en.wikipedia.org/wiki/Widget_toolkit>toolkits</a>. Experiment 2 uses OpenNMT to fine-tune the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. We report our results for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> with the evaluation script but mostly discuss the linguistic properties of the terminology dataset provided for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. We provide evidence of the importance of text genres across scores, having replicated the evaluation scripts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.81.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--81 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.81 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.81/>Dynamic Terminology Integration for COVID-19 and Other Emerging Domains<span class=acl-fixed-case>COVID</span>-19 and Other Emerging Domains</a></strong><br><a href=/people/t/toms-bergmanis/>Toms Bergmanis</a>
|
<a href=/people/m/marcis-pinnis/>Mārcis Pinnis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--81><div class="card-body p-3 small">The majority of <a href=https://en.wikipedia.org/wiki/Domain_of_discourse>language domains</a> require prudent use of terminology to ensure clarity and adequacy of information conveyed. While the correct use of terminology for some languages and domains can be achieved by adapting general-purpose MT systems on large volumes of in-domain parallel data, such quantities of domain-specific data are seldom available for less-resourced languages and niche domains. Furthermore, as exemplified by COVID-19 recently, no domain-specific parallel data is readily available for emerging domains. However, the gravity of this recent calamity created a high demand for reliable translation of critical information regarding pandemic and infection prevention. This work is part of WMT2021 Shared Task : <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a> using Terminologies, where we describe Tilde MT systems that are capable of dynamic terminology integration at the time of translation. Our systems achieve up to 94 % COVID-19 term use accuracy on the test set of the EN-FR language pair without having access to any form of in-domain information during system training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.82.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--82 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.82 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.82/>CUNI Systems for WMT21 : Terminology Translation Shared Task<span class=acl-fixed-case>CUNI</span> Systems for <span class=acl-fixed-case>WMT</span>21: Terminology Translation Shared Task</a></strong><br><a href=/people/j/josef-jon/>Josef Jon</a>
|
<a href=/people/m/michal-novak/>Michal Novák</a>
|
<a href=/people/j/joao-paulo-aires/>João Paulo Aires</a>
|
<a href=/people/d/dusan-varis/>Dusan Varis</a>
|
<a href=/people/o/ondrej-bojar/>Ondřej Bojar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--82><div class="card-body p-3 small">This paper describes Charles University sub-mission for Terminology translation Shared Task at WMT21. The objective of this task is to design a <a href=https://en.wikipedia.org/wiki/System>system</a> which translates certain terms based on a provided <a href=https://en.wikipedia.org/wiki/Terminology_database>terminology database</a>, while preserving high overall translation quality. We competed in English-French language pair. Our approach is based on providing the desired translations alongside the input sentence and training the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to use these provided terms. We lemmatize the terms both during the <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training</a> and inference, to allow the model to learn how to produce correct surface forms of the words, when they differ from the forms provided in the <a href=https://en.wikipedia.org/wiki/Terminology_database>terminology database</a>. Our submission ranked second in Exact Match metric which evaluates the ability of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to produce desired terms in the translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.83.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--83 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.83 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.83/>PROMT Systems for WMT21 Terminology Translation Task<span class=acl-fixed-case>PROMT</span> Systems for <span class=acl-fixed-case>WMT</span>21 Terminology Translation Task</a></strong><br><a href=/people/a/alexander-molchanov/>Alexander Molchanov</a>
|
<a href=/people/v/vladislav-kovalenko/>Vladislav Kovalenko</a>
|
<a href=/people/f/fedor-bykov/>Fedor Bykov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--83><div class="card-body p-3 small">This paper describes the PROMT submissions for the WMT21 Terminology Translation Task. We participate in two directions : <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/French_language>French</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> to <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a>. Our final submissions are MarianNMT-based neural systems. We present two <a href=https://en.wikipedia.org/wiki/Technology>technologies</a> for terminology translation : a modification of the Dinu et al. (2019) soft-constrained approach and our own approach called PROMT Smart Neural Dictionary (SmartND). We achieve good results in both directions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.84/>SYSTRAN @ WMT 2021 : Terminology Task<span class=acl-fixed-case>SYSTRAN</span> @ <span class=acl-fixed-case>WMT</span> 2021: Terminology Task</a></strong><br><a href=/people/m/minh-quang-pham/>Minh Quang Pham</a>
|
<a href=/people/j/josep-m-crego/>Josep Crego</a>
|
<a href=/people/a/antoine-senellart/>Antoine Senellart</a>
|
<a href=/people/d/dan-berrebbi/>Dan Berrebbi</a>
|
<a href=/people/j/jean-senellart/>Jean Senellart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--84><div class="card-body p-3 small">This paper describes SYSTRAN submissions to the WMT 2021 terminology shared task. We participate in the English-to-French translation direction with a standard Transformer neural machine translation network that we enhance with the ability to dynamically include terminology constraints, a very common industrial practice. Two state-of-the-art terminology insertion methods are evaluated based (i) on the use of placeholders complemented with morphosyntactic annotation and (ii) on the use of target constraints injected in the source stream. Results show the suitability of the presented approaches in the evaluated scenario where <a href=https://en.wikipedia.org/wiki/Terminology>terminology</a> is used in a <a href=https://en.wikipedia.org/wiki/System>system</a> trained on generic data only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.85/>TermMind : Alibaba’s WMT21 Machine Translation Using Terminologies Task Submission<span class=acl-fixed-case>T</span>erm<span class=acl-fixed-case>M</span>ind: <span class=acl-fixed-case>A</span>libaba’s <span class=acl-fixed-case>WMT</span>21 Machine Translation Using Terminologies Task Submission</a></strong><br><a href=/people/k/ke-wang/>Ke Wang</a>
|
<a href=/people/s/shuqin-gu/>Shuqin Gu</a>
|
<a href=/people/b/boxing-chen/>Boxing Chen</a>
|
<a href=/people/y/yu-zhao/>Yu Zhao</a>
|
<a href=/people/w/weihua-luo/>Weihua Luo</a>
|
<a href=/people/y/yuqi-zhang/>Yuqi Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--85><div class="card-body p-3 small">This paper describes our work in the WMT 2021 Machine Translation using Terminologies Shared Task. We participate in the shared translation terminologies task in English to Chinese language pair. To satisfy terminology constraints on <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, we use a terminology data augmentation strategy based on Transformer model. We used <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tags</a> to mark and add the term translations into the matched sentences. We created synthetic terms using phrase tables extracted from bilingual corpus to increase the proportion of term translations in training data. Detailed pre-processing and filtering on data, in-domain finetuning and <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble method</a> are used in our system. Our submission obtains competitive results in the terminology-targeted evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.86.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--86 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.86 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.86/>FJWU Participation for the WMT21 Biomedical Translation Task<span class=acl-fixed-case>FJWU</span> Participation for the <span class=acl-fixed-case>WMT</span>21 Biomedical Translation Task</a></strong><br><a href=/people/s/sumbal-naz/>Sumbal Naz</a>
|
<a href=/people/s/sadaf-abdul-rauf/>Sadaf Abdul Rauf</a>
|
<a href=/people/s/sami-ul-haq/>Sami Ul Haq</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--86><div class="card-body p-3 small">In this paper we present the FJWU&#8217;s system submitted to the biomedical shared task at WMT21. We prepared state-of-the-art multilingual neural machine translation systems for three languages (i.e. German, <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/French_language>French</a>) with <a href=https://en.wikipedia.org/wiki/English_language>English</a> as target language. Our NMT systems based on Transformer architecture, were trained on combination of in-domain and out-domain parallel corpora developed using Information Retrieval (IR) and domain adaptation techniques.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.88.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--88 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.88 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.88/>Huawei AARC’s Submissions to the WMT21 Biomedical Translation Task : Domain Adaption from a Practical Perspective<span class=acl-fixed-case>AARC</span>’s Submissions to the <span class=acl-fixed-case>WMT</span>21 Biomedical Translation Task: Domain Adaption from a Practical Perspective</a></strong><br><a href=/people/w/weixuan-wang/>Weixuan Wang</a>
|
<a href=/people/w/wei-peng/>Wei Peng</a>
|
<a href=/people/x/xupeng-meng/>Xupeng Meng</a>
|
<a href=/people/q/qun-liu/>Qun Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--88><div class="card-body p-3 small">This paper describes Huawei Artificial Intelligence Application Research Center&#8217;s neural machine translation systems and submissions to the WMT21 biomedical translation shared task. Four of the submissions achieve state-of-the-art BLEU scores based on the official-released automatic evaluation results (EN-FR, EN-IT and ZH-EN). We perform experiments to unveil the practical insights of the involved domain adaptation techniques, including finetuning order, terminology dictionaries, and ensemble decoding. Issues associated with <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> and under-translation are also discussed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.92.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--92 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.92 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.92/>HW-TSC’s Participation at WMT 2021 Quality Estimation Shared Task<span class=acl-fixed-case>HW</span>-<span class=acl-fixed-case>TSC</span>’s Participation at <span class=acl-fixed-case>WMT</span> 2021 Quality Estimation Shared Task</a></strong><br><a href=/people/y/yimeng-chen/>Yimeng Chen</a>
|
<a href=/people/c/chang-su/>Chang Su</a>
|
<a href=/people/y/yingtao-zhang/>Yingtao Zhang</a>
|
<a href=/people/y/yuxia-wang/>Yuxia Wang</a>
|
<a href=/people/x/xiang-geng/>Xiang Geng</a>
|
<a href=/people/h/hao-yang/>Hao Yang</a>
|
<a href=/people/s/shimin-tao/>Shimin Tao</a>
|
<a href=/people/g/guo-jiaxin/>Guo Jiaxin</a>
|
<a href=/people/w/wang-minghan/>Wang Minghan</a>
|
<a href=/people/m/min-zhang/>Min Zhang</a>
|
<a href=/people/y/yujia-liu/>Yujia Liu</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--92><div class="card-body p-3 small">This paper presents our work in WMT 2021 Quality Estimation (QE) Shared Task. We participated in all of the three sub-tasks, including Sentence-Level Direct Assessment (DA) task, Word and Sentence-Level Post-editing Effort task and Critical Error Detection task, in all language pairs. Our systems employ the framework of Predictor-Estimator, concretely with a pre-trained XLM-Roberta as Predictor and task-specific classifier or regressor as Estimator. For all tasks, we improve our systems by incorporating post-edit sentence or additional high-quality translation sentence in the way of <a href=https://en.wikipedia.org/wiki/Multitask_learning>multitask learning</a> or encoding it with predictors directly. Moreover, in zero-shot setting, our data augmentation strategy based on Monte-Carlo Dropout brings up significant improvement on DA sub-task. Notably, our submissions achieve remarkable results over all <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.94/>The JHU-Microsoft Submission for WMT21 Quality Estimation Shared Task<span class=acl-fixed-case>JHU</span>-<span class=acl-fixed-case>M</span>icrosoft Submission for <span class=acl-fixed-case>WMT</span>21 Quality Estimation Shared Task</a></strong><br><a href=/people/s/shuoyang-ding/>Shuoyang Ding</a>
|
<a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a>
|
<a href=/people/m/matt-post/>Matt Post</a>
|
<a href=/people/c/christian-federmann/>Christian Federmann</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--94><div class="card-body p-3 small">This paper presents the JHU-Microsoft joint submission for WMT 2021 quality estimation shared task. We only participate in Task 2 (post-editing effort estimation) of the shared task, focusing on the target-side word-level quality estimation. The techniques we experimented with include Levenshtein Transformer training and <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a> with a combination of forward, backward, round-trip translation, and pseudo post-editing of the MT output. We demonstrate the competitiveness of our <a href=https://en.wikipedia.org/wiki/System>system</a> compared to the widely adopted OpenKiwi-XLM baseline. Our <a href=https://en.wikipedia.org/wiki/System>system</a> is also the top-ranking system on the MT MCC metric for the English-German language pair.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.98.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--98 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.98 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.98/>Papago’s Submission for the WMT21 Quality Estimation Shared Task<span class=acl-fixed-case>WMT</span>21 Quality Estimation Shared Task</a></strong><br><a href=/people/s/seunghyun-lim/>Seunghyun Lim</a>
|
<a href=/people/h/hantae-kim/>Hantae Kim</a>
|
<a href=/people/h/hyunjoong-kim/>Hyunjoong Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--98><div class="card-body p-3 small">This paper describes Papago submission to the WMT 2021 Quality Estimation Task 1 : Sentence-level Direct Assessment. Our multilingual Quality Estimation system explores the combination of Pretrained Language Models and Multi-task Learning architectures. We propose an iterative training pipeline based on pretraining with large amounts of in-domain synthetic data and <a href=https://en.wikipedia.org/wiki/Finetuning>finetuning</a> with gold (labeled) data. We then compress our <a href=https://en.wikipedia.org/wiki/System>system</a> via knowledge distillation in order to reduce parameters yet maintain strong performance. Our submitted multilingual systems perform competitively in multilingual and all 11 individual language pair settings including zero-shot.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.99.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--99 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.99 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.99/>NICT Kyoto Submission for the WMT’21 Quality Estimation Task : Multimetric Multilingual Pretraining for Critical Error Detection<span class=acl-fixed-case>NICT</span> <span class=acl-fixed-case>K</span>yoto Submission for the <span class=acl-fixed-case>WMT</span>’21 Quality Estimation Task: Multimetric Multilingual Pretraining for Critical Error Detection</a></strong><br><a href=/people/r/raphael-rubino/>Raphael Rubino</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a>
|
<a href=/people/b/benjamin-marie/>Benjamin Marie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--99><div class="card-body p-3 small">This paper presents the NICT Kyoto submission for the WMT&#8217;21 Quality Estimation (QE) Critical Error Detection shared task (Task 3). Our approach relies mainly on QE model pretraining for which we used 11 language pairs, three sentence-level and three word-level translation quality metrics. Starting from an XLM-R checkpoint, we perform continued training by modifying the learning objective, switching from masked language modeling to QE oriented signals, before finetuning and ensembling the models. Results obtained on the test set in terms of <a href=https://en.wikipedia.org/wiki/Correlation_coefficient>correlation coefficient</a> and <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> show that automatic metrics and synthetic data perform well for pretraining, with our submissions ranked first for two out of four language pairs. A deeper look at the impact of each metric on the downstream task indicates higher performance for token oriented metrics, while an ablation study emphasizes the usefulness of conducting both self-supervised and QE pretraining.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.101.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--101 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.101 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.101/>Direct Exploitation of Attention Weights for Translation Quality Estimation</a></strong><br><a href=/people/l/lisa-yankovskaya/>Lisa Yankovskaya</a>
|
<a href=/people/m/mark-fishel/>Mark Fishel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--101><div class="card-body p-3 small">The paper presents our submission to the WMT2021 Shared Task on Quality Estimation (QE). We participate in sentence-level predictions of human judgments and post-editing effort. We propose a glass-box approach based on attention weights extracted from <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation systems</a>. In contrast to the previous works, we directly explore attention weight matrices without replacing them with general metrics (like entropy). We show that some of our <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> can be trained with a small amount of a high-cost labelled data. In the absence of training data our approach still demonstrates a moderate linear correlation, when trained with synthetic data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--102 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.102/>IST-Unbabel 2021 Submission for the Quality Estimation Shared Task<span class=acl-fixed-case>IST</span>-Unbabel 2021 Submission for the Quality Estimation Shared Task</a></strong><br><a href=/people/c/chrysoula-zerva/>Chrysoula Zerva</a>
|
<a href=/people/d/daan-van-stigt/>Daan van Stigt</a>
|
<a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/ana-c-farinha/>Ana C Farinha</a>
|
<a href=/people/p/pedro-ramos/>Pedro Ramos</a>
|
<a href=/people/j/jose-g-c-de-souza/>José G. C. de Souza</a>
|
<a href=/people/t/taisiya-glushkova/>Taisiya Glushkova</a>
|
<a href=/people/m/miguel-vera/>Miguel Vera</a>
|
<a href=/people/f/fabio-kepler/>Fabio Kepler</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--102><div class="card-body p-3 small">We present the joint contribution of IST and Unbabel to the WMT 2021 Shared Task on Quality Estimation. Our team participated on two tasks : Direct Assessment and Post-Editing Effort, encompassing a total of 35 submissions. For all submissions, our efforts focused on training multilingual models on top of OpenKiwi predictor-estimator architecture, using pre-trained multilingual encoders combined with adapters. We further experiment with and uncertainty-related objectives and features as well as training on out-of-domain direct assessment data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.103.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--103 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.103 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.103/>The IICT-Yverdon System for the WMT 2021 Unsupervised MT and Very Low Resource Supervised MT Task<span class=acl-fixed-case>IICT</span>-Yverdon System for the <span class=acl-fixed-case>WMT</span> 2021 Unsupervised <span class=acl-fixed-case>MT</span> and Very Low Resource Supervised <span class=acl-fixed-case>MT</span> Task</a></strong><br><a href=/people/a/alex-r-atrio/>Àlex R. Atrio</a>
|
<a href=/people/g/gabriel-luthier/>Gabriel Luthier</a>
|
<a href=/people/a/axel-fahy/>Axel Fahy</a>
|
<a href=/people/g/giorgos-vernikos/>Giorgos Vernikos</a>
|
<a href=/people/a/andrei-popescu-belis/>Andrei Popescu-Belis</a>
|
<a href=/people/l/ljiljana-dolamic/>Ljiljana Dolamic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--103><div class="card-body p-3 small">In this paper, we present the systems submitted by our team from the Institute of ICT (HEIG-VD / HES-SO) to the Unsupervised MT and Very Low Resource Supervised MT task. We first study the improvements brought to a <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline system</a> by techniques such as <a href=https://en.wikipedia.org/wiki/Back-translation>back-translation</a> and initialization from a parent model. We find that both techniques are beneficial and suffice to reach performance that compares with more sophisticated <a href=https://en.wikipedia.org/wiki/System>systems</a> from the 2020 task. We then present the application of this system to the 2021 task for low-resource supervised Upper Sorbian (HSB) to German translation, in both directions. Finally, we present a contrastive system for HSB-DE in both directions, and for unsupervised German to Lower Sorbian (DSB) translation, which uses multi-task training with various training schedules to improve over the baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.105.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--105 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.105 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.105/>The <a href=https://en.wikipedia.org/wiki/Ludwig_Maximilian_University_of_Munich>LMU Munich Systems</a> for the WMT21 Unsupervised and Very Low-Resource Translation Task<span class=acl-fixed-case>LMU</span> <span class=acl-fixed-case>M</span>unich Systems for the <span class=acl-fixed-case>WMT</span>21 Unsupervised and Very Low-Resource Translation Task</a></strong><br><a href=/people/j/jindrich-libovicky/>Jindřich Libovický</a>
|
<a href=/people/a/alexander-fraser/>Alexander Fraser</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--105><div class="card-body p-3 small">We present our submissions to the WMT21 shared task in Unsupervised and Very Low Resource machine translation between <a href=https://en.wikipedia.org/wiki/German_language>German</a> and Upper Sorbian, <a href=https://en.wikipedia.org/wiki/German_language>German and Lower Sorbian</a>, and <a href=https://en.wikipedia.org/wiki/Russian_language>Russian</a> and <a href=https://en.wikipedia.org/wiki/Chuvash_language>Chuvash</a>. Our low-resource systems (GermanUpper Sorbian, RussianChuvash) are pre-trained on high-resource pairs of related languages. We fine-tune those systems using the available authentic parallel data and improve by iterated back-translation. The unsupervised GermanLower Sorbian system is initialized by the best Upper Sorbian system and improved by iterated back-translation using monolingual data only.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.109.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--109 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.109 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.wmt-1.109.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.109" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.109/>cushLEPOR : customising hLEPOR metric using Optuna for higher agreement with human judgments or pre-trained language model LaBSE<span class=acl-fixed-case>LEPOR</span>: customising h<span class=acl-fixed-case>LEPOR</span> metric using Optuna for higher agreement with human judgments or pre-trained language model <span class=acl-fixed-case>L</span>a<span class=acl-fixed-case>BSE</span></a></strong><br><a href=/people/l/lifeng-han/>Lifeng Han</a>
|
<a href=/people/i/irina-sorokina/>Irina Sorokina</a>
|
<a href=/people/g/gleb-erofeev/>Gleb Erofeev</a>
|
<a href=/people/s/serge-gladkoff/>Serge Gladkoff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--109><div class="card-body p-3 small">Human evaluation has always been expensive while researchers struggle to trust the <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>automatic metrics</a>. To address this, we propose to customise traditional <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> by taking advantages of the pre-trained language models (PLMs) and the limited available human labelled scores. We first re-introduce the hLEPOR metric factors, followed by the Python version we developed (ported) which achieved the automatic tuning of the weighting parameters in hLEPOR metric. Then we present the customised hLEPOR (cushLEPOR) which uses Optuna hyper-parameter optimisation framework to fine-tune hLEPOR weighting parameters towards better agreement to pre-trained language models (using LaBSE) regarding the exact MT language pairs that cushLEPOR is deployed to. We also optimise cushLEPOR towards professional human evaluation data based on MQM and pSQM framework on English-German and Chinese-English language pairs. The experimental investigations show cushLEPOR boosts hLEPOR performances towards better agreements to PLMs like LABSE with much lower cost, and better agreements to human evaluations including MQM and pSQM scores, and yields much better performances than <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. Official results show that our submissions win three language pairs including English-German and Chinese-English on News domain via cushLEPOR(LM) and English-Russian on TED domain via hLEPOR. (data available at https://github.com/poethan/cushLEPOR)</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.110/>MTEQA at WMT21 Metrics Shared Task<span class=acl-fixed-case>MTEQA</span> at <span class=acl-fixed-case>WMT</span>21 Metrics Shared Task</a></strong><br><a href=/people/m/mateusz-krubinski/>Mateusz Krubiński</a>
|
<a href=/people/e/erfan-ghadery/>Erfan Ghadery</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a>
|
<a href=/people/p/pavel-pecina/>Pavel Pecina</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--110><div class="card-body p-3 small">In this paper, we describe our submission to the WMT 2021 Metrics Shared Task. We use the automatically-generated questions and answers to evaluate the quality of Machine Translation (MT) systems. Our submission builds upon the recently proposed MTEQA framework. Experiments on WMT20 evaluation datasets show that at the system-level the MTEQA metric achieves performance comparable with other state-of-the-art solutions, while considering only a certain amount of information from the whole translation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.111.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--111 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.111 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.wmt-1.111" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.111/>Are References Really Needed? Unbabel-IST 2021 Submission for the Metrics Shared Task<span class=acl-fixed-case>IST</span> 2021 Submission for the Metrics Shared Task</a></strong><br><a href=/people/r/ricardo-rei/>Ricardo Rei</a>
|
<a href=/people/a/ana-c-farinha/>Ana C Farinha</a>
|
<a href=/people/c/chrysoula-zerva/>Chrysoula Zerva</a>
|
<a href=/people/d/daan-van-stigt/>Daan van Stigt</a>
|
<a href=/people/c/craig-stewart/>Craig Stewart</a>
|
<a href=/people/p/pedro-ramos/>Pedro Ramos</a>
|
<a href=/people/t/taisiya-glushkova/>Taisiya Glushkova</a>
|
<a href=/people/a/andre-f-t-martins/>André F. T. Martins</a>
|
<a href=/people/a/alon-lavie/>Alon Lavie</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--111><div class="card-body p-3 small">In this paper, we present the joint contribution of Unbabel and IST to the WMT 2021 Metrics Shared Task. With this year&#8217;s focus on Multidimensional Quality Metric (MQM) as the ground-truth human assessment, our aim was to steer COMET towards higher correlations with MQM. We do so by first pre-training on Direct Assessments and then fine-tuning on z-normalized MQM scores. In our experiments we also show that reference-free COMET models are becoming competitive with reference-based models, even outperforming the best COMET model from 2020 on this year&#8217;s development data. Additionally, we present COMETinho, a lightweight COMET model that is 19x faster on <a href=https://en.wikipedia.org/wiki/Central_processing_unit>CPU</a> than the original model, while also achieving state-of-the-art correlations with <a href=https://en.wikipedia.org/wiki/MQM>MQM</a>. Finally, in the <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE</a> as a metric track, we also participated with a <a href=https://en.wikipedia.org/wiki/Quantum_electrodynamics>QE model</a> trained using the OpenKiwi framework leveraging MQM scores and word-level annotations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.120.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--120 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.120 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.120/>Simultaneous Neural Machine Translation with Constituent Label Prediction</a></strong><br><a href=/people/y/yasumasa-kano/>Yasumasa Kano</a>
|
<a href=/people/k/katsuhito-sudoh/>Katsuhito Sudoh</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--120><div class="card-body p-3 small">Simultaneous translation is a task in which <a href=https://en.wikipedia.org/wiki/Translation>translation</a> begins before the speaker has finished speaking, so it is important to decide when to start the <a href=https://en.wikipedia.org/wiki/Translation>translation process</a>. However, deciding whether to read more input words or start to translate is difficult for language pairs with different word orders such as <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. Motivated by the concept of pre-reordering, we propose a couple of simple decision rules using the label of the next constituent predicted by incremental constituent label prediction. In experiments on English-to-Japanese simultaneous translation, the proposed method outperformed <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> in the <a href=https://en.wikipedia.org/wiki/Latency_(engineering)>quality-latency trade-off</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.wmt-1.121.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--wmt-1--121 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.wmt-1.121 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.wmt-1.121/>Contrastive Learning for Context-aware Neural Machine Translation Using Coreference Information</a></strong><br><a href=/people/y/yongkeun-hwang/>Yongkeun Hwang</a>
|
<a href=/people/h/hyeongu-yun/>Hyeongu Yun</a>
|
<a href=/people/k/kyomin-jung/>Kyomin Jung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--wmt-1--121><div class="card-body p-3 small">Context-aware neural machine translation (NMT) incorporates contextual information of surrounding texts, that can improve the translation quality of document-level machine translation. Many existing works on context-aware NMT have focused on developing new model architectures for incorporating additional contexts and have shown some promising results. However, most of existing works rely on cross-entropy loss, resulting in limited use of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a>. In this paper, we propose CorefCL, a novel data augmentation and contrastive learning scheme based on <a href=https://en.wikipedia.org/wiki/Coreference>coreference</a> between the source and contextual sentences. By corrupting automatically detected coreference mentions in the contextual sentence, CorefCL can train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to be sensitive to coreference inconsistency. We experimented with our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> on common context-aware NMT models and two document-level translation tasks. In the experiments, our method consistently improved BLEU of compared models on <a href=https://en.wikipedia.org/wiki/English_as_a_second_or_foreign_language>English-German and English-Korean tasks</a>. We also show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> significantly improves <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a> in the English-German contrastive test suite.</div></div></div><hr></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>