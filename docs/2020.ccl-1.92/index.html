<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning" name=citation_title><meta content="Xiuhong Li" name=citation_author><meta content="Zhe Li" name=citation_author><meta content="Jiabao Sheng" name=citation_author><meta content="Wushour Slamu" name=citation_author><meta content="Proceedings of the 19th Chinese National Conference on Computational Linguistics" name=citation_conference_title><meta content="2020/10" name=citation_publication_date><meta content="https://aclanthology.org/2020.ccl-1.92.pdf" name=citation_pdf_url><meta content="994" name=citation_firstpage><meta content="1005" name=citation_lastpage><meta property="og:title" content="Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning"><meta property="og:image" content="https://aclanthology.org/thumb/2020.ccl-1.92.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2020.ccl-1.92"><meta property="og:description" content="Xiuhong Li, Zhe Li, Jiabao Sheng, Wushour Slamu. Proceedings of the 19th Chinese National Conference on Computational Linguistics. 2020."><link rel=canonical href=https://aclanthology.org/2020.ccl-1.92></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2020.ccl-1.92.pdf>Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning</a>
<a id=af_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Name</a>
<a id=am_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>ቋንቋ</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>تصنيف نص منخفض الموارد عبر ضبط نموذج اللغة عبر اللغات بدقة</a>
<a id=az_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>T톛k-Kaynakl캼 Metin S캼n캼fland캼rmas캼 칂톛rz dil Modeli 캻yi-Yap캼land캼rmas캼 vasit톛sil톛</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Класификация на текста с нисък ресурс чрез фина настройка на междуезичния езиков модел</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klasifikacija teksta s niskim resursima preko prekršnog jezičkog modela fino podešavanje</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Classificació de text de baix recursos mitjançant ajustament del model de llenguatge translingüístic</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klasifikace textu s nízkými zdroji prostřednictvím jemného ladění modelu mezi jazyky</a>
<a id=da_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klassificering af tekst med lav ressource via tværsproget sprogmodel Finjustering</a>
<a id=de_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Ressourcenarme Textklassifikation durch Feinabstimmung des sprachübergreifenden Modells</a>
<a id=el_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Ταξινόμηση κειμένου χαμηλής περιεκτικότητας σε πόρους μέσω της διακρατικής γλωσσικής μοντελοποίησης</a>
<a id=es_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Clasificación de textos con pocos recursos mediante el ajuste del modelo lingüístico multilingüe</a>
<a id=et_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Vähese ressursiga teksti klassifitseerimine keeleülese keele mudeli peenhäälestuse kaudu</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>کلاس‌سازی متن کم منبع از طریق مدل زبان‌های زیادی</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Vähävaraisen tekstin luokittelu monikielisen kielimallin hienosäätön avulla</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Classification de textes à faibles ressources grâce à un ajustement précis du modèle linguistique multilingue</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Aicmiú Téacs Íseal-Acmhainne trí Mhionchoigeartú Samhail Teanga Trastheangach</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>KCharselect unicode block name</a>
<a id=he_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>שימוש טקסט משאבים נמוכים באמצעות שיפות מסוימות</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>क्रॉस-भाषाई भाषा मॉडल ठीक ट्यूनिंग के माध्यम से कम संसाधन पाठ वर्गीकरण</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klasificija teksta niskog resursa preko prekršnog jezičkog modela fino prilagođenja</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Alacsony erőforrású szövegosztályozás a többnyelvű nyelvű modell segítségével Finomhangolás</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Նվագ ռեսուրսների տեքստի դասակարգման միջոցով լեզվի միջոցով</a>
<a id=id_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klasifikasi Teks Sumber Terrendah melalui Penyesuaian Model Bahasa Selata Bahasa</a>
<a id=is_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Classificazione del testo a basso contenuto di risorse tramite modello linguistico multilingue</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>クロスリンガルモデルファインチューニングによる低リソーステキスト分類</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>undo-type</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Name</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Төмен ресурс мәтін классификациясы тіл үлгісін таңдау үшін</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>다중 언어 모델을 바탕으로 미세하게 조정된 저자원 텍스트 분류</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Mažai išteklių turintis teksto klasifikavimas naudojant tarpkalbinį kalbos modelį</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Класификација на текст со ниски ресурси преку промена на моделот на меѓујазик</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>ക്രോസ്- ഭാഷ മോഡില്‍ കുറഞ്ഞ വിഭവങ്ങളുടെ പദാവലി ക്ലാസ്സിഷന്‍</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Төвөрмөц хэл загвараар бага боловсруулагч текст хэлбэрээр хуваалцах</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klasifikasi Teks Sumber Terrendah melalui Penyesuaian Model Bahasa Sembahasa</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klassifikazzjoni tat-Test b’Riżorsi Bażi permezz ta’ Aġġustament Irfinat tal-Mudell tal-Lingwa Translingwi</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Classificatie van tekst met weinig hulpbronnen via aanpassing van het meertalige taalmodel</a>
<a id=no_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Name</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klasyfikacja tekstu niskich zasobów poprzez dostosowanie modelu języka wielojęzycznego</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Classificação de texto de poucos recursos por meio de ajuste fino do modelo de linguagem multilíngue</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Clasificarea textului cu resurse reduse prin intermediul modelului de limbă translingvistică</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Низкоресурсная классификация текста с помощью тонкой настройки межъязыковой языковой модели</a>
<a id=si_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Name</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klasifikacija besedila z nizkimi viri prek medjezičnega jezikovnega modela finega nastavitve</a>
<a id=so_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Fine-tuning</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klasifikimi i tekstit me burime të ulta nëpërmjet rregullimit të modelit të gjuhës ndërgjuhësore</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klasifikacija teksta niskog resursa preko prekršnog jezičkog modela fino podešavanje</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Klassificering av text med låg resurs via flerspråkig språkmodell Finjustering</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Usalama wa maandishi ya chini ya rasilimali kupitia Modeli ya Lugha yenye lugha</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>கிராஸ்- மொழி மொழி மாதிரி மூலம் குறைந்த மூலம் உரை வகைப்படுத்தல்</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Çot Diller modi Fin-tuning</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Name</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>KCharselect unicode block name</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2020.ccl-1.92.pdf>语言微调者低资源文本分类</a></h2><p class=lead><a href=/people/x/xiuhong-li/>Xiuhong Li</a>,
<a href=/people/z/zhe-li/>Zhe Li</a>,
<a href=/people/j/jiabao-sheng/>Jiabao Sheng</a>,
<a href=/people/w/wushour-slamu/>Wushour Slamu</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Text classification tends to be difficult when data are inadequate considering the amount of manually labeled text corpora. For low-resource agglutinative languages including <a href=https://en.wikipedia.org/wiki/Uyghur_language>Uyghur</a>, <a href=https://en.wikipedia.org/wiki/Kazakh_language>Kazakh</a>, and Kyrgyz (UKK languages), in which words are manufactured via <a href=https://en.wikipedia.org/wiki/Word_stem>stems</a> concatenated with several suffixes and <a href=https://en.wikipedia.org/wiki/Word_stem>stems</a> are used as the representation of text content, this feature allows infinite derivatives vocabulary that leads to high uncertainty of writing forms and huge redundant features. There are major challenges of low-resource agglutinative text classification the lack of labeled data in a target domain and morphologic diversity of derivations in language structures. It is an effective solution which fine-tuning a pre-trained language model to provide meaningful and favorable-to-use <a href=https://en.wikipedia.org/wiki/Feature_extraction>feature extractors</a> for downstream text classification tasks. To this end, we propose a low-resource agglutinative language model fine-tuning AgglutiFiT, specifically, we build a low-noise fine-tuning dataset by morphological analysis and stem extraction, then fine-tune the cross-lingual pre-training model on this dataset. Moreover, we propose an attention-based fine-tuning strategy that better selects relevant semantic and syntactic information from the pre-trained language model and uses those <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> on downstream text classification tasks. We evaluate our methods on nine Uyghur, Kazakh, and Kyrgyz classification datasets, where they have significantly better performance compared with several strong baselines.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Teks klassifikasie het gevaal om moeilik te wees wanneer data is onvoldoende onderwerp van die hoeveelheid van handgemerkte teks korpora. Vir lae-hulpbron aglutinatiewe tale insluitend Uyghur, Kazakh, en Kyrgyz (UKK tale), waarin woorde deur stamme geproduseer word deur stamme wat verskeie aglutiefasiliteite en stamme gebruik word as die voorstelling van teks inhoud, hierdie funksie laat onbepaalde afgeleide woordeboek toe wat lei na hoë onbevestigheid van skryfvorms en groot onbevestigheid funksies. Daar is groot uitdagings van lae-hulpbron aglutinatiewe teks klassifikasie die ontbreek van gemerkte data in 'n doel domein en morfologiese verskeidigheid van afgeleide in taal strukture. Dit is 'n effektief oplossing wat fyn- tuning van 'n vooraf- opgelei taal model om betekenlike en gunsbaarde funksie-uitvoerders te verskaf vir onderstreem teks klassifikasie taak. Op hierdie einde voorstel ons 'n lae hulpbron aglutinatiewe taal model fin-tuning AgglutiFiT, spesifieke, bou ons 'n lae-ruis fin-tuning datastel deur morfologiese analisie en stam-uittrekking, dan fin-tuning die kruislinglike voor-onderwerking model op hierdie datastel. Ook, ons voorstel 'n aandag-gebaseerde fyn-tuning strategie wat beter relevante semantiese en sintaktieke inligting kies van die voorafgevorderde taal model en gebruik daardie funksies op onderstreem teks klasifikasie taak. Ons evalueer ons metodes op nege Uyghur, Kazakh en Kyrgyz klassifikasie datastelle waar hulle betekeurig beter prestasie het vergelyk met verskeie sterk
baseline.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>የጽሑፍ መግለጫ የጽሑፍ ክፍተት ቁጥር በጨዋታ አይበቃም፡፡ ለጥቂት resource agglutinative ቋንቋዎች ጉጉጉር፣ ካዛክ እና ቂርጊዝ (የዩኩክ ቋንቋ) ያሉ ቃላት በብዙ ጉዳይ እና ድምጾች በተገኘ ድምፅ እና ድምፅ በተለየ ድምፅ ማቀናቀል ይደረጋሉ፡፡ በቋንቋዎች አካባቢ እና የሞፎሎጂ ልዩ ልዩ ልዩነት የጽሑፍ መክፈቻ የጎግሎት የጽሑፍ መግለጫ የሚያስፈልገው የድምፅ መረጃዎች በቋንቋ አካባቢዎች ውስጥ የጽሑፍ ጥያቄዎች አለባቸው፡፡ በጽሑፍ መክፈቻ ስራዎችን ለመስጠት የሚታይ እና የተወደደ የፊደል ቋንቋ ሞዴል በመጠቀም የሚያስፈልገው የጽሑፍ መክፈቻ ስርዓት የሚያስፈልገውን ማቀናጃ ያስተካክላል፡፡ ለዚህ ምክንያት የጎግሎቲዊ ቋንቋ ምሳሌ አግglutiFiT የተመሳሳይ እናስጀራለን፡፡ በተጨማሪም፣ ደግሞም የተጠቃሚ የቋንቋ ምሳሌ የተለየውን የጽሑፍ ትክክለኛ ስርዓት ለመምረጥ እና የተጠቃሚ መረጃዎችን ለመምረጥ እና እነዚህን ምርጫዎች በበታችኛው ጽሑፍ መግለጫ ስራዎችን ለመጠቀም እናስጠጋለን፡፡ የዘጠኝ ዩጉር፣ ካዛክ እና ቂርጊስ የክፍላጻ ዳታዎችን እናስተምርላቸዋለን፡፡
መሀከል መስመር</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>يميل تصنيف النص إلى أن يكون صعبًا عندما تكون البيانات غير كافية مع الأخذ في الاعتبار مقدار مجموعة النص المصنفة يدويًا. بالنسبة للغات التراصية منخفضة الموارد بما في ذلك الأويغور والكازاخستانية والقيرغيزية (لغات UKK) ، حيث يتم تصنيع الكلمات عبر السيقان المتسلسلة مع العديد من اللواحق والسيقان المستخدمة لتمثيل محتوى النص ، تسمح هذه الميزة بمشتقات لا حصر لها من المفردات التي تؤدي إلى ارتفاع عدم اليقين في أشكال الكتابة والسمات الزائدة الضخمة. هناك تحديات كبيرة لتصنيف النص التراصي منخفض الموارد ونقص البيانات المصنفة في المجال المستهدف والتنوع المورفولوجي للاشتقاقات في الهياكل اللغوية. إنه حل فعال يقوم بضبط نموذج لغة مُدرَّب مسبقًا لتوفير مستخلصات ميزات هادفة ومواتية للاستخدام لمهام تصنيف النص النهائي. تحقيقا لهذه الغاية ، نقترح نموذج لغة تراصية منخفضة الموارد لضبط AgglutiFiT ، على وجه التحديد ، نقوم ببناء مجموعة بيانات ضبط دقيق منخفضة الضوضاء عن طريق التحليل الصرفي واستخراج الجذع ، ثم ضبط نموذج التدريب المسبق متعدد اللغات على مجموعة البيانات هذه. علاوة على ذلك ، نقترح استراتيجية ضبط دقيقة قائمة على الانتباه والتي تختار بشكل أفضل المعلومات الدلالية والنحوية ذات الصلة من نموذج اللغة المدربة مسبقًا وتستخدم هذه الميزات في مهام تصنيف النص النهائي. نقوم بتقييم أساليبنا على تسعة مجموعات بيانات تصنيفية من الأويغور والكازاخستانية والقرغيزية ، حيث تتمتع بأداء أفضل بكثير مقارنة بالعديد من مجموعات البيانات القوية.
خطوط الأساس.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mətn klasifikasiyası, məlumat əl etiketli mətn korporasının dəyişikliyini düşünməyə kifayət olduğunda çətin olar. Uyghur, Kazakh və Kirgiz dilləri (UKK dilləri) içərisində düşük kaynaqlar agglutinativ dillər üçün, sözləri bir neçə suffiks və stems ilə birləşdirilmiş stems vasitəsilə təhsil edilir, bu özellik mətn məlumatının göstəricisi olaraq istifadə edilir. Bu təhsil yazmaq formların və böyük qüvvətli özelliklərin təhsil edilməsini sağlar. Tək ressurs agglutinativ metin klasifikasyonun böyük çətinlikləri var ki, məqsəd domeində etiket edilmiş məlumatların yoxdur və dil strukturlarında dəyişiklik məlumatların çoxluğunu. Bu, əvvəlcə təhsil edilmiş dil modelini təmizləmək üçün mənfəətli və faydalı istifadə etmək üçün fərqli çətinlikdir. Bu səbəbdə, düşük ressurs agglutinativ dil modeli AgglutiFiT'i təsdiqləyirik, əlbəttə ki, morfolojik analizi və stem ekstraksiyası ilə düşük səslər tədricləyici verilər qurduq, sonra bu verilər qutusunda çoxlu dil əvvəl tədricləyici modeli tədricləyirik. Daha sonra, biz təhsil edilmiş dil modelindən daha yaxşı olan semantik və sintaktik məlumatları seçən, təhsil edilən təhsil-təhsil təhsil-təhsil stratejisini təklif edirik. Biz doqquz Uyghur, Kazakh və Kirgiz klasifikasyonu verilən tərzlərimizi değerləşdiririk. Onlar çoxlu güclü tərzlər ilə müqayisədə daha xeyirli performanslar var.
baselines.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Класификацията на текста обикновено е трудна, когато данните са недостатъчни, като се има предвид количеството ръчно обозначени текстови корпуси. За аглутинационни езици с нисък ресурс, включително уйгурски, казахски и киргизки (езици на УКК), в които думите се произвеждат чрез стъбла, обвързани с няколко наставки и стъбла се използват като представяне на текстово съдържание, тази функция позволява безкрайни производни речник, който води до висока несигурност на писмените форми и огромни излишни характеристики. Съществуват големи предизвикателства при класификацията на аглутинационния текст с нисък ресурс, липсата на етикетирани данни в целевата област и морфологичното разнообразие на дериватите в езиковите структури. Това е ефективно решение, което фино настройва предварително обучен езиков модел, за да осигури смислени и благоприятни за използване екстрактори за задачи по класификация на текста надолу по веригата. За тази цел предлагаме модел на аглутинационен език с нисък ресурс, който фино настройва по-специално изграждаме набор от данни за фино настройване с нисък шум чрез морфологичен анализ и екстракция на стъблото, след което фино настройваме междуезичния модел на предобучение на този набор от данни. Освен това предлагаме стратегия за фино настройване, основана на вниманието, която по-добре избира съответната семантична и синтактична информация от предварително обучения езиков модел и използва тези функции при задачите за класификация на текста надолу по веригата. Ние оценяваме нашите методи на девет уйгурски, казакски и киргизки класификационни набора данни, където те имат значително по-добри резултати в сравнение с няколко силни
базови линии.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>টেক্সট ক্লাস্ফিকেশন ব্যবহার করে যখন ডাটা হাতে লেবেল করা টেক্সট কর্পোরার পরিমাণ যথেষ্ট না থাকে। লেখার প্রতিনিধি হিসেবে ব্যবহার করা হয়েছে যার মধ্যে উগুর, কাজাক এবং কিরগিজ (যুক্তরাজ্য ভাষা), যেখানে শব্দ উৎপাদন করা হয়েছে বেশ কয়েকটি ভক্স এবং স্টেমের মাধ্যমে, এই বৈশিষ্ট্যের বৈশিষ্ট্যের প্রতিনিধিত্ব হিসে টার্গেট ডোমেইনে লেবেলেড ডাটার অভাব এবং ভাষার কাঠামোর বৈচিত্র্যের বৈচিত্র্যের প্রধান চ্যালেঞ্জ রয়েছে। এটা একটি কার্যকর সমাধান যা পূর্ব প্রশিক্ষিত ভাষার মডেলের সুন্দর প্রদান করা যায় যাতে অর্থহীন এবং পছন্দ করা ব্যবহারের বৈশিষ্ট্য বিশেষ বিনি এই পর্যন্ত আমরা একটি নিম্ন সম্পদ গ্লুগুটিভ ভ ভাষার মডেল প্রস্তাব করি, বিশেষ করে আমরা মোরফোলিক্যালিকাল বিশ্লেষণ এবং স্টেম বের করে নির্মাণ করি, তারপর এই ডাটাসেটে ক্রিভাষাভাষার প্রেক্ষাপটে এছাড়াও, আমরা একটি মনোযোগ প্রস্তাব করি ভিত্তিক ভিত্তিক ভিত্তিক ভিত্তিক সুন্দর কৌশল যা পূর্ববর্তী প্রশিক্ষিত ভাষার মডেল থেকে সামান্যিক এবং সিন্ আমরা নয়টি ইউগুর, কাজাক এবং কিরগিজের ক্লাসাফিকেশন ডাটাসেটে আমাদের পদ্ধতি মূল্যায়ন করি, যেখানে তাদের বেশ কয়েকটি শক্তিশালী প্রদর্
বেসেলাইন।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Text classification tends to be difficult when data are inadequate considering the amount of manually labeled text corpora. For low-resource agglutinative languages including Uyghur, Kazakh, and Kyrgyz (UKK languages), in which words are manufactured via stems concatenated with several suffixes and stems are used as the representation of text content, this feature allows infinite derivatives vocabulary that leads to high uncertainty of writing forms and huge redundant features. There are major challenges of low-resource agglutinative text classification the lack of labeled data in a target domain and morphologic diversity of derivations in language structures. It is an effective solution which fine-tuning a pre-trained language model to provide meaningful and favorable-to-use feature extractors for downstream text classification tasks. To this end, we propose a low-resource agglutinative language model fine-tuning AgglutiFiT, specifically, we build a low-noise fine-tuning dataset by morphological analysis and stem extraction, then fine-tune the cross-lingual pre-training model on this dataset. Moreover, we propose an attention-based fine-tuning strategy that better selects relevant semantic and syntactic information from the pre-trained language model and uses those features on downstream text classification tasks. ང་ཚོའི་ཐབས་ལམ་དེ་ཚོ་ནི་ཡུ་གུ་རུ་(Uyghur)ཀཛུཀ་དང་། ཀར་གྲུ་ཛུང་གི་དབྱེ་སྟངས་ཆ་འཕྲིན་ཡིག་ཆ་ལྟར་ཞིབ་བྱེད་ཀྱི་ཡོད།
རྨང་གཞི་ཚོགས་རེད།</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Klasifikacija teksta često je teško kad podaci nisu dovoljni s obzirom na količinu ručno označene tekstne korpore. Za manje resurse agglutinativne jezike uključujući Uyghur, Kazakh i Kirgiz (UKK jezike), u kojima se riječi proizvode putem stazama povezanih sa nekoliko sufiksa i stazama koriste kao predstavljanje sadržaja teksta, ta karakteristika omogućava beskonačnu derivativnu rečenicu koja dovodi do visoke nesigurnosti oblika pisanja i ogromnih redundantnih karakteristika. Postoje veliki izazovi takve klasifikacije teksta s niskim resursima, nedostatak označenih podataka u ciljnom domenu i morfološkom raznolikosti derivacija u jezičkim strukturama. To je učinkovito rješenje koje ispravlja predobučeni jezički model kako bi omogućilo smislene i favorilne ekstraktore karakteristike za klasifikaciju teksta. Za taj cilj predlažemo model AgglutiFiT-a koji finalizira s niskim resursima agglutinativnim jezikom, posebno, izgradimo nizak zvuk finalizirajući podatke sa morfološkom analizom i ekstrakcijom matičnih izvora, a zatim finaliziramo preko jezika predobuku na ovom setu podataka. Osim toga, predlažemo strategiju za finaliziranje pažnje koja bolje odabere relevantne semantičke i sintaktičke informacije iz predobučenog jezičkog modela i koristi te karakteristike na zadacima klasifikacije teksta. Procjenjujemo naše metode na devet Uyghura, Kazakha i Kirgijskih klasifikacijskih podataka, gdje imaju značajno bolje izvedbe u usporedbi s nekoliko jakih
osnovne linije.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>La classificació del text tendeix a ser difícil quan les dades són insuficients considerant la quantitat de corpores de text etiquetats manualment. Per a llengües aglutinatives de baix recursos, incloent Uyghur, Kazakh i Kirguis (llengües UKK), en les quals les paraules es fabriquen a través de troncs concatenats amb diversos sufixxos i troncs s'utilitzen com a representació del contingut de text, aquesta característica permet un vocabulari infinit de derivats que porta a una gran incertitud de formes d'escriptura i grans característiques redundants. Hi ha els principals reptes de la classificació del text aglutinatiu de baix recursos la falta de dades etiquetades en un domini d'objectiu i la diversitat morfològica de derivacions en les estructures lingüístices. És una solució eficaç que ajusta un model de llenguatge pré-entrenat per proporcionar extractors de característiques significatius i favorables a l'ús per a tasques de classificació de textos a avall. Per això, proposem un model de llenguatge aglutinatiu de baix recursos que ajuste AgglutiFiT, específicament, construïm un conjunt de dades d'ajuste baix soroll mitjançant anàlisi morfològica i extracció de troncs, i després ajustem el model de pré-entrenament translingüístic d'aquest conjunt de dades. A més, proposem una estratègia d'ajustament basada en l'atenció que seleccioni millor la informació semàntica i sinàctica pertinent del model de llenguatge pré-entrenat i utilitza aquestes característiques en tasques de classificació de textos avall. Evaluam els nostres mètodes en nou conjunts de dades de classificació d'Uyghur, Kazakh i Kirguis, on tenen un rendiment significativament millor comparat amb diverses fortes
línies de base.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Klasifikace textu bývá obtížná, pokud jsou data nedostatečná vzhledem k množství ručně označených textových korpusů. U agglutinativních jazyků s nízkými zdroji včetně Ujgurštiny, Kazaštiny a Kyrgyzštiny (jazyků UKK), ve kterých jsou slova vytvářena pomocí stonků spojených s několika příponami a stonky se používají jako reprezentace textového obsahu, tato funkce umožňuje nekonečné odvození slovní zásoby, která vede k vysoké nejistotě psaní forem a obrovským nadbytečným rysům. Hlavní výzvy klasifikace aglutinativních textů s nízkými zdroji jsou nedostatek označených dat v cílové doméně a morfologická diverzita odvození jazykových struktur. Jedná se o efektivní řešení, které jemně ladí předškolený jazykový model tak, aby poskytovalo smysluplné a příznivé extraktory funkcí pro následné úlohy klasifikace textu. Za tímto účelem navrhujeme model AgglutiFiT s nízkými zdroji, konkrétně vytvoříme datovou sadu s nízkým šumem pomocí morfologické analýzy a extrakce kmenů, poté doladíme model předškolení mezi jazyky na této sadě. Navíc navrhujeme strategii jemného ladění založenou na pozornosti, která lépe vybírá relevantní sémantické a syntaktické informace z předškoleného jazykového modelu a využívá tyto funkce při následných úlohách klasifikace textu. Naše metody hodnotíme na devíti ujgurských, kazašských a kyrgyzských klasifikačních datových sadách, kde mají výrazně lepší výkon ve srovnání s několika silnými daty.
základní linie.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tekstklassifikation har tendens til at være vanskelig, når data er utilstrækkelige i betragtning af mængden af manuelt mærkede tekstkorpora. For lav ressource agglutinative sprog, herunder uigurisk, kasakhsisk og kirgisisk (UKK sprog), hvor ord fremstilles via stængler sammenkoblet med flere suffikser og stængler bruges som repræsentation af tekstindhold, denne funktion tillader uendelige derivater ordforråd, der fører til høj usikkerhed i skrivning formularer og enorme overflødige funktioner. Der er store udfordringer ved agglutitiv tekstklassifikation med lav ressource, manglen på mærkede data i et måldomæne og morfologisk mangfoldighed af derivater i sprogstrukturer. Det er en effektiv løsning, der finjusterer en forududdannet sprogmodel for at give meningsfulde og gunstige-til-brug feature extractors til downstream tekst klassifikationsopgaver. Til dette formål foreslår vi en lav ressource agglutinative sprogmodel, der finjusterer AgglutiFiT, specifikt bygger vi et støjsvagt finjusterende datasæt ved morfologisk analyse og stamekstraktion, og finjusterer derefter den tværsprogede pre-training model på dette datasæt. Desuden foreslår vi en opmærksomhedsbaseret finjusterende strategi, der bedre vælger relevante semantiske og syntaktiske oplysninger fra den forudgående sprogmodel og bruger disse funktioner på downstream tekstklassifikationsopgaver. Vi evaluerer vores metoder på ni ugguriske, kasakhsiske og kirgisiske klassificeringsdatasæt, hvor de har betydeligt bedre performance sammenlignet med flere stærke
basislinjer.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Die Textklassifizierung ist in der Regel schwierig, wenn Daten angesichts der Menge der manuell markierten Textkorpora unzureichend sind. Für ressourcenarme agglutinative Sprachen wie Uigurisch, Kasachisch und Kirgisisch (UKK-Sprachen), in denen Wörter über Stämme hergestellt werden, die mit mehreren Suffixen verkettet sind und als Darstellung von Textinhalten verwendet werden, ermöglicht diese Funktion unendliche Ableitungen von Vokabeln, die zu einer hohen Unsicherheit der Schreibformen und riesigen redundanten Merkmalen führen. Es gibt große Herausforderungen bei der ressourcenarmen agglutinativen Textklassifizierung, das Fehlen markierter Daten in einer Zieldomäne und die morphologische Vielfalt der Ableitungen in Sprachstrukturen. Es ist eine effektive Lösung, die ein vortrainiertes Sprachmodell verfeinert, um aussagekräftige und benutzerfreundliche Feature Extractor für nachgelagerte Textklassifizierungsaufgaben bereitzustellen. Zu diesem Zweck schlagen wir ein ressourcenarmes AgglutiFiT-Modell vor, das AgglutiFiT feinabstimmt. Insbesondere bauen wir einen rauscharmen Feinstimmungsdatensatz durch morphologische Analyse und Stammextraktion auf und verfeinern dann das sprachübergreifende Vortrainingsmodell auf diesem Datensatz. Darüber hinaus schlagen wir eine aufmerksamkeitsbasierte Fine-Tuning-Strategie vor, die relevante semantische und syntaktische Informationen aus dem vortrainierten Sprachmodell besser auswählt und diese Funktionen für nachgelagerte Textklassifizierungsaufgaben nutzt. Wir evaluieren unsere Methoden auf neun uigurischen, kasachischen und kirgisischen Klassifizierungsdatensätzen, wo sie im Vergleich zu mehreren starken
Grundlinien.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Η ταξινόμηση κειμένου τείνει να είναι δύσκολη όταν τα δεδομένα είναι ανεπαρκή λαμβάνοντας υπόψη την ποσότητα των χειροκίνητα επισημασμένων σωμάτων κειμένου. Για τις χαμηλής περιεκτικότητας σε συγκολλητικές γλώσσες, συμπεριλαμβανομένων των Ουιγούρων, των Καζακικών και των Κιργιζικών (γλώσσες UKK), στις οποίες οι λέξεις κατασκευάζονται μέσω στελεχών που αλληλοσυνδέονται με διάφορα επιθήματα και οι μίσχοι χρησιμοποιούνται ως αναπαράσταση του περιεχομένου κειμένου, αυτό το χαρακτηριστικό επιτρέπει άπειρα παράγωγα λεξιλόγιο που οδηγεί σε υψηλή αβεβαιότητα των μορφών γραφής και τεράστια περιττά χαρακτηριστικά. Υπάρχουν μεγάλες προκλήσεις της ταξινόμησης κολλητικών κειμένων χαμηλής περιεκτικότητας σε πόρους, η έλλειψη επισημασμένων δεδομένων σε έναν τομέα-στόχο και η μορφολογική ποικιλομορφία παραγώγων στις γλωσσικές δομές. Είναι μια αποτελεσματική λύση που ρυθμίζει ένα προ-εκπαιδευμένο γλωσσικό μοντέλο για να παρέχει ουσιαστικές και ευνοϊκές εξαγωγές χαρακτηριστικών για μεταγενέστερες εργασίες ταξινόμησης κειμένου. Για το σκοπό αυτό, προτείνουμε ένα μοντέλο χαμηλής περιεκτικότητας σε συγκολλητική γλώσσα, το οποίο συντονίζει το AgglutiFiT, συγκεκριμένα, χτίζουμε ένα σύνολο δεδομένων χαμηλού θορύβου με μορφολογική ανάλυση και εξαγωγή στελεχών, και στη συνέχεια συντονίζουμε το δισγλωσσικό μοντέλο προεκπαίδευσης σε αυτό το σύνολο δεδομένων. Επιπλέον, προτείνουμε μια στρατηγική συντονισμού βασισμένη στην προσοχή που επιλέγει καλύτερα σχετικές σημασιολογικές και συντακτικές πληροφορίες από το προ-εκπαιδευμένο γλωσσικό μοντέλο και χρησιμοποιεί αυτά τα χαρακτηριστικά σε μεταγενέστερες εργασίες ταξινόμησης κειμένου. Αξιολογούμε τις μεθόδους μας σε εννέα σύνολα δεδομένων ταξινόμησης Ουιγούρων, Καζακστάν και Κιργιζίας, όπου έχουν σημαντικά καλύτερη απόδοση σε σύγκριση με αρκετά ισχυρά
Βασικές γραμμές.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>La clasificación del texto tiende a ser difícil cuando los datos son inadecuados, teniendo en cuenta la cantidad de cuerpos de texto etiquetados manualmente. Para los idiomas aglutinantes de pocos recursos, incluidos el uigur, el kazajo y el kirguís (idiomas del Reino Unido), en los que las palabras se fabrican a través de raíces concatenadas con varios sufijos y se utilizan raíces como representación del contenido del texto, esta característica permite un vocabulario derivado infinito que conduce a una alta incertidumbre de escribir formularios y enormes funciones redundantes. Existen grandes desafíos en la clasificación de textos aglutinantes de bajos recursos, la falta de datos etiquetados en un dominio objetivo y la diversidad morfológica de las derivaciones en las estructuras del lenguaje. Es una solución eficaz que ajusta un modelo de lenguaje previamente entrenado para proporcionar extractores de funciones significativos y favorables para las tareas de clasificación de texto posteriores. Con este fin, proponemos un modelo de lenguaje aglutinativo de bajos recursos que afine AgglutiFit, específicamente, construimos un conjunto de datos de ajuste fino de bajo ruido mediante análisis morfológico y extracción de tallos, luego ajustamos el modelo de preentrenamiento multilingüe en este conjunto de datos. Además, proponemos una estrategia de ajuste de precisión basada en la atención que selecciona mejor la información semántica y sintáctica relevante del modelo de lenguaje previamente entrenado y utiliza esas características en las tareas de clasificación de textos posteriores. Evaluamos nuestros métodos en nueve conjuntos de datos de clasificación uigur, kazajo y kirguís, donde tienen un rendimiento significativamente mejor en comparación con varios
líneas de base.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Teksti klassifitseerimine kipub olema keeruline, kui andmed on käsitsi märgistatud tekstikorpuste hulka arvestades ebapiisavad. Vähese ressursiga aglutineeritud keelte puhul, sealhulgas uõguuri, kasahhi ja kirgiisi keeled (UKK keeled), kus sõnu valmistatakse mitme sufiksiga seotud varre kaudu ning tekstisisu esitamiseks kasutatakse varre kaudu, võimaldab see funktsioon lõputult tuletisi sõnavara, mis toob kaasa kirjutamisvormide suure ebakindluse ja tohutute üleliigsete omaduste. Vähese ressursiga aglutinaatilise teksti klassifitseerimisel on suured probleemid, märgistatud andmete puudumine sihtvaldkonnas ja tuletiste morfoloogiline mitmekesisus keelestruktuurides. Tegemist on tõhusa lahendusega, mis täpsustab eelõpetatud keelemudelit, et pakkuda sisukaid ja soodsaid funktsioonide ekstraktoreid teksti klassifitseerimise ülesannete jaoks. Selleks pakume välja madala ressursiga aglutinaatse keele mudeli AgglutiFiT täpsustamiseks, täpsemalt ehitame morfoloogilise analüüsi ja tüve ekstraheerimise abil madala müraga peenhäälestuse andmekogumi, seejärel häälestame selle andmekogumi keeleülese eelkoolituse mudeli. Lisaks pakume välja tähelepanupõhise peenhäälestusstrateegia, mis valib eelõpetatud keelemudelist paremini asjakohase semantilise ja süntaktilise teabe ning kasutab neid funktsioone teksti järgmise klassifitseerimise ülesannetes. Hindame oma meetodeid üheksa Uõguuri, Kasahhi ja Kõrgõisi klassifikatsiooni andmekogumi põhjal, kus need on oluliselt paremad kui mitmed tugevad
lähtejooned.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>زمانی که داده‌ها با توجه به اندازه مقدار جسد متن به دستی برگزیده شده‌اند، محدودیت متن سخت می‌شود. برای زبانهای کمترین منبع آغاز کننده‌ای که شامل ویگور، Kazakh و کیرگیز (زبانهای UKK) می‌باشند، در آن کلمات‌ها از طریق استخوان‌های متعدد و استخوان‌ها به عنوان نمایش محتوای متن استفاده می‌شوند، این ویژگی اجازه می‌دهد کلمات ناپدید آغاز‌کننده‌های بی‌نهایی که به شکل‌های نوشتن و ویژه‌های بزرگی ناپدید می‌ چالش‌های بزرگی از ترکیب متن‌های کم منبع آلوده‌کننده‌ای وجود دارد که ناتوانی داده‌های نقاشی در یک دامنه هدف و مختلف متنوع مورفولوژیک از تولید‌ها در ساختارهای زبان وجود دارد. این یک راه حل موثری است که یک مدل پیش از آموزش زبان را تغییر می دهد تا استفاده کنندگان ویژه‌های معنی و مناسب برای استفاده از ویژه‌های ویژه‌های مختصات متن پایین استفاده کند. برای این قسمت، ما یک مدل زبان آگلوتیفیت را پیشنهاد می‌کنیم که یک مدل آگلوتیفیت با کمترین منابع آگلوتیفیت را تنظیم کنیم، مخصوصا، یک مجموعه اطلاعات نیکو تنظیم صدا را با تحلیل مورفیک و استخراج استم بسازیم، سپس مدل پیش آموزشی متوسط زبان را بر این مجموعه داده‌ ما پیشنهاد می کنیم استراتژی اصلاح توجه بر اساس توجه که بهتر اطلاعات semantic و syntactic مربوط به عنوان مدل زبان پیش آموزش شده را انتخاب کند و از این ویژگی ها در مسائل جدایی متن پایین استفاده می کند. ما روش‌هایمان را در مورد نو اویگور، کازاک و داده‌های مختصات کرگی ارزیابی می‌کنیم، جایی که آنها عملکرد بسیار بهتر در مقایسه با چندین قوی دارند
خط پایین</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tekstin luokittelu on yleensä vaikeaa, kun tiedot ovat puutteellisia, kun otetaan huomioon manuaalisesti merkittyjen tekstikorpusten määrä. Vähävaraisten agglutinaattien kielten, kuten uyguurin, kazakin ja kirgisian (UKK-kielet), joissa sanoja valmistetaan varren kautta, jotka on yhdistetty useisiin sufixeihin ja varret käytetään tekstin sisällön esittämiseen, tämä ominaisuus mahdollistaa loputtoman johdannaissanaston, joka johtaa kirjoitusmuotojen korkeaan epävarmuuteen ja valtaviin tarpeettomiin ominaisuuksiin. Vähävaraisen agglutinaattisen tekstin luokitteluun liittyy suuria haasteita, sillä kohdealueella ei ole merkittyä tietoa ja kielirakenteiden johdannaisten morfologinen monimuotoisuus. Se on tehokas ratkaisu, joka hienosäätää ennalta koulutettua kielimallia tuottamaan mielekkäitä ja käyttökelpoisia ominaisuusuuttimia loppupään tekstiluokitustehtäviin. Tätä varten ehdotamme AgglutiFiT:n hienosäätöä hyödyntävää agglutinatiivista kielimallia, erityisesti rakennamme hiljaisen hienosäätöaineiston morfologisella analyysillä ja varrenpoistolla, minkä jälkeen hienosäädämme monikielisen esikoulutusmallin tähän aineistoon. Lisäksi ehdotamme huomiota perustuvaa hienosäätöstrategiaa, joka valitsee paremmin relevantin semanttisen ja syntaktisen tiedon esikoulutetusta kielimallista ja käyttää näitä ominaisuuksia loppupään tekstiluokittelutehtävissä. Arvioimme menetelmiämme yhdeksästä uiguurin, Kazakstanin ja Kirgisian luokitusaineistosta, joissa niiden suorituskyky on huomattavasti parempi verrattuna useisiin vahvoihin luokituksiin.
peruslinjat.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>La classification de texte a tendance à être difficile lorsque les données sont inadéquates compte tenu de la quantité de corpus de texte étiquetés manuellement. Pour les langues agglutinantes à faibles ressources, y compris l'ouïghour, le kazakh et le kirghize (langues UKK), dans lesquelles les mots sont fabriqués via des tiges concaténées avec plusieurs suffixes et des radicaux sont utilisés comme représentation du contenu du texte, cette fonctionnalité permet un vocabulaire de dérivés infinis qui conduit à une incertitude élevée d'écriture de formulaires et d'énormes fonctionnalités redondantes. La classification des textes agglutinants à faibles ressources, le manque de données étiquetées dans un domaine cible et la diversité morphologique des dérivations dans les structures linguistiques présentent des défis majeurs. Il s'agit d'une solution efficace qui permet de peaufiner un modèle de langage pré-formé afin de fournir des extracteurs de caractéristiques significatifs et favorables à l'utilisation pour les tâches de classification de texte en aval. À cette fin, nous proposons un modèle de langage agglutinant à faibles ressources pour affiner AgglutiFIT, en particulier, nous construisons un ensemble de données de réglage fin à faible bruit par analyse morphologique et extraction de tige, puis nous affinons le modèle de pré-apprentissage multilingue sur cet ensemble de données. De plus, nous proposons une stratégie de réglage fin basée sur l'attention qui sélectionne mieux les informations sémantiques et syntaxiques pertinentes à partir du modèle de langage pré-formé et utilise ces fonctionnalités dans les tâches de classification de texte en aval. Nous évaluons nos méthodes sur neuf ensembles de données de classification ouïghours, kazakhs et kirghizes, où elles présentent des performances nettement meilleures par rapport à plusieurs
lignes de base.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Is gnách go mbíonn sé deacair téacs a rangú nuair nach leor na sonraí ag cur san áireamh an méid corpora téacs a lipéadaítear de láimh. I gcás teangacha comhglutineacha íseal-acmhainne lena n-áirítear Uyghur, Kazakh, agus Cirgisis (teangacha UKK), ina ndéantar focail a mhonarú trí ghais atá comhcheangailte le roinnt iarmhíreanna agus ina n-úsáidtear gais mar léiriú ar ábhar téacs, ceadaíonn an ghné seo díorthaigh gan teorainn stór focal a eascraíonn as ard. éiginnteacht foirmeacha scríbhneoireachta agus gnéithe iomarcacha ollmhóra. Tá dúshláin mhóra ann maidir le haicmiú téacs comhglutineach ar acmhainní íseal, easpa sonraí lipéadaithe i spriocfhearann agus éagsúlacht mhoirfeolaíoch díorthach i struchtúir teanga. Is réiteach éifeachtach é a dhéanann mionchoigeartú ar mhúnla teanga réamhoilte chun sainfháiscirí a bhfuil brí agus fabhrach le húsáid a sholáthar do thascanna aicmithe téacs iartheachtacha. Chuige sin, molaimid múnla teanga glutinative íseal-acmhainní a mhionchoigeartú AgglutiFiT, go sonrach, tógaimid tacar sonraí mionchoigeartaithe íseal-torainn trí anailís mhoirfeolaíoch agus asbhaint gas, ansin mionchoigeartú ar an tsamhail réamh-oiliúna tras-teangach ar. an tacar sonraí seo. Ina theannta sin, molaimid straitéis mhionchoigeartaithe aird-bhunaithe a roghnaíonn faisnéis shéimeantach agus chomhréire ábhartha ón tsamhail teanga réamhoilte agus a úsáideann na gnéithe sin ar thascanna iartheachtacha aicmithe téacs. Déanaimid luacháil ar ár modhanna ar naoi dtacar sonraí aicmithe Uyghur, Kazakh, agus Cirgisis, áit a bhfuil feidhmíocht i bhfad níos fearr acu i gcomparáid le roinnt tacar sonraí láidre.
bunlínte.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tsarin matsayi ya zama mai tsanani idan data ba su isa daidai ba, idan ana ƙayyade girmar matsayin da hannayen aka rubũta. @ info: whatsthis Kuna da masu girma wa classified matsayin na wuri-resource agagagutinative na ƙaranci da danne da aka rubũta shi a lokacin da aka yi amfani da shi, da diffai masu motsi cikin tsarin harshe. @ action: button Ga wannan, Munã buɗa wani misali na aggliutinative lugha mai kyau-tuning AggliutiFiT, kuma da ƙayyade, muna sami wani danne mai sauri-sauni da ake samun rarraba fasalin morfologi da kuma za'a yi amfani da shi, sa'an nan kuma muna sami misãlin mai amfani da ke kan wannan dataset. Za kuma, za mu buɗa wani akan tunkuɗe wa masu fasahan aikin da aka samar da aikin muhimmi da masu husika na semantic da syntactic daga misalin harshen da aka yi wa zaman-wa'anar na samu kuma Muke amfani da su masu tsari kan aikin classified matsayin da ke ƙarƙashe. Tuna ƙaddara hanyoyinmu a kan masaluman tara na Uighur, Kazakh da kyrgiz, inda sun sami mafiya kyakkyawan fasarin da wasu mãsu ƙarfi
asalin.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>מסווג טקסט נוטה להיות קשה כאשר נתונים אינם מספיקים בהתחשב בכמות של גופורה טקסט מסוימת ידנית. עבור שפות אגלוטינציות נמוכות כוללות אויגור, קזאק, וקירגיז (שפות UKK), שבהן מילים יוצרות באמצעות גזעים משותפים עם מספר ספיקסים וסטים משתמשים כמייצג של תוכן טקסט, התחום הזה מאפשר מילון הנגזרים אינסופיים שמוביל לאי-בטוחות גבוהה של צורות כתיבות ומיוחדים ענקים. ישנם אתגרים גדולים של מסווג טקסט אגלוטינטיבי משאבים נמוכים, חוסר נתונים מסומנים בתחום המטרה, ומגווון מורפולוגי של התוצאות במבני שפה. זה פתרון יעיל שמתאים את דוגמנית שפת מאומנת מראש כדי לספק משמעותיים וחוברים לשימוש מחלקי תכונות עבור משימות מסווג טקסט למטה. למטרה זו, אנו מציעים מודל שפה אגלוטינטיבי עם משאבים נמוכים מתאים גבוה AgglutiFiT, במיוחד, אנו בונים קבוצת נתונים מתאים גבוה עם רעש נמוך על ידי ניתוח מורפולוגי וחילוץ סטם, ואז מתאים את מודל התאמה לפני השפה הצלבית על קבוצת נתונים זו. בנוסף, אנו מציעים אסטרטגיה מתאימה מבוססת תשומת לב שמבוחרת יותר מידע סמנטי וסינטקטי רלוונטי ממודל השפה המאמן מראש We evaluate our methods on nine Uyghur, Kazakh, and Kyrgyz classification datasets, where they have significantly better performance compared with several strong
קווי הבסיס.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>पाठ वर्गीकरण तब मुश्किल होता है जब डेटा मैन्युअल रूप से लेबल किए गए टेक्स्ट कॉर्पोरेट की मात्रा पर विचार करते हुए अपर्याप्त होता है। उइघुर, कजाख, और किर्गिज़ (यूकेके भाषाओं) सहित कम-संसाधन एग्लूटीनेटिव भाषाओं के लिए, जिसमें शब्दों को कई प्रत्ययों के साथ संयोजित तने के माध्यम से निर्मित किया जाता है और उपजी का उपयोग पाठ सामग्री के प्रतिनिधित्व के रूप में किया जाता है, यह सुविधा अनंत डेरिवेटिव शब्दावली की अनुमति देती है जो लेखन रूपों और विशाल अनावश्यक सुविधाओं की उच्च अनिश्चितता की ओर जाता है। कम संसाधन agglutinative पाठ वर्गीकरण की प्रमुख चुनौतियां हैं एक लक्ष्य डोमेन में लेबल किए गए डेटा की कमी और भाषा संरचनाओं में व्युत्पत्ति की आकृति विज्ञान विविधता। यह एक प्रभावी समाधान है जो डाउनस्ट्रीम पाठ वर्गीकरण कार्यों के लिए सार्थक और अनुकूल-से-उपयोग सुविधा एक्सट्रैक्टर्स प्रदान करने के लिए एक पूर्व-प्रशिक्षित भाषा मॉडल को ठीक-ठीक ट्यूनिंग करता है। इस अंत के लिए, हम एक कम-संसाधन agglutinative भाषा मॉडल ठीक ट्यूनिंग AgglutiFiT का प्रस्ताव, विशेष रूप से, हम रूपात्मक विश्लेषण और स्टेम निष्कर्षण द्वारा एक कम शोर ठीक ट्यूनिंग डेटासेट का निर्माण, तो ठीक धुन इस डेटासेट पर पार-भाषाई पूर्व प्रशिक्षण मॉडल. इसके अलावा, हम एक ध्यान-आधारित ठीक-ट्यूनिंग रणनीति का प्रस्ताव करते हैं जो पूर्व-प्रशिक्षित भाषा मॉडल से प्रासंगिक शब्दार्थ और वाक्यात्मक जानकारी का बेहतर चयन करता है और डाउनस्ट्रीम पाठ वर्गीकरण कार्यों पर उन सुविधाओं का उपयोग करता है। हम नौ उइघुर, कजाख और किर्गिज़ वर्गीकरण डेटासेट पर अपने तरीकों का मूल्यांकन करते हैं, जहां उनके पास कई मजबूत की तुलना में काफी बेहतर प्रदर्शन है
बेसलाइन।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Klasifikacija teksta je često teško kad podaci nisu dovoljni s obzirom na količinu ručno označenog tekstnog tijela. Za manje resurse agglutinativne jezike uključujući Uyghur, Kazakh i Kirgiz (UKK jezike), u kojima se riječi proizvode putem stakla povezanih s nekoliko sufiksa i stakla koriste kao predstavljanje sadržaja teksta, ta osobina omogućava beskonačnu riječ derivata koji dovodi do visoke nesigurnosti oblika pisanja i ogromnih redundantnih funkcija. Postoje veliki izazovi klasifikacije teksta s niskim resursima aglutinativnih teksta nedostatak označenih podataka u ciljnom domenu i morfološkom raznolikosti derivacija u jezičkim strukturama. To je učinkovito rješenje koje ispravlja predobučeni jezički model za pružanje značajnih i favorilnih ekstraktora funkcija za klasifikaciju teksta. Za taj cilj predlažemo model AgglutiFiT-a s niskim resursima agglutinativnim jezikom, posebno, izgradimo nizak zvuk fino-prilagođavanje podataka morfološkom analizu i ekstrakcijom matičnih izvora, zatim srediti preko jezika predobučeni model na ovom setu podataka. Osim toga, predlažemo strategiju za finaliziranje pažnje koja bolje odabere relevantne semantičke i sintaktičke informacije iz predobučenog jezičkog modela i koristi te karakteristike na zadatkima klasifikacije teksta. Procjenjujemo naše metode na devet Uyghura, Kazakha i kirgijskih klasifikacijskih podataka, gdje imaju značajno bolje učinkovitosti u usporedbi s nekoliko jakih
osnovne linije.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A szövegosztályozás általában nehéz, ha az adatok nem megfelelőek, tekintettel a manuálisan feliratozott szövegkorpuszok mennyiségére. Az alacsony erőforrású agglutinált nyelvek esetében, beleértve az ujgur, kazah és kirgiz nyelveket (UKK nyelveket), amelyekben a szavak több utótaggal és szárral összekapcsolt száron keresztül készülnek a szövegtartalom ábrázolására, ez a funkció végtelen származékos szókincset tesz lehetővé, ami nagy bizonytalanságot eredményez az írási formák és hatalmas redundáns funkciók. Az alacsony erőforrású agglutinált szövegosztályozás jelentős kihívásokkal jár, a céltartományban megjelölt adatok hiánya és a nyelvi struktúrák származékainak morfológiai sokfélesége. Ez egy hatékony megoldás, amely finomhangolja az előre képzett nyelvi modellt, hogy értelmes és kedvezően használható funkciókivonókat biztosítson a downstream szövegosztályozási feladatokhoz. Ennek érdekében egy alacsony erőforrású agglutinatív nyelvi modellt javasolunk, amely finomhangolja AgglutiFiT-t, konkrétan alacsony zajszintű finomhangoló adatkészletet építünk morfológiai elemzéssel és szárkinyeréssel, majd finomhangoljuk ezen adatkészleten a keresztnyelvű előképzési modellt. Emellett egy figyelem alapú finomhangolási stratégiát javasolunk, amely jobban kiválasztja a releváns szemantikai és szintaktikai információkat az előképzett nyelvi modellből, és ezeket a funkciókat a downstream szövegosztályozási feladatokhoz használja. Módszereinket kilenc ujgur, kazah és kirgiz osztályozási adatkészleten értékeljük, ahol jelentősen jobb teljesítménnyel rendelkeznek több erős adathoz képest.
alapvető vonalak.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Տեքստի դասակարգումը հակված է դժվար լինել, երբ տվյալները բավարար են, հաշվի առնելով ձեռքով նշված տեքստի մարմնի քանակը: Այս հատկությունը թույլ է տալիս անսահմանափակ ծառայությունների բառարան, որը հանգեցնում է գրողական ձևերի մեծ անորոշությունը և հսկայական անհնար հատկություններին: Նվագ ռեսուրսների ագլուտիվ տեքստի դասակարգման մեծ մարտահրավերներ կան նպատակային ոլորտում նշանակված տվյալների բացակայությունը և լեզվի կառուցվածքների մորֆոլոգիական բազմազանությունը: Դա արդյունավետ լուծում է, որը վերափոխում է նախապատրաստված լեզվի մոդելը, որպեսզի տրամադրի իմաստալից և օգտագործելի առանձնահատկությունների վերացուցիչներ ներքևի տեքստի դասակարգումների համար: Այս նպատակով, մենք առաջարկում ենք ցածր ռեսուրսներով ագլյուտինատիվ լեզվի մոդել, որը կազմակերպում է ագգլյուտիֆիթ, հատկապես, մենք կառուցում ենք ցածր աղմուկի ագլյուտիզացիոն տվյալների համակարգ՝ մորֆոլոգիական վերլուծությամբ և ստորաձև վերացնելով, հետո Ավելին, մենք առաջարկում ենք ուշադրության վրա հիմնված բարելավման ռազմավարություն, որն ավելի լավ ընտրում է նշանակալի սեմանտիկ և սինտակտիկ տեղեկատվություն նախապատրաստված լեզվի մոդելի միջոցով և օգտագործում է այդ հատկությունները հետագա տեքստի դասա Մենք գնահատում ենք մեր մեթոդները 9 Ոյգուրի, Կազախի և Կիրգիզի դասակարգման տվյալների համակարգերի վրա, որտեղ դրանք շատ ավելի լավ արդյունք ունեն, համեմատած մի քանի ուժեղ
հիմնական գծերը:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Klasifikasi teks cenderung menjadi sulit ketika data tidak cukup mempertimbangkan jumlah teks yang ditabel secara manual corpora. Untuk bahasa agglutinatif sumber daya rendah termasuk Uyghur, Kazakh, dan Kyrgyz (bahasa UKK), di mana kata-kata diproduksi melalui stem yang dikoncatenasi dengan beberapa suffix dan stem digunakan sebagai perwakilan konten teks, fitur ini memungkinkan vocabulari derivat yang tidak terbatas yang mengarah ke ketidakpastian tinggi bentuk menulis dan fitur-fitur redundant besar. Ada tantangan utama dari klasifikasi teks agglutinatif sumber rendah kekurangan data yang ditabel dalam domain target dan diversitas morfologi dari derivasi dalam struktur bahasa. Ini adalah solusi efektif yang menyesuaikan model bahasa yang terlatih untuk menyediakan ekstraktor fitur yang berguna dan berguna untuk menggunakan untuk tugas klasifikasi teks turun. Untuk tujuan ini, kami mengusulkan model bahasa AgglutiFiT dengan sumber daya rendah memperbaiki AgglutiFiT, secara spesifik, kami membangun set data memperbaiki suara rendah dengan analisis morfologi dan ekstraksi stem, kemudian memperbaiki model prapelatihan salib bahasa pada set data ini. Selain itu, kami mengusulkan strategi penyesuaian yang berdasarkan perhatian yang lebih baik memilih informasi semantis dan sintaksi relevan dari model bahasa yang dilatih-dilatih dan menggunakan fitur-fitur tersebut pada tugas klasifikasi teks turun. Kami mengevaluasi metode kami pada sembilan dataset klasifikasi Uyghur, Kazakh dan Kyrgyz, di mana mereka memiliki prestasi yang jauh lebih baik dibandingkan dengan beberapa kuat
garis dasar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>La classificazione del testo tende ad essere difficile quando i dati sono inadeguati considerando la quantità di corpi di testo etichettati manualmente. Per le lingue agglutinative a basso contenuto di risorse, tra cui uiguro, kazako e kirghizista (lingue UKK), in cui le parole sono prodotte tramite steli concatenati con diversi suffissi e steli sono utilizzati come rappresentazione del contenuto del testo, questa caratteristica consente infiniti vocaboli derivati che portano ad un'elevata incertezza delle forme di scrittura e enormi funzionalità ridondanti. Ci sono grandi sfide legate alla classificazione dei testi agglutinanti a basso contenuto di risorse, alla mancanza di dati etichettati in un dominio target e alla diversità morfologica delle derivazioni nelle strutture linguistiche. È una soluzione efficace che perfeziona un modello linguistico pre-addestrato per fornire estrattori di funzionalità significativi e favorevoli all'uso per le attività di classificazione del testo a valle. A tal fine, proponiamo un modello di linguaggio agglutinativo a basso contenuto di risorse che perfeziona AgglutiFiT, nello specifico, costruiamo un set di dati a basso rumore di fine-tuning mediante analisi morfologica ed estrazione dello stel, quindi perfezionamo il modello di pre-formazione cross-lingual su questo set di dati. Inoltre, proponiamo una strategia di fine-tuning basata sull'attenzione che seleziona meglio le informazioni semantiche e sintattiche rilevanti dal modello linguistico pre-addestrato e utilizza tali funzionalità nelle attività di classificazione del testo a valle. Valutiamo i nostri metodi su nove set di dati di classificazione uiguri, kazaki e kirghizi, dove hanno prestazioni significativamente migliori rispetto a diversi
linee di base.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>手動でラベル付けされたテキストコーラの量を考慮すると、データが不十分な場合、テキスト分類は難しい傾向があります。 ウイグル語、カザフ語、キルギス語（ UKK言語）を含む低資源の集約言語では、複数の接尾辞とステムで連結されたステムを介して単語が製造され、テキストコンテンツの表現として使用されるため、この機能は、書き込みフォームの不確実性の高さと巨大な冗長機能につながる無限の派生語彙を可能にします。 低資源凝集性テキスト分類の主要な課題は、標的ドメインにおける標識データの欠如及び言語構造における派生の形態学的多様性である。 これは、事前にトレーニングされた言語モデルを微調整して、下流のテキスト分類タスクに有意義で使いやすい機能抽出を提供する効果的なソリューションです。 この目的のために、私たちは低リソースの凝集性言語モデルを提案し、AgglutiFiTを微調整します。具体的には、形態分析とステム抽出によって低ノイズの微調整データセットを構築し、このデータセット上のクロスリンガル事前トレーニングモデルを微調整します。 さらに、事前にトレーニングされた言語モデルから関連する意味情報と構文情報をより適切に選択し、それらの機能を下流のテキスト分類タスクに使用する、注意に基づく微調整戦略を提案します。 私たちは、9つのウイグル語、カザフ語、キルギス語の分類データセットで方法を評価します。これらのデータセットは、いくつかの強力なデータセットと比較して有意に優れたパフォーマンスを持っています。
ベースライン。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>email-custom-header-Security Perusahaan langgambar kelas karo akeh-Ressource error message It is an Effect Resolution Fine Label Awak dh챕w챕 챕ntuk dh챕w챕 챕ntuk ak챕w챕 ning ngerasai liyane UYhur, Kasakh karo data seneng kerghiz, kawur dh챕w챕 wis ana luwih operasi sing gak dh챕w챕 karo ngono sing paling dh챕w챕
vertical-aligntextattr</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ტექსტის კლასიფიკაცია უფრო რთული იქნება, როცა მონაცემები არაფექტი იქნება, როცა ტექსტის კოპორაციის მარტივი მარტივი მარტივი მარტივი ზ მარტივი რესურსების ადგლუტინატიური ენებისთვის, რომლებიც სიგური, კაზაკი და კურგიზი (UKK ენები), რომლებიც სიტყვებების გამოყენება, რამდენიმე სუფიქსის და სტექსის გამოყენება, რომლებიც ტექსტის შემდგომარების გამოყენებაში გამოყენებულია, ეს ფუნქცია უნ არსებობს მნიშვნელოვანი გამოცდილებები მარტივი რესურსების ადგლუტინატიური ტექსტიკის კლასიფიკაციაში მარტივი დიომინში და მორფოლოგიური განსხვავებების განსხვავება ენის სტრ ეს ეფექტიური პასუხი, რომელიც წინ განაკეთებული ენის მოდელის შესაძლებელი და მნიშვნელოვანი გამოყენებელი ფუნქტურების ექსტრეკტორები ტექსტის კლასიფიკაციის დავალებებისთვის დაა ამ მიზეზისთვის, ჩვენ მინდომარესური ადლუტინატიური ენის მოდელის წარმოდგლუტიფიფიT, განსაკუთრებით, ჩვენ მოპოროლოგიური ანალიზაციის და სტემი ექსტრექციის მანძილური მოდელის შესახებ დავკეთებთ ამ მონაცემების საშუალოდ დამატებით, ჩვენ დავწყვებთ ინტერნეციის დასაწყებული კონტაქტიური სტრატიგია, რომელიც უფრო უფრო უკეთესი სმენტიური და სინტაქტიური ინფორმაციის მოდულიდან წინაწყებული ენის მოდელ ჩვენ ჩვენი მეტოვები 9 სიგური, კაზაკური და კურგიზი კლასიფიკაციის მონაცემების შესახებ გავაკეთებთ, სადაც ისინი მნიშვნელოვანია უკეთესი კონფიკაცია
ბაზიანი.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Мәтін klassifikaциясы, деректер қолмен жарлық мәтін корпорасының қаншасын қарастыруға мүмкіндік емес болғанда қиын болады. Уигур, Казах және Кыргыз тілдері (UKK тілдері) үшін төмен ресурстар аглутизациялық тілдер үшін сөздерді бірнеше жұрнақтар және стимлер мәтін мазмұнын көрсету үшін қолданылады. Бұл мүмкіндік жазу пішімдері мен үлкен қалқымалы мүмкіндіктер үшін шектелмеген дерективтер сөздер Мақсатты доменде жарлықталған деректер жоқ және тіл құрылғыларындағы морфологиялық түрлі түрлі құрылғылардың маңызды мәтін бағыттауының негізгі өзгерістері бар. Мәтін классификациялау тапсырмалары үшін мәліметті және қолданатын мүмкіндіктерді қолдану үшін алдыңғы тіл үлгісін баптау эффективні шешімі. Бұл үшін біз мәліметтің артық ресурстар аглутизациялық тіл үлгісін таңдау үлгісін таңдаймыз. Мәлімет, морфологиялық анализ және стим тарқату арқылы тыс дыбыс түзету деректерін құрамыз, сондықтан мәліметтің алдын- тілікті алды Сонымен қатар, біз бақылау тәртіпке негізделген тәртіпке баптау стратегиясын таңдаймыз. Алдын- ала оқылған тіл үлгісінен қатынау және синтактикалық мәліметтерді таңдап, оларды төменгі мәтін кл Біз 9 Uyghur, Kazakh және Кыргыз классификациялық деректер қорларында тәртіптерімізді оқу үшін, олар бірнеше күшті жұмыс салыстырып тұрады.
негізгі жолдар.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>인공적으로 표시된 텍스트 자료 라이브러리의 수량을 고려하면 데이터가 부족할 때 텍스트 분류는 비교적 어렵다.위구르어, 카자흐어, 키르기스어 (UKK 언어) 를 포함한 저자원 접착성 언어의 경우, 이들 언어의 단어는 어간과 몇 개의 접미사를 통해 연결되고, 어간은 작문본 내용의 표시로 사용되며, 이 기능은 무제한 파생 어휘를 허용하여 쓰기 형식의 높은 불확실성과 대량의 불필요한 기능을 초래한다.저자원 접착성 텍스트 분류가 직면한 주요 도전은 목표역에서 표기된 데이터의 부족과 언어 구조에서 파생된 단어의 형태의 다양성이다.미리 훈련된 언어 모델을 미세하게 조정하여 후속 텍스트 분류 임무에 의미 있고 유리한 특징 추출기를 제공하는 것은 효과적인 해결 방안이다.이를 위해 우리는 저자원 응집 언어 모델인 마이크로스피커 응집 의합을 제시했다. 즉, 형태학 분석과 어간 추출을 통해 저소음 마이크로스피커 데이터 집합을 구축하고 이 데이터 집합에서 마이크로스피커 크로스 언어 예훈련 모델을 구축했다.그 밖에 우리는 주의를 바탕으로 하는 마이크로 조정 전략을 제시하여 미리 훈련된 언어 모델에서 관련 의미와 문법 정보를 더욱 잘 선택하고 이러한 특징을 하위 텍스트 분류 임무에 사용할 수 있다.우리는 9개의 위구르족, 카자흐족, 키르기스족 분류 데이터 집합에서 우리의 방법을 평가했다. 이런 데이터 집합에서 몇 개의 강력한 분류 데이터 집합에 비해 그들의 성능이 현저히 좋다.
베이스라인.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Teksto klasifikavimas paprastai yra sudėtingas, kai duomenų nepakanka, atsižvelgiant į rankiniu būdu pažymėto teksto korpuso kiekį. Nedidelių išteklių aglutinatyvių kalbų, įskaitant Uyghur ą, Kazachą ir Kirgiziją (UKK kalbas), kuriose žodžiai gaminami naudojant stiebus, sutvirtintus keliais stiebliais ir stiebliais, naudojami kaip teksto turinio reprezentacija, šis požymis leidžia neribotą išvestinių medžiagų žodyną, dėl kurio kyla didelis rašymo formų neapibrėžtumas ir dideli pertekliniai požymiai. There are major challenges of low-resource agglutinative text classification the lack of labeled data in a target domain and morphologic diversity of derivations in language structures. Tai veiksmingas sprendimas, kuriuo tiksliai pritaikomas iš anksto parengtas kalbų model is, siekiant suteikti prasmingus ir palankius naudojimo savybių ekstraktorius tolesnėms teksto klasifikavimo užduotims. Šiuo tikslu siūlome mažai išteklių aglutinacinį kalbos model į, tiksliai tikslinantį AgglutiFiT, kuriame mažo triukšmo tikslinimo duomenų rinkinį atliekant morfologinę analizę ir kamieno ekstrakciją, o vėliau tikslinantį šio duomenų rinkinio tarpkalbinį pasirengimo mokymui modelį. Be to, siūlome dėmesio pagrindu pagrįstą patobulinimo strategiją, kuri geriau atrinktų svarbią semantinę ir sintaksinę informaciją iš iš anksto parengto kalbos modelio ir panaudotų šias charakteristikas tolesnio teksto klasifikavimo užduotims. Vertiname savo metodus devyniuose Uyghuro, Kazacho ir Kirgizijos klasifikavimo duomenų rinkiniuose, kuriuose jie gerokai geresni, palyginti su keliomis stipriomis
bazinės linijos.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Класификацијата на текстот е тешка кога податоците не се доволни со оглед на количината на рачно означени текстови. За јазици со ниски ресурси, вклучително и Уигур, Казах и Киргиз (јазици на УКК), во кои зборовите се произведуваат преку столбови со неколку суфикси и столбови се користат како претставување на текстова содржина, оваа карактеристика овозможува бесконечен резултат на дерибутиви што води до висока несигурност на формуларите за пишување и огромни претер Постојат големи предизвици од класификацијата на ниските ресурси на аглутинативниот текст, недостатокот на означени податоци во целна домена и морфолошката различност на деринациите во јазичните структури. Тоа е ефективно решение кое финетизира предобучен јазички модел за да обезбеди значајни и поволни екстрактори на карактеристики за понатамошна класификација на текстот. За оваа цел, предложуваме мало ресурсно аглутинативен јазички модел за финетизирање на AgglutiFiT, специфично, ние изградуваме ниско звукно финетизирање на податоците со морфолошка анализа и екстракција на стомак, а потоа финетизирање на крстојазичниот модел за предобука на овој податок. Покрај тоа, предложуваме стратегија за финетизирање базирана на вниманието која подобро ги избира релевантните семантични и синтактички информации од предобучениот јазички модел и ги користи овие карактеристики на задачите за класификација на текстот. Ги проценуваме нашите методи на девет податоци за класификација на Уигур, Казах и Киргиз, каде што тие имаат значително подобра резултат во споредба со неколку силни
Основни линии.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>വിവരങ്ങള്‍ കൈയ്യില്‍ ലേബിള്‍ ചെയ്ത ടെക്സ്റ്റ് കോര്‍പ്പോരിയുടെ എണ്ണം കാണാന്‍ ആവശ്യമില്ലെങ്കില്‍ പദാവലി ക്ല കുറഞ്ഞ വിഭവങ്ങള്‍ക്ക്, ഉഗ്ഗുര്‍, കസാഖ്, കിര്‍ഗ്ഗിസ് എന്നിവയുള്ള ഭാഷകള്‍ക്കും, വാക്കുകള്‍ ഉണ്ടാക്കിയിരിക്കുന്നു. വാക്കുകള്‍ കൂടുതല്‍ സ്റ്റേജുകള്‍ കൂട്ടിചേര്‍ക്കുന്ന സ്റ്റേജുകള്‍ക്കും സ്റ്റേഡുകള്‍ക്കും വ ലക്ഷ്യസ്ഥാനത്തില്‍ ലേബിള്‍ ഡേറ്റായിട്ടുള്ള വിവരങ്ങളുടെ കുറഞ്ഞ വിഭവങ്ങളുടെ വിവിധ വ്യത്യാസങ്ങളുണ്ട്. ഭാഷയുടെ അടിസ്ഥാനങ്ങളില്‍ മോര്‍ പ്രധാനപ്പെട്ട ഭാഷയുടെ മോഡല്‍ മുമ്പ് പരിശീലിക്കപ്പെട്ട ഭാഷയുടെ പ്രധാനപ്പെടുത്തുന്ന ഒരു പ്രധാനപ്പെട്ട തീരുമാനമാണിത്. ഡെസ ഈ അവസാനത്തിനു വേണ്ടി ഞങ്ങള്‍ ഒരു കുറഞ്ഞ വിഭവങ്ങള്‍ ആഗ്ലുട്ടിയിട്ടുള്ള ഭാഷ മോഡല്‍ പ്രൊദ്ദേശിപ്പിക്കുന്നു. പ്രത്യേകിച്ച്, നമ്മള്‍ മോര്‍ഫോളജിക്കല്‍ അന്വേഷണവും സ്റ്റേമിക്കല്‍ പു അതുകൊണ്ട്, നമ്മള്‍ ശ്രദ്ധിക്കുന്ന ഒരു നല്ല തിരഞ്ഞെടുക്കാനുള്ള ഗുണപൂര്‍ണ്ണമായ വിവരങ്ങള്‍ മുന്‍പരിശീലന ഭാഷ മോഡലില്‍ നിന്നും ഉത്തരവാദിത്തം തെരഞ ഞങ്ങള്‍ നമ്മുടെ രീതികളെ വിലാസപ്പെടുത്തുന്നു. നമ്മുടെ രീതികള്‍ ഒമ്പതു ഉഗ്ഹുര്‍, കസാഖ്, കിര്‍ഗിസ് ക്ലാസ്ഫിക്കല്‍ ഡാറ്റാസറ്റുകള
അടിസ്ഥാനങ്ങള്‍.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Текст хуваалтын хэмжээ нь өгөгдлийн хуваалтын хэмжээ гараар нэрлэгдсэн текст корпора гэдгийг ойлгож чадахгүй байхад хэцүү байдаг. Уйгур, Казах, Киргиз (УKK хэл) гэх мэт бага нөөцийн аглуктив хэл дээр хэлбэрээр хэлбэрээр хэлбэрээр хэлбэрээр бүтээж, хэд хэдэн давхар, стэмс нь текст бүтээгдэхүүний төлөвлөгөө болгон ашигладаг. Энэ төлөвлөгөө нь бичиж буй хэлбэрээс маш их тодорхойгүй байдлыг болон маш их Холбооны бүтээгдэхүүн дээр нэрлэгдсэн өгөгдлийн байдал болон хэлний бүтээгдэхүүний морфологик олон төрлийн төлөвлөгөөс бага багасгах мөнгө бүтээгдэхүүний төлөвлөгөөний асуудал бий. Энэ бол сургалтын өмнө сургалтын хэл загварыг тодорхойлж, утгатай, ашиглах хэрэгтэй өргөмжийг ашиглах боломжтой арга загваруудыг дамжуулах эффективны шийдэл юм. Энэ төгсгөлд бид бага боломжтой аглутинацийн хэл загварыг аглутифифиT тодорхойлж, ялангуяа бид морфологик шинжилгээ, стэм татаж бага чимээгүй өгөгдлийн санг бүтээж, дараа нь энэ өгөгдлийн сангийн аль хэлний сургалтын загварыг тодорхойлж чадна. Дараа нь бид анхаарлын төвөгтэй сайжруулах стратегийг илүү сайжруулдаг бөгөөд урьд сургалтын хэл загвараас хамааралтай семантик болон синтактик мэдээллийг сонгож, тэдгээрийг доорх текст хуваалтын ажил дээр ашигладаг. Бид 9 Уйгур, Казах, Киргиз хуваалтын өгөгдлийн сангийн аргыг үнэлгээж байна. Тэд хэдэн хүчтэй харьцуулахад илүү сайн үйл ажиллагаатай байдаг.
суурь шугам.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Klasifikasi teks cenderung menjadi sukar bila data tidak cukup mengingat jumlah teks yang ditabel secara manual. Untuk bahasa agglutinatif sumber rendah termasuk Uyghur, Kazakh, dan Kyrgyz (bahasa UKK), di mana perkataan dihasilkan melalui stem yang bersamaan dengan beberapa suffiks dan stem digunakan sebagai perwakilan kandungan teks, ciri ini membolehkan vocabulari derivatif tak terbatas yang membawa kepada ketidakpastian tinggi bagi bentuk tulisan dan ciri-ciri yang berlebihan besar. Terdapat cabaran utama bagi klasifikasi teks agglutinatif sumber rendah kekurangan data labeled dalam domain sasaran dan pelbagai morfologi derivasi dalam struktur bahasa. Ia adalah penyelesaian yang berkesan yang menyesuaikan model bahasa pra-dilatih untuk menyediakan pengekstraktor ciri-ciri yang bermakna dan berguna untuk digunakan untuk tugas klasifikasi teks turun. Untuk tujuan ini, kami cadangkan model bahasa yang rendah-sumber agglutinatif penyesuaian AgglutiFiT, secara khusus, kami membina set data penyesuaian rendah-bunyi dengan analisis morfologik dan ekstraksi stem, kemudian penyesuaian model praselatihan saling bahasa pada set data ini. Lagipun, kami cadangkan strategi penyesuaian yang berdasarkan perhatian yang lebih baik memilih maklumat semantik dan sintaktik yang relevan dari model bahasa yang dilatih-dilatih dan menggunakan ciri-ciri tersebut pada tugas kelasukan teks turun. Kami menilai kaedah kami pada sembilan set data klasifikasi Uyghur, Kazakh, dan Kyrgyz, di mana mereka mempunyai prestasi yang jauh lebih baik dibandingkan dengan beberapa kuat
garis dasar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Il-klassifikazzjoni tat-test għandha t-tendenza li tkun diffiċli meta d-dejta ma tkunx adegwata meta jitqies l-ammont ta’ korpura tat-test ittikkettat manwalment. Għall-lingwi agglutinattivi b’riżorsi baxxi inklużi l-Uyghur, il-Każak, u l-Kirgiż (lingwi UKK), li fihom il-kliem jiġu manifatturati permezz ta’ stems konċinatati b’diversi suffissi u stems jintużaw bħala r-rappreżentazzjoni tal-kontenut tat-test, din il-karatteristika tippermetti vokabulari tad-derivattivi infiniti li jwassal għal in ċertezza għolja tal-formoli tal-kitba u karatteristiċi kbar żejda. Hemm sfidi ewlenin tal-klassifikazzjoni tat-test agglutinattiv b’riżorsi baxxi n-nuqqas ta’ dejta ttikkettata f’dominju fil-mira u d-diversità morfoloġika tad-derivazzjonijiet fl-istrutturi lingwistiċi. Hija soluzzjoni effettiva li tiffina mudell lingwistiku mħarreġ minn qabel biex tipprovdi estratturi ta’ karatteristiċi sinifikanti u favorevoli għall-użu għal kompiti ta’ klassifikazzjoni tat-test downstream. Għal dan il-għan, qed nipproponu mudell lingwistiku agglutinattiv b’riżorsi baxxi li jaġġusta l-AgglutiFiT, speċifikament, a ħna nibnu sett ta’ dejta ta’ aġġustament fin b’ħoss baxx permezz ta’ analiżi morfoloġika u estrazzjoni tal-istokk, u mbagħad naġġustaw il-mudell ta’ qabel it-taħriġ translingwistiku fuq dan is-sett ta’ dejta. Barra minn hekk, qed nipproponu strateġija ta’ rfinar ibbażata fuq l-attenzjoni li tagħżel aħjar informazzjoni semantika u sintattika rilevanti mill-mudell lingwistiku mħarreġ minn qabel u tuża dawk il-karatteristiċi fuq kompiti ta’ klassifikazzjoni tat-test downstream. Aħna jevalwaw il-metodi tagħna fuq disa' settijiet ta' dejta tal-klassifikazzjoni Uyghur, Każak u Kirgiż, fejn għandhom prestazzjoni sinifikanti aħjar meta mqabbla ma' diversi settijiet qawwija
linji bażi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tekstklassificatie is vaak moeilijk wanneer gegevens onvoldoende zijn gezien de hoeveelheid handmatig gelabelde tekstcorpora's. Voor agglutinatieve talen met weinig bronnen, waaronder Oeigoers, Kazachst en Kirgizisch (UKK-talen), waarin woorden worden vervaardigd via stengels die zijn verbonden met verschillende achtervoegsels en stelen worden gebruikt als de weergave van tekstinhoud, maakt deze functie oneindige afgeleide woordenschat mogelijk die leidt tot een hoge onzekerheid van schrijfvormen en enorme overbodige kenmerken. Er zijn grote uitdagingen van agglutinatieve tekstclassificatie met lage resources, het ontbreken van gelabelde gegevens in een doeldomein en morfologische diversiteit van afgeleidingen in taalstructuren. Het is een effectieve oplossing die een vooraf getraind taalmodel verfijnt om betekenisvolle en gunstig te gebruiken feature extractors te bieden voor downstream tekstclassificatietaken. Om dit doel te bereiken stellen we een agglutinatieve taalmodel voor dat AgglutiFiT verfijnt, specifiek bouwen we een low-noise fine-tuning dataset door morfologische analyse en stamextractie, en verfijnen vervolgens het cross-lingual pre-training model op deze dataset. Bovendien stellen we een attentie-based fine-tuning strategie voor die relevante semantische en syntactische informatie beter selecteert uit het voorgetrainde taalmodel en die functies gebruikt bij downstream tekstclassificatietaken. We evalueren onze methoden op negen Oeigoerse, Kazachstaanse en Kirgizische classificatiedatasets, waar ze aanzienlijk betere prestaties hebben vergeleken met verschillende sterke
basislijnen.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tekstklassifikasjonen er vanskeleg når data er ikkje tilstrekkeleg ved å sjekke kor mange manuelt merket tekstkorpora. For låg ressursaglutinativ språk, inkludert Uyghur, Kazakh, og Kyrgyz (UKK-språk), der ord vert produsert via stemmer samsvara med fleire suffiksar og stemmer brukt som representasjonen av tekstinnhaldet, kan denne funksjonen tillata uendelige deriverte ordbokstavar som fører til høg usikkerhet på skriveform og store redundant funksjonar. Det finst store utfordringar i låg ressursaglutinativt tekstklassifikasjon for mangling av merkelige data i eit måldområde og morfologisk mangfolding av derivasjonar i språkstrukturer. Det er ein effektiv løysing som finn opp eit før- treng språk- modell for å gje meningsverdiar og favoritt- bruk- funksjonsekstraktorar for understremde tekstklassifikasjonar. I denne slutten foreslår vi ein låg ressurs agglutinativ språk modell for fin-tuning av AgglutiFiT, spesifikke, bygger vi ei låg lys fin-tuning dataset ved morfologisk analyse og stem-ekstraksjon, og så finn opp den krysspråk føreøvingsmodellen på denne datasettet. I tillegg foreslår vi ein oppmerksbasert finnstillingsstrategi som betre veljer relevante semantiske og syntaktiske informasjon frå den først trengte språk-modellen og brukar dei funksjonane på understremde tekstklassifikasjonar. Vi evaluerer metodane våre på ni Uyghur, Kazakh og Kyrgyz-klassifikasjonsdata, der dei har mykje bedre utvikling samanlikna med fleire sterke
baseline.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Klasyfikacja tekstu jest zwykle trudna, gdy dane są niewystarczające biorąc pod uwagę ilość ręcznie oznaczonych korpusów tekstowych. W przypadku języków aglutynatywnych o niskich zasobach, w tym języków ujgurskich, kazachskich i kirgiskich (języków UKK), w których słowa produkowane są za pomocą łodyg połączonych z kilkoma przyrostkami i łodyg są używane jako reprezentacja treści tekstowej, funkcja ta pozwala na nieskończone pochodne słownictwo, co prowadzi do wysokiej niepewności form pisania i ogromnych nadmiernych cech. Istnieją poważne wyzwania związane z klasyfikacją tekstu o niskich zasobach aglutynacyjnych braku oznaczonych danych w domenie docelowej oraz różnorodnością morfologiczną pochodnych struktur językowych. Jest to skuteczne rozwiązanie, które dostosowuje wstępnie przeszkolony model językowy, aby zapewnić znaczące i korzystne w użyciu ekstraktory funkcji do dalszych zadań klasyfikacji tekstu. W tym celu proponujemy mało zasobowy model AgglutiFiT dostrajający AgglutiFiT, w szczególności zbudowujemy zbiór danych o niskim poziomie szumu poprzez analizę morfologiczną i ekstrakcję macierzy, a następnie dostrajamy model przedszkoleniowy między językami na tym zbiorze danych. Ponadto proponujemy strategię dopracowania opartą na uwadze, która lepiej wybiera istotne informacje semantyczne i składniowe z przeszkolonego modelu językowego i wykorzystuje te cechy w dalszych zadaniach klasyfikacji tekstu. Nasze metody oceniamy na dziewięciu uzgurskich, kazachskich i kirgiskich zestawach danych klasyfikacyjnych, gdzie mają one znacznie lepszą wydajność w porównaniu z kilkoma silnymi
Podstawowe linie.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A classificação de texto tende a ser difícil quando os dados são inadequados considerando a quantidade de corpora de texto rotulados manualmente. Para idiomas aglutinantes de poucos recursos, incluindo uigur, cazaque e quirguiz (línguas do Reino Unido), em que as palavras são fabricadas por meio de radicais concatenados com vários sufixos e os radicais são usados como representação do conteúdo do texto, esse recurso permite um vocabulário de derivados infinitos que leva a incerteza de escrever formas e enormes recursos redundantes. Existem grandes desafios de classificação de texto aglutinativo de baixo recurso a falta de dados rotulados em um domínio de destino e diversidade morfológica de derivações em estruturas de linguagem. É uma solução eficaz que ajusta um modelo de linguagem pré-treinado para fornecer extratores de recursos significativos e favoráveis ao uso para tarefas de classificação de texto downstream. Para este fim, propomos um modelo de linguagem aglutinativa de baixo recurso que ajusta o AglutiFiT, especificamente, construímos um conjunto de dados de ajuste fino de baixo ruído por análise morfológica e extração de haste, depois ajustamos o modelo de pré-treinamento linguístico em este conjunto de dados. Além disso, propomos uma estratégia de ajuste fino baseada na atenção que seleciona melhor as informações semânticas e sintáticas relevantes do modelo de linguagem pré-treinado e usa esses recursos em tarefas de classificação de texto a jusante. Avaliamos nossos métodos em nove conjuntos de dados de classificação uigur, cazaque e quirguiz, onde eles têm um desempenho significativamente melhor em comparação com vários conjuntos de dados fortes.
linhas de base.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Clasificarea textului tinde să fie dificilă atunci când datele sunt inadecvate având în vedere cantitatea de corpuri text etichetate manual. Pentru limbile aglutinative cu resurse reduse, inclusiv uigură, kazahă și kârgâză (limbile UKK), în care cuvintele sunt fabricate prin tulpini concatenate cu mai multe sufixe și tulpini sunt utilizate ca reprezentare a conținutului text, această caracteristică permite vocabularul derivat infinit care duce la incertitudine ridicată a formelor de scriere și caracteristici redundante uriașe. Există provocări majore legate de clasificarea textelor aglutinative cu resurse reduse, lipsa datelor etichetate într-un domeniu țintă și diversitatea morfologică a derivațiilor în structurile lingvistice. Este o soluție eficientă care reglează fin un model lingvistic pre-instruit pentru a oferi extractoare de caracteristici semnificative și favorabile utilizării pentru sarcinile de clasificare a textului în aval. În acest scop, propunem un model de limbă aglutinativă cu resurse reduse care reglează fin AgglutiFiT, în special, construim un set de date de reglare fină cu zgomot redus prin analiza morfologică și extracția tulpinilor, apoi reglăm fin modelul de pre-formare translingvistică pe acest set de date. În plus, propunem o strategie de reglare fină bazată pe atenție, care selectează mai bine informațiile semantice și sintactice relevante din modelul lingvistic pre-instruit și utilizează aceste caracteristici în sarcinile de clasificare a textului în aval. Evaluăm metodele noastre pe nouă seturi de date de clasificare uigură, kazahă și kârgâză, unde acestea au performanțe semnificativ mai bune comparativ cu mai multe seturi de date puternice
liniile de bază.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Классификация текста, как правило, затруднена, когда данные неадекватны с учетом количества текстов, помеченных вручную. Для малоресурсных агглютинативных языков, включая уйгурский, казахский и кыргызский (языки UKK), в которых слова изготавливаются через стебли, объединенные несколькими суффиксами, и стебли используются в качестве представления текстового содержания, эта особенность позволяет бесконечный словарь производных, что приводит к высокой неопределенности письменных форм и огромным избыточным признакам. Существуют серьезные проблемы, связанные с агглютинативной классификацией текста с низким уровнем ресурсов, отсутствием маркированных данных в целевой области и морфологическим разнообразием производных в языковых структурах. Это эффективное решение, которое тонко настраивает предварительно обученную лингвистическую модель, чтобы обеспечить значимые и благоприятные для использования экстракторы признаков для задач последующей классификации текста. С этой целью мы предлагаем низкоресурсную модель агглютинативного языка с точной настройкой AgglutiFiT, в частности, мы создаем низкошумный набор данных с точной настройкой путем морфологического анализа и извлечения ствола, а затем точно настраиваем кросс-лингвальную модель предварительного обучения на этом наборе данных. Кроме того, мы предлагаем стратегию тонкой настройки, основанную на внимании, которая лучше выбирает соответствующую семантическую и синтаксическую информацию из предварительно обученной языковой модели и использует эти особенности в задачах последующей классификации текста. Мы оцениваем наши методы на девяти наборах данных классификации уйгуров, казахстанцев и киргизов, где они имеют значительно лучшую производительность по сравнению с несколькими сильными
базовые линии.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>පණිවිඩය පරීක්ෂණය අමාරුයි. දත්ත නැති වෙලාවට පණිවිඩය පණිවිඩය ලේබ් කරලා තියෙන්න පුළුවන් පරීක් For low-source agglutinative language comprises UYGUR, KAZAHK, and Kyrgy (UKK language), in where Words are Maded by stems Concathed with Severous suffixes and stems are Used as the reposition of text continence, this character allows Infinit Darwin Vocatives Vocativery that lead to high uncerity of write form and great red undiundint Featurals. අඩු සම්බන්ධ ප්‍රශ්නයක් සම්බන්ධ පැත්තක් විශේෂණයේ ප්‍රධාන අවශ්‍යය තියෙනවා ඉලක්ෂාත්මක ප්‍රශ්නයක් සහ භා ඒක ප්‍රයෝජනය විසඳීමක් වෙන්න පුළුවන් භාෂාව ප්‍රයෝජනය කරන්න පුළුවන් භාෂාව ප්‍රයෝජනය සහ ප්‍රයෝජනය කරන්න පුළු මේ අවසානයෙන්, අපි ප්‍රශ්නයක් කරනවා low-source agglutinative language Model Fin-tuning AgglitiFiT, විශේෂයෙන්, අපි මොර්ෆෝලෝගික විශ්ලේෂණය සහ ස්ටීම් ප්‍රශ්නයක් නිර්මාණය කරනවා, ඊට පස්සේ ප තවත්, අපි අවධානය අධ්‍යාත්මක විශ්වාස කරන්න පුළුවන් සැමැන්ටික් සහ සංකේතික තොරතුරු තෝරාගන්න පුළුවන් භාෂා මොඩේලයෙන අපි උයිගුර්, කාසාක්, කිර්ගිස් විශේෂ දත්ත සේට් එක්ක අපේ විධානය අවශ්‍ය කරනවා, ඔවුන් ගොඩක් හොඳ විධානය තියෙනව
ප්‍රධාන ප්‍රමාණය.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Klasifikacija besedila je težka, kadar so podatki nezadostni glede na količino ročno označenih korpusov besedila. Za aglutinativne jezike z nizkimi viri, vključno z ujgurščino, kazaščino in kirgiščino (UKK jeziki), v katerih se besede izdelujejo preko stebel, povezanih z več priponami in stebel se uporabljajo kot predstavitev besedilne vsebine, ta funkcija omogoča neskončno izpeljano besedišče, ki vodi do visoke negotovosti pisanih oblik in ogromnih odvečnih značilnosti. Obstajajo veliki izzivi pri klasifikaciji aglutinacijskega besedila z nizkimi viri, pomanjkanje označenih podatkov v ciljni domeni in morfološka raznolikost derivacij v jezikovnih strukturah. Je učinkovita rešitev, ki natančno nastavi vnaprej usposobljen jezikovni model, da zagotovi smiselne in ugodne izvlečke funkcij za opravila klasifikacije besedila v nadaljnji fazi. V ta namen predlagamo model aglutinacijskega jezika z nizkimi viri za fino nastavitev AgglutiFiT, natančneje, z morfološko analizo in ekstrakcijo stebla zgradimo nizko-hrupni fini nastavitveni nabor podatkov, nato na tem naboru podatkov natančno nastavimo medjezični model predusposabljanja. Poleg tega predlagamo na pozornosti temelječo strategijo finega uravnavanja, ki bolje izbira relevantne semantične in sintaktične informacije iz vnaprej usposobljenega jezikovnega modela in uporablja te funkcije pri opravilih klasifikacije besedila po koncu toka. Naše metode ocenjujemo na devetih ujgurskih, kazahskanskih in kirgiških klasifikacijskih zbirkah podatkov, kjer imajo bistveno boljšo učinkovitost v primerjavi z več močnimi klasifikacijskimi zbirkami.
osnovne linije.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Text classification tends to be difficult when data are inadequate considering the amount of manually labeled text corpora. Luqadaha hoose ee luqadaha agglutinatida ee ku qoran luqadaha Uyghur, Kazakh iyo Kyrgyz (luqadaha UKK), kuwaas oo lagu sameeyo hadal lagu qoro kooxaha kala duwan iyo kooxaha ay ku qoran yihiin, waxaa loo isticmaalaa sida loo qeybeeyo macluumaadka qoraalka, kaasi waxyaabaha aan la’aanta aheyn wuxuu ka heli karaa hadal aan la’aanta ah oo ka soo jeeda foomka qorniinka iyo tababaro aad u weyn. Waxaa jira dhibaatooyin badan oo ku saabsan qoraalka hoose-resource agglutinative, baahida macluumaadka la qoray oo ku qoran meelaha lagu qoray iyo kala duduwan dhaqdhaqaaqa oo afka lagu qoray. Waa xal faa’iido ah oo ku hagaajinta model afka hore oo la tababaray, in lagu siiyo guryaha isticmaalka oo faa’iido leh oo loo jecel yahay isticmaalka shaqooyinka fasaxa qoraalka hoose-bannaanta. Taas darteed waxaynu soo jeedaynaa model afka agglutinati oo aad u qoran tahay AgglutiFiT, si gaar ah, waxaynu dhisaynaa macluumaad aad u sameyneyso baaritaanka morphologiga iyo soo bixinta, kadibna waxaynu sameynaa modelka hore-tababarida ee afka kala baxa, taasoo ku qoran taariikhdan. Sidoo kale waxaynu horumarinaynaa qoraal aad u fiirsan karto, taasoo si wanaagsan u doorta macluumaad la xiriira semantic iyo syntactic oo ka mid ah modelka afka hore lagu baray, waxaas oo lagu isticmaalaa shaqooyinkaas ku saabsan shaqooyinka tababarida qoraalka hoose. Waxaynu qiimeynaynaa qaababkayaga ku qoran sagaal Uyghur, Kazakh iyo kooxaha fasalka Kirgyz, kuwaas oo ay ka fiican tahay tababar sameynta qaar xoog badan.
aasaasyo.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Klasifikimi i tekstit ka tendencë të jetë i vështirë kur të dhënat janë të papërshtatshme duke konsideruar sasinë e korprave teksti të etiketuara manualisht. Për gjuhët aglutinative me burime të ulëta duke përfshirë Uyghur, Kazakh dhe Kirgizën (gjuhët UKK), në të cilat fjalët prodhohen nëpërmjet shtyllave të bashkangjitura me disa shtylla dhe shtylla përdoren si përfaqësim i përmbajtjes së tekstit, kjo funksion lejon fjalorin e përcaktuar të derivateve që shpie në pasiguri të lartë të formulave të shkrimit dhe karakteristikave të mëdha të tepërta. Ka sfida të mëdha të klasifikimit të tekstit aglutinativ me burime të ulta mungesën e të dhënave të etiketuara në një domeni objektiv dhe diversitetin morfologjik të derivatave në strukturat gjuhësore. Ky është një zgjidhje efektive që rregullon një model gjuhësh të paratrajnuar për të siguruar nxjerrës të kuptueshëm dhe të favorshëm për përdorim për detyrat e klasifikimit të tekstit në vazhdim. Për këtë qëllim, ne propozojmë një model gjuhësh aglutinative me burime të ulëta të rregullimit të AgglutiFiT, veçanërisht, ne ndërtojmë një set të dhënash të rregullimit të ulëtë të zhurmës me analizë morfologjike dhe nxjerrje të shtyllës, pastaj të rregullojmë modelin ndërgjuhësor të paratrajnimit në këtë set të dhënash. Përveç kësaj, ne propozojmë një strategji mirërregullimi bazuar në vëmendje që zgjedh më mirë informacionin e duhur semantik dhe sintaktik nga modeli i gjuhës së paratrajnuar dhe përdor këto elemente në detyrat e klasifikimit të tekstit poshtë. Ne vlerësojmë metodat tona në nëntë grupe të dhënash klasifikuese Uyghur, Kazakh dhe Kirgiz, ku ato kanë performancë më të mirë në krahasim me disa të forta
linjat bazë.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Klasifikacija teksta je često teška kada podaci nisu dovoljni s obzirom na količinu ručno označene tekstne korpore. Za jezike niskih resursa, uključujući Uyghur, Kazakh i Kirgiz (UKK jezika), u kojima se reči proizvode putem stazama povezanih sa nekoliko sufiksa i stazama koriste kao predstavljanje sadržaja teksta, ta karakteristika omogućava beskonačnu derivativsku rečenicu koja dovodi do visoke nesigurnosti oblika pisanja i ogromnih redundantnih karakteristika. Postoje veliki izazovi takve klasifikacije teksta sa niskim resursima, nedostatak označenih podataka u ciljnom domenu i morfološkom raznolikosti derivacija u jezičkim strukturama. To je efikasno rješenje koje ispravlja predobučeni jezički model kako bi pružilo smislene i favorilne ekstraktore funkcija za klasifikaciju teksta. Za taj cilj predlažemo model AgglutiFiT-a sa niskim resursima aglutinativnim jezikom, posebno, izgradimo nizak zvuk fino-tuniranje podataka morfološkom analizu i ekstrakcijom matičnih izvora, zatim sredimo preko jezika pre-obuku modela na ovom setu podataka. Osim toga, predlažemo strategiju za finaliziranje pažnje koja bolje odabere relevantne semantičke i sintaktičke informacije iz predobučenog jezičkog modela i koristi te karakteristike na zadacima klasifikacije teksta. Procjenjujemo naše metode na devet Uyghura, Kazakha i Kirgijskih klasifikacijskih podataka, gde imaju značajno bolje izvedbe u usporedbi sa nekoliko jakih
osnovne linije.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Textklassificering tenderar att vara svår när data är otillräckliga med tanke på mängden manuellt märkta textkorpor. För lågresursagglutinerande språk inklusive uiguriska, kazakiska och kirgiziska (UKK-språk), där ord tillverkas via stammar sammankopplade med flera suffix och stammar används som representation av textinnehåll, möjliggör denna funktion oändliga derivat ordförråd som leder till hög osäkerhet i skrivande former och enorma redundanta funktioner. Det finns stora utmaningar med agglutinerande textklassificering med låg resurs, bristen på märkta data inom en måldomän och morfologisk mångfald av härledningar i språkstrukturer. Det är en effektiv lösning som finjusterar en färdigutbildad språkmodell för att ge meningsfulla och gynnsamma funktionsextraktorer för efterföljande textklassificeringsuppgifter. För detta ändamål föreslår vi en lågresurs agglutinative språkmodell som finjusterar AgglutiFiT, specifikt bygger vi ett lågbrusigt finjusterande dataset genom morfologisk analys och stamextraktion, och finjusterar sedan den tvärspråkliga pre-training modellen på denna datauppsättning. Dessutom föreslår vi en uppmärksamhetsbaserad finjusteringsstrategi som bättre väljer relevant semantisk och syntaktisk information från den förberedda språkmodellen och använder dessa funktioner i efterföljande textklassificeringsuppgifter. Vi utvärderar våra metoder på nio uiguriska, kazakiska och kirgiziska klassificeringsdataset, där de har betydligt bättre prestanda jämfört med flera starka
baslinjer.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Utawala wa maandishi unakuwa vigumu pale taarifa hazitafaa kuzingatia kiasi cha makampuni ya maandishi yaliyoandikwa kwa mikononi. Kwa lugha za asili za chini za rasilimali ikiwa ni pamoja na Uyghur, Kazakh, na Kyrgyz (lugha za Uingereza), ambazo maneno yanatengenezwa kupitia vituo vinavyohusiana na viungo kadhaa vinatumika kama uwakilishi wa maudhui ya maandishi, kipengele hiki kinaruhusu lugha isiyo na ujuzi ambao hupelekea usio na uhakika mkubwa wa aina za kuandika na vipengele vikubwa vinavyopungua. Kuna changamoto kubwa za usambazaji wa maandishi yenye rasilimali ya chini kwa kutangaza ukosefu wa taarifa zilizowekwa katika maeneo yanayolenga na utofauti wa viwanda vya kimaadili katika miundombinu ya lugha. Ni suluhisho yenye ufanisi ambalo hutumia mwelekeo wa lugha iliyoelekezwa kabla wa kutoa wataalamu wenye maana na wanaopendwa kwa ajili ya kazi za usambazaji wa maandishi ya chini. Kwa mwisho huu, tunapendekeza modeli ya lugha yenye asili ya kibaguzi mzuri AgglutiFiT, hususani, tunajenga taarifa zilizotengenezwa vizuri vya sauti kwa uchambuzi wa kimaadilojia na utekelezaji wa vigogo, kisha tunatumia modeli ya mafunzo ya kabla ya lugha katika seti hii ya data. Zaidi ya hayo, tunapendekeza mkakati wa mafunzo mzuri ambao unachagua taarifa zinazohusiana na kimapenzi kutoka katika mtindo wa lugha iliyoendelea na kutumia hizo vipengele kwenye kazi za usambazaji wa maandishi. Tutathmini mbinu zetu kuhusu takwimu za usambazaji tisa za Uyghur, Kazakh na Kyrgyz, ambazo zina ufanisi bora zaidi ukilinganishwa na baadhi ya nguvu
mistari ya msingi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Text classification tends to be difficult when data are inadequate considering the amount of manually labeled text corpora. @ info: whatsthis குறைந்த வளங்கள் agglutinative text வகைப்பாட்டின் மிகப்பெரிய சவால்கள் இருக்கிறது மொழி அமைப்புகளில் உள்ள குறிப்பிடப்பட்ட தரவுகளின் குறைவான தகவல் கு இது ஒரு விருப்பமான தீர்வு தான் முன் பயிற்சி மொழி மாதிரியை சரியாக முடிக்கும் முன்பு பயிற்சி மாதிரியை பயன்படுத்துவதற்காக சரியான இந்த முடிவிற்கு, நாம் ஒரு குறைந்த மூலத்தின் agglutinative மொழி மாதிரி பரிந்துரைக்கிறோம் குறிப்பிட்டு, குறிப்பிட்டு, நாம் ஒரு குறைந்த ஒலி நன்று தூண்டும் தகவல் அமைப்பை மார்போலியல மேலும், நாம் முன்பயிற்சி மொழி மாதிரியிலிருந்து தேர்ந்தெடுக்கும் பொருத்தமான பாமான்டிக் மற்றும் ஒத்திசைக்கும் தகவலை தேர்ந்தெடுக் நாங்கள் எங்கள் முறைகளை எடுத்துக் கொள்கிறோம் ஒன்பது உக்குர், கஜக், மற்றும் கிரிக்கிஸ் வகுப்பு தரவுத்தளங்களில், அதிகமாக சிறந்த செ
அடிப்படை கோடுகள்.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Metin sahypalary elimden etilen metin corpora hasaplamak üçin kynçylyk däldir. Uygur, Kazakh we Kirghiz dilleri (UKK dilleri) içinde kelimeler birnäçe sufiks we süzmeler bilen meňzeş sözler bilen üretilýär. Açyk ressurs aglumatçy metin klasifikasynda hasaplanýan maglumat domaýynda we dil strukturlarynda näçe görnüşler bar. Bu ýer täsirli çözüm. Bir öňünden eğlenen dil nusgasyny a şmaly metin klasifikasyonaty üçin möhüm we gowymy üçin ullanjak täsirlere süýtgetmek üçin janlaşdyrylýan çözüm. Bu üçin biz esasy üçin iň derejede agglutinatçy dil nusgasyny AgglutiFiT'i suýlaýarys. Şüphesiz morfolojik analiziýa we استm ekstrasyona görä düşük sesli hasaplanjak düzgün hasaplanjak düzgün hasaplanjak üçin gurnuyoruz we soňra bu data düzgünde cross-dil öň-okuwçy nusgasyny çykarýarys. Mundan hem, biz üns altynda nämli taýýarlama stratejiýasyny gowy görkezýäris we ol semantik we sintaktik maglumatlary öňünden eğlenen dil nusgadan saýlaýar we ol özellikleri aşaky tekst klasifikasynda ullanýar. Biz öz ýürlerimizi 9 Uýgur, Kazakh we Kirgiz klasifikasyýasynda deňleýäris. Bu ýerde birnäçe güýçli ukyplaryň bilen görä has gowy hereketlerimiz bar.
üýtgeşik</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>جب ڈیٹا ناپاکیزہ ہوتے ہیں تو متن کلاسپیٹ کا ذریعہ اپنے ہاتھ سے لکھا ہوا متن کرپورا کا ذریعہ سمجھتے ہیں۔ یوگھر، کازاق اور کرجیز (UKK زبان) کے شامل کم منبع کے گلوٹینٹیوں زبانوں کے لئے، جہاں کلمات لکھنے کے مطابق بہت سی سوفکس اور استمز کے مطابق استعمال کئے جاتے ہیں، یہ فائدہ ایک حد سے زیادہ غیر منبع لکھنے کی اجازت دیتا ہے کہ لکھنے کے فرموں اور بہت زیادہ غیر معلومات کی وجہ سے بہت بڑی غیر معلوما نیچے رسورسوس اگلوٹینٹیو ٹیکسٹ کلاسپیٹ کی بہترین چالوں ہیں کہ ایک ٹیکسٹ ڈومین میں لابلیٹ ڈیٹ کی ناکامی اور زبان ساختاروں میں دریافت کی مورپولوژیکی مختلفیت ہے. یہ ایک اثرات حل ہے جو ایک پیش آموزش کی زبان موڈل کو مطلوب اور پسندیدہ استعمال کرنے کے لئے ڈونسٹریم ٹیکسٹ کلاسیفون کے کاموں کے لئے استعمال کرنے کے لئے اضافہ کرتا ہے. اس کے لئے ہم ایک کم منبع آگلوٹینٹی زبان کی موڈل اگلوٹی فیٹ کو پیشنهاد کرتے ہیں، مخصوصاً ہم ایک کم آواز پاکیزگی ٹونڈ ڈیٹ سٹ بناتے ہیں morfological analysis اور stem extraction کے ذریعہ، پھر اس ڈیٹ سٹ پر cross-lingual pre-training موڈل کو ٹھیک ٹھیک کر دیتے ہیں۔ اور اس کے علاوہ، ہم ایک توجه کی بنیاد پاکیزہ تنظیم استراتژی پیشنهاد کرتے ہیں جو اچھا معاملہ سیمانٹیکی اور سینٹکتیک معلومات کو پیش آموزش کی زبان موڈل سے انتخاب کرتا ہے اور ان فرصت کو نیچے سینٹریم ٹکس کلاسیفون کے کاموں پر است ہم نے نو Uyghur, Kazakh اور Kyrgyz classification datasets کے ذریعے اپنے طریقے کا ارزش کیا ہے جہاں وہ بہت زیادہ اچھی عملکرد رکھتے ہیں
بنیاس لین.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ma ľlumotlar qo Ľlbola qo Ľlbola yordamida qo Ľlbola belgilangan matn kompaniyasini aniqlashda matn darajalashtirish juda qiyin edi. Name There are major challenges of low-resource agglutinative text classification the lack of labeled data in a target domain and morphologic diversity of derivations in language structures. Name Mana shu paytda, biz yaxshi manba agglutinativ tilning modeli AgglutiFiT'ni yaxshi ko'rinishimizni talab qilamiz. Hullas, biz morfological analyysi va tizimni chiqarish orqali yaratib, keyin bu ma ľlumotlarning bir necha tillar oldini ta ľminlovchi modelini yaramiz. Ko'rib, biz birinchi ta ľminlovchi tillar modelidan muhim semantik va syntactic ma ľlumotini tanlashni istaysizmi, va bu tashkilotlarni quyidagi matn classification vazifalardan foydalanamiz. Biz 9 Uyghur, Kazakh va Kyrgyz klassifik ma ľlumotlar tarkibini qiymatmiz. Bu yerda ularning ko'plab ko'pchilik bir qanchalik ko'plab bajarishiga juda yaxshi
asboblar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Đoạn phân loại có xu hướng khó khăn khi dữ liệu chưa đủ so với lượng chữ viết bằng nhãn bằng tay Hạ sĩ. Đối với các ngôn ngữ đậm chất thấp, bao gồm Uyghur, Kazak, và Kyrgyz (ngôn ngữ UKK), nơi mà từ được tạo ra qua các gốc được kết hợp bằng các dòng đủ và các dòng chảy được dùng làm đại diện cho nội dung văn bản, tính năng này cho phép nhiều từ dẫn tới sự mơ hồ của chữ viết và những tính năng thừa thải khổng lồ. Có những thách thức lớn của việc phân loại văn bản đậm đặc với nguồn thấp kém. sự thiếu dữ liệu được đánh dấu trong một miền đích và sự đa dạng hoá học của chế độ phân phát trong cấu trúc ngôn ngữ. Đây là một giải pháp hiệu quả tinh chỉnh mô hình ngôn ngữ được đào tạo sẵn để cung cấp các còn lại đặc trưng có ý nghĩa và thuận lợi cho các nhiệm vụ phân loại văn bản xuôi dòng. Chúng tôi đề xuất một kiểu ngôn ngữ đậm đặc biệt phức tạp, tinh chỉnh thấp độ của AglutiFiT, cụ thể, chúng tôi xây dựng dữ liệu độ âm thanh âm thanh thấp bằng cách phân tích lịch học và chiết xuất gốc, rồi chỉnh lại mô hình tiền đào tạo xuyên ngôn ngữ trên bộ dữ liệu này. Hơn nữa, chúng tôi đề nghị một chiến lược thúc-tinh chỉnh tập trung để chọn thông tin ngữ pháp và pháp liên quan tốt hơn từ mô hình ngôn ngữ được đào tạo trước và sử dụng những tính năng đó trong các nhiệm vụ phân loại văn bản xuôi dòng. Chúng tôi đánh giá các phương pháp của chúng tôi về chín trường dữ liệu phân loại Uthế, Kazak, và Kyrgyz, nơi chúng có hiệu quả tốt hơn nhiều so với một số lượng lớn.
tầng hầm.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>及念手动所记文本语料库数不足,文本分类往往为难。 维吾尔语、哈萨克语、吉尔吉斯语(UKK语)内低资源凝集性语,其中单词因数后缀之词干为之,而词干以文本者,许衍生词汇,而书高不确定性巨冗余也。 低资源凝性文本分类存大挑战,即域中乏标记数据及语言结构中派生之形多样性。 有效之解决方案,微调预训之语,为下流文本分类之义而易用之提取器。 为发微调AgglutiFiT低资源凝言具体来说,以形析词干,取一低噪声微调数集,然后集上于跨语预练之。 此外立微策,择语义、句法信息,施于下流文本之类。 吾于九维吾尔族、哈萨克族、吉尔吉斯族之类集上论吾法,比于强数,其有明善矣。
基线。</span></div></div><dl><dt>Anthology ID:</dt><dd>2020.ccl-1.92</dd><dt>Volume:</dt><dd><a href=/volumes/2020.ccl-1/>Proceedings of the 19th Chinese National Conference on Computational Linguistics</a></dd><dt>Month:</dt><dd>October</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Haikou, China</dd><dt>Venue:</dt><dd><a href=/venues/ccl/>CCL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Chinese Information Processing Society of China</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>994–1005</dd><dt>Language:</dt><dd>English</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.ccl-1.92>https://aclanthology.org/2020.ccl-1.92</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">li-etal-2020-low</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Xiuhong Li, Zhe Li, Jiabao Sheng, and Wushour Slamu. 2020. <a href=https://aclanthology.org/2020.ccl-1.92>Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning</a>. In <i>Proceedings of the 19th Chinese National Conference on Computational Linguistics</i>, pages 994–1005, Haikou, China. Chinese Information Processing Society of China.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2020.ccl-1.92>Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning</a> (Li et al., CCL 2020)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.ccl-1.92.pdf>https://aclanthology.org/2020.ccl-1.92.pdf</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.ccl-1.92.pdf title="Open PDF of 'Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Low-Resource+Text+Classification+via+Cross-lingual+Language+Model+Fine-tuning" title="Search for 'Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning](https://aclanthology.org/2020.ccl-1.92) (Li et al., CCL 2020)</p><ul class=mt-2><li><a href=https://aclanthology.org/2020.ccl-1.92>Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning</a> (Li et al., CCL 2020)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Xiuhong Li, Zhe Li, Jiabao Sheng, and Wushour Slamu. 2020. <a href=https://aclanthology.org/2020.ccl-1.92>Low-Resource Text Classification via Cross-lingual Language Model Fine-tuning</a>. In <i>Proceedings of the 19th Chinese National Conference on Computational Linguistics</i>, pages 994–1005, Haikou, China. Chinese Information Processing Society of China.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>