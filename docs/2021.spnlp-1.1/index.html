<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>RewardsOfSum : Exploring Reinforcement Learning Rewards for SummarisationRewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="RewardsOfSum : Exploring Reinforcement Learning Rewards for SummarisationRewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation" name=citation_title><meta content="Jacob Parnell" name=citation_author><meta content="Inigo Jauregi Unanue" name=citation_author><meta content="Massimo Piccardi" name=citation_author><meta content="Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)" name=citation_conference_title><meta content="2021/8" name=citation_publication_date><meta content="https://aclanthology.org/2021.spnlp-1.1.pdf" name=citation_pdf_url><meta content="1" name=citation_firstpage><meta content="11" name=citation_lastpage><meta content="10.18653/v1/2021.spnlp-1.1" name=citation_doi><meta property="og:title" content="RewardsOfSum : Exploring Reinforcement Learning Rewards for SummarisationRewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation"><meta property="og:image" content="https://aclanthology.org/thumb/2021.spnlp-1.1.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2021.spnlp-1.1"><meta property="og:description" content="Jacob Parnell, Inigo Jauregi Unanue, Massimo Piccardi. Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021). 2021."><link rel=canonical href=https://aclanthology.org/2021.spnlp-1.1></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum : Exploring Reinforcement Learning Rewards for Summarisation<span class=acl-fixed-case>R</span>ewards<span class=acl-fixed-case>O</span>f<span class=acl-fixed-case>S</span>um: Exploring Reinforcement Learning Rewards for Summarisation</a>
<a id=af_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>Herstel OfSum: Ondersoek Verbevesting Leer Herstelling vir Opsomming</a>
<a id=am_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>Sum: Exploring Reinforcement Learning Rewards for Summary</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: استكشاف مكافآت التعلم المعززة للتلخيص</a>
<a id=az_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>Награди за обобщаване на знанията за подсилване</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>পুনরায় অফ সাম: সামারসংক্রান্ত শিক্ষা শিক্ষা প্রত্যাবর্তন করা হচ্ছে</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: བསྐྱར་གསོ་སྡུད་ཀློག་སྟངས་ལ་བསྡུས་དང་།</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: istraživanje pojačanja učenja nagrade za sažetak</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Explorar Rewards d'Aprendiment de reforç per a resumir</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Prozkoumání odměn za posílení učení pro shrnutí</a>
<a id=da_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Udforsk forstærket læringsbelønninger til opsummering</a>
<a id=de_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Exploring Reinforcement Learning Belohnungen zur Zusammenfassung</a>
<a id=el_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>ΑνταμοιβέςOfSum: Εξερευνώντας Ανταμοιβές Μάθησης Ενίσχυσης για Σύνοψη</a>
<a id=es_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Explorando las recompensas de aprendizaje por refuerzo para resumir</a>
<a id=et_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Tugevdamise õppe uurimine</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>بازگشت‌سازیOfSum: تحقیق توسعه یادگیری پاداش یادگیری برای جمع‌سازی</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Vahvistusoppimisen tutkiminen</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum : Explorer les récompenses d'apprentissage par renforcement pour la synthèse</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Ag Iniúchadh ar Fhoghlaim Treisithe Luaíochtaí le haghaidh Achoimre</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>Sum: Exploring reInheritor Learn Learn for Summary</a>
<a id=he_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>פרסים של סכום: לחקור פרסים ללימודים מחדשים לסיום</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: सारांशीकरण के लिए सुदृढीकरण सीखने के पुरस्कारों की खोज</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: istraživanje pojačanja učenja nagrade za sažetak</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: A megerősítési tanulási jutalmak feltárása az összefoglaláshoz</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RerewsOFSum: Exploring BookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBookBook</a>
<a id=id_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation</a>
<a id=is_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Esplorare le ricompense per l'apprendimento di rinforzo per la sintesi</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum ：要約のための強化学習報酬の探求</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>undo-type</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>გადავიწყება</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>Қайталау</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>보상 총결산: 탐색 강화 학습 보상 총결산</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Reinforcement Learning reward for Summary</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>Одплатите од сумата: Истражување на наградите за зајакнување на учењето за резултат</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>തിരിച്ചുവരുന്ന OfSum: Exploring Reinforcement Learning Rewards for Summary</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Баталгаа суралцах үйл ажиллагааг судлах</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RigwardsOfSum: L-esplorazzjoni tar-Rigwardji tat-Tagħlim għat-Tisħiħ għas-Sommarju</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Het verkennen van Reinforcement Learning Rewards voor samenvatting</a>
<a id=no_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>GjenopprettaOfSum: Utforskar forstørring av forstørringar tilbake for sammendrag</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Badanie nagród uczenia się wzmocnienia w celu podsumowania</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Explorando Recompensas de Aprendizado por Reforço para Resumir</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RecompensOfSum: Explorarea recompenselor de învățare pentru rezumare</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Изучение наград за обучение подкреплению для подведения итогов</a>
<a id=si_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsofSum: ප්‍රශ්නයක් විශ්වාස කරනවා ආපහු ප්‍රශ්නයක් ඉගෙනගන්න ආපහු ප්‍රශ්නයක්</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Raziskovanje okrepitve učenja Nagrade za povzetek</a>
<a id=so_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Exploring Reinforcement Learning Rewards for Summary</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Shqyrtimi i shpërblimeve të mësimit të forcuar për përmbledhjen</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>Vratite se OfSum: istraživanje pojačanja učenja nagrade za sažetak</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: Exploring Förstärkning Lärande belöningar för Sammanfattning</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>Upande wa Mpya: Kuchunguza Kufundisha Kufundisha Kujifunza</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>மீண்டும் OfSum: Exploring Reinforcement Learning Rewards for Summary</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>Täzeleşme</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>RewardsOfSum: reinforcement Learning Reward for Summarisation</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>OfSum: Exploring Reinforcement Learning Rewards for Summary</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>Phần bổ sung:</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2021.spnlp-1.1.pdf>赏总结:探总结</a></h2><p class=lead><a href=/people/j/jacob-parnell/>Jacob Parnell</a>,
<a href=/people/i/inigo-jauregi-unanue/>Inigo Jauregi Unanue</a>,
<a href=/people/m/massimo-piccardi/>Massimo Piccardi</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>To date, most abstractive summarisation models have relied on variants of the negative log-likelihood (NLL) as their training objective. In some cases, <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> has been added to train the <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> with an objective that is closer to their evaluation measures (e.g. ROUGE). However, the <a href=https://en.wikipedia.org/wiki/Reward_system>reward function</a> to be used within the reinforcement learning approach can play a key role for performance and is still partially unexplored. For this reason, in this paper, we propose two reward functions for the task of abstractive summarisation : the first function, referred to as RwB-Hinge, dynamically selects the samples for the gradient update. The second <a href=https://en.wikipedia.org/wiki/Function_(mathematics)>function</a>, nicknamed RISK, leverages a small pool of strong candidates to inform the reward. In the experiments, we probe the proposed approach by fine-tuning an NLL pre-trained model over nine summarisation datasets of diverse size and nature. The experimental results show a consistent improvement over the negative log-likelihood baselines.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Op dag het die meeste abstraktiewe opsomming modele op veranderinge van die negatiewe log-waarskynlik (NLL) as hul opsomming-objek opgelê. In sommige gevalle is die versterking leer bygevoeg om die modele te oefen met 'n doel wat naby is aan hulle evalueringsmaat (bv. ROUGE). Maar, die loon funksie wat binne die versterking leer toegang gebruik moet word, kan speel 'n sleutel rol vir prestasie en is nog gedeeltelik onverkondig. Vir hierdie rede, in hierdie papier, voorstel ons twee vergelde funksies vir die taak van abstraktiewe opsomming: die eerste funksie, verwys na as RwB- Hinge, dinamies kies die voorbeelde vir die gradient opdateer. Die tweede funksie, bynaam RISK, verwyder 'n klein pool van sterke kandidate na informasie die loon. In die eksperimente probeer ons die voorgestelde toegang deur 'n NLL voor-opgelei model te fin-tuning oor nege opsomming datastelle van verskeie grootte en natuur. Die eksperimentale resultate vertoon 'n konsistente verbetering oor die negatiewe log-waarskynlike basisline.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>To date, most abstractive summarisation models have relied on variants of the negative log-likelihood (NLL) as their training objective. አንዳንድ ጉዳይ፣ ማስተማር ተጨማሪው ተጨማሪው በሞዴላዎችን ማስተምር የተጨመረ ነው፡፡ ነገር ግን የደመወዝ ሥራ በማድረግ ውስጥ የሚጠቀሙት የድጋፍ ማድረግ ማድረግ ማድረግ የሚችል የቁልፍ ሚዛን ይጫወታል፡፡ ስለዚህ ምክንያት፣ በዚህ ገጽ፣ ለጥያቄ አካባቢ ስርዓት ሁለት የዋጋ ፍትወቶችን እናሳውቃለን፤ የመጀመሪያው ክፍል RwB-Hinge የተባለውን ምሳሌዎችን ለቀዳሚ አዲስ ማሻሻሻል ይምረጣል፡፡ ሁለተኛይቱ ስርዓት RISK የተባለው የደመወዙን ለማሳወቅ የብርቱ አማካሪዎችን ታናሽ የውኃ ሙቀት ሰጥቷል፡፡ በተፈተናው ውስጥ የዘጠኝ አነስተኛነት ዳታዎችን በተለየ መጠን እና በሥርዓት ላይ የተለየውን የNLL ሞዴል በመጠቀም በተፈተናው ልማት እና በሥርዓት የተለየ ጥያቄዎችን እናሳውቃለን፡፡ የሞከሩ ውጤቶች የnegative የሎግ-ምናልባት መቀመጫዎች ላይ የሚተካክለውን ትክክል ያሳያል፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>حتى الآن ، اعتمدت معظم نماذج التلخيص التجريدية على متغيرات احتمالية السجل السلبي (NLL) كهدف تدريبي لها. في بعض الحالات ، تمت إضافة التعلم المعزز لتدريب النماذج بهدف أقرب إلى تدابير التقييم الخاصة بهم (مثل ROUGE). ومع ذلك ، فإن وظيفة المكافأة التي سيتم استخدامها في نهج التعلم المعزز يمكن أن تلعب دورًا رئيسيًا في الأداء ولا تزال غير مستكشفة جزئيًا. لهذا السبب ، في هذه الورقة ، نقترح وظيفتين للمكافأة لمهمة التلخيص التجريدي: الوظيفة الأولى ، المشار إليها باسم RwB-Hinge ، تختار ديناميكيًا العينات لتحديث التدرج. الوظيفة الثانية ، الملقبة بـ RISK ، تستفيد من مجموعة صغيرة من المرشحين الأقوياء لإبلاغ المكافأة. في التجارب ، قمنا بفحص النهج المقترح من خلال ضبط نموذج NLL مدرب مسبقًا على تسع مجموعات بيانات تلخيص ذات أحجام وطبيعة متنوعة. تظهر النتائج التجريبية تحسنًا ثابتًا على خطوط الأساس السلبية للسجل المحتمل.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Şimdiye qədər, çox abstraktiv qurğulama modelleri onların təhsil məqsədili olaraq negatif log-likeliğinin variablarına təvəkkül edir. Bazı vaxtlarda, modelləri təhsil etmək üçün daha yaxın bir məqsəd ilə öyrənmək öyrənmək üçün artırmaq öyrənməsi artırıldı (məsələn, ROUGE). Ancaq mükafat funksiyası artırmaq öyrənmə tərzində istifadə ediləcək tərzdə performans üçün anahtar rolü oynaya bilər və hələ də bir qismi a çıq-aydın deyildir. Bu səbəbdən, bu kağızda abstraktiv təmizləmə görevi üçün iki mükafat funksiyasını təbliğ edirik: ilk funksiyası, RwB-Hinge adlandırılmış, dinamik olaraq gradient güncelləməsi üçün nümunələri seçir. İkinci funksiyası, RISK adlı nickname, mükafatı bildirmək üçün güclü kandidátların küçük bir havuna istifadə edir. İşlemlərdə, NLL əvvəlcə təhsil edilmiş modeli doqquz müxtəlif ölçü və təbiəti ilə təhsil edilmiş verilən verilən qurbanların təhsil edilməsini təsdiq edirik. Müxtəlif sonuçlar negatif log-likeliğinin səhifələrində müəyyən bir improvement göstərir.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Към днешна дата повечето абстрактни модели за обобщаване разчитат на варианти на отрицателната логаритметична вероятност (НЛЛ) като цел на обучение. В някои случаи е добавено засилено обучение за обучение на моделите с цел, която е по-близо до мерките за оценка (напр. Въпреки това, функцията за възнаграждение, която трябва да се използва в рамките на подхода за укрепване на обучението, може да играе ключова роля за изпълнението и все още е частично неизследвана. Поради тази причина в настоящата статия предлагаме две възнаграждаващи функции за задачата на абстрактно обобщаване: първата функция, наречена Панта, динамично избира образците за обновяване на градиента. Втората функция, наречена РИСК, използва малък набор от силни кандидати, за да информира наградата. В експериментите изследваме предложения подход чрез фина настройка на предварително обучен модел на НЛЛ върху девет обобщени набора от данни с различен размер и природа. Експерименталните резултати показват последователно подобрение спрямо базовите линии на отрицателната log-вероятност.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>এখন পর্যন্ত নেতিবাচক লগ-সম্ভাবনা নির্ভর করে বেশীরভাগ সংক্রান্ত মডেল তাদের প্রশিক্ষণের লক্ষ্য হিসেবে নির্ভর করেছে। কিছু ক্ষেত্রে শিক্ষা বাড়িয়ে দিয়েছে মডেলের শিক্ষা প্রশিক্ষণের জন্য যোগ করা হয়েছে যা তাদের মূল্যায়নের কাছাকাছি (যেমন রোজের ক কিন্তু ক্ষেত্রে ব্যবহার করা পুরস্কারের কাজের ক্ষেত্রে ব্যবহার করা যায় তা প্রদর্শনের জন্য একটি গুরুত্বপূর্ণ ভূমিকা খেলতে প এই কারণের জন্য, এই কাগজটিতে আমরা অস্বীকৃতিক সংক্রান্ত কাজের জন্য দুটি পুরস্কার প্রস্তাব করছি: প্রথম ফাংশন, যার নাম রিউবি-হিঙ্গ বলা হচ্ছে, গ্রেডিয়েন্ড আপড দ্বিতীয় ফাংশন, রিএসকে নাম দেয়া হয়েছে, পুরস্কার জানানোর জন্য একটি ছোট পুলের প্রার্থীদের একটি পুল প্রদান করেছে। পরীক্ষার মধ্যে আমরা প্রস্তাবিত পদ্ধতি প্রমাণ করি একটি এনএল পূর্ব প্রশিক্ষিত মডেলের মাধ্যমে নয়টি সংক্ষিপ্ত তথ্যের বিভিন্ন আকার ও প পরীক্ষার ফলাফল নেতিবাচক লগ-সম্ভাবনার বেসাইনের ব্যাপারে একেবারে উন্নতি প্রদর্শন করে।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ད་ལྟ་མི་མང་ཆེ་བ་དག་གི་བསྡུས་བརྗོད་སྟངས་ཀྱི་ནང་དུ་ཉུང་འབྲེལ་བ་དང་མཉམ་དུ་འགྱུར་བ་ཡིན། གལ་སྲིད་ལ་ཤས་ཀྱི་ནང་དུ། མིག་རྩལ་བསྐྱེད་ཚད་སྒྲིག་གི་མཐུན་རྐྱེན་ཐབས་ལམ་ལ་ཁྱད་ནས་མཐུན་རྐྱེན་བཟོ་བྱས་ཡོད། ཡིན་ནའང་། རྒྱབ་སྐྱོར་གྱི་གཟུགས་རིས་སྟོན་པར་ལག་ལེན་འཐབ་དགོས་པ་དེ་ལས་ཀྱང་སྐྱོང་ཆེན་པོ་ཞིག་རྩེ་བ་ལས། འོན་ཀྱང་། རྒྱུ་མཚན་འདིའི་ནང་གི་ཤོག་བུ་འདིའི་ནང་དུ་ང་ཚོའི་རྗེས་སུ་འབྲངས་ཀ་གཉིས་འཆར་འདོད་པ་ཡིན། the first function, referred to as RwB-Hinge, dynamically selects the samples for the gradient update. དངོས་པོ་གཉིས་པ་་ཡིན་པའི་མིང་མིང RISK་་་་ཆ་རྐྱེན་སྐོར་ཆེན་པོ་ཆུང་ཀུ་ཉེ་ཆར་སྤྲོད་ཡོད། འུ་ཅག་གིས་བརྟག་ཞིབ་བྱས་པའི་སྔོན་གྲངས་སྒྲིག་འགོད་བྱས་པའི་གཟུགས་རིས་ལྟར་ཞིབ་བཤེར་བྱེད་ཀྱི་ཡོད། གྲུབ་ཕྱོགས་གྱི་གནད་སྡུད་དུ་ཚད་རྐྱེན་ཐོག་ནས་གནས་སྟངས་གང་ཟག་ཞིག་མཐོང་ནུས་</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Do sada, većina apstraktivnih modela za sažetak oslanjala se na variante negativne mogućnosti dnevnika (NLL) kao njihov cilj obuke. U nekim slučajevima, dodana je učenje pojačanja kako bi obučila modele objektivnim ciljem koji je bliži njihovim mjerama procjene (npr. ROUGE). Međutim, funkcija nagrade koja se koristi unutar pristupa pojačanja učenja može imati ključnu ulogu za izvođenje i još je djelomično neočekivana. Zbog ovog razloga, u ovom papiru predlažemo dve nagrade funkcije za zadatak abstraktivne sažetke: prva funkcija, pod nazivom RwB-Hinge, dinamički odabere uzorke za aktualizaciju gradienta. Druga funkcija, nadimak RISK, utiče na mali bazen jakih kandidata da obavijesti nagradu. U eksperimentima, istražujemo predloženi pristup tako što ćemo ispraviti predobučeni model NLL preko devet rezimetriranih podataka različitih veličina i prirode. Eksperimentalni rezultati pokazuju konsekventno poboljšanje na osnovnim linijama negativne mogućnosti dnevnika.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>To date, most abstractive summarisation models have relied on variants of the negative log-likelihood (NLL) as their training objective. En alguns casos, s'ha afegit l'aprenentatge de reforç per formar els models amb un objectiu que és més proper a les seves mesures d'evaluació (per exemple ROUGE). No obstant això, la funció de recompensa que s'ha d'utilitzar en l'enfocament d'aprenentatge de reforç pot jugar un paper clau per al rendiment i encara no ha estat explorada en part. Per això, en aquest paper proposem dues funcions de recompensa per la tasca de resum abstractiu: la primera funció, anomenada RwB-Hinge, selecciona dinàmicament les mostres per a l'actualització del gradient. La segona funció, anomenada RISK, utilitza un petit grup de candidats forts per informar la recompensa. En els experiments, investigam l'enfocament proposat ajustando un model pré-entrenat de NLL sobre nou conjunts de dades de resum de mida i naturalesa diverses. Els resultats experimentals mostren una millora constantsobre les línies de base negatives de probabilitat de registre.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dosud se většina abstraktivních souhrnných modelů spoléhala na varianty negativní log-pravděpodobnosti (NLL) jako cíl výcviku. V některých případech bylo přidáno posílení učení pro trénink modelů s cílem, který je blíže jejich hodnotícím opatřením (např. ROUGE). Funkce odměny, která má být použita v rámci přístupu k posilování učení, však může hrát klíčovou roli pro výkon a je stále částečně neprozkoumaná. Z tohoto důvodu v tomto článku navrhujeme dvě funkce odměny pro úkol abstraktivního shrnutí: první funkce, označovaná jako RwB-Hinge, dynamicky vybírá vzorky pro aktualizaci gradientu. Druhá funkce, přezdívaná RISK, využívá malou skupinu silných kandidátů, aby informovala odměnu. V experimentech zkoumáme navržený přístup jemným laděním NLL předtrénovaného modelu na devíti souhrnných datových sadách různé velikosti a povahy. Experimentální výsledky ukazují konzistentní zlepšení oproti negativním log-pravděpodobnosti základních linií.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Hidtil har de fleste abstrakte opsummeringsmodeller baseret sig på varianter af den negative log-sandsynlighed (NLL) som deres træningsmål. I nogle tilfælde er der tilføjet forstærket læring for at uddanne modellerne med et mål, der ligger tættere på deres evalueringsforanstaltninger (f.eks. ROUGE). Belønningsfunktionen, der skal bruges inden for forstærkningslæringsmetoden, kan dog spille en nøglerolle for ydeevnen og er stadig delvist uudforsket. Derfor foreslår vi i denne artikel to belønningsfunktioner til opgaven med abstraktiv opsummering: Den første funktion, kaldet RwB-Hinge, udvælger dynamisk prøverne til gradient opdateringen. Den anden funktion, kaldet RISK, udnytter en lille pulje af stærke kandidater til at informere belønningen. I eksperimenterne undersøger vi den foreslåede tilgang ved at finjustere en NLL præ-trænet model over ni sammenfattende datasæt af forskellig størrelse og natur. De eksperimentelle resultater viser en konsekvent forbedring i forhold til de negative log-sandsynlighed basislinjer.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bisher haben sich die meisten abstraktiven Zusammenfassungsmodelle auf Varianten der negativen Log-Likelihood (NLL) als Trainingsziel verlassen. In einigen Fällen wurde Verstärkungslernen hinzugefügt, um die Modelle mit einem Ziel zu trainieren, das näher an ihren Evaluationsmaßnahmen liegt (z.B. ROUGE). Die Belohnungsfunktion, die im Rahmen des Verstärkungslernens eingesetzt werden soll, kann jedoch eine Schlüsselrolle für die Leistung spielen und ist teilweise noch unerforscht. Aus diesem Grund schlagen wir in diesem Beitrag zwei Belohnungsfunktionen für die Aufgabe der abstraktiven Zusammenfassung vor: Die erste Funktion, genannt RwB-Scharnier, wählt dynamisch die Samples für die Verlaufsaktualisierung aus. Die zweite Funktion, genannt RISK, nutzt einen kleinen Pool starker Kandidaten, um die Belohnung zu informieren. In den Experimenten untersuchen wir den vorgeschlagenen Ansatz durch Feinabstimmung eines NLL-vortrainierten Modells über neun Zusammenfassungsdatensätze unterschiedlicher Größe und Natur. Die experimentellen Ergebnisse zeigen eine konsistente Verbesserung gegenüber den negativen Log-Likelihood Baselines.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Μέχρι σήμερα, τα περισσότερα μοντέλα αφηρημένης σύνοψης βασίστηκαν σε παραλλαγές της αρνητικής πιθανότητας καταγραφής (NLL) ως εκπαιδευτικό στόχο τους. Σε ορισμένες περιπτώσεις, προστέθηκε η ενισχυτική μάθηση για την κατάρτιση των μοντέλων με στόχο πιο κοντά στα μέτρα αξιολόγησης τους (π.χ. ROUGE). Ωστόσο, η λειτουργία ανταμοιβής που θα χρησιμοποιηθεί στο πλαίσιο της προσέγγισης ενίσχυσης μάθησης μπορεί να διαδραματίσει βασικό ρόλο για την απόδοση και εξακολουθεί εν μέρει να είναι ανεξερεύνητη. Για το λόγο αυτό, στην παρούσα εργασία, προτείνουμε δύο λειτουργίες ανταμοιβής για το έργο της αφηρημένης σύνοψης: η πρώτη συνάρτηση, που αναφέρεται ως αρθρωτή, επιλέγει δυναμικά τα δείγματα για την ενημέρωση της διαβάθμισης. Η δεύτερη λειτουργία, με το ψευδώνυμο ΚΙΝΔΥΝΟΣ, αξιοποιεί μια μικρή ομάδα ισχυρών υποψηφίων για να ενημερώσει την ανταμοιβή. Στα πειράματα, εξετάζουμε την προτεινόμενη προσέγγιση με την τελειοποίηση ενός προ-εκπαιδευμένου μοντέλου σε εννέα σύνολα δεδομένων σύνοψης διαφορετικού μεγέθους και φύσης. Τα πειραματικά αποτελέσματα δείχνουν συνεπή βελτίωση σε σχέση με τις αρνητικές γραμμές καταγραφής πιθανοτήτων βάσης.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Hasta la fecha, la mayoría de los modelos de resumen abstractivo se han basado en variantes de la probabilidad logarítmica negativa (NLL) como objetivo de entrenamiento. En algunos casos, se ha añadido el aprendizaje por refuerzo para entrenar a los modelos con un objetivo más cercano a sus medidas de evaluación (por ejemplo, ROUGE). Sin embargo, la función de recompensa que se utilizará dentro del enfoque de aprendizaje por refuerzo puede desempeñar un papel clave para el rendimiento y aún no se ha explorado parcialmente. Por esta razón, en este artículo proponemos dos funciones de recompensa para la tarea de resumen abstractivo: la primera función, denominada RWB-Hinge, selecciona dinámicamente las muestras para la actualización del gradiente. La segunda función, apodada RISK, aprovecha un pequeño grupo de candidatos fuertes para informar la recompensa. En los experimentos, probamos el enfoque propuesto ajustando un modelo previamente entrenado de NLL en nueve conjuntos de datos de resumen de diversos tamaños y naturaleza. Los resultados experimentales muestran una mejora constante con respecto a las líneas de base de probabilidad logarítmica negativa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Praeguseks on enamik abstraktseid kokkuvõtlusmudeleid oma koolituse eesmärgina tuginenud negatiivse logitõenäosuse variantidele. Mõnel juhul on lisatud tugevdatud õppimine, et koolitada mudeleid eesmärgiga, mis on lähemal nende hindamismeetmetele (nt ROUGE). Siiski võib tugevdamisõppe meetodi raames kasutatav tasuvusfunktsioon mängida tulemuslikkuse seisukohalt võtmerolli ja seda on veel osaliselt uurimata. Sel põhjusel pakume selles töös välja kaks preemiafunktsiooni ülesandeks abstraktne kokkuvõte: esimene funktsioon, mida nimetatakse RwB-Hinge, valib dünaamiliselt näidised gradienti uuendamiseks. Teine funktsioon, hüüdnimega RISK, võimendab väikest tugevate kandidaatide kogumit, et teavitada tasu. Katsetes analüüsime kavandatud lähenemisviisi, täpsustades NLL eelõpetatud mudelit üheksa erineva suuruse ja iseloomuga kokkuvõtliku andmekogumi kaudu. Katsetulemused näitavad pidevat paranemist negatiivse logaritmilise tõenäosuse baasjoonega võrreldes.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>تا حال، بیشترین مدل‌های جمع‌آوری مطلق به عنوان هدف آموزش آنها بر تغییرات احتمال منفی (NLL) اعتماد دارند. در بعضی موارد، یادگیری پشتیبانی برای آموزش مدلها با هدف نزدیکتر به اندازه‌های ارزیابی آنها (مثال ROUGE) اضافه شده است. ولی، عملکرد پاداش که در دستور یادگیری افزایش استفاده می‌شود، می‌تواند نقش کلید برای اجرا را را بازی کند و هنوز قسمتی غیر توضیح داده می‌شود. به خاطر این دلیل، در این کاغذ، دو تابع پاداش برای کار جمع‌آوری abstractive پیشنهاد می‌کنیم: اولین تابع، به عنوان RwB-Hinge، دینامیک نمونه‌ها را برای جدید‌سازی گراده انتخاب می‌کند. عملکرد دوم، اسم RISK، یک استخره کوچک از کاندیدای قوی برای اطلاعات پاداش را ارائه می دهد. در این آزمایشات، ما روش پیشنهاد را با تنظیم یک مدل پیش آموزش NLL بیش از نو مجموعه داده‌های جمع کردن اندازه و طبیعت مختلف تحقیق می‌کنیم. نتیجه آزمایشی بر اساس پایین‌های احتمال منفی بهتر شدن را نشان می‌دهد.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tähän mennessä useimmat abstraktit yhteenvetomallit ovat käyttäneet koulutustavoitteena negatiivisen log-todennäköisyyden variantteja. Joissakin tapauksissa mallien kouluttamiseen on lisätty tehostettua oppimista tavoitteena, joka on lähempänä niiden arviointitoimenpiteitä (esim. ROUGE). Vahvistusoppimisessa käytettävällä palkitsemistoiminnolla voi kuitenkin olla keskeinen rooli suorituskyvyn kannalta, ja sitä ei ole vielä osittain tutkittu. Tästä syystä tässä työssä ehdotamme kahta palkitsemisfunktiota abstraktiivisen yhteenvedon tehtävään: ensimmäinen funktio, jota kutsutaan nimellä RwB-Hinge, valitsee dynaamisesti näytteet gradientin päivitystä varten. Toinen toiminto, lempinimeltään RISK, hyödyntää pientä vahvojen ehdokkaiden joukkoa ilmoittamaan palkkiosta. Kokeiluissa tutkitaan ehdotettua lähestymistapaa hienosäätämällä esikoulutettua NLL-mallia yhdeksään erikokoiseen ja luonteeltaan erikokoiseen yhteenvetoaineistoon. Kokeelliset tulokset osoittavat jatkuvan parannuksen negatiivisen log-todennäköisyyden perusviivoista.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>À ce jour, la plupart des modèles de synthèse abstraits se sont appuyés sur des variantes du log de vraisemblance négatif (NLL) comme objectif de formation. Dans certains cas, l'apprentissage par renforcement a été ajouté pour former les modèles avec un objectif plus proche de leurs mesures d'évaluation (par exemple ROUGE). Cependant, la fonction de récompense à utiliser dans le cadre de l'approche d'apprentissage par renforcement peut jouer un rôle clé pour la performance et est encore partiellement inexplorée. Pour cette raison, dans cet article, nous proposons deux fonctions de récompense pour la tâche de synthèse abstraite : la première fonction, appelée RWB-hinge, sélectionne dynamiquement les échantillons pour la mise à jour du gradient. La deuxième fonction, surnommée RISK, tire parti d'un petit bassin de candidats solides pour informer la récompense. Dans les expériences, nous étudions l'approche proposée en ajustant un modèle pré-entraîné NLL sur neuf ensembles de données de synthèse de différentes tailles et de différentes natures. Les résultats expérimentaux montrent une amélioration constante par rapport aux valeurs de base du log de vraisemblance négative.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Go dtí seo, bhí an chuid is mó de na samhlacha achoimre teibí ag brath ar leaganacha de na dóchúlachtaí logála diúltacha (NLL) mar chuspóir oiliúna acu. I gcásanna áirithe, cuireadh leis an bhfoghlaim atreisithe chun na samhlacha a oiliúint le cuspóir atá níos gaire dá mbearta meastóireachta (e.g. ROUGE). Mar sin féin, féadfaidh an fheidhm luach saothair atá le húsáid laistigh den chur chuige foghlama treisithe príomhról a imirt maidir le feidhmíocht agus tá sé fós gan iniúchadh i bpáirt. Ar an ábhar sin, sa pháipéar seo, molaimid dhá fheidhm luaíochta don tasc a bhaineann le hachoimriú teibí: roghnaíonn an chéad fheidhm, ar a dtugtar RwB-Hinge, na samplaí don nuashonrú grádáin go dinimiciúil. Déanann an dara feidhm, ar a dtugtar RISK, giaráil ar líon beag iarrthóirí láidre chun an luach saothair a chur ar an eolas. Sna turgnaimh, déanaimid iniúchadh ar an gcur chuige atá beartaithe trí mhionchoigeartú a dhéanamh ar shamhail réamh-oilte NLL thar naoi tacar sonraí achoimriúcháin de mhéid agus nádúr éagsúil. Léiríonn na torthaí turgnamhacha feabhas comhsheasmhach thar na bonnlínte diúltacha dóchúlachta logála.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ga yanzu, mafi yawan motsi na ƙidãya masu kanana sun dõgara a kan variant na masu yiwur-rubutun-rubutun (NLL) kamar abun na tsari. In some cases, reinforcement learning has been added to train the models with an objective that is closer to their evaluation measures (e.g. ROUGE). A lokacin da ake amfani da aikin ijãrar da aka ƙara cikin hanyarwa da za'a yi amfani da shi, ko kuma yana da rabon aiki wanda ba'a buƙata ba. Saboda haka, a cikin wannan takarda, Munã buɗar da misalin biyu na musamman wa aikin da aka tsare masu kanana: aikin na farko, wanda aka kallo na RwB-Hige, yana zãɓi misãlai na don a sake tsari na-sãɓa. Fara na biyu, wanda aka sunan RiSK, yana samar da wani abu mai ƙaranci daga kandida mai ƙarfi dõmin ya sanar da ijãra. Daga jarrabai, za'a jarraba kafin da za'a sami wata misalin na NLL a gaba-wa'anar da taki tara shekarar data masu turu'in girma da halin. Mataimakin jarrabai na nuna wata gyãra mai daidai a kan salummai mai yiwuwa-log-negative.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>עד היום, רוב הדוגמנים המתוקפים המתוקפים הסמכו על שונות של סבירות השלילית של לוג-סבירות (NLL) כמטרה האימונים שלהם. במקרים מסוימים, הוספה למידת גיבוי כדי לאמן את הדוגמנים עם מטרה שהיא קרובה יותר לאמצעי הערכה שלהם (למשל ROUGE). בכל אופן, התפקיד הפרס שימש בתוך גישת הלימוד התגבורה יכול לשחק תפקיד מפתח לביצוע ועדיין לא חוקר חלקית. מסיבה זו, בעיתון הזה, אנו מציעים שתי תפקידים פרס למשימה של סכם אוסטרקטיבי: הפונקציה הראשונה, שנקראת RwB-Hinge, בוחרת דינמית את הדגימות למעדכון המדרגות. הפונקציה השנייה, בשם RISK, משתמשת בבריכה קטנה של מועמדים חזקים כדי להודיע על הפרס. בניסויים, אנו חוקרים את הגישה המוצעת על ידי שיפור מודל מאומן מראש של NLL מעל תשע קבוצות מידע של גודל ומגוון טבע. תוצאות הניסויים מראות שיפור קבוע מעל קווי הבסיס של סבירות לוג שליליים.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>आज तक, अधिकांश अमूर्त सारांश मॉडल ने अपने प्रशिक्षण उद्देश्य के रूप में नकारात्मक लॉग-संभावना (एनएलएल) के रूपों पर भरोसा किया है। कुछ मामलों में, सुदृढीकरण सीखने को एक उद्देश्य के साथ मॉडल को प्रशिक्षित करने के लिए जोड़ा गया है जो उनके मूल्यांकन उपायों (जैसे रूज) के करीब है। हालांकि, सुदृढीकरण सीखने के दृष्टिकोण के भीतर उपयोग किए जाने वाले इनाम फ़ंक्शन प्रदर्शन के लिए एक महत्वपूर्ण भूमिका निभा सकते हैं और अभी भी आंशिक रूप से अज्ञात हैं। इस कारण से, इस पेपर में, हम अमूर्त सारांशीकरण के कार्य के लिए दो इनाम कार्यों का प्रस्ताव करते हैं: पहला फ़ंक्शन, जिसे आरडब्ल्यूबी-हिंज के रूप में जाना जाता है, गतिशील रूप से ग्रेडिएंट अपडेट के लिए नमूनों का चयन करता है। दूसरा फ़ंक्शन, जिसका उपनाम जोखिम है, इनाम को सूचित करने के लिए मजबूत उम्मीदवारों के एक छोटे से पूल का लाभ उठाता है। प्रयोगों में, हम विभिन्न आकार और प्रकृति के नौ सारांश डेटासेट पर एक एनएलएल पूर्व-प्रशिक्षित मॉडल को ठीक करके प्रस्तावित दृष्टिकोण की जांच करते हैं। प्रयोगात्मक परिणाम नकारात्मक लॉग-संभावना बेसलाइन पर एक सुसंगत सुधार दिखाते हैं।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Do sada, većina apstraktivnih modela za sažetak oslanjala se na variante negativne mogućnosti dnevnika (NLL) kao njihov cilj obuke. U nekim slučajevima dodana je učenje pojačanja za obuku modela s ciljem koji je bliži njihovim mjerama procjene (npr. ROUGE). Međutim, funkcija nagrade koja se koristi unutar pristupa pojačanja učenja može imati ključnu ulogu za učenje i još je djelomično neočekivana. Zbog ovog razloga, u ovom papiru predlažemo dvije nagrade za zadatak abstraktivne sažetke: prva funkcija, naziva se RwB-Hinge, dinamički odabere uzorke za aktualizaciju gradienta. Druga funkcija, nadimak RISK, utiče na mali bazen jakih kandidata da obavijesti nagradu. U eksperimentima, istražujemo predloženi pristup finaliziranjem predobučenog model NLL-a preko devet rezimetriranih podataka različitih veličina i prirode. Eksperimentalni rezultati pokazuju konsekventno poboljšanje na osnovnim linijama negativne mogućnosti dnevnika.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Eddig a legtöbb absztraktív összefoglaló modell a negatív log-valószínűség (NLL) variánsaira támaszkodott képzési célként. Egyes esetekben kiegészítették a megerősített tanulást annak érdekében, hogy a modelleket olyan célkitűzéssel képezzék, amely közelebb áll az értékelési intézkedésekhez (pl. ROUGE). A megerősítő tanulási megközelítésben alkalmazott jutalmazási funkció azonban kulcsszerepet játszhat a teljesítmény szempontjából, és részben még mindig feltáratlan. Ezért ebben a tanulmányban két jutalomfüggvényt javasolunk az absztraktív összefoglalás feladatához: az első függvény, amelyet RwB-Hinge néven említenek, dinamikusan választja ki a gradiens frissítéséhez szükséges mintákat. A második funkció, a Kockázat beceneve, erős jelöltek kis csoportját használja fel a jutalom tájékoztatására. A kísérletek során a javasolt megközelítést úgy vizsgáljuk, hogy egy NLL előre képzett modell finomhangolásával kilenc, különböző méretű és természetű összefoglaló adatkészleten alapul. A kísérleti eredmények következetes javulást mutatnak a negatív log-valószínűség alapjaihoz képest.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Այսօր վերացրական համառոտագրման մոդելների մեծ մասը հիմնված է բացասական լոգ-հավանականության (ՆԼԼ) տարբերակների վրա որպես իրենց ուսուցման նպատակ: Որոշ դեպքերում ուժեղացված ուսումնասիրությունը ավելացվել է, որպեսզի մոդելները վարժեցնեն նպատակով, որը ավելի մոտ է նրանց գնահատման չափումներին (օրինակ ROUge). Այնուամենայնիվ, վարձի գործառույթը, որը պետք է օգտագործվի ուսուցման ուժեղացման մոտեցում, կարող է ունենալ արտադրողության կարևոր դեր և դեռ մասամբ չի ուսումնասիրել: Այս պատճառով, այս թղթի մեջ մենք առաջարկում ենք երկու հատուկ ֆունկցիաներ վերացական համառոտագրման խնդրի համար. առաջին ֆունկցիան, որը կոչվում է RwB-Հինգ, դինամիկ կերպով ընտրում է նմուշները դասավորման վերականգնման համար: Երկրորդ ֆունկցիան, որը կոչվում է RIsk, օգտագործում է մի փոքրիկ հավաքածու ուժեղ թեկնածուներ, որպեսզի տեղեկացնեն պարգևը: Փորձարկումների ընթացքում մենք ուսումնասիրում ենք առաջարկած մոտեցումը, բարելավելով ՆԼԼ-ի նախապատրաստված մոդելը տարբեր չափերի և բնության ինն համառոտագրման տվյալների համակարգերի միջոցով: Փորձարկվող արդյունքները ցույց են տալիս, որ բացասական լոգ-հավանականության հիմնական գծերի հետ կապված բարելավում է:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Sehingga saat ini, kebanyakan model resumsi abstraktif telah bergantung pada varian dari kemungkinan log negatif (NLL) sebagai tujuan latihan mereka. Dalam beberapa kasus, pemerintahan belajar telah ditambah untuk melatih model dengan tujuan yang lebih dekat dengan tindakan evaluasi mereka (misalnya ROUGE). However, the reward function to be used within the reinforcement learning approach can play a key role for performance and is still partially unexplored. Untuk alasan ini, di kertas ini, kami mengusulkan dua fungsi hadiah untuk tugas resumen abstraktif: fungsi pertama, disebut sebagai RwB-Hinge, dinamik memilih sampel untuk pemutakhiran gradien. The second function, nicknamed RISK, leverages a small pool of strong candidates to inform the reward. Dalam eksperimen ini, kami menyelidiki pendekatan yang direncanakan dengan memperbaiki model NLL yang terlatih lebih dari sembilan dataset ringkasan ukuran dan alam yang berbeda. Hasil percobaan menunjukkan peningkatan konsisten atas garis dasar log-kemungkinan negatif.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ad oggi, la maggior parte dei modelli di sintesi astratti si è basata su varianti della probabilità di log negativa (NLL) come obiettivo formativo. In alcuni casi, è stato aggiunto il rafforzamento dell'apprendimento per formare i modelli con un obiettivo più vicino alle loro misure di valutazione (ad esempio ROUGE). Tuttavia, la funzione di ricompensa da utilizzare all'interno dell'approccio di apprendimento di rinforzo può svolgere un ruolo chiave per le prestazioni ed è ancora parzialmente inesplorata. Per questo motivo, in questo articolo, proponiamo due funzioni di ricompensa per il compito di sintesi astratta: la prima funzione, detta RwB-Hinge, seleziona dinamicamente i campioni per l'aggiornamento gradiente. La seconda funzione, soprannominata RISK, sfrutta un piccolo pool di candidati forti per informare la ricompensa. Negli esperimenti, sondiamo l'approccio proposto mettendo a punto un modello pre-addestrato NLL su nove set di dati di sintesi di diverse dimensioni e natura. I risultati sperimentali mostrano un miglioramento costante rispetto ai valori di riferimento negativi per la probabilità di log.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>これまで、ほとんどの抽象的な要約モデルは、トレーニング目標として負の対数尤度（ NLL ）のバリアントに依存してきた。 場合によっては、モデルを評価尺度に近い目標で訓練するために強化学習が追加されている（例：ルージュ）。 しかし、強化学習アプローチ内で使用される報酬関数は、パフォーマンスのための重要な役割を果たすことができ、まだ部分的には探求されていません。 このため、本稿では、抽象的な要約のタスクのための2つの報酬関数を提案する。RwB - Hingeと呼ばれる最初の関数は、勾配更新のためのサンプルを動的に選択する。 2つ目の関数はリスクと呼ばれ、強力な候補者の小さなプールを活用して報酬を通知します。 実験では、多様なサイズと性質の9つの要約データセットにわたってNLL事前にトレーニングされたモデルを微調整することによって、提案されたアプローチを探索する。 実験結果は、負の対数尤度ベースラインよりも一貫した改善を示す。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Daftar, akeh di sistem absolute dadi resumen model sing wis dianggawe karo variant dadi log-like (NLL) nganggo nggawe aksil ngucap Slacky Nanging, bongkin nggo dianggap sistem kang nggawe Jejaring Awak dhéwé, ning basa iki, kita mulungi iki bakal nggawe bakal dhéwé basa sing perusahaan karo hal-perusahaan: nambah tanggal tuatah, nambah RwB-hint deep Nang ujaran ning rabi, kita ngubah akses nggawe layakno ning kebutuhan NLL kuwi model sing wis antara awak dhéwé dadi nggawe barang sampek kanggo kalagayut karo bumi. Rejalaké sing diperaksi wong kaé mpungasane kanggo ngilangno sistem sing gawe kaya nguasakno</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ახლა, უფრო მეტი აბსტრაქტიური სიმბოლოების მოდელები გადარწმუნდება, როგორც საკუთარი საკუთარი საკუთარი მიზეზების გარიანტებით. რამდენიმე შემთხვევაში სწავლება დამატებულია, რომ მოდელები გასწავლათ მისი მიზეზი, რომელიც უფრო კიდევ მისი გაუმუშავება (მაგალითად ROUGE). მაგრამ, სამუშაო ფუნქცია, რომელიც სწავლად სწავლას შესაძლებლობად გამოიყენება, შეიძლება გავაკეთოთ სამუშაო პროლის გასაკეთებლად და უკვე ნაწილა ამ მიზეზით, ამ დომენტში, ჩვენ აბსტრაქტიგური სიმბოლოების დასაწყისთვის ორი სამუშაო ფუნქციების შესაძლებლობა: პირველი ფუნქცია, რომელიც RwB-Hinge, დინამიკურად განახლებელად მეორე ფუნქცია, სახელი RISK, ძალიან ძალიან კანდიდენტების მარტივი ბასონი იმპორტირება. ექსპერიმენტებში, ჩვენ შევცვალობთ წარმოიდგინული პროგრამა, რომელიც NLL-ს წარმოადგილებული მოდელის შესახებ, ცხრა განსხვავებული განსხვავებული განსხვავებული განსხვავებული მო ექსპერიმენტიური წარმოდგენების შესაძლებლობა მინდომენტიური ლოგური შესაძლებლობაზე შესაძლებელი დააწყება.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Күніне, абстрактивтік жазу үлгілерінің көпшілігі негативті журнал мүмкіндігі (NLL) бақылау мақсаты ретінде тәуелді. Кейбір жағдайда, үлгілерді оқыту мақсаттарына жақын оқыту үшін қолданылады (мысалы, ROUGE). Бірақ қолданылатын көмектесу тәсілдерінде қолданылатын көмектесу функциясы оқу тәсілдерінің көмектесу үшін көмектесу рөлін ойлап, бірақ бөлімі күтпеген Бұл себептен, бұл қағазда, абстрактивті тұжырымдамасының тапсырмасының екі жоғары функциясын ұсынамыз: RwB- Hinge деп аталатын бірінші функциясы, динамикалық градиенттің жаңарту үшін үлгіле Екінші функциясы, RISK атауы үшін үлкен кандидаттардың кішкентай жиілігін қолданады. Тәжірибелерде, біз NLL алдын- ала оқылған үлгілерді тоғыз көлемі мен табиғатты қосымша деректер жинақтарының түрлі түрлі мәліметі мен қасиеттің түрлі түрлендіру арқылы ұсын Тәжірибелі нәтижелер негативті журнал мүмкіндіктерінің негативті жолдарында тұрақты жақсарту көрсетеді.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>지금까지 대부분의 추상적인 총결산 모델은 마이너스 대수 유사(NLL) 변수에 의존하여 훈련 목표로 삼았다.어떤 경우 훈련 목표가 평가 지표에 더욱 가까워지도록 학습을 강화하는 모델(예를 들어 연지)을 추가했다.그러나 학습 방법을 강화하는 데 사용되는 장려 함수는 실적에 관건적인 역할을 할 수 있기 때문에 아직 일부는 탐색하지 못했다.따라서 본고에서 우리는 추상적으로 임무를 정리하는 데 사용되는 두 가지 보상 함수를 제시했다. 첫 번째 함수는 RwB 경첩이라고 하고 동적 선택 사다리 업데이트에 사용되는 견본이다.두 번째 기능은 리스크라는 별명으로 강력한 후보들을 이용해 보상을 알리는 것이다.실험에서 우리는 9개의 서로 다른 크기와 성질의 집합 데이터 집합의 NLL 예비 훈련 모델을 미세하게 조정함으로써 제시한 방법을 탐색했다.실험 결과 마이너스 대수의 유사 기선에 비해 이 방법은 일치된 개선을 보였다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Iki šiol dauguma abstrakcinių santraukų modelių, kaip mokymo tikslas, rėmėsi neigiamos log tikimybės variantais. Kai kuriais atvejais mokymasis stiprinamas siekiant modelių mokymo tikslo, kuris yra artimesnis jų vertinimo priemonėms (pvz., ROUGE). Vis dėlto darbo užmokesčio funkcija, naudojama taikant mokymosi stiprinimu metodą, gali atlikti svarbų vaidmenį veiklos rezultatams ir vis dar iš dalies neištirta. Dėl šios priežasties šiame dokumente siūlome dvi atlyginimo funkcijas už abstrakcinės santraukos užduotį: pirmoji funkcija, vadinama RwB-Hinge, dinamiškai atrenka mėginius gradientui atnaujinti. Antroji funkcija, vadinama RISK, suteikia galimybę informuoti mažą grupę stiprių kandidatų apie atlygį. Eksperimentuose ištiriame siūlomą metodą tiksliai pritaikant iš anksto parengtą NLL model į į devynis įvairių dydžių ir gamtos duomenų rinkinius. Eksperimentiniai rezultatai rodo nuoseklų pagerėjimą palyginti su neigiama log tikimybės bazine verte.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Досега, повеќето апстрактивни модели за резултат се потпираа на варијантите на негативната веројатност на логот (НЛЛ) како нивна обука. Во некои случаи, додадено е зајакнување на учењето за обука на моделите со цел кој е поблиску до нивните мерки за проценка (np. ROUGE). Сепак, функцијата на награда која треба да се користи во рамките на пристапот на зајакнување на учењето може да одигра клучна улога за изведувањето и сé уште е делумно неиспитана. Од оваа причина, во овој весник, предложуваме две функции на награда за задачата на апстрактивна резултатација: првата функција, наречена RwB-Hinge, динамично ги избира примероците за градијантното ажурирање. Втората функција, наречена РИСК, користи мал базен силни кандидати за да ја информира наградата. Во експериментите, го испитуваме предложениот пристап со финетизирање на предобучениот модел на НЛЛ преку девет резултати на податоци од различна големина и природа. Експерименталните резултати покажуваат константно подобрување во однос на базите на негативните лог-веројатности.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ഇന്നത്തേക്കുള്ള സമയത്ത്, ഏറ്റവും അസാധ്യതയില്ലാത്ത ചിഹ്നങ്ങളുടെ മോഡല്‍ അവരുടെ പരിശീലനത്തിന്റെ ലക്ഷ്യം ആയിരിക്കുന്നു. In some cases, reinforcement learning has been added to train the models with an objective that is closer to their evaluation measures (e.g. ROUGE). എന്നാലും, കൂടുതല്‍ പഠിക്കുന്നതിനുള്ളില്‍ ഉപയോഗിക്കുന്ന പ്രതിഫലം പ്രദര്‍ശനത്തിനുള്ള താക്കോല്‍ പ്രവർത്തിക്കുന്നത ഈ പേപ്പറില്‍ നമ്മള്‍ രണ്ട് പ്രതിഫല പ്രവര്‍ത്തനങ്ങള്‍ പ്രായശ്ചിത്തം ചെയ്യുന്നു: ആദ്യത്തെ പ്രവര്‍ത്തനത്തിന് RwB-ഹിങ്ങ് എന്ന് വിളിക്കുന്നു. ഗ്രേഡിഗ്ര രണ്ടാമത്തെ പ്രവൃത്തിക്ക്, RISK എന്ന പേര്, പ്രതിഫലം അറിയിക്കാന്‍ ശക്തിയുള്ള പ്രാര്‍ത്ഥികളുടെ ഒരു ചെറിയ പൂള പരീക്ഷണങ്ങളില്‍, നമ്മള്‍ പ്രൊദ്ദേശിക്കപ്പെട്ട ഒരു NLL മുമ്പ് പരിശീലിക്കപ്പെട്ട മോഡലിനെ പരിശോധിക്കുന്നത് നമ്മള്‍ പരിശോധിക്ക പരീക്ഷണത്തിന്റെ ഫലങ്ങള്‍ നെഗറ്റീവ് ലോഗ് സാധ്യതയുള്ള അടിസ്ഥാനത്തില്‍ മെച്ചപ്പെടുത്തുന്നത് കാണിക്</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Өнөөдөр ихэнх абстрактив тодорхойлолтын загварууд нь тэдний сургалтын зорилго болгон сөрөг боломжтой (NLL) логины хувилбаруудыг хамаарна. Зарим тохиолдолд, загваруудыг оюутнуудын шалгалтын шалгалтын тулд хамгийн ойрхон зорилготой зорилгоор сургалтыг нэмэгдүүлсэн. Гэхдээ шагналын функцийг нэмэгдүүлэх суралцах аргын дотор ашиглах шагналын функц нь үйл ажиллагаанд хамгийн чухал үүрэг тоглож чадна. Гэхдээ хэсэг хэсэг нь тодорхойгүй. Ийм шалтгаанаас, энэ цаасан дээр бид abstractive summary-ын даалгаварын хоёр шагналын функцүүдийг санал болгож байна: RwB-Hinge гэдэг анхны функц, динамик градиентын жагсаалтын жишээг сонгож байна. Хоёр дахь функц, RISK нэртэй нэртэй нэртэй, шагналыг мэдээллийн тулд хүчирхэг хүндрэгчдийн жижиг хэмжээг ашигладаг. Эдгээр туршилтуудын тулд бид NLL сургалтын өмнө сургалтын загварыг 9 дахин олон хэмжээст болон байгалийн тодорхойлолтын хэмжээний өгөгдлийн сангуудыг судалж байна. Үүний туршилтын үр дүнд сөрөг лог магадлалын суурь шугам дээр тогтмол сайжруулалт гаргадаг.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Sehingga kini, kebanyakan model ringkasan abstraktif telah bergantung pada varian kemungkinan log negatif (NLL) sebagai objektif latihan mereka. In some cases, reinforcement learning has been added to train the models with an objective that is closer to their evaluation measures (e.g. ROUGE). However, the reward function to be used within the reinforcement learning approach can play a key role for performance and is still partially unexplored. Untuk sebab ini, dalam kertas ini, kami cadangkan dua fungsi hadiah untuk tugas ringkasan abstraktif: fungsi pertama, yang disebut sebagai RwB-Hinge, memilih secara dinamik sampel untuk kemaskini gradien. Fungsi kedua, bernama RISK, menggunakan sekumpulan kecil calon kuat untuk memberitahu hadiah. Dalam percubaan ini, kita menguji pendekatan yang direncanakan dengan menyesuaikan model NLL yang dilatih-dilatih lebih dari sembilan set data ringkasan saiz dan sifat berbeza. Hasil percubaan menunjukkan peningkatan konsisten atas garis dasar log-kemungkinan negatif.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Sal-lum, il-biċċa l-kbira tal-mudelli abstrattivi ta’ sommarju ddependew fuq varjanti tal-probabbiltà negattiva ta’ reġistrazzjoni (NLL) bħala l-għan tat-taħriġ tagħhom. F’xi każijiet, it-tagħlim ta’ rinfurzar ġie miżjud biex jitħarrġu l-mudelli b’objettiv li huwa eqreb g ħall-miżuri ta’ evalwazzjoni tagħhom (pereżempju ROUGE). Madankollu, il-funzjoni ta’ premju li għandha tintuża fl-approċċ tat-tagħlim ta’ rinfurzar jista’ jkollha rwol ewlieni għall-prestazzjoni u għadha parzjalment mhux esplorata. Għal din ir-raġuni, f’dan id-dokument, qed nipproponu żewġ funzjonijiet ta’ premju għall-kompitu ta’ sommarju astrattiv: l-ewwel funzjoni, imsejħa RwB-Hinge, tagħżel dinamikament il-kampjuni għall-aġġornament tal-gradjenti. It-tieni funzjoni, magħrufa bħala RISK, tagħti spinta lil grupp żgħir ta’ kandidati b’saħħithom biex jinfurmaw il-premju. Fl-esperimenti, aħna nistudjaw l-approċċ propost billi nirranġaw mudell imħarreġ minn qabel tal-NLL fuq disa’ settijiet ta’ dejta ta’ sommarju ta’ daqs u natura varji. Ir-riżultati sperimentali juru titjib konsistenti fuq il-linji bażi negattivi tal-probabbiltà log.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tot op heden hebben de meeste abstracte samenvattingsmodellen als trainingsdoelstelling gebruik gemaakt van varianten van de negatieve log-waarschijnlijkheid (NLL). In sommige gevallen is versterking learning toegevoegd om de modellen te trainen met een doel dat dichter bij hun evaluatiemaatregelen ligt (bijv. ROUGE). De beloningsfunctie die moet worden gebruikt binnen de versterkende leeraanpak kan echter een belangrijke rol spelen voor prestaties en is gedeeltelijk nog onontdekt. Daarom stellen we in dit artikel twee beloningsfuncties voor de taak van abstractieve samenvatting voor: de eerste functie, aangeduid als RwB-scharnier, selecteert dynamisch de samples voor de gradiënt update. De tweede functie, genaamd RISK, maakt gebruik van een kleine pool van sterke kandidaten om de beloning te informeren. In de experimenten onderzoeken we de voorgestelde aanpak door een NLL voorgetraind model af te stemmen over negen samenvattingsdatasets van verschillende grootte en aard. De experimentele resultaten tonen een consistente verbetering ten opzichte van de negatieve log-waarschijnlijkheid baselines.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Til dag har dei fleste abstraktive sammendragsmodulane relieve på variantane av den negativ loggsannsynligheten (NLL) som opplæringsmålet. I enkelte tilfeller er stipringslæring lagt til for å lære modellen med eit mål som er nærmere deres evalueringsmål (f.eks. ROUGE). Dette funksjonen kan imidlertid brukast i læringstilnærminga for reinforcement kan spela ein nøkkelrolle for utviklinga og er fortsatt delvis uventa. I denne papiret foreslår vi to løftefunksjonar for oppgåva av abstraktive sammendrag: den første funksjonen, kalla som RwB-Hinge, dynamisk veljer prøver for oppdateringa av fargeovergangane. Den andre funksjonen, kallenamnet RISK, leverer ein liten pool av sterke kandidatar for å informere renten. I eksperimentene prøver vi den foreslåde tilnærminga ved å finne opp ein NLL-føretrained modell over ni samanseringsdata med ulike storleik og natur. Eksperimentale resultatet viser ein konsistent forbedring over den negative logsannsynlege baselinjene.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Do tej pory większość abstrakcyjnych modeli podsumowania opierała się na wariantach negatywnej prawdopodobieństwa logowania (NLL) jako celu szkoleniowego. W niektórych przypadkach dodano uczenie się wzmacniające w celu szkolenia modeli z celem bliższym do ich środków oceny (np. ROUGE). Jednakże funkcja nagrody, która ma być wykorzystywana w ramach podejścia do uczenia się wzmacniającego, może odgrywać kluczową rolę dla wydajności i jest nadal częściowo niezbadana. Z tego względu w niniejszym artykule proponujemy dwie funkcje nagrody dla zadania abstrakcyjnego podsumowania: pierwsza funkcja, zwana RwB-Zawiasem, dynamicznie dobiera próbki do aktualizacji gradientu. Druga funkcja, zwana RISK, wykorzystuje niewielką pulę silnych kandydatów do informowania o nagrodzie. W eksperymentach badamy proponowane podejście poprzez dostrojenie wstępnie przeszkolonego modelu NLL na dziewięciu zestawach danych podsumowujących o różnej wielkości i charakterze. Wyniki eksperymentalne wskazują na stałą poprawę w stosunku do ujemnych linii bazowych log-prawdopodobieństwa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Até o momento, a maioria dos modelos de sumarização abstrativos baseou-se em variantes do log-likelihood negativo (NLL) como seu objetivo de treinamento. Em alguns casos, o aprendizado por reforço foi adicionado para treinar os modelos com um objetivo mais próximo de suas medidas de avaliação (por exemplo, ROUGE). No entanto, a função de recompensa a ser usada dentro da abordagem de aprendizado por reforço pode desempenhar um papel fundamental para o desempenho e ainda é parcialmente inexplorada. Por esta razão, neste artigo, propomos duas funções de recompensa para a tarefa de sumarização abstrativa: a primeira função, denominada RwB-Hinge, seleciona dinamicamente as amostras para a atualização do gradiente. A segunda função, apelidada de RISK, aproveita um pequeno grupo de candidatos fortes para informar a recompensa. Nos experimentos, sondamos a abordagem proposta ajustando um modelo pré-treinado de NLL em nove conjuntos de dados de sumarização de diversos tamanhos e naturezas. Os resultados experimentais mostram uma melhoria consistente sobre as linhas de base de probabilidade logarítmica negativa.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Până în prezent, cele mai multe modele abstractive de sinteză s-au bazat pe variante ale probabilității log negative (NLL) ca obiectiv de formare. În unele cazuri, s-a adăugat consolidarea învățării pentru a instrui modelele cu un obiectiv mai aproape de măsurile lor de evaluare (de exemplu, ROUGE). Cu toate acestea, funcția de recompensare care trebuie utilizată în cadrul abordării de învățare de consolidare poate juca un rol cheie pentru performanță și este încă parțial neexplorată. Din acest motiv, în această lucrare, propunem două funcții de recompensare pentru sarcina de rezumare abstractivă: prima funcție, denumită RwB-Balage, selectează dinamic eșantioanele pentru actualizarea gradientului. A doua funcție, poreclită RISK, mobilizează un mic grup de candidați puternici pentru a informa recompensa. În cadrul experimentelor, analizăm abordarea propusă prin reglarea fină a unui model pre-instruit NLL pe nouă seturi de date de rezumare de dimensiuni și natură diverse. Rezultatele experimentale arată o îmbunătăţire constantă faţă de valorile de referinţă ale probabilităţii jurnaliste negative.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>На сегодняшний день большинство моделей абстрактного обобщения опираются на варианты отрицательного логарифмического правдоподобия (НЛЛ) в качестве цели обучения. В некоторых случаях было добавлено дополнительное обучение для обучения моделей с целью, которая ближе к их оценке (например, ROUGE). Однако функция вознаграждения, которая будет использоваться в подходе к обучению подкреплению, может играть ключевую роль для эффективности и все еще частично не изучена. По этой причине в данной работе мы предлагаем две функции вознаграждения для задачи абстрактного обобщения: первая функция, называемая RwB-Hinge, динамически выбирает выборки для обновления градиента. Вторая функция, прозванная РИСК, использует небольшой пул сильных кандидатов, чтобы информировать награду. В экспериментах мы исследуем предлагаемый подход путем тонкой настройки предварительно обученной модели NLL для девяти сводных наборов данных различного размера и природы. Экспериментальные результаты показывают постоянное улучшение по сравнению с отрицательными исходными уровнями логарифмической правдоподобия.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>අවස්ථානයෙන්, ගොඩක් අවස්ථාවක් සංශ්‍ය සංශ්‍ය විද්‍යාපිත විද්‍යාපිත විද්‍යාපිත විද්‍යාපිත විද්‍යාපිත ව සමහර අවස්ථාවක් වලින්, විශ්වාස කරන්න පුළුවන් ඉගෙනගන්න ලැබුණා මොඩේල් එක්ක අභියෝගයක් සඳහා ඔවුන්ගේ විශ්වා නමුත්, ප්‍රතිචාර ප්‍රතිචාර ප්‍රයෝජනය විශ්වාස කරන්න පුළුවන් ප්‍රයෝජනය වෙනුවෙන් ප්‍රතිචාර ප්‍රයෝජන මේ හේතුවෙන්, මේ පත්තරයේ අපි ප්‍රතිචාර දෙකක් ප්‍රතිචාර කරන්න ප්‍රතිචාර දෙකක් ප්‍රතිචාර කරනවා: පළමු ප්‍රතිචාර ක්‍රියාව, RwB-හින දෙවෙනි ප්‍රකාර, RISK නාමය, ප්‍රතිචාරයක් තියෙන්න පුළුවන් පුළුවන් පුළුවන් පුළුවන් පුළුවන පරීක්ෂණයේ අපි ප්‍රශ්නයක් පරීක්ෂණය කරනවා NLL ප්‍රශ්නයක් ප්‍රශ්නයක් විවිධ ප්‍රමාණයක් සහ ස්වභාවිතයෙන් අනුවෙ පරීක්ෂණ ප්‍රතිචාර ප්‍රතිචාර ප්‍රතිචාර ප්‍රතිචාරයක් පෙන්වන්න පුළුවන් විශාලනයක් ව</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Do danes se je večina abstraktivnih modelov povzetka zanašala na različice negativne log verjetnosti (NLL) kot cilj usposabljanja. V nekaterih primerih je bilo dodano okrepitveno učenje za usposabljanje modelov s ciljem, ki je bližji njihovim ukrepom ocenjevanja (npr. ROUGE). Vendar pa lahko funkcija nagrajevanja, ki se uporablja v okviru pristopa krepitvenega učenja, igra ključno vlogo za uspešnost in je še vedno delno neraziskana. Zato v tem prispevku predlagamo dve nagradni funkciji za nalogo abstraktivnega povzetka: prva funkcija, imenovana RwB-Hinge, dinamično izbere vzorce za posodobitev gradienta. Druga funkcija, imenovana RISK, vzvodi majhen nabor močnih kandidatov za obveščanje o nagradi. V poskusih smo preučili predlagani pristop z natančnim nastavitvijo vnaprej usposobljenega modela NLL na devetih naborih povzetkov podatkov različnih velikosti in narave. Poskusni rezultati kažejo konsistentno izboljšanje glede na izhodiščne vrstice negativne log verjetnosti.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Maanta la joogo waxaa ku xiran qaababka habaarka ah oo ay ku xiran yihiin noocyo kala duduwan qoraalka diidmada ah (NLL) oo ah goal waxbarashadooda. Xaaladaha qaarkood waxaa lagu darsaday inaad kordhiso waxbarashada lagu barto tusaalayaasha, kaas oo ku dhow qaababka qiimeynta (tusaale ahaan ROUGE). Si kastaba ha ahaatee shaqada mushaarka lagu isticmaali karo habka kordhiska waxbarashadu waxay noqon kartaa qayb ka mid ah qayb la'aan. Sababtaas darteed waxan warqaddan ku qoran, waxaan u soo jeedaynaa laba shaqooyin oo mushaar ah oo u qoran shaqo la'aan oo la soo saaro cayiman ah: kooxda ugu horeeyay ee lagu magacaabay RwB-Hinge, oo lagu magacaabay dhaqdhaqaaqa samooyinka sawirida. Shaqooyinka labaad, magaca RISK, wuxuu soo saaraa balli yar oo ka mid ah kandidiyayaasha xoogga badan si uu mushahaarada u ogeysiiyo. Imtixaanka, waxaynu tijaabinaynaa dhaqdhaqaaqa la soo jeeday si fiican looga sameynayo model la tababaray NLL oo ka sarreeya sagaal xagaaminta macluumaad oo kala duduwan tirada iyo dabiicadda kala duduwan. resultinta imtixaanka waxaa ka muuqda horumarinta ku socota qorshaha suurtagalka diidiga ah.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Deri tani, shumica e modeleve abstraktive të përmbledhjes janë mbështetur në variantet e probabilitetit negativ të regjistrimit (NLL) si objektivin e tyre të trajnimit. Në disa raste, mësimi i forcimit është shtuar për të trajnuar modelet me një objektiv që është më pranë masave të tyre të vlerësimit (për shembull ROUGE). Megjithatë, funksioni i shpërblimit që do të përdoret brenda qasjes së forcimit të mësimit mund të luajë një rol kyç për performancën dhe është ende pjesërisht i papërshqyrtuar. For this reason, in this paper, we propose two reward functions for the task of abstractive summarisation: the first function, referred to as RwB-Hinge, dynamically selects the samples for the gradient update. Funksioni i dytë, i quajtur RISK, nxjerr një grup të vogël kandidatësh të fortë për të informuar shpërblimin. Në eksperimente, ne vëzhgojmë qasjen e propozuar duke përshtatur një model të paratrajnuar NLL mbi nëntë grupe të dhënash të përmbledhur të madhësisë dhe natyrës së ndryshme. Rezultatet eksperimentale tregojnë një përmirësim konsistent lidhur me linjat bazë negative të probabilitetit të log.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Do sada, većina apstraktivnih modela za sažetak oslanjaju se na variante negativne verovatnosti dnevnika (NLL) kao njihov cilj obuke. U nekim slučajevima, dodana je učenje pojačanja kako bi obučila modele objektivnim ciljem koji je bliži njihovim mjerama procjene (npr. ROUGE). Međutim, funkcija nagrade koja se koristi unutar pristupa pojačanja učenja može imati ključnu ulogu za izvođenje i još je djelomično neočekivana. Zbog ovog razloga, u ovom papiru predlažemo dve nagrade za zadatak abstraktivne sažetke: prva funkcija, pod nazivom RwB-Hinge, dinamički odabere uzorke za aktualizaciju gradienta. Druga funkcija, nadimak RISK, utiče na mali bazen jakih kandidata da obavijesti nagradu. U eksperimentima, istražujemo predloženi pristup kako bi finalizirali predobučeni model NLL preko devet rezimetriranih podataka različitih veličina i prirode. Eksperimentalni rezultati pokazuju konsekventno poboljšanje na osnovnim linijama negativne verovatnosti.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Hittills har de flesta abstrakta sammanfattningsmodeller förlitat sig på varianter av negativ log-sannolikhet (NLL) som träningsmål. I vissa fall har förstärkt lärande lagts till för att utbilda modellerna med ett mål som ligger närmare deras utvärderingsåtgärder (t.ex. ROUGE). Den belöningsfunktion som används inom förstärkningsstrategin kan dock spela en nyckelroll för prestationen och är fortfarande delvis outforskad. Av denna anledning föreslår vi i denna uppsats två belöningsfunktioner för uppgiften med abstraktiv sammanfattning: den första funktionen, kallad RwB-Hinge, väljer dynamiskt proverna för gradientuppdateringen. Den andra funktionen, smeknamnet RISK, utnyttjar en liten pool av starka kandidater för att informera belöningen. I experimenten undersöker vi det föreslagna tillvägagångssättet genom att finjustera en NLL-förberedd modell över nio sammanfattningsdatauppsättningar av olika storlek och natur. De experimentella resultaten visar en konsekvent förbättring jämfört med den negativa log-sannolikhetsbaselinen.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mpaka sasa, mifano mingi ya muhtasari usio na maana yametegemea tofauti ya uwezekano wa log hasi (NLL) kama lengo la mafunzo yao. Katika baadhi ya matukio mengine, mafunzo ya kuuza umeongezeka kufundisha mifano kwa lengo ambalo ni karibu zaidi na hatua za uchunguzi (kwa mfano, ROUGE). Hata hivyo, jukumu la malipo linalotumiwa ndani ya mbinu za kujifunza linaweza kucheza jukumu la msingi kwa ajili ya utendaji na bado haujajua. Kwa sababu hii, katika gazeti hili, tunapendekeza kazi mbili za malipo kwa ajili ya jukumu la muhtasari wa kidini: kazi ya kwanza, inayoitwa RwB-Hinge, kwa nguvu tunachagua mifano kwa ajili ya upya wa kisasa. Kifungu cha pili, kinachoitwa jina la RISK, kina kichwa kidogo cha wagombea wenye nguvu kutoa taarifa ya malipo. Katika majaribio hayo, tunajaribu mbinu zilizopendekezwa kwa kuunganisha muundo wa zamani wa NLL kwa zaidi ya seti za taarifa za muhtasari tisa za ukubwa na asili tofauti. Matokeo ya majaribio yanaonyesha maendeleo yanayoendelea zaidi ya uwezekano wa kuandika hasi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>இந்த நேரத்திற்கு பெரும்பாலான சுருக்கம் மாதிரிகள் எதிர்மறை பதிவு சாத்தியமான (NLL) மாறிகளை அவர்களுடைய பயிற்சி பொருளாக நம் சில நிகழ்ச்சிகளில், கல்வி கற்றுக்கொள்ள மாதிரிகளை பயிற்சியுடன் சேர்க்கப்பட்டுள்ளது, அது அவர்களுடைய மதிப்பீட்டு அளவுகளுக்கு அருகில ஆயினும், வலுப்படுத்தல் முறையில் பயன்படுத்த வேண்டிய கூலி செயல்பாடு செயல்பாட்டிற்கான முக்கிய விளையாட்டை விளையாட இந்த காரணத்தில், நாம் செயல்பாட்டிற்கான இரண்டு கூலி செயல்பாடுகளை பரிந்துரைக்கிறோம்: முதல் செயல்பாடு, RwB- Hinge என்று குறிப்பிடப்பட்டால், தானாகவே சார் இரண்டாவது செயல்பாடு, RISK புனைப்பெயர், கூலி அறிவிப்பதற்கு ஒரு சிறிய குளியீட்டை வழங்குகிறது. பரிசோதனைகளில், நாம் பரிந்துரைக்கப்பட்ட செயல்பாட்டை சரியாக ஒரு NLL முன் பயிற்சி மாதிரியை ஒன்பது முறை சுருக்கி தகவல் அமைப்புகளில இந்த சோதனையின் முடிவுகள் எதிர்மறை பதிவு சாத்தியமான அடிப்படைகளை காட்டும்.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Şu wagt, iň köp abstraktiw jemgyýet nusgalary negatif log-sanlykynyň (NLL) üýtgetmek maksady bolup geçirdi. Käbir ýagdaýda, nusgalary düzenlemek üçin köpräk öwrenmeler (meseläm ROUGE) düzenlemek golaýynda golaýlaşdyryldy. Ýöne, taýýarlanmak öwrenmek approwasynda ulanylýan täsirli işleýän fonksiýa performans üçin a çyk roli oýnap biler we heniz hem be ýleki şekilde tanamaýar. Bu sebäpden, bu kagyzda abstraktiw toplantyň zadynyň üçin iki täsirli fonksiyony teklip edip görýäris: ilkinji funksiýa, RwB-Hinge diýilip atlanýar, dinamik görnöşi üçin örnekleri saýlaýar. Ikinji faýly, RISK atlandyrylýan, tämiýeti biljek üçin güýçli kandidýalaryň kiçi howluny süýtgedýär. Deneylerde, NLL öňünden eğlenen nusga 9 topar ululyk we tebigatyň üstünde teklip eden nusgasyny çykaryp barýarys. Deneymeli netijeler negatif günlük mümkinçiliki üssüňlerde sürekli gelişmeleri görkezýär.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>یہاں تک، اکثر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غیر غ بعض موقعیت میں، مدلس کی تعلیم کی زیادتی کے ساتھ اضافہ کی گئی ہے ایک موقعیت کے ساتھ جو ان کے ارزش اندازے کے قریب ہے (مثل ROUGE). However, the reward function to be used in reinforcement learning approach can play a key role for performance and still partially unexplored. اس وجہ سے، ہم اس کاغذ میں دو اجرت فناوری پیشنهاد کرتے ہیں کہ ابتراذی تعداد کے کام کے لئے دو اجرت فناوری: پہلی فناوری، RwB-Hinge کا نام لیا گیا ہے، سینامیک طور پر گراڈینٹ اوڈڈیٹ کے لئے نمونے انتخاب کرتے ہیں. دوسری فنقش، RISK کے نام کا نیک نام، مزدوری سے خبردار کرنے کے لئے ایک چھوٹا پائل ہے۔ آزمائش میں، ہم نے ایک NLL پیش آموزش کی مدل کو نو مختلف اندازے اور طبیعت کے ذریعہ سے اپنا پیش آموزش دینے والی ڈیٹ سٹ کے ذریعہ پیش آموزش دینے کے ذریعہ پیش آموزش دیے ہیں. آزمائش نتیجے منفی لاگ-احتمالات بنیس لین پر ایک ثابت قدم ترقی دکھاتے ہیں.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bu yerda ko'pchilik muvaffaqiyatsiz modellari ularning taʼminlovchi maqsadiga (NLL) oʻzgarishga ishlatadi. Ba'zi holatda, o'rganishni qoʻshish qoʻshildi, modellarni qiymatning qiymatlariga quyidagi maqsad bilan o'rganish mumkin. Lekin, o'rganish usulida foydalanish muvaffaqiyatli o'rganish qoidasi bajarish uchun muhim roli o'ynashi mumkin va ammo qismi unutilmaydi. For this reason, in this paper, we propose two reward functions for the task of abstractive summarisation: the first function, referred to as RwB-Hinge, dynamically selects the samples for the gradient update. Ikkinchi funksiyat RISK nomli, muammolar haqida o'zgartirish uchun kichkina qismi qo'shiladi. Tajribalar davomida, biz birinchi taʼminlovchi NLL modelini yaxshi ko'ra ko'proq taʼminlovchi 9 ta'minlovchi taʼminlovchi maʼlumotlar tarkibini o'rganish mumkin. Sertifikatlar natijalari negativ logning asosiy asosiy sonlarida davom etishni ko'rsatadi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Cho đến nay, hầu hết các mô hình tóm tắt trừu tượng đã dựa vào các biến thể của khả năng nhật ký âm (NLL) làm mục tiêu huấn luyện của chúng. Trong một số trường hợp, việc học gia cường đã được thêm vào để huấn luyện các mô- đun với một mục tiêu g ần hơn với các biện pháp đánh giá của chúng (v.d. ROSELE). Tuy nhiên, chức năng phần thưởng được sử dụng trong phương pháp huấn luyện gia tăng có thể đóng vai trò quan trọng trong việc vận hành và vẫn chưa được khám phá một phần. Vì vậy, trong tờ giấy này, chúng tôi đề xuất hai chức năng thưởng cho nhiệm vụ tổng hợp trừu tượng: chức năng đầu tiên, được gọi là RwB-Hinge, sẽ chọn theo động lực các mẫu cho lần cập nhật dốc. Lần thứ hai, biệt danh RISK, cầm một nhóm nhỏ các ứng viên mạnh để góp phần thưởng. Trong các thí nghiệm, chúng tôi thăm dò phương pháp được đề nghị bằng cách tinh chỉnh một mô hình Trước ICL hơn chín tập hợp dữ liệu với kích thước và tự nhiên khác nhau. Xét nghiệm kết quả cho thấy tỉ lệ tốt hơn so với các căn cứ về duyên khẩu âm.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>迄今为止,多抽象总结模形,皆赖负对数似然(NLL)变体以为训练。 加之以化学,近之以估(如ROUGE)。 然于化学之赏函数可以起关键作用,而犹未穷也。 由此言之,抽象总结二奖函数:一函数谓之RwB-Hinge,动择梯度新样本。 第二职,绰号为RISK,因一小群强选以通奖。 实验之中,因九大小性质之总数集上微调NLL预练模形以探其术。 实验结果表明,比负对数似然基线,实验终改善。</span></div></div><dl><dt>Anthology ID:</dt><dd>2021.spnlp-1.1</dd><dt>Volume:</dt><dd><a href=/volumes/2021.spnlp-1/>Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)</a></dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/acl/>ACL</a>
| <a href=/venues/ijcnlp/>IJCNLP</a>
| <a href=/venues/spnlp/>spnlp</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>1–11</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.spnlp-1.1>https://aclanthology.org/2021.spnlp-1.1</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2021.spnlp-1.1 title="To the current version of the paper by DOI">10.18653/v1/2021.spnlp-1.1</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">parnell-etal-2021-rewardsofsum</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Jacob Parnell, Inigo Jauregi Unanue, and Massimo Piccardi. 2021. <a href=https://aclanthology.org/2021.spnlp-1.1>RewardsOfSum : Exploring Reinforcement Learning Rewards for SummarisationRewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation</a>. In <i>Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)</i>, pages 1–11, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2021.spnlp-1.1>RewardsOfSum : Exploring Reinforcement Learning Rewards for SummarisationRewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation</a> (Parnell et al., spnlp 2021)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.spnlp-1.1.pdf>https://aclanthology.org/2021.spnlp-1.1.pdf</a></dd><dt>Data</dt><dd><a href=https://paperswithcode.com/dataset/newsroom>NEWSROOM</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.spnlp-1.1.pdf title="Open PDF of 'RewardsOfSum : Exploring Reinforcement Learning Rewards for SummarisationRewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=RewardsOfSum+%3A+Exploring+Reinforcement+Learning+Rewards+for+SummarisationRewardsOfSum%3A+Exploring+Reinforcement+Learning+Rewards+for+Summarisation" title="Search for 'RewardsOfSum : Exploring Reinforcement Learning Rewards for SummarisationRewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'RewardsOfSum : Exploring Reinforcement Learning Rewards for SummarisationRewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[RewardsOfSum : Exploring Reinforcement Learning Rewards for SummarisationRewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation](https://aclanthology.org/2021.spnlp-1.1) (Parnell et al., spnlp 2021)</p><ul class=mt-2><li><a href=https://aclanthology.org/2021.spnlp-1.1>RewardsOfSum : Exploring Reinforcement Learning Rewards for SummarisationRewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation</a> (Parnell et al., spnlp 2021)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Jacob Parnell, Inigo Jauregi Unanue, and Massimo Piccardi. 2021. <a href=https://aclanthology.org/2021.spnlp-1.1>RewardsOfSum : Exploring Reinforcement Learning Rewards for SummarisationRewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation</a>. In <i>Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)</i>, pages 1–11, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>