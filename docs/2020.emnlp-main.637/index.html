<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Cold-start Active Learning through Self-supervised Language Modeling - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Cold-start Active Learning through Self-supervised Language Modeling" name=citation_title><meta content="Michelle Yuan" name=citation_author><meta content="Hsuan-Tien Lin" name=citation_author><meta content="Jordan Boyd-Graber" name=citation_author><meta content="Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)" name=citation_conference_title><meta content="2020/11" name=citation_publication_date><meta content="https://aclanthology.org/2020.emnlp-main.637.pdf" name=citation_pdf_url><meta content="7935" name=citation_firstpage><meta content="7948" name=citation_lastpage><meta content="10.18653/v1/2020.emnlp-main.637" name=citation_doi><meta property="og:title" content="Cold-start Active Learning through Self-supervised Language Modeling"><meta property="og:image" content="https://aclanthology.org/thumb/2020.emnlp-main.637.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2020.emnlp-main.637"><meta property="og:description" content="Michelle Yuan, Hsuan-Tien Lin, Jordan Boyd-Graber. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."><link rel=canonical href=https://aclanthology.org/2020.emnlp-main.637></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2020.emnlp-main.637.pdf>Cold-start Active Learning through Self-supervised Language Modeling</a>
<a id=af_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Kold- begin Aktiewe Leer deur Selfbeheerde Taal Modelering</a>
<a id=am_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>CategoryName</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>البدء البارد في التعلم النشط من خلال نمذجة اللغة تحت الإشراف الذاتي</a>
<a id=az_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Öz-gözləyirli Dil Modelindən soğuq-başlayış Etkinlik Öyrənməsi</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Студено стартиране на активно обучение чрез самостоятелно езиково моделиране</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>স্বয়ংক্রিয়ভাষা মডেলিং এর মাধ্যমে শিক্ষা শিক্ষা শিক্ষার্থী</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>རང་ཉིད་ལྟ་རྟོག་པའི་སྐད་རིགས་མ་དབྱིབས་བསྒྱུར་བའི་འཕྲོ་བ་དེ་འགོ་འཛུགས་པ</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Hladni početak Aktivno učenje kroz samopouzdanje jezika</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>L'aprenentatge activa de començament fred a través de la modelació de llenguatges autosupervisada</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Aktivní učení za studena prostřednictvím modelování jazyků pod vlastním dohledem</a>
<a id=da_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Koldstart aktiv læring gennem selvstyret sprogmodellering</a>
<a id=de_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Kaltstart Aktives Lernen durch selbstüberwachte Sprachmodellierung</a>
<a id=el_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Ενεργή μάθηση υπό ψυχρή εκκίνηση μέσω Αυτοεποπτικής μοντελοποίησης γλωσσών</a>
<a id=es_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Aprendizaje activo de inicio en frío mediante modelos lingüísticos autosupervisados</a>
<a id=et_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Külmkäivitus aktiivne õpe enesejärelevalvega keele modelleerimise kaudu</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>یادگیری فعال‌آغاز سرد از طریق مدل زبان خودکنترل</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Kylmäkäynnistyksen aktiivinen oppiminen itseohjatun kielimallinnuksen avulla</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Apprentissage actif à froid grâce à la modélisation linguistique auto-supervisée</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Tús Fuar Foghlaim Ghníomhach trí Shamhaltú Teanga Féin-mhaoirsithe</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>KCharselect unicode block name</a>
<a id=he_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>לימוד פעיל בהתחלה קרה באמצעות מודל שפת משגיח על עצמו</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>स्व-पर्यवेक्षित भाषा मॉडलिंग के माध्यम से कोल्ड-स्टार्ट एक्टिव लर्निंग</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Hladno početno aktivno učenje kroz samopouzdanje jezika</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Hidegen indított aktív tanulás önfelügyelt nyelvi modellezéssel</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Սառը սկիզբ ակտիվ սովորելու միջոցով ինքնավերահսկվող լեզվի մոդելներ</a>
<a id=id_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Pelajaran Aktif dimulai dingin melalui Modeling Bahasa yang diawasi oleh diri sendiri</a>
<a id=is_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Apprendimento attivo a freddo attraverso la modellazione linguistica self-supervisionata</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>自己監督型言語モデリングによるコールドスタート型アクティブラーニング</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Mulai-Mulai Aksi si layang Ngawe Peringatan Self-super-model Language</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Name</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Өзінің бақылау тілін моделдеу арқылы белсенді оқыту</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>자기 감독 언어 모델링을 통해 주동적인 학습을 시작하다</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Šalto pradžios aktyvus mokymasis naudojant savarankiškai prižiūrimą kalbų modeliavimą</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Активно учење на ладен почеток преку моделирање на јазик под самоупотреба</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>സ്വയം നിരീക്ഷിക്കപ്പെട്ട ഭാഷ മോഡലിങിലൂടെ തണുപ്പു് തുടങ്ങുന്ന സജ്ജീവമായ പഠിക്കുക</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Хүйтэн-эхлэл Актив Сургууль Өөрийгөө удирдлагатай хэл загварын аргаар</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Pelajaran Aktif Mula-Dingin melalui Modelan Bahasa Dipengawasi Sendiri</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Tagħlim Attiv bi startjar kiesaħ permezz tal-Mudellar tal-Lingwi li huwa ssorveljat minnu nnifsu</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Cold-start Active Learning door middel van zelfbegeleide taalmodellering</a>
<a id=no_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Name</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Aktywne uczenie się na zimnym start poprzez samodzielne modelowanie języka</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Aprendizagem ativa a frio por meio de modelagem de linguagem autossupervisionada</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Învățarea activă cu pornire la rece prin modelarea limbilor străine auto-supravegheate</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Активное обучение в холодном запуске с помощью самоконтролируемого языкового моделирования</a>
<a id=si_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Name</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Hladni zagon aktivnega učenja s samonadzorovanim jezikovnim modeliranjem</a>
<a id=so_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Barshada waxqabadka ee iskuul-billaabista</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Mësimi aktiv i fillimit të ftohtë nëpërmjet modelimit të gjuhës me mbikqyrje të vetë</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Hladno-početno aktivno učenje kroz samopouzdanje jezika</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Kallstart Aktivt lärande genom självövervakad språkmodellering</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Kujifunza Kitivo cha Kadi kupitia Modeling of Language Self-monitored</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>சார்ந்து கண்காணிக்கப்பட்ட மொழி மாற்றியமைப்பு வழியாக குளிர் துவங்கும் இயங்கும் கற்றல்</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Otomatik Başlyg Dili Görkezilen Öwrenmek</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Name</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>Luyện tập bằng cách chế ngôn ngữ tự giám sát</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2020.emnlp-main.637.pdf>盖自督言建模者冷启动自学也</a></h2><p class=lead><a href=/people/m/michelle-yuan/>Michelle Yuan</a>,
<a href=/people/h/hsuan-tien-lin/>Hsuan-Tien Lin</a>,
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a>. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> is impractical because of model instability and data scarcity. Fortunately, modern <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP</a> provides an additional source of information : pre-trained language models. The pre-training loss can find examples that surprise the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> and should be labeled for efficient <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for <a href=https://en.wikipedia.org/wiki/Text_classification>text classification</a>. Compared to other baselines, our approach reaches higher <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> within less <a href=https://en.wikipedia.org/wiki/Sampling_(statistics)>sampling iterations</a> and <a href=https://en.wikipedia.org/wiki/Time_complexity>computation time</a>.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktiewe leer strep om annotasie koste te verklein deur die mees kritiese voorbeelde te kies om etiket te merk. Die aktiewe leer strategie is ingevolg op die klassifikasie model. Byvoorbeeld, onbevestigheid-sampling afhang van slegte kalibreerde model vertrou-poeiers. In die koue-begin instelling is aktiewe leer onaktiewe vanweë model instabiliteit en data skaars. Gelukkig, moderne NLP verskaf 'n addisionele bron van inligting: voor- opgelei taal modele. Die verlies van voorbereining kan voorbeelde vind wat die model verbaas en moet vir effektief fintuning etiket word. Daarom, ons behandel die taal modeling verlies as 'n volmag vir klasifikasie onbevestigheid. Met BERT, ontwikkel ons 'n eenvoudige strategie gebaseer op die maskeerde taal modeling verlies wat minimiseer etiketting koste vir teks klasifikasie. Vergelyk met ander basisline, ons toegang bereik hoër presisie binne minder versameling iterasies en rekenaar tyd.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>የመምረጥ ምርጫዎች ለመምረጥ የአስታተር ውጤቶችን ለማጎድል ይሞክራል፡፡ በተጨማሪው፣ የተለመደ ትምህርት strategy በተለያዩ ሞዴል ላይ ግንኙነት ነው፡፡ ለምሳሌ፣ የማይታወቀው ምሳሌ በመስኮት ላይ የተጠቃሚ የሞዴል ትምህርት ሁኔታ ነው፡፡ በብርድ ጀምሮ በመጠቀም ላይ የተግባር ትምህርት በሞዴል ስህተት እና የዳታ ድህነት ስህተት ነው፡፡ በተከፋች ጊዜ አዲስ NLP የጨማሪው የመረጃ ምንጭ ሰጥቷል፤ በፊት ተማርቷል የቋንቋ ምሳሌዎች፡፡ የፊተኛ ትምህርት ጉዳይ ምሳሌዎችን ማግኘት ይችላል፡፡ ስለዚህ የቋንቋውን ምሳሌ ሳንቆርጥ ለመፍጠር የማይታወቅ ፕሮክሲን እናስባለን፡፡ With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. ከሌሎችም መሠረቶች በተያያይዙ፣ የሥርዓታችን ግንኙነት ከጥቂት ምሳሌዎች እና ከቁጥጥር ሰዓት ውስጥ ወደ ከፍተኛ እርግጠት ይደርሳል፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>يسعى التعلم النشط إلى تقليل تكاليف التعليقات التوضيحية عن طريق اختيار الأمثلة الأكثر أهمية للتسمية. عادة ، تعتمد استراتيجية التعلم النشط على نموذج التصنيف. على سبيل المثال ، يعتمد أخذ عينات عدم اليقين على درجات ثقة النموذج سيئة المعايرة. في بيئة البداية الباردة ، يكون التعلم النشط غير عملي بسبب عدم استقرار النموذج وندرة البيانات. لحسن الحظ ، يوفر البرمجة اللغوية العصبية الحديثة مصدرًا إضافيًا للمعلومات: نماذج اللغة المدربة مسبقًا. يمكن أن تجد خسارة ما قبل التدريب أمثلة تفاجئ النموذج ويجب أن يتم تصنيفها من أجل الضبط الدقيق الفعال. لذلك ، نتعامل مع فقدان نمذجة اللغة كبديل لعدم اليقين في التصنيف. مع BERT ، نطور إستراتيجية بسيطة تعتمد على فقدان نمذجة اللغة المقنعة التي تقلل من تكاليف وضع العلامات لتصنيف النص. بالمقارنة مع خطوط الأساس الأخرى ، يصل نهجنا إلى دقة أعلى في أقل تكرارات أخذ العينات ووقت الحساب.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Etkili öyrənmək üçün ən kritik məsəlləri seçən məsəlləri azaltmaq üçün hərəkət edir. Tipik olaraq, aktif öyrənmə stratejisi klasifikasiya modeli ilə bağlı olur. Misal olaraq, təhlükəsizlik nümunələri kötü kalibrlənmiş modellərin təhlükəsizlik nöqtələrinə bağlı. Soğuk başlatma quruluşunda, fəaliyyət öyrənməsi modellərin istifadəsi və məlumatların zəif olmasına görə əsasən deyildir. Necə ki, modern NLP, əvvəlcə təhsil edilmiş dil modelləri daha çox məlumatı verir. Əvvəlcə təhsil edilməsi modelini təəccübləndirən məsələlər tapır və müvəffəqiyyətli düzəltmək üçün etiketlənməli olar. Beləliklə, biz dil modellərinin itirməsini klasifikasiya təhlükəsizlik üçün proksi kimi hesab edirik. BERT ilə, gizli dil modellərinin kaybına dayanan basit bir strateji təhsil edirik ki, metin klasifikasyonu üçün etiketləmə maliyyətlərini azaldırır. Başqa sətirlərlə qarşılaşdığımız tərzimiz daha az nümunələr və hesablama zamanı içində daha yüksək doğruluğa çatdı.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Активното обучение се стреми да намали разходите за анотация, като избира най-критичните примери за етикетиране. Обикновено стратегията за активно обучение зависи от модела на класификация. Например вземането на проби от неопределеност зависи от лошо калибрираните резултати на доверието на модела. В условията на студен старт активното учене е непрактично поради нестабилността на модела и недостига на данни. За щастие съвременната НЛП предоставя допълнителен източник на информация: предварително обучени езикови модели. Загубата преди тренировка може да намери примери, които изненадват модела и трябва да бъдат етикетирани за ефективно фино настройване. Затова разглеждаме загубата на езиково моделиране като прокси за класификационната несигурност. С разработваме проста стратегия, базирана на маскираната загуба на езиково моделиране, която свежда до минимум разходите за етикетиране за класификация на текста. В сравнение с други базови линии, нашият подход достига по-висока точност в рамките на по-малко итерации на вземане на проби и време за изчисление.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>লেবেলের সবচেয়ে গুরুত্বপূর্ণ উদাহরণ নির্বাচনের মাধ্যমে সক্রিয় শিক্ষা শেখার চেষ্টা করছে। স্বাভাবিকভাবে সক্রিয় শিক্ষা কৌশল ক্লাসাফিকেশন মডেলে যুক্ত। উদাহরণস্বরূপ, নিশ্চিত নম্পলিং খারাপ ক্যালিবার্ট মডেলের বিশ্বাস স্কোরের উপর নির্ভর করে। শীতল শুরু করার সময় সক্রিয় শিক্ষা অকার্যকর কারণ মডেলের অস্থিরতা এবং তথ্যের ক্ষতির কারণে। সৌভাগ্যবান, আধুনিক এনএলপি আরো তথ্যের উৎস প্রদান করেছে: পূর্বে প্রশিক্ষিত ভাষার মডেল। প্রশিক্ষণের পূর্বে ক্ষতিগ্রস্ত হারানোর উদাহরণ খুঁজে বের করতে পারে যে মডেল বিস্ময়কর এবং কার্যকর সুন্দর প্রতিষ্ঠ Therefore, we treat the language modeling loss as a proxy for classification uncertainty. বেরেটের মাধ্যমে আমরা একটি সাধারণ কৌশল তৈরি করি মুখোশিত ভাষার মডেলের ক্ষতির উপর ভিত্তিক যা টেক্সট গ্রাফিকেশনের জন্য লেব অন্যান্য বেসেলাইনের সাথে তুলনায়, আমাদের প্রতিযোগিতা কম নম্পালিটেশন এবং গণনার সময়ের মধ্যে উচ্চপরিসূচিতে পৌঁছা</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ཤུལ་བ་ཡོད་པའི་སྦྱོར་བའི་ཚད་ལྡན་སྔོན་བསྐྱེད་པའི་དབྱིབས་ཆེན་ཤུགས་ཀྱི་དཔེ་བས། སྤྱིར་བཏང་ན། དབྱེ་སྟངས་གྱི་ཐབས་ལམ་དེ་དམིགས་འཛུགས་བྱེད་ཀྱི་མིག་དཔེ་དབྱིབས་དང་ཁྱད་ཡོད། དཔེར་ན། uncertainty sampling depends on poorly calibrated model confidence scores. དྲགས་པ་ལས་འགོ་འཛུགས་པའི་སྒྲིག ལེགས་སྐྱོང་ན། ད་ལྟོའི་NLP་གིས་གསལ་བཤད་ཁྱད་པར་ཐོག་མའི་གནས་ཚུལ་ཞིག་བྱིན་པ། སྔོན་གྱིས་སྔོན་ག The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. དེར་བརྟེན། With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. ང་ཚོའི་གཟུགས་རིས་གཞན་དང་མཉམ་དུ་མཐོང་ན། ང་ཚོའི་གཟུགས་རིས་དེ་ལས་ཉུང་བའི་རྐྱེན་རིས་དང་རྩིས་འཁོར</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktivni učenje pokušava smanjiti troškove annotacije birajući najkritičnije primjere za etiketu. Obično, aktivna strategija učenja je povezana s klasifikacijskim modelom. Na primjer, uzorak neodređenosti ovisi o lošim kalibriranim rezultatima modela povjerenja. U nastavku hladnopočetka, aktivno učenje je neprikladno zbog modela nestabilnosti i nedostatka podataka. Srećom, moderni NLP pruža dodatni izvor informacija: predobučeni jezički modeli. Gubitak predobuke može pronaći primjere koji iznenađuju model i trebaju biti označeni za učinkovito ispravljanje. Stoga, tretiramo gubitak jezika modela kao proksi za klasifikaciju nesigurnosti. Sa BERT-om, razvijamo jednostavnu strategiju baziranu na gubitku maskiranog jezika koji minimizira troškove označavanja za klasifikaciju teksta. U usporedbi s drugim osnovnim linijama, naš pristup stiže do višeg preciznosti unutar manjih iteracija i vremena računala.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Active learning strives to reduce annotation costs by choosing the most critical examples to label. Normalment, l'estratègia d'aprenentatge actiu depèn del model de classificació. Per exemple, la mostra d'incertituddepèn de puntuacions de confiança mal calibrats del model. En un entorn de començament fred, l'aprenentatge actiu és impractic a causa de l'instabilitat model i l'escassetat de dades. Afortunadament, la NLP moderna proporciona una fontde informació adicional: models de llenguatge pré-entrenats. La pèrdua previa a l'entrenament pot trobar exemples que sorprenen el model i haurien de ser etiquetats per ajustar eficientment. Per tant, tractem la pèrdua de modelació de llenguatges com un indicador de la incertitud de classificació. Amb BERT, desenvolupem una estratègia senzilla basada en la pèrdua mascarada de modelar idiomes que minimitza els costos d'etiquetar la classificació de text. Comparat amb altres línies de base, el nostre enfocament arriba a una més precisió en menys iteracions de mostratge i temps de calcul.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktivní učení se snaží snížit náklady na anotaci výběrem nejkritičtějších příkladů k označení. Strategie aktivního učení je obvykle závislá na klasifikačním modelu. Například vzorkování nejistoty závisí na špatně kalibrovaných skórích důvěry modelu. V nastavení studeného startu je aktivní učení nepraktické kvůli nestabilitě modelu a nedostatku dat. Naštěstí moderní NLP poskytuje další zdroj informací: předškolené jazykové modely. Ztráta před tréninkem může najít příklady, které model překvapí a měly by být označeny pro efektivní jemné ladění. Proto považujeme ztrátu jazykového modelování za proxy klasifikační nejistoty. S BERT vyvíjíme jednoduchou strategii založenou na maskované ztrátě jazykového modelování, která minimalizuje náklady na popisování pro klasifikaci textu. Ve srovnání s jinými základními liniemi dosahuje náš přístup vyšší přesnosti během kratších iterací vzorků a výpočetního času.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktiv læring stræber efter at reducere annoteringsomkostningerne ved at vælge de mest kritiske eksempler at mærke. Typisk er den aktive læringsstrategi betinget af klassifikationsmodellen. Eksempelvis afhænger usikkerhedsprøveudtagning af dårligt kalibrerede modellens tillidsscorer. I koldstartsindstillingen er aktiv læring upraktisk på grund af modelstabilitet og dataknaphed. Heldigvis giver moderne NLP en ekstra kilde til information: forududdannede sprogmodeller. Tabet før træning kan finde eksempler, der overrasker modellen og bør mærkes for effektiv finjustering. Derfor behandler vi sprogmodelleringstab som en proxy for klassificeringsusikkerhed. Med BERT udvikler vi en enkel strategi baseret på tabet af maskeret sprogmodellering, der minimerer mærkningsomkostningerne til tekstklassificering. Sammenlignet med andre basislinjer opnår vores tilgang højere nøjagtighed inden for mindre prøveudtagning iterationer og beregningstid.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktives Lernen zielt darauf ab, die Annotationskosten durch Auswahl der kritischsten Beispiele zu senken. Typischerweise hängt die aktive Lernstrategie vom Klassifizierungsmodell ab. Beispielsweise hängt Unsicherheitssampling von schlecht kalibrierten Modellvertrauenswerten ab. Im Kaltstart-Setting ist aktives Lernen aufgrund von Modellinstabilität und Datenknappheit unpraktisch. Glücklicherweise bietet das moderne NLP eine zusätzliche Informationsquelle: vortrainierte Sprachmodelle. Der Verlust vor dem Training kann Beispiele finden, die das Modell überraschen und für eine effiziente Feinabstimmung beschriftet werden sollten. Daher behandeln wir den Verlust der Sprachmodellierung als Proxy für Klassifizierungsunsicherheit. Mit BERT entwickeln wir eine einfache Strategie basierend auf dem maskierten Sprachmodellierungsverlust, die die Etikettierungskosten für die Textklassifizierung minimiert. Im Vergleich zu anderen Baselines erreicht unser Ansatz eine höhere Genauigkeit innerhalb weniger Sampling-Iterationen und Berechnungszeiten.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Η ενεργή μάθηση προσπαθεί να μειώσει το κόστος σχολιασμού επιλέγοντας τα πιο κρίσιμα παραδείγματα για την επισήμανση. Συνήθως, η στρατηγική ενεργού μάθησης εξαρτάται από το μοντέλο ταξινόμησης. Για παράδειγμα, η δειγματοληψία αβεβαιότητας εξαρτάται από τις κακώς βαθμονομημένες βαθμολογίες εμπιστοσύνης του μοντέλου. Στο πλαίσιο ψυχρής εκκίνησης, η ενεργή μάθηση δεν είναι πρακτική λόγω της αστάθειας του μοντέλου και της έλλειψης δεδομένων. Ευτυχώς, το σύγχρονο σύστημα παρέχει μια πρόσθετη πηγή πληροφοριών: προ-εκπαιδευμένα γλωσσικά μοντέλα. Η απώλεια προ-εκπαίδευσης μπορεί να βρει παραδείγματα που εκπλήσσουν το μοντέλο και θα πρέπει να επισημανθεί για αποδοτικό συντονισμό. Ως εκ τούτου, αντιμετωπίζουμε την απώλεια γλωσσικής μοντελοποίησης ως αντιπρόσωπο της αβεβαιότητας ταξινόμησης. Με την ανάπτυξη μιας απλής στρατηγικής βασισμένης στην απώλεια μοντελοποίησης γλώσσας που ελαχιστοποιεί το κόστος επισήμανσης για την ταξινόμηση κειμένου. Σε σύγκριση με άλλες γραμμές βάσης, η προσέγγισή μας επιτυγχάνει υψηλότερη ακρίβεια μέσα σε λιγότερες επαναλήψεις δειγματοληψίας και χρόνο υπολογισμού.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>El aprendizaje activo se esfuerza por reducir los costos de anotación al elegir los ejemplos más importantes para etiquetar. Por lo general, la estrategia de aprendizaje activo depende del modelo de clasificación. Por ejemplo, el muestreo de incertidumbre depende de puntuaciones de confianza del modelo mal calibradas. En el entorno de arranque en frío, el aprendizaje activo no es práctico debido a la inestabilidad del modelo y la escasez de datos. Afortunadamente, la PNL moderna proporciona una fuente de información adicional: modelos lingüísticos previamente entrenados. La pérdida previa al entrenamiento puede encontrar ejemplos que sorprenden al modelo y deben etiquetarse para un ajuste fino eficiente. Por lo tanto, tratamos la pérdida del modelado del lenguaje como un indicador de la incertidumbre de clasificación. Con BERT, desarrollamos una estrategia simple basada en la pérdida del modelado del lenguaje enmascarado que minimiza los costos de etiquetado para la clasificación de textos. En comparación con otras líneas de base, nuestro enfoque alcanza una mayor precisión en menos iteraciones de muestreo y tiempo de cálculo.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktiivne õpe püüab vähendada märgistamiskulusid, valides märgistamiseks kõige kriitilisemad näited. Tavaliselt sõltub aktiivse õppe strateegia klassifitseerimismudelist. Näiteks sõltub määramatuse proovivõtmine halvasti kalibreeritud mudeli usaldusskooridest. Külmkäivituses on aktiivne õppimine ebapraktiline mudeli ebastabiilsuse ja andmete nappuse tõttu. Õnneks pakub kaasaegne NLP täiendavat teabeallikat: eelkoolitud keelemudeleid. Treeningueelne kaotus võib leida näiteid, mis üllatavad mudelit ja tuleks märgistada tõhusa peenhäälestuse jaoks. Seetõttu käsitleme keele modelleerimise kadu klassifikatsiooni ebakindluse proxyina. BERTiga töötame välja lihtsa strateegia, mis põhineb maskeeritud keele modelleerimise kaotusel, mis minimeerib teksti klassifitseerimise märgistamiskulusid. Võrreldes teiste lähtejoontega saavutab meie lähenemine suurema täpsuse vähem proovivõtutiteratsioone ja arvutusaega.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>تلاش یادگیری فعال برای کاهش هزینه‌های اخطار با انتخاب بیشترین مثالهای مهم برای نقاشی است. معمولاً استراتژی یادگیری فعال روی مدل مختصر است. برای مثال، نمونه‌های مطمئنی بستگی به نمونه‌های مطمئن مدل‌های بد اندازه‌گیری شده است. در تنظیم شروع سرد، یادگیری فعالی به دلیل ناتوانی مدل و کمبود داده ها غیر دقیق است. خوشبختانه NLP مدرن یک منبع اطلاعات اضافه را پیش آموزش داد: مدل زبان پیش آموزش داده است. خسارت پیش آموزش می‌تواند مثالهایی را پیدا کند که مدل را سورپرایز می‌کند و باید برای تغییر‌سازی موثر برچسب شود. بنابراین، ما از دست دادن نمونه‌های زبان به عنوان یک پروکسی برای مطمئن شدن محرمانه رفتار می‌کنیم. با BERT، ما یک استراتژی ساده را بر اساس از دست دادن مدل زبان ماسک توسعه می‌کنیم که هزینه‌های برچسب برای برچسب‌بندی متن را کمترین می‌کند. در مقایسه با خطوط پایین دیگر، نزدیک ما به دقیق بالاتر در زمان نمونه‌های کمتر و محاسبات رسیده است.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktiivinen oppiminen pyrkii vähentämään merkintöjen kustannuksia valitsemalla kriittisimmät merkit. Tyypillisesti aktiivisen oppimisen strategia riippuu luokitusmallista. Esimerkiksi epävarmuuden näytteenotto riippuu huonosti kalibroiduista mallin luottamuspisteistä. Kylmäkäynnistyksessä aktiivinen oppiminen on epäkäytännöllistä mallin epävakauden ja datan niukkuuden vuoksi. Onneksi nykyaikainen NLP tarjoaa ylimääräisen tietolähteen: esikoulutetut kielimallit. Esiharjoitustappiosta löytyy esimerkkejä, jotka yllättävät mallin ja jotka tulisi merkitä tehokkaaseen hienosäätöön. Siksi käsittelemme kielimallinnuksen menetystä luokitusepävarmuuden proxynä. BERT:n avulla kehitämme yksinkertaisen strategian, joka perustuu maskeerattuun kielimallinnushäviöön, joka minimoi tekstin luokittelun merkintäkustannukset. Muihin lähtölinjoihin verrattuna lähestymistapamme saavuttaa suuremman tarkkuuden pienemmällä näytteenottoiteraatiolla ja laskentaajalla.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Active Learning s'efforce de réduire les coûts d'annotation en choisissant les exemples les plus critiques à étiqueter. Généralement, la stratégie d'apprentissage actif dépend du modèle de classification. Par exemple, l'échantillonnage d'incertitude dépend de scores de confiance du modèle mal calibrés. Dans le contexte du démarrage à froid, l'apprentissage actif n'est pas pratique en raison de l'instabilité du modèle et de la rareté des données. Heureusement, la PNL moderne fournit une source d'information supplémentaire : des modèles linguistiques préformés. La perte avant l'entraînement peut trouver des exemples qui surprennent le modèle et doivent être étiquetés pour un réglage fin efficace. Par conséquent, nous traitons la perte liée à la modélisation linguistique comme une approximation de l'incertitude de classification. Avec BERT, nous développons une stratégie simple basée sur la perte de modélisation du langage masqué qui minimise les coûts d'étiquetage pour la classification de texte. Comparée à d'autres niveaux de référence, notre approche atteint une plus grande précision en moins d'itérations d'échantillonnage et de temps de calcul.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Féachann foghlaim ghníomhach le costais anótála a laghdú trí na samplaí is tábhachtaí a roghnú le lipéadú. Go hiondúil, bíonn an straitéis foghlama gníomhaí ag brath ar mhúnla an rangú. Mar shampla, braitheann sampláil éiginnteachta ar scóir mhuiníne samhlacha atá calabraithe go dona. Sa suíomh fuarthosaithe, níl an fhoghlaim ghníomhach praiticiúil mar gheall ar éagobhsaíocht na samhla agus ganntanas sonraí. Go fortunately, soláthraíonn NLP nua-aimseartha foinse breise faisnéise: samhlacha teanga réamh-oilte. Is féidir leis an gcaillteanas réamhoiliúna samplaí a fháil a chuireann iontas ar an tsamhail agus ba cheart iad a lipéadú le haghaidh mionchoigeartaithe éifeachtach. Mar sin, caithfimid an caillteanas samhaltaithe teanga mar sheachfhreastalaí d’éiginnteacht aicmithe. Le BERT, forbraímid straitéis shimplí bunaithe ar an gcaillteanas samhaltaithe teanga chumhdaigh a íoslaghdaíonn costais lipéadaithe le haghaidh aicmiú téacs. I gcomparáid le bunlínte eile, sroicheann ár gcur chuige cruinneas níos airde laistigh de níos lú atriallta samplála agus am ríomh.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ana iya amfani da yin amfani da shi, sai ya yi ƙwauro ga ƙarai masu nau'in zartar da alama zuwa label. Ana iya ƙayyade, akan da ake haɗi cikin shirin fasallar. Misali, misali da ba'a sani ba yana ƙayyade ga score na tsarin ayuka da aka yi raba kalmar. Idan an daidaita kwanan farawa, za'a yi amfani da karatun aiki ba ne mai amfani da kwamfyutan misali da haske na data. Bayan bushãra, NLP na samar da wani source na ƙaranci wa information: misãlai na zaman-tsari. Takacin da ya gabãta, yana iya sãmun misãlai, da za a yi mãmãki game da shirin ayuka kuma a kamata a yi masa sunan wa tunkuɗawa masu da inganci. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. Da BERT, za mu buɗe wani takwai mai sauƙi a kan misalin misalin harshen da aka rufe shi da ƙaranci masu yin alama ga fasalin matsayin. Ga sami da misalin wasu misalin, hanyoyinmu yana zuwa tsari mafi girma a lokacin da aka samu masu ƙaranci da lokacin lissafi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>הלימודים הפעילים מתאמצים להפחית עלות הערות על ידי בחירת הדוגמאות הקריטיות ביותר לתווית. בדרך כלל, אסטרטגיה הלימודים הפעילה תלויה במודל ההקלטה. לדוגמא, דוגמאות לא בטוחות תלויות בתוצאות ביטחון מודל קליבריות גרועות. בהתחלה קרה, הלימודים הפעילים אינם פעילים בגלל חוסר יציבות מודל וחסר נתונים. למרבה המזל, NLP מודרני מספק מקור נוסף של מידע: דוגמנים לשפה מאומנים מראש. הפסד לפני האימונים יכול למצוא דוגמאות שמפתיעות את הדוגמא והיא צריכה להיות מוצבת לתאים יעיל. לכן, אנחנו מתייחסים לאובדל השפה כפרוקסי ללא בוודאות מסווג. עם BERT, אנחנו מפתחים אסטרטגיה פשוטה מבוססת על אובדן מודל שפת מסוכנת שמפחיד את עלויות התיקוי לקליפסיה טקסט. בהשוואה לקווי בסיס אחרים, הגישה שלנו מגיעה לדיוקת גבוהה יותר בתוך פחות חוזרות דגימות וזמן החישוב.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>सक्रिय शिक्षा लेबल करने के लिए सबसे महत्वपूर्ण उदाहरणों का चयन करके एनोटेशन लागत को कम करने का प्रयास करती है। आमतौर पर, सक्रिय सीखने की रणनीति वर्गीकरण मॉडल पर आकस्मिक होती है। उदाहरण के लिए, अनिश्चितता का नमूना खराब कैलिब्रेटेड मॉडल आत्मविश्वास स्कोर पर निर्भर करता है। कोल्ड-स्टार्ट सेटिंग में, मॉडल अस्थिरता और डेटा की कमी के कारण सक्रिय शिक्षा अव्यावहारिक है। सौभाग्य से, आधुनिक एनएलपी जानकारी का एक अतिरिक्त स्रोत प्रदान करता है: पूर्व-प्रशिक्षित भाषा मॉडल। पूर्व-प्रशिक्षण हानि ऐसे उदाहरण पा सकती है जो मॉडल को आश्चर्यचकित करते हैं और कुशल ठीक-ट्यूनिंग के लिए लेबल किया जाना चाहिए। इसलिए, हम वर्गीकरण अनिश्चितता के लिए एक प्रॉक्सी के रूप में भाषा मॉडलिंग हानि का इलाज करते हैं। BERT के साथ, हम नकाबपोश भाषा मॉडलिंग हानि के आधार पर एक सरल रणनीति विकसित करते हैं जो पाठ वर्गीकरण के लिए लेबलिंग लागत को कम करता है। अन्य आधार रेखाओं की तुलना में, हमारा दृष्टिकोण कम नमूना पुनरावृत्तियों और गणना समय के भीतर उच्च सटीकता तक पहुंचता है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktivni učenje pokušava smanjiti troškove annotacije birajući najkritičnije primjere za etiketu. Obično je aktivna strategija učenja povezana s klasifikacijskim modelom. Na primjer, uzorak neodređenosti ovisi o lošim kalibriranim rezultatima poverenja modela. U nastavku hladnopočetka aktivno učenje je neprikladno zbog modela nestabilnosti i nedostatka podataka. Srećom, moderni NLP pruža dodatni izvor informacija: predobučeni jezički modeli. Gubitak predobuke može pronaći primjere koji iznenađuju model i trebaju se označiti za učinkovito ispravno ispravljanje. Stoga, tretiramo gubitak jezika modeliranja kao proksi za klasifikaciju nesigurnosti. S BERT-om razvijamo jednostavnu strategiju na temelju gubitka maskiranog jezika koji minimizira troškove označavanja za klasifikaciju teksta. U usporedbi s drugim osnovnim linijama, naš pristup stiže do višeg preciznosti unutar manjih iteracija i vremena računala.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Az aktív tanulás arra törekszik, hogy csökkentse a jegyzetelési költségeket a legkritikusabb példák kiválasztásával, amelyeket címkézni szeretne. Az aktív tanulási stratégia jellemzően az osztályozási modelltől függ. A bizonytalansági mintavétel például a rosszul kalibrált modell konfidencia pontszámoktól függ. Hidegindítási körülmények között az aktív tanulás a modell instabilitása és az adathűség miatt nem praktikus. Szerencsére a modern NLP további információforrást nyújt: az előképzett nyelvi modelleket. Az edzés előtti veszteség olyan példákat találhat, amelyek meglepik a modellt, és címkézni kell a hatékony finomhangolás érdekében. Ezért a nyelvmodellezési veszteséget a besorolási bizonytalanság proxyként kezeljük. A BERT segítségével egy egyszerű stratégiát dolgozunk ki a maszkos nyelvmodellezési veszteség alapján, amely minimalizálja a szövegosztályozás címkézési költségeit. Más alapvonalakhoz képest a megközelítésünk nagyobb pontosságot ér el kevesebb mintavételi iteráció és számítási idő alatt.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ակտիվ ուսումնասիրությունը փորձում է նվազեցնել annoտացիայի արժեքները՝ ընտրելով պիտակ տալու ամենակարևոր օրինակները: Սովորաբար ակտիվ ուսուցման ռազմավարությունը կախված է դասակարգման մոդելի վրա: For instance, uncertainty sampling depends on poorly calibrated model confidence scores. Սառը սկզբնական միջավայրում ակտիվ սովորելը անպրակտիկ է մոդելի անկայունության և տվյալների բացակայության պատճառով: Բարեբախտաբար, ժամանակակից ՆԼՊ-ը պարունակում է ավելին տեղեկատվության աղբյուր՝ նախապատրաստված լեզվի մոդելներ: Նախապատրաստման կորուստը կարող է գտնել օրինակներ, որոնք զարմանում են մոդելը և պիտի պիտակում են արդյունավետ բարելավման համար: Այսպիսով, մենք վերաբերում ենք լեզվի մոդելավորման կորստին որպես դասակարգման անորոշության մեծամասնություն: ԲԵՌԹ-ի միջոցով մենք ստեղծում ենք մի պարզ ռազմավարություն, որը հիմնված է թաքնված լեզվի մոդելավորման կորուստի վրա, որը նվազեցնում է տեքստի դասակարգման պիտակների արժեքը: Համեմատելով այլ հիմնական գծերի հետ, մեր մոտեցումը հասնում է ավելի ճշգրտության ավելի քիչ նմուշների վերացման և հաշվարկների ժամանակի ընթացքում:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Belajar aktif berusaha untuk mengurangi biaya anotasi dengan memilih contoh yang paling kritis untuk label. Biasanya, strategi belajar aktif tergantung pada model klasifikasi. Contohnya, sampel ketidakpastian tergantung pada skor kepercayaan model yang kurang kalibrasi. Dalam permulaan dingin, pembelajaran aktif tidak praktis karena ketidakstabilan model dan kekurangan data. Untungnya, NLP modern menyediakan sumber informasi tambahan: model bahasa yang terlatih sebelumnya. Kehilangan prapelatihan dapat menemukan contoh yang mengejutkan model dan harus ditabel untuk penyesuaian efisien. Oleh karena itu, kita memperlakukan kehilangan model bahasa sebagai proxi untuk ketidakpastian klasifikasi. Dengan BERT, kita mengembangkan strategi sederhana berdasarkan kehilangan model bahasa bertopeng yang mengurangi biaya label untuk klasifikasi teks. dibandingkan dengan garis dasar lainnya, pendekatan kita mencapai akurasi yang lebih tinggi dalam kurang mengumpulkan iterasi dan waktu komputasi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>L'apprendimento attivo mira a ridurre i costi di annotazione scegliendo gli esempi più critici da etichettare. In genere, la strategia di apprendimento attivo è subordinata al modello di classificazione. Ad esempio, il campionamento dell'incertezza dipende da punteggi di confidenza del modello scarsamente calibrati. Nell'ambiente di avvio a freddo, l'apprendimento attivo è impraticabile a causa dell'instabilità dei modelli e della scarsità di dati. Fortunatamente, il moderno PNL fornisce un'ulteriore fonte di informazioni: modelli linguistici pre-formati. La perdita pre-allenamento può trovare esempi che sorprendono il modello e dovrebbe essere etichettata per una messa a punto efficiente. Pertanto, trattiamo la perdita di modellazione linguistica come un proxy per l'incertezza di classificazione. Con BERT sviluppiamo una strategia semplice basata sulla perdita di modellazione del linguaggio mascherato che minimizza i costi di etichettatura per la classificazione del testo. Rispetto ad altre linee di base, il nostro approccio raggiunge una maggiore precisione entro meno iterazioni di campionamento e tempi di calcolo.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>アクティブラーニングは、ラベル付けする最も重要な例を選択することで、注釈コストを削減するように努めます。典型的には、能動的学習戦略は分類モデルに依存する。例えば、不確実性サンプリングは、較正が不十分なモデル信頼度スコアに依存する。コールドスタートの設定では、モデルの不安定性とデータ不足のためにアクティブラーニングは実用的ではありません。幸いなことに、現代のNLPは、事前にトレーニングされた言語モデルという追加の情報源を提供します。事前トレーニングの損失は、モデルを驚かせる例を見つけることができ、効率的な微調整のためにラベルを付ける必要があります。したがって、モデリング言語の損失を分類の不確実性の代用として扱う。BERTでは、テキスト分類のラベリングコストを最小限に抑える、マスクされた言語モデリングロスに基づいたシンプルな戦略を開発しています。他のベースラインと比較して、当社のアプローチは、より少ないサンプリング反復と計算時間の中でより高い精度に達します。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>politenessoffpolite"), and when there is a change ("assertivepoliteness Slackfree sample Nang mulai-Mulai, akeh iso mulai aksi ora dadi ngono model kuwi kalem dong. Lahor sedhaya, nambah NLP ngewehke sumelang informasi tambah: model sing gak banter language. Ngubah-ngubah layang kelas nyong bisa mbut-bisa kuwi diantepakan seneng pisan neng model kuwi mau kudu etiket nggawe ngubah ora nggawe ngubah lah BERT, dino sing luwih akeh sistem sing sampeyan luwih dumadhi kapan bangsane sampeyan mrogram kuwi nggawe barang nggawe gerakan kanggo Kemerdekaan Teks Tambah karo akeh liyané sing paling sampeyan, dadi diagonal terus dakasar kelas nêmên karo nggawe gerakan ukusun.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>აქტიური სწავლების ძალიან, რომელიც აკეთებულიან უფრო კრიტიკური მაგალითების შესაძლებელად დააკეთება. ტიპულად, აქტიური სწავლის სტრატიგია კლასიფიკაციის მოდელზე კონტეგენტია. მაგალითად, შეცდომარეობის გამოყენება დარწმუნებულია მაგალითად კალიბრებულია მოდელის დარწმუნება. მოდელური დაწყვეტილება და მონაცემების მუშაობის შესაძლებლობით, აქტიური სწავლება არაფექტიურია. შესაბამისიდ, მოდინარე NLP დამატებული ინფორმაციის გამოსახულება: წინ შესწავლა ენის მოდელები. წინ განვითარება შეიძლება იპოვოთ მაგალითები, რომელიც მოდელის შეცდომა და უნდა იყოს ეფექტიური კონფიგურაციისთვის მაგალითი. ამიტომ, ჩვენ მუშაობით ენის მოდელის დასრულება როგორც კლასიფიკაციის უცნობიერება. BERT-ის შესახებ, ჩვენ განვითარებთ მასკრებული ენის მოდელური დაკავშირება, რომელიც ტექსტის კლასიფიკაციაციისთვის მასკრებული ენის მოდელური დაკავშირებაზე, ჩვენი მიღება უფრო მეტი ფესური ხაზების შემდგომარებით, ჩვენი მიღება უფრო მეტი სიმართლეობა, რომელიც ახლა გამოიყენება იტერაციების და კო</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Белсенді оқыту үшін белгілеу мәселелерін шектеу үшін ең жақсы мысалдарды таңдап тұрады. Әдетте, белсенді оқыту стратегиясы классификациясының үлгісіне байланысты. Мысалы, мәліметтерді түрлендіру керек калибрлеу үлгісінің сенімдік нәтижелеріне тәуелді. Шығыс бастау параметрінде, белсенді оқыту үлгісі дұрыс емес және деректердің қажеттілігі себебі. Қазіргі NLP қосымша мәліметтің көзі келтіреді: алдын- оқылған тіл үлгілері. Алдыңғы оқыту жоғалуы үлгісін күліп, оқыту үшін мәселелерді таба алады. Сондықтан, біз тілді модельдің жоғалуын классификациялық тәуелсіздік деп қалаймыз. BERT арқылы, мәтін классификациясының жарлықтың бағаларын төмендету үшін қарапайым стратегия құрамыз. Басқа негізгі сызықтарымен салыстырып, біздің тәсіліміздің дұрыстығын төмендету мен есептеу уақытында артық болады.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>주동적으로 학습은 가장 관건적인 예시를 선택하여 표시를 함으로써 주석 원가를 낮추려고 노력한다.일반적으로 능동 학습 전략은 분류 모델에 달려 있다.예를 들어 불확실성 표본 추출은 교정 불량 모델의 신뢰도 점수에 달려 있다.냉각 가동 환경에서 모델이 불안정하고 데이터가 부족하기 때문에 주동적으로 학습하는 것은 비현실적이다.다행히도 현대 NLP는 사전 훈련된 언어 모델을 추가로 제공했다.훈련 전의 손실은 모델을 놀라게 하는 예를 찾을 수 있으며 효과적인 마이크로스피커로 표시해야 한다.따라서 우리는 언어 모델링 손실을 분류 불확실성의 대리로 본다.BERT를 통해 우리는 복면 언어 모델링 손실을 바탕으로 하는 간단한 전략을 개발했는데 이 전략은 텍스트 분류의 표기 비용을 최소화할 수 있다.다른 기선에 비해 우리의 방법은 더욱 적은 샘플링 교체 횟수와 계산 시간 내에 더욱 높은 정밀도에 이르렀다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktyvus mokymasis siekia sumažinti anotacijos išlaidas pasirinkdamas svarbiausius ženklinimo pavyzdžius. Paprastai aktyvaus mokymosi strategija priklauso nuo klasifikavimo modelio. Pavyzdžiui, neapibrėžtumo mėginių ėmimas priklauso nuo prastai kalibruotų modelio patikimumo rezultatų. Šalto pradžios sąlygomis aktyvus mokymasis nėra praktiškas dėl modelio nestabilumo ir duomenų trūkumo. Laimei, šiuolaikinis NLP teikia papildomą informacijos šaltinį: iš anksto parengtus kalbų modelius. Išankstinio mokymo praradimas gali rasti pavyzdžių, kurie nustebina model į ir turėtų būti ženklinami veiksmingam tikslinimui. Todėl kalbų modeliavimo praradimą vertiname kaip klasifikacijos neapibrėžtumo įrodymą. Be BERT, parengiame paprastą strategiją, pagrįstą paslėpta kalbų modeliavimo praradimu, kuris sumažina teksto klasifikavimo ženklinimo išlaidas. Palyginti su kitomis bazinėmis linijomis, mūsų metodas pasiekia didesnį tikslumą per mažiau mėginių ėmimo kartojimų ir skaičiavimo laiko.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Active learning strives to reduce annotation costs by choosing the most critical examples to label. Обично, активната стратегија за учење е зависна од моделот на класификација. На пример, примерувањето на несигурноста зависи од лошо калибрираните оценки на доверба на моделот. Во системот на ладен почеток, активното учење е непрактично поради нестабилноста на моделот и недостатокот на податоци. За среќа, модерната НЛП обезбедува дополнителен извор на информации: предобучени јазички модели. Изгубите од предобуката можат да најдат примери кои го изненадуваат моделот и треба да бидат означени за ефикасно финетизирање. Затоа, ја третираме загубата на моделирањето на јазикот како прокси за несигурност во класификацијата. Со БЕРТ, развиваме едноставна стратегија базирана на маскираната загуба на моделирање на јазикот која ги минимизира трошоците на етикетирање за класификација на текст. Во споредба со другите основни линии, нашиот пристап достигнува повисока точност во помалку примероци и времето на пресметка.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ലേബിളിലേക്കുള്ള ഏറ്റവും പ്രധാനപ്പെട്ട ഉദാഹരണങ്ങള്‍ തെരഞ്ഞെടുക്കുന്നതിനാല്‍ സജ്ജീവമായ പഠിക്കുന സാധാരണ, സജ്ജീവമായ പഠിക്കുന്ന പദ്ധതി ക്ലാസ്ഫിക്കല്‍ മോഡലില്‍ ബന്ധപ്പെടുന്നു. ഉദാഹരണത്തിനായി, തിരിച്ചറിയാത്ത സമാമ്പിള്‍ മോഡലിന്റെ വിശ്വാസ സ സ്കോര്‍സിനെ മാത്രമാണ് ആശ്രയിച്ചത്. തണുപ്പ് തുടങ്ങുന്ന ക്രമീകരണത്തില്‍ സജ്ജീവമായ പഠനത്തില്‍ പ്രധാനപ്പെടുത്തുന്നത് മോഡല്‍ സ്ഥിരതയും ഡേ ഭാഗ്യവശാല്‍, ആധുനിക NLP വിവരങ്ങളുടെ കൂടുതല്‍ വിവരങ്ങള്‍ നല്‍കുന്നു: മുന്‍പ് പരിശീലന ഭാഷ മോഡലുകള്‍. പരിശീലനത്തിനു മുമ്പുള്ള നഷ്ടത്തിന് ഉപമകള്‍ കണ്ടെത്താന്‍ സാധിക്കുന്നു. മോഡലിനെ അത്ഭുതപ്പെടുത്തുന്നു. അതി Therefore, we treat the language modeling loss as a proxy for classification uncertainty. ബെര്‍ട്ടിയോടൊപ്പം, മുഖ്യഭാഷ മോഡലിങ്ങിന്റെ നഷ്ടത്തിന്റെ അടിസ്ഥാനത്ത് നമ്മള്‍ ഒരു സാധാരണമായ ഒരു ക്രമത്തിന്റെ പ്ര മറ്റുള്ള അടിസ്ഥാനങ്ങളോടൊപ്പം നമ്മുടെ അടിസ്ഥാനം കൂടുതല്‍ കൂടുതല്‍ കൃത്യമായി എത്തുന്നു. നമ്മുടെ സമാധാനം കൂടുതല</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Хамгийн чухал жишээг сонгоход сэтгэл хөдлөлийн зардалыг багасгах гэж хичээдэг. Олон тохиолдолд, актив суралцах стратеги нь хуваалцах загвараас хамааралтай. Жишээлбэл, тодорхойгүй байдлыг тодорхойлох нь зөвхөн тодорхойлогдсон загварын итгэл итгэл үндсэнээс хамааралтай. Хямдан эхлэх хэмжээнд, актив суралцах нь загварын тогтворгүй байдал болон өгөгдлийн хамааралтай учраас буруу юм. Харамсалтай нь орчин үеийн NLP нь нэмэлт мэдээллийн эх үүсвэр хангадаг: өмнө сургалтын хэл загвар. Өмнөх сургалтын алдагдал нь загварыг гайхшруулж, үр дүнтэй сайжруулах хэрэгтэй жишээ олж болно. Тиймээс бид хэл загварын алдагдлыг хуваалцах тодорхойгүй байдлыг прокси гэж үздэг. BERT-тэй хамт бид хэлний загварын алдагдлын үндсэн энгийн стратегийг хөгжүүлнэ. Өөр суурь шулуунтай харьцуулахад бидний арга зам нь бага хэмжээний жишээ болон тооцоолох цаг дотор илүү тодорхой байдаг.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Pembelajaran aktif berusaha untuk mengurangi biaya anotasi dengan memilih contoh yang paling kritik untuk label. Biasanya, strategi pembelajaran aktif bergantung pada model klasifikasi. Contohnya, pengumpulan ketidakpastian bergantung pada skor kepercayaan model berkalibrat teruk. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Untungnya, NLP modern menyediakan sumber maklumat tambahan: model bahasa pra-dilatih. Kehilangan praselatihan boleh mencari contoh yang mengejutkan model dan patut ditabel untuk penyesuaian yang efisien. Oleh itu, kita memperlakukan kehilangan model bahasa sebagai proksi untuk ketidakpastian klasifikasi. Dengan BERT, kita mengembangkan strategi sederhana berdasarkan kehilangan model bahasa bertopeng yang mengurangi biaya label untuk kelasukan teks. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>It-tagħlim attiv ifittex li jnaqqas l-ispejjeż tal-annotazzjoni billi jagħżel l-aktar eżempji kritiċi li għandhom jiġu ttikkettati. Tipikament, l-istrateġija ta’ tagħlim attiv hija kontinġenti fuq il-mudell ta’ klassifikazzjoni. Pereżempju, it-teħid ta’ kampjuni ta’ inċertezza jiddependi fuq punteġġi ta’ kunfidenza tal-mudell ikkalibrati ħażin. Fl-ambjent ta’ startjar kiesaħ, it-tagħlim attiv mhuwiex prattiku minħabba l-instabbiltà tal-mudell u l-iskarsezza tad-dejta. Fortunatament, il-NLP moderna tipprovdi sors addizzjonali ta’ informazzjoni: mudelli lingwistiċi mħarrġa minn qabel. It-telf ta’ qabel it-taħriġ jista’ jsib eżempji li jissuspettaw il-mudell u għandhom jiġu ttikkettati għal aġġustament effiċjenti. Għalhekk, jittrattaw it-telf tal-mudellar tal-lingwi bħala proxy għall-inċertezza tal-klassifikazzjoni. Bil-BERT, a ħna niżviluppaw strateġija sempliċi bbażata fuq telf ta' mudellar tal-lingwi maskrat li jimminimizza l-ispejjeż tat-tikkettar għall-klassifikazzjoni tat-test. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Actief leren streeft ernaar om annotatiekosten te verlagen door de meest kritische voorbeelden te kiezen om te labelen. Meestal is de actieve leerstrategie afhankelijk van het classificatiemodel. Bijvoorbeeld, onzekerheidsbemonstering hangt af van slecht gekalibreerde model confidentie scores. In de koude start omgeving is actief leren onpraktisch vanwege modelstabiliteit en dataschaarste. Gelukkig biedt moderne NLP een extra informatiebron: voorgetrainde taalmodellen. Het pre-training verlies kan voorbeelden vinden die het model verrassen en moet worden gelabeld voor efficiënte fine-tuning. Daarom behandelen we het verlies van taalmodellering als een proxy voor classificatieonzekerheid. Met BERT ontwikkelen we een eenvoudige strategie op basis van het gemaskeerde taalmodelleringsverlies dat de etiketteringskosten voor tekstclassificatie minimaliseert. Vergeleken met andere baselines, bereikt onze aanpak hogere nauwkeurigheid binnen minder sampling iteraties en berekeningstijd.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktiv læringsstreng for å redusera notasjonskostnader ved å velja dei mest kritiske eksemplane som skal merke. Vanlegvis er det aktive læringsstrategiet avhengig av klassifikasjonsmodulen. For eksempel avhengig av uverkjende prøver på feil kalibrerte tiltruksscorer for modellen. I kald-startinnstillinga er det aktive læring ugyldig på grunn av modellen for instabilitet og data feil. For godt, modern NLP tilbyr ei ekstra kjelde for informasjon: først trengte språk- modeller. Førehandsvisinga kan finna eksemplar som overraskar modellen og skal merke til effektivt finning. Det er derfor vi behandlar tapt på språk som mellomtenar for klassifikasjons usikkerhet. Med BERT utviklar vi ein enkel strategi basert på tapt av maskerte språk som minimerer merkelappaste for tekstklassifikasjon. Sammenlignet med andre grunnlinjer, når tilnærminga vårt når høgare nøyaktighet innenfor mindre samplingsiterasjonar og rekningstid.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktywne uczenie się dąży do obniżenia kosztów adnotacji poprzez wybór najbardziej krytycznych przykładów do etykietowania. Zazwyczaj strategia aktywnego uczenia się zależy od modelu klasyfikacji. Na przykład próbkowanie niepewności zależy od słabo skalibrowanych wyników zaufania modelu. W warunkach zimnego startu aktywne uczenie się jest niepraktyczne ze względu na niestabilność modelu i niedobór danych. Na szczęście nowoczesny NLP zapewnia dodatkowe źródło informacji: wstępnie przeszkolone modele językowe. Strata przedtreningowa może znaleźć przykłady, które zaskakują model i powinny być oznaczone dla efektywnego dostrajania. Dlatego też utratę modelowania językowego traktujemy jako zastępcę niepewności klasyfikacji. Z BERT opracowujemy prostą strategię opartą na zamaskowanej strategii modelowania języka, która minimalizuje koszty etykietowania dla klasyfikacji tekstu. W porównaniu z innymi liniami bazowymi nasze podejście osiąga większą dokładność w mniejszym czasie iteracji próbkowania i obliczeń.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>O aprendizado ativo se esforça para reduzir os custos de anotação escolhendo os exemplos mais críticos para rotular. Normalmente, a estratégia de aprendizagem ativa depende do modelo de classificação. Por exemplo, a amostragem de incerteza depende de pontuações de confiança do modelo mal calibradas. Na configuração de partida a frio, o aprendizado ativo é impraticável devido à instabilidade do modelo e à escassez de dados. Felizmente, a PNL moderna fornece uma fonte adicional de informações: modelos de linguagem pré-treinados. A perda pré-treinamento pode encontrar exemplos que surpreendem o modelo e devem ser rotulados para um ajuste fino eficiente. Portanto, tratamos a perda de modelagem de linguagem como uma proxy para incerteza de classificação. Com o BERT, desenvolvemos uma estratégia simples baseada na perda de modelagem de linguagem mascarada que minimiza os custos de rotulagem para classificação de texto. Em comparação com outras linhas de base, nossa abordagem atinge maior precisão com menos iterações de amostragem e tempo de computação.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Învățarea activă se străduiește să reducă costurile de adnotare prin alegerea celor mai critice exemple de etichetat. De obicei, strategia de învățare activă este condiționată de modelul de clasificare. De exemplu, eșantionarea incertitudinii depinde de scorurile de încredere ale modelului slab calibrate. În condițiile de pornire la rece, învățarea activă nu este practică din cauza instabilității modelului și a deficitului de date. Din fericire, PNL modern oferă o sursă suplimentară de informații: modele lingvistice pre-instruite. Pierderea pre-antrenament poate găsi exemple care surprind modelul și ar trebui etichetate pentru o reglare fină eficientă. Prin urmare, tratăm pierderea modelării limbajului ca un proxy pentru incertitudinea clasificării. Cu BERT, dezvoltăm o strategie simplă bazată pe pierderea modelării limbajului mascat, care minimizează costurile de etichetare pentru clasificarea textului. În comparație cu alte linii de referință, abordarea noastră atinge o precizie mai mare în mai puține iterații de eșantionare și timp de calcul.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Активное обучение направлено на снижение затрат на аннотации путем выбора наиболее важных примеров для маркировки. Как правило, стратегия активного обучения зависит от модели классификации. Например, выборка неопределенностей зависит от плохо откалиброванных показателей достоверности модели. В условиях &lt; &lt; холодного старта > > активное обучение нецелесообразно ввиду нестабильности моделей и нехватки данных. К счастью, современный NLP предоставляет дополнительный источник информации: предварительно обученные языковые модели. Предварительная потеря в обучении может найти примеры, которые удивляют модель и должны быть помечены для эффективной тонкой настройки. Поэтому мы рассматриваем потерю языкового моделирования как косвенный фактор неопределенности классификации. С БЕРТОМ мы разрабатываем простую стратегию, основанную на потере моделирования маскированного языка, которая минимизирует затраты на маркировку для классификации текста. По сравнению с другими базовыми линиями, наш подход достигает более высокой точности в пределах меньшего количества итераций выборки и времени вычислений.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>සක්‍රිය ඉගෙනීමේ ප්‍රයෝජනය ලේබුල් එක්ක ගොඩක් විශේෂ උදාහරණයක් තෝරාගන්න. සාමාන්‍ය විදියට, සක්‍රිය ඉගෙනීමේ සාධාන්‍ය විද්‍යාපනය විශේෂ විද්‍යාපනය වෙනුවෙන්. උදාහරණයෙන්, නිර්දේශතාවක් සැම්පල් එක අවශ්‍ය විශ්වාස විශ්වාස විශ්වාස විශ්වාස විශ්වාස සීන්ඩ- පටන් ගන්න සැකසුමේදී, සක්‍රිය ඉගෙනගන්නේ නිර්මාණය සහ දත්ත අවස්ථාවක් නිසා නිසා. සතුටුයි, අද්‍යුතික NLP විශාල තොරතුරු සම්පූර්ණයක් දෙනවා: ප්‍රීක්ෂණිත භාෂා මොඩේල්. ප්‍රධානය නැතිවෙන්න පුළුවන් උදාහරණයක් හොයාගන්න, මොඩේල් එක පුදුමක් වෙන්න, ඒ වගේම ප්‍රශ්නයක් ඉතින්, අපි භාෂාව ප්‍රමාණයක් නැති වෙනුවෙන් ප්‍රොක්සියක් විදිහට පරීක්ෂා කරනවා. BERT එක්ක, අපි සරල භාෂාව ප්‍රමාණය සඳහා සාමාන්‍ය ව්‍යාපෘතියක් විශ්වාස කරනවා. මුහුණු භාෂාව ප්‍රමාණය අනිත් මූලිකාව සමඟ සම්පූර්ණයෙන්, අපේ ප්‍රවේශනය තරම් වැඩි සැම්ප්ලික් සහ ගණනාකරණ වෙලාවට අඩුවෙන</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktivno učenje si prizadeva zmanjšati stroške označevanja z izbiro najpomembnejših primerov za označevanje. Običajno je strategija aktivnega učenja odvisna od klasifikacijskega modela. Vzorčenje negotovosti je na primer odvisno od slabo umerjenih ocen zaupanja modela. Pri hladnem zagonu je aktivno učenje nepraktično zaradi nestabilnosti modela in pomanjkanja podatkov. Na srečo sodobna NLP zagotavlja dodaten vir informacij: vnaprej usposobljeni jezikovni modeli. Izguba pred usposabljanjem lahko najde primere, ki presenetijo model in jih je treba označiti za učinkovito fino nastavitev. Zato izgubo jezikovnega modeliranja obravnavamo kot približek klasifikacijske negotovosti. Z BERT razvijamo preprosto strategijo, ki temelji na maskirani izgubi jezikovnega modeliranja, ki zmanjšuje stroške označevanja za klasifikacijo besedila. V primerjavi z drugimi osnovnimi linijami naš pristop doseže večjo natančnost v manj iteracijah vzorčenja in času izračuna.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Waxbarashada waxqabadka ah wuxuu ku dadaalaa in uu hoos u dhigo kharashka caafimaadka marka uu doorto tusaalaha ugu muhiimsan ee calaamadda. Sida caadiga ah qorshaha waxbarashada waxqabadka ah waxaa ku xiran qaababka fasalka. Tusaale ahaan sameynta aan la garanayn waxay ku xiran tahay noocyada kalsoonaanta. Xafiiska bilowga qabowga ah waxbarashada waxqabadka ah waa mid aan suurtogal ahayn, sababtoo ah qallafsanaanshaha sameynta iyo baahida macluumaadka. Nasiib weyn NLP wuxuu bixiyaa noocyo kale oo macluumaad dheeraad ah: noocyada afka hore oo la tababaray. Lacagta waxbarashada ka horeysa waxay heli karaan tusaale ahaan loo yaabo tusaale ahaan, kaas oo lagu magacaabi karo hab-dhismo oo faa’iido ah. Sidaa darteed waxaynu u isticmaalnaa khasaarada tusaale ahaan tijaabada fasaxa. BERT waxaynu horumarinaynaa qoraal fudud oo ku saleysan tusaale ahaan lumbarka luuqada maskaxda ah oo ugu yaraanaya kharashka fasaxa qoraalka. Isbarbardhig u dhigma saldhigyada kale, dhaqdhaqaalahayagu wuxuu gaadhaa saxda aad u sareeya waqtiga ka yar sameynta iyo wakhtiga xisaabinta.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mësimi aktiv përpiqet të reduktojë kostot e anotacionit duke zgjedhur shembujt më kritikë për të etiketuar. Zakonisht, strategjia aktive e mësimit është e varur nga modeli i klasifikimit. Për shembull, mossiguria varet nga rezultatet e konfidencisë të modelit të kalibruar keq. Në ambientin e fillimit të ftohtë, mësimi aktiv është i paprektishëm për shkak të paqëndrueshmërisë së modelit dhe mungesës së të dhënave. Fatmirësisht, NLP moderne ofron një burim shtesë informacioni: modele gjuhësh të paratrajnuara. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Prandaj, ne trajtojmë humbjen e modelimit gjuhësor si një proxy për pasigurinë klasifikuese. Me BERT, ne zhvillojmë një strategji të thjeshtë bazuar në humbjen e modelimit të gjuhës së maskuar që minimizon kostot e etiketave për klasifikimin e tekstit. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktivni učenje pokušava smanjiti troškove annotacije birajući najkritičnije primjere za etiketu. Tipično, aktivna strategija učenja je povezana sa klasifikacijskim modelom. Например, несигурност одбора проби зависи от лоших калибрираних показатела довери модела. U nastavku hladnopoèetka, aktivno učenje je neprikladno zbog modela nestabilnosti i nedostatka podataka. Srećom, moderni NLP pruža dodatni izvor informacija: predobučeni jezički modeli. Gubitak pre obuke može pronaći primjere koji iznenađuju model i trebaju biti označeni za efikasno ispravljanje. Stoga, tretiramo gubitak jezika modela kao proksi za klasifikaciju nesigurnosti. Sa BERT-om razvijamo jednostavnu strategiju na osnovu gubitka maskiranog jezika koji minimizira troškove označavanja za klasifikaciju teksta. U usporedbi sa drugim osnovnim linijama, naš pristup stiže do višeg preciznosti unutar manjih iteracija i vremena računala.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Aktivt lärande strävar efter att minska annoteringskostnaderna genom att välja de mest kritiska exemplen att märka. Vanligtvis är strategin för aktivt lärande beroende av klassificeringsmodellen. Exempelvis beror osäkerhetsprovtagning på dåligt kalibrerade modellkonfidenspoäng. I kallstartssituationen är aktivt lärande opraktiskt på grund av modellinstabilitet och databrist. Lyckligtvis ger modern NLP ytterligare en informationskälla: förberedda språkmodeller. Förlusten före träning kan hitta exempel som överraskar modellen och bör märkas för effektiv finjustering. Därför behandlar vi språkmodelleringsförlusten som en proxy för klassificeringsosäkerhet. Med BERT utvecklar vi en enkel strategi baserad på förlust av maskerad språkmodellering som minimerar märkningskostnader för textklassificering. Jämfört med andra baslinjer uppnår vårt tillvägagångssätt högre noggrannhet inom mindre provtagningsiterationer och beräkningstid.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kujifunza haraka hujitahidi kupunguza gharama za uchunguzi kwa kuchagua mifano muhimu zaidi ya kuandika. Kwa kawaida, mkakati wa kujifunza unahusiana na muundo wa usafi. Kwa mfano, sampuli isiyo na uhakika inategemea vipimo vya matumaini vibaya vilivyotajwa na muundo wa imani. Katika mazingira ya mwanzo wa baridi, kujifunza kwa haraka ni jambo lisilo sahihi kutokana na uwezekano wa mifano na ukosefu wa taarifa. Kwa bahati nzuri, NLP wa sasa anatoa chanzo cha taarifa zaidi: mifano ya lugha zilizofunzwa kabla. Tatizo la mafunzo ya kabla linaweza kutafuta mifano ambayo inashangaza model na inapaswa kuchaguliwa kwa mafunzo yenye ufanisi. Kwa hiyo, tunakabiliwa na kupoteza mifano ya lugha kama mtihani wa utambulisho usio na uhakika. Kwa kutumia BERT, tunatengeneza mkakati rahisi unaotengeneza hasara ya mifano ya lugha inayoonyesha kupunguza gharama za kutangaza usambazaji wa maandishi. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>விளக்கச்சீட்டிற்கு மிக முக்கியமான உதாரணங்களை தேர்ந்தெடுத்து அறிவிப்பு செலவை குறைக்க முயற்சிக வகுப்பு மாதிரியில் செயலில் கற்றுக்கொள்ளும் திட்டம் சேர்ந்தது. உதாரணத்திற்கு, நம்பிக்கை மாதிரி மாதிரி மதிப்புகளை சார்ந்திருக்கிறது. குளிர் துவங்கும் அமைப்பில், இயல்பான கற்றல் மாதிரி நிலைமையான மற்றும் தரவு குறைவான காரணத்தால் செயல்படு அதிர்ஷ்டவசமாக, modern NLP ஒரு கூடுதல் தகவல் மூலம் வழங்குகிறது: முன் பயிற்சி மொழி மாதிரிகள். முன் பயிற்சி இழப்பு மாதிரியை ஆச்சரியப்படுத்த முடியும் மற்றும் தேவையான நன்மை தூண்டுதலுக்கு குறிப்பிட வ ஆகையால், நாம் மொழி மாதிரி இழப்பை வகைப்படுத்தல் நம்பிக்கையில் பிராக்சி ஆக்கிக் கொள்கிறோம். BERT உடன், நாம் ஒரு எளிய திட்டம் உரை வகைப்படுத்தலின் செலவ்களை சிறிதாக்குகிறது, முகப்பு மொழி மாதிரி தோல்வியை அடிப்படையில மற்ற அடிப்படைக்கோடுகளை ஒப்பிட்டு, எங்கள் நெருக்கம் சிறிய மாதிரிப்பு பொருள்கள் மற்றும் கணக்கீட்டு நேரத்திற</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Iň wajyp mysal saýlarak täzelikleri azaltmak üçin öwrenmek üçin çabalar. Adatça, aktif öwrenmek stratejiýasy klasifikasyon nusgasynda bar. Örneğin, kesinliksiz örnekler kötü kalibreli model güvenlik notlarına bağlı. Sowuk başlangyç düzeninde, aktiw öwrenme nusgasy we maglumat ýetmeginiň sebäbi däldir. Gynansakda, modern NLP şu wagtyň esasy maglumatyň çeşmesini saýlaýar: öňünden eğlenen dil nusgalary. Öňki okuw ýitişi nusgalary tapyp biler nusgalary getirýär we nusgalary etkinlik taýýarlamak üçin etitlenmeli. Şol sebäpli, biz dil modellendirmegini klasifikasiýa garşylygy üçin bir vekil olarak gözleýäris. BERT bilen maskeli diller modelleýäniň ýitilmegine dayanan basit bir strategiýany geliştirdik. Bu da metin klasifikasy üçin etimlendirme täsirlerini azaltýar. Başga üssli hatlarla karşılaşdyrylýar, biziň ýagdaýymyz düşük näme üçin deňleşme we hasaplamak wagtyna ulaşýar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>فعال سیکھنے کی کوشش کرتی ہے کہ لیبل کے بہترین مثالیں منتخب کریں۔ قابل طور پر، فعال سیکھنے کا استراتژی کلاسپیک موڈل پر موجود ہے۔ مثال کے طور پر، غیر قطعی نمونہ نمونہ برابر کیلیبراریٹ موڈل اعتماد اسکور پر مضبوط ہے. ٹھنڈ-شروع سٹینٹ میں، فعال سیکھنا ناکام ہے نمڈل ناکامی اور ڈیٹا کمزوری کے سبب۔ خوش شانس، مدرن NLP نے اضافہ اطلاعات کا سورج پیش آموزش کی زبان موڈل پیدا کیا ہے. پیش ترینس کے خسارہ کے لئے مثالیں پیدا کر سکتے ہیں جو مدل کو تعجب کرتی ہیں اور ان کے لئے اثرات پاکیزہ تنظیم کے لئے لیبل کرتی ہیں۔ لہٰذا ہم زبان کی مدل گھاٹ کو کلاسی غیر یقین کے لئے پروکسی بناتے ہیں۔ BERT کے ساتھ ہم ایک ساده استراتژی ایجاد کریں جو ماسک کی زبان نمڈلینگ خسارہ پر بنیاد ہے جو ٹیکس کلاسپیٹ کے لئے لابلینگ کے قیمت کو کم کر دیتا ہے۔ اور دوسری بنسس لینوں کے مقابلہ میں، ہماری تقریبا کم نمونہ کے بارے میں بہت بالا دقیق پہنچتی ہے۔</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>@ info Oddiy, aktiv o'rganish strategiya darajalashtirish modelidagi murakkab bo'ladi. Masalan, haqiqatgina sampli kalitlangan model ishonchining imkoniyatlarini yaratadi. Qushqi boshlash moslamalarida, aktiv o'rganish model umumiy va maʼlumot yoqtirligi sababchi emas. Uzunasizki, yangi NLP qoʻshimcha maʼlumot manbasiga ega: pre-trained tillar modellari. Oldingi taʼminlovchi tap topish modelni ajoyib chiqarishi mumkin va yaxshi suhbat bilan ishlab chiqarishi kerak. Shunday qilib, biz tilning modelini tasavvur qilamiz, tasdiqlash haqiqatgina proksi deb o'ylaymiz. BERT bilan, biz matn darajasining qiymatini yaratish uchun oddiy strategiya yaratishimiz mumkin. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Việc học tập đang làm giảm chi phí ghi chú bằng cách chọn các trường hợp quan trọng nhất để ghi chú. Thông thường, chiến lược học tập hoạt động phụ thuộc vào mô hình phân loại. Ví dụ, việc thử nghiệm độ mơ hồ phụ thuộc vào điểm tin cậy sai chuẩn. Trong môi trường bắt đầu lạnh, việc học hành không thực tế được vì không ổn định mô hình và thiếu dữ liệu. May mắn thay, một phiên bản ngôn ngữ hiện đại cung cấp thêm một nguồn thông tin: The pre-huấn luyện loss có thể tìm ví dụ Ngạc nhiên với mô- đun và should be nhãn for efficity fine-tunation. Do đó, chúng tôi coi mất người mẫu ngôn ngữ là ủy nhiệm cho việc chưa chắc chắn. Với BERT, chúng tôi phát triển một chiến lược đơn giản dựa trên sự giảm thiểu khả năng tạo mẫu ngôn ngữ đeo mặt nạ, thu nhỏ chi phí được mô tả cho việc phân loại văn bản. So với những đường cơ bản khác, cách tiếp cận của chúng ta đạt độ chính xác cao hơn trong vòng lặp lần thử nghiệm ít hơn và thời gian tính to án.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>学以至要示例勉降注释。 常自学习策略决于分类。 如不确定性抽样在校准不当者置信度分数。 冷启动之设也,不定数稀缺,学之不切也。 幸今世NLP供额外信息,预训语言模样。 预训练损失可寻示例,且宜标记效微调。 故以言建模损为不确定性摄。 借 BERT者,开屏蔽之言建模损之策,可极减文本分类之标本。 比之他基线,吾道少采样迭代,计日高准确性。</span></div></div><dl><dt>Anthology ID:</dt><dd>2020.emnlp-main.637</dd><dt>Volume:</dt><dd><a href=/volumes/2020.emnlp-main/>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venue:</dt><dd><a href=/venues/emnlp/>EMNLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>7935–7948</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.emnlp-main.637>https://aclanthology.org/2020.emnlp-main.637</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2020.emnlp-main.637 title="To the current version of the paper by DOI">10.18653/v1/2020.emnlp-main.637</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">yuan-etal-2020-cold</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Michelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-Graber. 2020. <a href=https://aclanthology.org/2020.emnlp-main.637>Cold-start Active Learning through Self-supervised Language Modeling</a>. In <i>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, pages 7935–7948, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2020.emnlp-main.637>Cold-start Active Learning through Self-supervised Language Modeling</a> (Yuan et al., EMNLP 2020)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.emnlp-main.637.pdf>https://aclanthology.org/2020.emnlp-main.637.pdf</a></dd><dt class=acl-button-row>Video:</dt><dd class=acl-button-row><a href=https://slideslive.com/38938687 class="btn btn-attachment btn-sm"><i class="fas fa-video"></i>&nbsp;https://slideslive.com/38938687</a></dd><dt>Code</dt><dd><a href=https://github.com/forest-snow/alps><i class="fab fa-github"></i>&nbsp;forest-snow/alps</a></dd><dt>Data</dt><dd><a href=https://paperswithcode.com/dataset/ag-news>AG News</a>,&nbsp;<a href=https://paperswithcode.com/dataset/imdb-movie-reviews>IMDb Movie Reviews</a>,&nbsp;<a href=https://paperswithcode.com/dataset/sst>SST</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.emnlp-main.637.pdf title="Open PDF of 'Cold-start Active Learning through Self-supervised Language Modeling'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Cold-start+Active+Learning+through+Self-supervised+Language+Modeling" title="Search for 'Cold-start Active Learning through Self-supervised Language Modeling' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-secondary d-flex flex-wrap justify-content-center" href="https://paperswithcode.com/paper/?acl=2020.emnlp-main.637" title="Code for 'Cold-start Active Learning through Self-supervised Language Modeling' on Papers with Code"><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-big" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg><span class="pl-sm-2 d-none d-sm-inline">Code</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Cold-start Active Learning through Self-supervised Language Modeling'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a>
<a class="btn btn-attachment d-flex flex-wrap justify-content-center" href=https://slideslive.com/38938687 title="Open video for 'Cold-start Active Learning through Self-supervised Language Modeling'"><span class="align-self-center px-1"><i class="fas fa-video"></i></span>
<span class=px-1>Video</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Cold-start Active Learning through Self-supervised Language Modeling](https://aclanthology.org/2020.emnlp-main.637) (Yuan et al., EMNLP 2020)</p><ul class=mt-2><li><a href=https://aclanthology.org/2020.emnlp-main.637>Cold-start Active Learning through Self-supervised Language Modeling</a> (Yuan et al., EMNLP 2020)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Michelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-Graber. 2020. <a href=https://aclanthology.org/2020.emnlp-main.637>Cold-start Active Learning through Self-supervised Language Modeling</a>. In <i>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>, pages 7935–7948, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>