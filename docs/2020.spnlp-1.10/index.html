<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>On the Discrepancy between Density Estimation and Sequence Generation - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="On the Discrepancy between Density Estimation and Sequence Generation" name=citation_title><meta content="Jason Lee" name=citation_author><meta content="Dustin Tran" name=citation_author><meta content="Orhan Firat" name=citation_author><meta content="Kyunghyun Cho" name=citation_author><meta content="Proceedings of the Fourth Workshop on Structured Prediction for NLP" name=citation_conference_title><meta content="2020/11" name=citation_publication_date><meta content="https://aclanthology.org/2020.spnlp-1.10.pdf" name=citation_pdf_url><meta content="84" name=citation_firstpage><meta content="94" name=citation_lastpage><meta content="10.18653/v1/2020.spnlp-1.10" name=citation_doi><meta property="og:title" content="On the Discrepancy between Density Estimation and Sequence Generation"><meta property="og:image" content="https://aclanthology.org/thumb/2020.spnlp-1.10.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2020.spnlp-1.10"><meta property="og:description" content="Jason Lee, Dustin Tran, Orhan Firat, Kyunghyun Cho. Proceedings of the Fourth Workshop on Structured Prediction for NLP. 2020."><link rel=canonical href=https://aclanthology.org/2020.spnlp-1.10></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2020.spnlp-1.10.pdf>On the Discrepancy between <a href=https://en.wikipedia.org/wiki/Density_estimation>Density Estimation</a> and Sequence Generation</a>
<a id=af_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Op die diskrepansie tussen Densiteit Estimatie en SewensieGenerasie</a>
<a id=am_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>ምርጫዎች</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>حول التناقض بين تقدير الكثافة وتوليد التسلسل</a>
<a id=az_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>S캼x캼nl캼q Hesab캼 v톛 S캼radan M톛xluqat캼 aras캼nda</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Относно разликата между оценката на плътността и генерирането на последователност</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>ডেন্সিটি গণনা এবং সেকেন্স জেনারেশনের মধ্যে স্বীকার</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Density Estimation and Sequence Generation དབར་གྱི་འཕེལ་རྩིས་འཁོར་ལ་དག</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>O diskreciji između procjene gustoće i generacije sekvence</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>En la discrepancia entre l'estimació de la densitat i la generació de seqüències</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Rozdíly mezi odhadem hustoty a generováním sekvencí</a>
<a id=da_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Om forskellen mellem tæthedsestimering og sekvensgenerering</a>
<a id=de_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Zur Diskrepanz zwischen Dichteschätzung und Sequenzgeneration</a>
<a id=el_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Σχετικά με τη διαφορά μεταξύ εκτίμησης πυκνότητας και δημιουργίας ακολουθίας</a>
<a id=es_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Sobre la discrepancia entre la estimación de la densidad y la generación de secuencias</a>
<a id=et_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Tiheduse hindamise ja järjestuse genereerimise vahelise lahknevuse kohta</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>در اختلاف بین ارزیابی گستردگی و نسل سطح</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Tiheys-arvion ja sekvenssien muodostumisen välinen ero</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Sur l'écart entre l'estimation de la densité et la génération de séquences</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Ar an Neamhréireacht idir Meastachán Dlúis agus Giniúint Seicheamh</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>@ action</a>
<a id=he_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>On the Discrepancy between Density Estimation and Sequence Generation</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>घनत्व अनुमान और अनुक्रम जनरेशन के बीच विसंगति पर</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>O diskreciji između procjene gustoće i generacije sekvence</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>A sűrűségbecslés és a szekvencia generáció közötti különbség</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>On the Discrepancy between Density Estimation and Sequence Generation</a>
<a id=id_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Pada diskrepansi antara Perkiraan Densitas dan Generasi Sekuensi</a>
<a id=is_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Sulla discrepanza tra stima della densità e generazione di sequenze</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>密度推定とシーケンス生成の不一致について</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Ngawe Perintah Panjenengan langgar Density</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>დისპრეპონციის განსაზღვრება და განსაზღვრების განსაზღვრება</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Жылтықтығы мен реттеу арасындағы дискрепциясы</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>밀도 추정과 서열 생성의 차이</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Dėl tankio vertinimo ir sekos generacijos skirtumo</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>За дискрепантноста помеѓу проценката на густината и генерацијата на секвенција</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>ഡെന്‍സിറ്റി എസ്റ്റിമേഷനും സെക്കന്റ് ജനിപ്പിനും തമ്മിലുള്ള ഡിസിക്കെന്‍സിയില്‍</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Хүмүүсийн дүгнэлт болон дарааллын төлөвлөгөөний хоорондын хувьд</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Pada Kegagalan diantara Perkiraan Densiti dan Jenerasi Sekuensi</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Dwar id-Diskrepanza bejn l-Istima tad-Densità u l-Ġenerazzjoni tas-Sekwenza</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Over het verschil tussen dichtheidsschatting en opeenvolgende generatie</a>
<a id=no_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>På diskrepansen mellom estimaen av tettleik og sekvensgenerering</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Na temat różnicy między oszacowaniem gęstości a generowaniem sekwencji</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Sobre a discrepância entre estimativa de densidade e geração de sequência</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Despre discrepanța dintre estimarea densității și generarea secvențelor</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>О несоответствии между оценкой плотности и формированием последовательности</a>
<a id=si_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>සීමාවත් අනුමාණය සහ අනුමාණය අතර විශාලය සඳහා විශාලය</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>O razliki med oceno gostote in ustvarjanjem zaporedja</a>
<a id=so_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Ka baaraandegista xisaabinta iyo xisaabinta</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Në diskrepancën midis vlerësimit të densitetit dhe gjenerimit të sekuencës</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>O diskreciji između procjene gustoće i generacije sekvence</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Om skillnaden mellan densitetsuppskattning och sekvensgenerering</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Kuhusu Ulipuko kati ya Uzazi na Uzalishaji</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>அடர்த்தி கணக்கீடு மற்றும் வரிசை உருவாக்கத்திற்கும் இடையே தீர்ப்பு</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>S철첵g체t Ta첵첵arlama we Ta첵첵arlama Suratynda</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>تنگی کا اندازہ اور سکنس کی نسل کے درمیان تفرقہ پر</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>On the Discrepancy between Density Estimation and Sequence Generation</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>Dựa vào độ phân tán giữa độ Density Esmation and Sequence Production</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2020.spnlp-1.10.pdf>其密度估计序生之间异也</a></h2><p class=lead><a href=/people/j/jason-lee/>Jason Lee</a>,
<a href=/people/d/dustin-tran/>Dustin Tran</a>,
<a href=/people/o/orhan-firat/>Orhan Firat</a>,
<a href=/people/k/kyunghyun-cho/>Kyunghyun Cho</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Many sequence-to-sequence generation tasks, including <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> and <a href=https://en.wikipedia.org/wiki/Speech_synthesis>text-to-speech</a>, can be posed as estimating the density of the output y given the input x : p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y * : R(y, y * | x). While we hope that a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that excels in <a href=https://en.wikipedia.org/wiki/Density_estimation>density estimation</a> also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. In this paper, by comparing several density estimators on five machine translation tasks, we find that the correlation between rankings of models based on <a href=https://en.wikipedia.org/wiki/Likelihood_function>log-likelihood</a> and BLEU varies significantly depending on the range of the model families being compared. First, <a href=https://en.wikipedia.org/wiki/Likelihood_function>log-likelihood</a> is highly correlated with <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a> when we consider <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> within the same family (e.g. autoregressive models, or <a href=https://en.wikipedia.org/wiki/Latent_variable_model>latent variable models</a> with the same parameterization of the prior).</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Baie volgorde-na-volgorde geneeringstaak, insluitend masjien vertaling en teks-na-woord, kan wees posisioneer as estimatiseer van die densiteit van die uitset y gegee die invoer x: p(y.x). Gien hierdie uitlegging, is dit natuurlik om sekwensie-na-sekwensie modele te evalueer deur voorwaardes log-waarskynlik op 'n toets stel te gebruik. Maar die doel van volgorde-na-volgorde generasie (of struktureerde voorskou) is om die beste uitvoer y gegee 'n invoer x te vind, en elke taak het sy eie onderstreem metriese R wat 'n model uitvoer tel deur vergelyking teen' n stel van verwysing y*: R( y, y *. x). Alhoewel ons hoop dat 'n model wat oorvloei in densiteit estimatie ook goed uitvoer op die onderstreem metriese, die eksakte korrelasie is nie onderwerp vir sekwensiegenerasie taak nie. In hierdie papier, deur vergelyking van verskeie densiteit estimatiërs op vyf masjien vertaling opdragte, vind ons dat die korrelasie tussen rankings van modele gebaseer op log-waarskynlik en BLEU verander betekeurig afhanklik van die omvang van die model geslagte wat vergelyk word. Eerste, log-waarskynlik is baie korrelasieer met BLEU wanneer ons beskou modele binne dieselfde familie (bv. autoregressiewe modele, of latent veranderlike modele met dieselfde parameter van die vooraf). Ons hou tog geen korrelasie tussen rankings van modele oor verskillende geslagte nie: (1) onder nie-autoregressiewe latente veranderlike modele, 'n fleksibel voorheede verspreiding is beter by densiteit estimatie maar gee verdere generasie kwaliteit as 'n eenvoudige voorheede en (2) autoregressiewe modele
gee die beste vertaling prestasie totaal, terwyl latente veranderlike modele met 'n normaliseerde vloei voorheen gee die hoogste gehou- uit log- likelikheid oor alle datastelle. Daarom, ons aanbeveel om 'n eenvoudige vooraf te gebruik vir die latente veranderlike nie- autoregressiewe model wanneer vinnige generasie spoed verwil word.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>የመረጃ ትርጉም እና የጽሑፍ-ለንግግር እና የውጤት ውጤት ውጤት ቁጥጥር በቁጥጥር ይቆጥራል፡፡ ይህንን ትርጓሜ በተሰጠ፣ የሥርዓት-ወደ-sequence models በተፈተና ላይ የሞክራዊ የlog-ምናልባት በመጠቀም ያስተካክል ነው፡፡ ምንም እንኳን የsequence-to-sequence ትውልድ (or structured prediction) የእርሱን የውጤት ውጤት መግኘት ነው፣ ለሁሉም ስራ የራሱ የውጤት ሜትሪክ R ነው፡፡ y* - y* - x በብርቱ ድምፅ ላይ የሚሻል ምሳሌ ከውኃው ማተሚያ ደግሞ መልካም እንዲሠራ ተስፋ እናደርጋለን፣ የእውነቱ ግንኙነት ለትውልድ ትውልድ ስራ አልተማረም፡፡ በዚህ ካላት፣ አምስት መሣሪያዎች ትርጉም ስራዎችን በማስተካከል ብዙዎች ድጋፍ፣ የሞዴላዎች ግንኙነት እና በቢሌዩም በሞዴል ቤተሰቦች ላይ በተያያያየው ግንኙነት በትክክል ይለያያል፡፡ መጀመሪያ፣ የlog-ምናልባት ከBLEU ጋር እጅግ ተገናኝቷል፡፡ ምንም እንኳን፣ በተለዩ ቤተሰቦች መካከል የሞላት ግንኙነት ምንም አናይም::
የተመረጠውን ትርጉም ማድረግ በሙሉ ያቀረቡ፤ በተቃውሞ የተለየ ሞዴላዎች በተመሳሳይ ፈሳሽ አስቀድሞ ከዳታ ማህበረሰብ ሁሉ በላይ የተመሳሳይ የlog-ምናልባት ይሰጣል፡፡ ስለዚህም የፍጥነት ትውልድ ፈጥኖ በተፈለገ ጊዜ የተለየ ለራሱ-ሥልጣን ሞዴል ሳይሆን ቀላል ቀድሞ ለመጠቀም እንመክራለን፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>يمكن طرح العديد من مهام إنشاء التسلسل إلى التسلسل ، بما في ذلك الترجمة الآلية وتحويل النص إلى كلام ، كتقدير لكثافة الإخراج y بالنظر إلى المدخلات x: p (y | x). بالنظر إلى هذا التفسير ، من الطبيعي تقييم نماذج التسلسل إلى التسلسل باستخدام احتمالية السجل الشرطي في مجموعة اختبار. ومع ذلك ، فإن الهدف من إنشاء التسلسل إلى التسلسل (أو التنبؤ المنظم) هو العثور على أفضل ناتج y مع إعطاء إدخال x ، ولكل مهمة مقياسها المتلقي الخاص بها R الذي يسجل مخرجات النموذج من خلال المقارنة مع مجموعة من المراجع y *: ص (ص ، ص * | س). بينما نأمل أن يعمل النموذج الذي يتفوق في تقدير الكثافة أيضًا بشكل جيد على مقياس المصب ، لم تتم دراسة الارتباط الدقيق لمهام إنشاء التسلسل. في هذا البحث ، من خلال مقارنة العديد من مقدرات الكثافة في خمس مهام ترجمة آلية ، وجدنا أن الارتباط بين تصنيفات النماذج بناءً على احتمالية السجل و BLEU يختلف اختلافًا كبيرًا اعتمادًا على نطاق عائلات النماذج التي تتم مقارنتها. أولاً ، ترتبط احتمالية السجل ارتباطًا وثيقًا بـ BLEU عندما نفكر في النماذج داخل نفس العائلة (على سبيل المثال ، نماذج الانحدار التلقائي ، أو النماذج المتغيرة الكامنة مع نفس المعلمات السابقة). ومع ذلك ، لم نلاحظ أي ارتباط بين تصنيفات النماذج عبر عائلات مختلفة: (1) بين النماذج المتغيرة الكامنة غير الانحدارية ، يكون التوزيع المسبق المرن أفضل في تقدير الكثافة ولكنه يعطي جودة توليد أسوأ من النماذج السابقة البسيطة و (2) الانحدار التلقائي
تقدم أفضل أداء للترجمة بشكل عام ، في حين أن النماذج المتغيرة الكامنة ذات التدفق الطبيعي السابق تعطي أعلى احتمالية لتسجيل الدخول في جميع مجموعات البيانات. لذلك ، نوصي باستخدام طريقة مسبقة بسيطة للنموذج الكامن المتغير غير الانحدار التلقائي عندما تكون سرعة التوليد السريع مطلوبة.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Makina çevirilməsi və mətn-sözünə daxil olan çox sequence-to-sequence nəsil işləri, y girdi x: p(y.x). Bu yorumlayıcıya görə, sınama qutusunda müəyyən bir log mümkünlüyünü istifadə edən sequence-to-sequence modellerini təhsil etmək təbiidir. Lakin, sequence-to-sequence nəsillərin (y a da strukturlu təsirlərin) ən y a x şısını x verilən y-ni tapmaq məqsədilədir, və hər işin öz düşük-düşük metrik R vardır ki modellərin çıxışını y*: R(y, y*.x ilə qarşılaşdırılır. Biz ümid edirik ki, yoğunluq değerlendirməsindən daha yaxşı bir modellər də a şağı metrikdə yaxşı işlər edir, həqiqətən, bu bağlantı seçmə nəsillərinin işləri üçün təhsil edilməmişdir. Bu kağızda, beş maşına çevirilmiş işlərdə bir neçə yoxluq hesablayıcını salıb, çoxlu modellərin səviyyələrinin və BLEU ehtimalına dayanan modellərin səviyyələrinin müqayisədə çoxlu dəyişikliyini görürük. Əvvəlcə, bir ailədə modelləri düşündüyümüz zaman çox çox BLEU ilə bağlı olaraq (həmçinin autoregressiv modelləri, ya da keçmişdəkilərin eyni parameterizləri ilə latent dəyişiklik modelləri). Ancaq biz müxtəlif ailələr arasında modellərin səviyyələri arasında heç bir bağlılığı görmürük: (1) autoregressiv olmayan latent modellərin arasında, fleksibil əvvəlki dağıtım yoxluq değerində daha yaxşıdır, amma nəsillərin basit əvvəlki modellərdən daha kötüsünü verir, və (2) autoregressiv modellərin
Ən yaxşı tercümə performansını təbliğ et, lakin latent dəyişiklik modelləri, əvvəl normalizasyon axışı ilə, bütün veri qurğularının ən yüksək mümkünlüyünü verir. Bu yüzden, hızlı nəsil sürəti istədiyi zaman, latent dəyişiklik olmayan autoregressiv modeli üçün basit bir əvvəl istifadə etməyi tavsiye edirik.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Много задачи за генериране на последователност по последователност, включително машинен превод и текст в реч, могат да се представят като оценка на плътността на изходния y при въвеждането х: p(y|x). Като се има предвид това тълкуване, естествено е моделите последователност към последователност да се оценяват с помощта на условна логаритметична вероятност върху тестов набор. Въпреки това, целта на генерирането последователност към последователност (или структурирано прогнозиране) е да се намери най-добрият изход у при дадено входно х, и всяка задача има свой собствен метричен показател надолу по веригата, който оценява изхода на модела чрез сравняване с набор от препратки у*: R(y, y* | x). Макар и да се надяваме, че модел, който се отличава в оценката на плътността, също се справя добре с метриката надолу по веригата, точната корелация не е проучена за задачите за генериране на последователност. В тази статия, сравнявайки няколко оценителя на плътността на пет задачи за машинен превод, откриваме, че корелацията между класирането на моделите въз основа на log-вероятността и варира значително в зависимост от обхвата на сравняваните семейства модели. Първо, лог-вероятността е силно корелирана с BLU, когато разглеждаме модели от едно и също семейство (например авторегресивни модели или латентни променливи модели със същата параметризация като предишните). Въпреки това, не наблюдаваме корелация между класирането на моделите в различните семейства: (1) при неавторегресивните латентни променливи модели гъвкавото предварително разпределение е по-добро при оценката на плътността, но дава по-лошо качество на генериране от обикновените предварителни модели, и (2) авторегресивни модели
предлагат най-добрата производителност на превода като цяло, докато латентните променливи модели с нормализиращ поток дават най-висока вероятност за излизане на запис във всички набори от данни. Ето защо препоръчваме използването на прост преди за латентната променлива неавторегресивен модел, когато е желана бърза скорост на генериране.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>অনেক প্রজন্মের কাজ, যার মধ্যে রয়েছে মেশিন অনুবাদ এবং টেক্সট-থেকে বক্তৃতার মধ্যে, ইনপুট এক্স: p( y) প্রদান করা আউটপুটের গভীর হিসেবে হিসেবে প এই ব্যাখ্যা দিয়ে এটা স্বাভাবিক যে পরীক্ষার সেটে সেকেন্স-থেকে সেকেন্স মোডেল ব্যবহার করে সংক্রান্ত ব্যবস্থা ব্যবহার কিন্তু প্রজন্মের উদ্দেশ্য হচ্ছে ইনপুট দেওয়া সবচেয়ে ভালো আউটপুটের প্রজন্ম (অথবা কাঠামো ভবিষ্যৎবাণী) খুঁজে বের করা যায়, আর প্রত্যেক ক কাজের নিজের নিজের নিজস্ব প্রান্তের মে যদিও আমরা আশা করি যে একটি মডেল যা গভীর হিসেবে বেশী প্রাপ্ত হয়েছে তা নিচের নদীর মেট্রিকে ভালোভাবে প্রকাশ করেছে, কিন্তু সেকেন্ড প্রজন্মের ক এই পত্রিকায়, পাঁচটি মেশিন অনুবাদের কাজের উপর কয়েকটি গুরুত্বপূর্ণ হিসেব গুরুত্বপূর্ণ হিসেবের মাধ্যমে আমরা দেখতে পাচ্ছি যে মডেলের রান্নাকারীদের মধ্যে যে সম প্রথমত, লোগ-সম্ভাবনা বিলুর সাথে অত্যন্ত সংশ্লিষ্ট যখন আমরা একই পরিবারের মধ্যে মডেল বিবেচনা করি (উদাহরণস্বয়ংক্রিয় মডেল, অথবা সাম্প্রতিক পরিবর্তনের মডে তবে আমরা বিভিন্ন পরিবারের মধ্যে মডেলের রাঙ্কের মাঝে কোন সম্পর্ক দেখতে পাই না: (১) স্বয়ংক্রিয়ভাবে সাম্প্রতিক পরিবর্তনের মডেলের মধ্যে একটি ফ্লিক্সিয়াল বিতরণের আগের গু
সবচেয়ে ভালো অনুবাদ প্রদর্শনের প্রচারণা প্রদান করুন, যদিও সাম্প্রতিক ভেরিয়েবল মডেল স্বাভাবিক প্রবাহের পূর্বে স্বাভাবিক প্রব Therefore, we recommend using a simple prior for the latent variable non-autoregressive model when fast generation speed is desired.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y.x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y*: R(y, y* .x). While we hope that a model that excels in density estimation also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. ཤོག་བྱང་འདིའི་ནང་དུ་རྩིས་འཁོར་གཞུང་གི་ཚད་རྩིས་ཅན་མི་འདྲ་ཞིག་དང་མཉམ་དུ་མཐུན་པ་ཡིན། First, log-likelihood is highly correlated with BLEU when we consider models within the same family (e.g. autoregressive models, or latent variable models with the same parameterization of the prior). Do not translate the keyword between brackets (e.g. ServerName, ServerAdmin, etc.) འོན་ཀྱང་། ང་ཚོར་བཅས་ཚང་མི་འདྲ་བར་གྱི་རིམ་པ་ལས་དབྱེ་རིམ་མཐུན་གྱི་མིག་ལམ་ལ་མཐུན་རྐྱེན་མེད།
རྗེས་མའི་འགྱུར་ཅན་གྱི་དཔེ་དབྱིབས་ཡོད་ཚད་ལྟ་བུའི་སྐྱེས་སྐར་ཡིག དེར་བརྟེན།</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mnogi zadatak generacije sekvence do sekvence, uključujući prevod strojeva i tekst do govora, mogu se postaviti kao procjena gustoće proizvoda y s obzirom na ulaz x: p(y.x). S obzirom na ovu interpretaciju, prirodno je procijeniti modele sekvence do sekvence koristeći uvjetnu vjerojatnost log a na testu. Međutim, cilj generacije sekvence-do-sekvence (ili strukturirane predviđanja) je pronaći najbolji izlaz y dajući ulaz x, a svaki zadatak ima svoj vlastiti niz metrički R koji rezultira izlaz modela uspoređujući s setom referencija y*: R(y, y*.x). Iako se nadamo da model koji nadmaže procjenu gustine također dobro izvršava na donjem metriku, tačna korelacija nije proučena zbog zadataka generacije sekvence. U ovom papiru, uspoređujući nekoliko procjena gustine na pet zadataka za prevod mašine, smatramo da se povezanost između redova modela baziranog na log-verovatnosti i BLEU značajno razlikuje ovisno o rasponu modelnih porodica u usporedbi. Prvo, vjerojatnost dnevnika je vrlo povezana sa BLEU kada razmatramo modele unutar iste porodice (npr. autoregresivne modele, ili latentne varijantne modele sa istim parameterizacijom prethodnog). Međutim, mi ne posmatramo korelaciju između redova modela u različitim porodicama: 1) među modelima ne autoregresivnih latentnih promjena, fleksibilna prethodna distribucija je bolja za procjenu gustosti, ali daje gore kvalitet generacije nego jednostavno ranije, i (2) autoregresivne modele
ponudite najbolje učinkovito prevođenja ukupno, dok latentni varijantni modeli sa normalizacijskim tokom prije daju najveću vjerojatnost izvršenog izveštaja u svim podacima. Stoga preporučujemo korištenje jednostavnog ranije za latentnu promjenu ne autoregresivnog model a kada se žele brzina generacije.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Moltes tasques de generació de seqüència a seqüència, incloent traducció de màquina i text a discurs, es poden posar com a estimació de la densitat de la producció y dada la entrada x: p(y[UNK]x). Tenint en compte aquesta interpretació, és natural avaluar models de seqüència a seqüència utilitzant probabilitats condicionals de registre en un conjunt de proves. Tanmateix, l'objectiu de la generació seqüència a seqüència (o predicció estructurada) és trobar la millor producció y dada una entrada x, i cada tasca té la seva pròpia R mètrica avall que puntueix una producció del model comparant-se amb un conjunt de referències y*: R(y, y* [UNK] x). Mentre esperem que un model que excel·li en l'estimació de densitat també funcioni bé en la mètrica avall, la correlació exact a no s'ha estudiat per a tasques de generació de seqüències. En aquest paper, comparant diverses estimadores de densitat en cinc tasques de traducció màquina, descobrim que la correlació entre les classificacions dels models basades en probabilitats de registre i BLEU varien significativament segons l'interval de les famílies models comparades. En primer lloc, la probabilitat de registre està molt correlacionada amb el BLEU quan considerem models dins la mateixa família (per exemple, models autoregressius o models variables latents amb la mateixa paràmetrització que la anterior). No obstant això, no observem correlació entre les classificacions dels models entre les diferents famílies: (1) entre els models latents i no autoregressius, una distribució anterior flexible és millor a la estimació de densitat però dóna una qualitat de generació pitjor que un model anterior simple, i (2) models autoregressius
ofereixen el millor rendiment de traducció en general, mentre que els models variables latents amb un flux de normalització anterior donen la més alta probabilitat de rexistruació de tots els conjunts de dades. Per tant, recomanem l'ús d'un simple anterior per al model variable latent no autoregressiu quan es desitge una velocitat de generació ràpida.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mnoho úloh generování sekvence na sekvenci, včetně strojového překladu a text-to-Speech, lze představit jako odhad hustoty výstupu y při vstupu x: p(y|x). Vzhledem k této interpretaci je přirozené vyhodnocovat modely sekvence na sekvenci pomocí podmíněné log-pravděpodobnosti na testovací sadě. Cílem generování sekvence na sekvenci (nebo strukturované predikce) však je najít nejlepší výstup y při vstupu x a každý úkol má svou vlastní následnou metriku R, která skóruje výstup modelu porovnáním se sadou referencí y*: R(y, y*, x). Zatímco doufáme, že model, který vyniká v odhadu hustoty, také funguje dobře na následné metrice, přesná korelace nebyla studována pro úlohy generování sekvencí. V tomto článku, porovnáním několika odhadů hustoty na pěti úlohách strojového překladu, zjišťujeme, že korelace mezi hodnocením modelů založenými na log-pravděpodobnosti a BLEU se výrazně liší v závislosti na rozsahu porovnávaných modelových rodin. Za prvé, log-pravděpodobnost je vysoce korelována s BLEU, pokud bereme v úvahu modely ve stejné rodině (např. autoregresivní modely nebo latentní proměnné modely se stejnou parametrizací jako předchozí). Nicméně, nezaznamenáváme žádnou korelaci mezi žebříčky modelů napříč různými rodinami: (1) mezi non-autoregresivními latentními proměnnými modely je flexibilní předchozí distribuce lepší při odhadu hustoty, ale poskytuje horší kvalitu generace než jednoduchý prior, a (2) autoregresivní modely
nabízejí celkově nejlepší překladový výkon, zatímco latentní proměnné modely s normalizujícím tokem předtím poskytují nejvyšší pravděpodobnost odhlášení přes všechny datové sady. Proto doporučujeme použít jednoduchý předpis pro latentní variabilní non-autoregresivní model, pokud je požadována rychlá generační rychlost.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mange sekvens-til-sekvens generationsopgaver, herunder maskinoversættelse og tekst-til-tale, kan stilles som estimering af tætheden af output y givet input x: p(y|x). I betragtning af denne fortolkning er det naturligt at vurdere sekvens-til-sekvensmodeller ved hjælp af betinget logsandsynlighed på et testsæt. Målet med sekvens-til-sekvens-generering (eller struktureret forudsigelse) er dog at finde det bedste output y givet et input x, og hver opgave har sin egen downstream metric R, der scorer en model output ved at sammenligne med et sæt referencer y*: R(y, y* | x). Selvom vi håber, at en model, der udmærker sig i densitetsestimering, også klarer sig godt på downstream metric, er den nøjagtige korrelation ikke blevet undersøgt for sekvensgenereringsopgaver. Ved at sammenligne flere densitetsestimatorer på fem maskinoversættelsesopgaver finder vi i denne artikel, at sammenhængen mellem placeringer af modeller baseret på log-sandsynlighed og BLEU varierer betydeligt afhængigt af rækkevidden af de modelfamilier, der sammenlignes. For det første er log-sandsynlighed stærkt korreleret med BLEU, når vi overvejer modeller inden for samme familie (f.eks. autoregressive modeller eller latente variable modeller med samme parametrisering som tidligere). Vi observerer dog ingen sammenhæng mellem placeringer af modeller på tværs af forskellige familier: (1) blandt ikke-autoregressive latente variable modeller er en fleksibel forudgående fordeling bedre til densitetsestimering, men giver dårligere generationskvalitet end en simpel forudgående, og (2) autoregressive modeller
tilbyder den bedste oversættelseseffekt generelt, mens latente variable modeller med en normaliserende flow forud giver den højeste holdt-out log-sandsynlighed på tværs af alle datasæt. Derfor anbefaler vi at bruge en simpel forud for den latente variabel ikke-autoregressive model, når der ønskes hurtig genereringshastighed.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Viele Sequenz-zu-Sequenz-Generierungsaufgaben, einschließlich maschineller Übersetzung und Text-in-Sprache, können als Schätzung der Dichte der Ausgabe y bei Eingabe x: p(y|x) dargestellt werden. Angesichts dieser Interpretation ist es natürlich, Sequenz-zu-Sequenz-Modelle mit bedingter Log-Likelihood auf einem Testsatz zu bewerten. Das Ziel der Sequenz-zu-Sequenz-Generierung (oder strukturierter Vorhersage) besteht jedoch darin, die beste Ausgabe y bei Eingabe x zu finden, und jede Aufgabe hat ihre eigene nachgelagerte Metrik R, die eine Modellausgabe durch Vergleich mit einer Menge von Referenzen y* bewertet: R(y, y*,x). Während wir hoffen, dass ein Modell, das sich in der Dichteschätzung auszeichnet, auch auf der nachgeschalteten Metrik gut abschneidet, wurde die genaue Korrelation für Sequenzgenerationsaufgaben nicht untersucht. Durch den Vergleich mehrerer Dichteschätzer für fünf maschinelle Übersetzungsaufgaben stellen wir in dieser Arbeit fest, dass die Korrelation zwischen Rankings von Modellen basierend auf Log-Likelihood und BLEU je nach Umfang der zu vergleichenden Modellfamilien signifikant variiert. Erstens ist Log-Likelihood stark mit BLEU korreliert, wenn wir Modelle innerhalb derselben Familie betrachten (z.B. autoregressive Modelle oder latente Variablenmodelle mit der gleichen Parametrisierung wie das vorherige). Wir beobachten jedoch keine Korrelation zwischen den Ranglisten von Modellen über verschiedene Familien: (1) unter nicht autoregressiven latenten Variablenmodellen ist eine flexible vorherige Verteilung besser bei der Dichteabschätzung, liefert aber schlechtere Generationsqualität als ein einfaches Prior, und (2) autoregressive Modelle
Sie bieten die beste Übersetzungsleistung insgesamt, während latente Variablenmodelle mit einem normalisierenden Flow Prevor die höchste festgehaltene Log-Likelihood für alle Datensätze bieten. Daher empfehlen wir für das latente variable nicht-autoregressive Modell ein einfaches Vorab zu verwenden, wenn eine schnelle Erzeugungsgeschwindigkeit gewünscht wird.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Πολλές εργασίες δημιουργίας αλληλουχίας σε αλληλουχία, συμπεριλαμβανομένης της μηχανικής μετάφρασης και της ομιλίας κειμένου, μπορούν να τεθούν ως εκτίμηση της πυκνότητας της εξόδου δεδομένου της εισόδου x: p(y|x). Δεδομένου αυτής της ερμηνείας, είναι φυσικό να αξιολογούνται μοντέλα ακολουθίας-ακολουθίας χρησιμοποιώντας πιθανή καταγραφή υπό όρους σε ένα σύνολο δοκιμών. Ωστόσο, ο στόχος της δημιουργίας αλληλουχίας σε αλληλουχία (ή δομημένης πρόβλεψης) είναι να βρεθεί η καλύτερη παραγωγή με δεδομένη εισαγωγή x, και κάθε εργασία έχει τη δική της μεταγενέστερη μετρική R που βαθμολογεί μια παραγωγή μοντέλου συγκρίνοντας με ένα σύνολο αναφορών y*: R(y, y*,x). Ενώ ελπίζουμε ότι ένα μοντέλο που υπερέχει στην εκτίμηση πυκνότητας αποδίδει επίσης καλά στην μεταγενέστερη μέτρηση, η ακριβής συσχέτιση δεν έχει μελετηθεί για εργασίες δημιουργίας αλληλουχιών. Σε αυτή την εργασία, συγκρίνοντας διάφορους εκτιμητές πυκνότητας σε πέντε εργασίες μηχανικής μετάφρασης, διαπιστώνουμε ότι η συσχέτιση μεταξύ των ταξινομήσεων των μοντέλων με βάση την πιθανότητα καταγραφής και την πιθανότητα καταγραφής ποικίλλει σημαντικά ανάλογα με το εύρος των οικογενειών μοντέλων που συγκρίνονται. Πρώτον, η πιθανότητα καταγραφής συσχετίζεται ιδιαίτερα με την BLEU όταν εξετάζουμε μοντέλα εντός της ίδιας οικογένειας (π.χ. αυτοανακριτικά μοντέλα, ή λανθάνοντα μεταβλητά μοντέλα με την ίδια παραμετροποίηση του προηγούμενου). Ωστόσο, δεν παρατηρούμε συσχέτιση μεταξύ των ταξινομήσεων των μοντέλων σε διαφορετικές οικογένειες: (1) μεταξύ των μη αυτοανακριτικών λανθάνοντων μεταβλητών μοντέλων, μια ευέλικτη προγενέστερη κατανομή είναι καλύτερη στην εκτίμηση της πυκνότητας αλλά δίνει χειρότερη ποιότητα παραγωγής από ένα απλό προηγούμενο, και (2) αυτοανακριτικά μοντέλα
προσφέρουν την καλύτερη απόδοση μετάφρασης συνολικά, ενώ τα λανθάνοντα μεταβλητά μοντέλα με ομαλοποίηση της ροής πριν παρέχουν την υψηλότερη πιθανότητα καταγραφής σε όλα τα σύνολα δεδομένων. Ως εκ τούτου, συστήνουμε τη χρήση ενός απλού προηγούμενου για το λανθάνουσα μεταβλητό μη-αυτοανακριτικό μοντέλο όταν η γρήγορη ταχύτητα παραγωγής είναι επιθυμητή.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Muchas tareas de generación de secuencia a secuencia, incluida la traducción automática y la conversión de texto a voz, se pueden plantear como una estimación de la densidad de la salida y dada la entrada x: p (y|x). Dada esta interpretación, es natural evaluar los modelos de secuencia a secuencia utilizando log-verosimilitud condicional en un conjunto de pruebas. Sin embargo, el objetivo de la generación de secuencia a secuencia (o predicción estructurada) es encontrar la mejor salida y dada una entrada x, y cada tarea tiene su propia métrica R descendente que puntúa una salida del modelo comparándola con un conjunto de referencias y*: R (y, y* | x). Si bien esperamos que un modelo que sobresalga en la estimación de la densidad también funcione bien en la métrica descendente, no se ha estudiado la correlación exacta para las tareas de generación de secuencias. En este artículo, al comparar varios estimadores de densidad en cinco tareas de traducción automática, encontramos que la correlación entre las clasificaciones de los modelos basadas en la probabilidad logarítmica y BLEU varía significativamente según el rango de las familias de modelos que se comparan. En primer lugar, la probabilidad logarítmica está altamente correlacionada con BLEU cuando consideramos modelos dentro de la misma familia (por ejemplo, modelos autorregresivos o modelos de variables latentes con la misma parametrización del anterior). Sin embargo, no observamos correlación entre las clasificaciones de los modelos de diferentes familias: (1) entre los modelos de variables latentes no autorregresivas, una distribución previa flexible es mejor en la estimación de la densidad, pero da peor calidad de generación que un modelo previo simple, y (2) los modelos autorregresivos
ofrecen el mejor rendimiento de traducción en general, mientras que los modelos de variables latentes con un flujo de normalización anterior proporcionan la probabilidad de registro más alta en todos los conjuntos de datos. Por lo tanto, recomendamos utilizar un prior simple para el modelo no autorregresivo de variables latentes cuando se desea una velocidad de generación rápida.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Paljud järjestuste genereerimise ülesanded, sealhulgas masintõlke ja teksti kõneks muutmine, võivad kujutada väljundi y tiheduse hindamiseks sisendi x järgi: p(y|x). Seda tõlgendust arvestades on loomulik hinnata jada-jada mudeleid, kasutades katsekogumil tingimuslikku logitõenäosust. Siiski on järjestuse genereerimise (või struktureeritud prognoosimise) eesmärk leida parim väljund y sisendi x korral ja igal ülesandel on oma allvoolu mõõdik R, mis hindab mudeli väljundit, võrreldes viitekogumiga y*: R(y, y* | x). Kuigi loodame, et tiheduse hindamisel silmapaistev mudel toimib hästi ka allvoolu mõõdikus, ei ole täpset korrelatsiooni jada genereerimise ülesannete puhul uuritud. Selles töös, võrreldes mitmeid tiheduse hindajaid viie masintõlketöö puhul, leiame, et korrelatsioon log-tõenäosusel põhinevate mudelite järjestuste ja BLEU vahel varieerub oluliselt sõltuvalt võrreldavate mudelite perekondade vahemikust. Esiteks on logaritmiline tõenäosus väga korrelatsioonis BLEU-ga, kui kaalume sama perekonna mudeleid (nt autoregressiivseid mudeleid või latentseid muutujamudeleid, mille parameetrid on sama kui eelmisel). Kuid me ei tähelda korrelatsiooni mudelite järjestuste vahel erinevate perekondade kaupa: (1) mitte-autoregressiivsete latentsete muutujate mudelite puhul on paindlik eelnev jaotus tiheduse hindamisel parem, kuid annab halvema generatsiooni kvaliteedi kui lihtsad eelnevad mudelid, ja (2) autoregressiivsed mudelid
pakuvad parimat tõlkimisjõudlust üldiselt, samas kui latentsed muutujate mudelid, mille voog on normaliseeritud, annavad kõigi andmekogumite puhul kõige suurema sisselogimise tõenäosuse. Seetõttu soovitame kasutada lihtsat eelvaadet latentse muutuja mittearregressiivse mudeli jaoks, kui soovitakse kiiret generatsioonikiirust.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>کار های بسیاری از نسل‌های مختلف به مختلف، شامل ترجمه‌های ماشین و متن-به-سخن، می‌توانند به عنوان ارزیابی density از نتیجه y به عنوان ورودی x: p(y.x) قرار دهند. با توجه به این تعبیر، این مدل‌ها را با استفاده از احتمال log شرایط در یک مجموعه آزمایش تحقیق کردن طبیعی است. با این حال، هدف نسل ردیابی به ردیابی (یا پیش‌بینی ساخته شده) بهترین نتیجه y را پیدا کردند که به عنوان یک ورودی x داده شده، و هر کاری متریک ردیابی خود را دارد که نتیجه مدل را با مقایسه کردن با مجموعه ردیابی y*: R(y, y* .x) می‌دهد. در حالی که امیدواریم یک مدل که در ارزیابی density زیاده‌تر می‌شود در متریک پایین، ارتباط دقیقا برای کار های نسل‌ها مطالعه نشده است. در این کاغذ، با مقایسه کردن تعدادی از ارزش density در پنج تابع ترجمه ماشین، ما یافتیم که ارتباط بین رشته‌های مدل بر اساس احتمال log و BLEU بستگی بستگی بستگی بستگی به مجموعه خانواده‌های مدل مقایسه می‌شود. اول، احتمال لوگو با BLEU بسیار ارتباط دارد وقتی ما مدل‌های داخل یک خانواده را در نظر می‌گیریم (مثلا مدل‌های autoregressive یا مدل‌های متغیر latent با همان پارامتریزی پیشینیان). با این حال، ما هیچ ارتباطی بین رشته‌های مدل در خانواده‌های مختلف را نمی‌بینیم: (۱) بین مدل‌های متغیر غیر خودگریزگریزگریزگریزگریزگریزگریزگریزگریزگریزگریزگریز، تقسیم پیشینیان flexible در ارتباط density بهتر است اما کیفیت نسل بدتر از یک مدل ساده
بهترین فعالیت ترجمه را در کل پیشنهاد می‌دهد، در حالی که مدل‌های متغیر latent با یک جریان عامل‌سازی پیش از آنکه بالاترین احتمال یافته‌ای در تمام مجموعه‌های داده‌ها را می‌دهد. بنابراین، ما پیشنهاد می‌دهیم که با استفاده از یک پیشینه ساده برای مدل متغیر latent غیر autoregressive زمانی که سرعت نسل سریع خواسته می‌شود استفاده کنیم.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Monien sekvenssin ja sekvenssin generointitehtävien, kuten konekääntämisen ja tekstin puheeksi muuntamisen, voidaan esittää arvioivan tulosteen y tiheyttä syötteellä x: p(y|x). Tämän tulkinnan perusteella on luonnollista arvioida sekvenssimalleja käyttämällä ehdollista log-todennäköisyyttä testisarjassa. Sekvenssin ja sekvenssin välisen generoinnin (tai strukturoidun ennusteen) tavoitteena on kuitenkin löytää paras tuotos y syötteellä x, ja jokaisella tehtävällä on oma loppupään metriikkansa R, joka tuottaa mallin tuotoksen vertaamalla viitesarjaan y*: R(y, y* | x). Vaikka toivomme, että tiheyden estimoinnissa erinomainen malli toimii hyvin myös loppupään metriikassa, tarkkaa korrelaatiota ei ole tutkittu sekvenssien luontitehtävissä. Vertailemalla useita tiheyden estimaattoreita viidessä konekäännöstehtävässä havaitsimme, että log-todennäköisyyteen perustuvien mallien ja BLEU:n sijoitusten korrelaatio vaihtelee merkittävästi vertailtavien malliperheiden välillä. Ensinnäkin log-todennäköisyys korreloi voimakkaasti BLEU:n kanssa, kun tarkastelemme samaan perheeseen kuuluvia malleja (esim. autoregressiivisia malleja tai latentteja muuttujamalleja, joilla on sama parametrisointi kuin aiemmalla). Emme kuitenkaan havainneet korrelaatiota mallien sijoitusten välillä eri perheissä: (1) ei-autoregressiivisten latenttien muuttujamallien kohdalla joustava ennakkojakauma on parempi tiheyden estimoinnissa, mutta huonompi sukupolvilaatu kuin yksinkertainen aiempi, ja (2) autoregressiiviset mallit
tarjoaa parhaan käännöstehokkuuden yleisesti ottaen, kun taas piilevät muuttujamallit, joissa on normalisoiva virtaus ennen, antavat suurimman lokin todennäköisyyden kaikissa tietosarjoissa. Siksi suosittelemme käyttämään yksinkertaista prioria piilevälle muuttujalle ei-autoregressiiviselle mallille, kun halutaan nopeaa generaationopeutta.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>De nombreuses tâches de génération de séquence à séquence, y compris la traduction automatique et la synthèse vocale, peuvent être considérées comme une estimation de la densité de la sortie y compte tenu de l'entrée x : p (y|x). Compte tenu de cette interprétation, il est naturel d'évaluer des modèles séquence à séquence en utilisant le log-vraisemblance conditionnel sur un ensemble de tests. Cependant, l'objectif de la génération séquence à séquence (ou prédiction structurée) est de trouver la meilleure sortie y compte tenu d'une entrée x, et chaque tâche possède sa propre métrique en aval R qui note une sortie de modèle en la comparant à un ensemble de références y* : R (y, y* | x). Bien que nous espérions qu'un modèle qui excelle dans l'estimation de la densité fonctionne également bien sur la métrique en aval, la corrélation exacte n'a pas été étudiée pour les tâches de génération de séquences. Dans cet article, en comparant plusieurs estimateurs de densité sur cinq tâches de traduction automatique, nous constatons que la corrélation entre les classements des modèles basés sur le log-vraisemblance et l'UEBL varie considérablement en fonction de la gamme des familles de modèles comparées. Tout d'abord, le log-vraisemblance est fortement corrélé avec l'UEBL lorsque l'on considère des modèles appartenant à la même famille (par exemple des modèles autorégressifs ou des modèles à variables latentes avec le même paramétrage que le modèle précédent). Cependant, nous n'observons aucune corrélation entre les classements des modèles de différentes familles : (1) parmi les modèles à variables latentes non autorégressifs, une distribution préalable flexible est meilleure pour l'estimation de la densité mais donne une qualité de génération inférieure à celle d'un modèle antérieur simple, et (2) des modèles autorégressifs
offrent les meilleures performances de traduction dans l'ensemble, tandis que les modèles à variables latentes avec un flux de normalisation préalable donnent la probabilité de log la plus élevée parmi tous les ensembles de données. Par conséquent, nous recommandons d'utiliser un simple prior pour le modèle non autorégressif à variable latente lorsqu'une vitesse de génération rapide est souhaitée.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Is féidir go leor tascanna giniúna seicheamh-go-seicheamh, lena n-áirítear aistriúchán meaisín agus téacs-go-hurlabhra, a chur mar mheastachán ar dhlús an aschuir y nuair a thugtar an t-ionchur x: p(y|x). I bhfianaise an léirmhínithe seo, tá sé nádúrtha samhlacha seicheamh-go-seicheamh a mheas ag baint úsáide as log-dóchúlacht choinníollach ar thacar tástála. Mar sin féin, is é an sprioc a bhaineann le giniúint seicheamh-go-seicheamh (nó tuartha struchtúrtha) ná an t-aschur is fearr y a fháil nuair a thugtar ionchur x, agus tá a mhéadracht iartheachtach R féin ag gach tasc a scórálann aschur samhla trí chomparáid a dhéanamh i gcoinne sraith tagairtí y *: R(y, y* | x). Cé go bhfuil súil againn go n-éireoidh go maith le samhail a sháraíonn i meastachán dlúis ar an méadrach iartheachtacha, níl staidéar déanta ar an gcomhghaolmhaireacht chruinn le haghaidh tascanna giniúna seichimh. Sa pháipéar seo, trí roinnt meastóirí dlúis a chur i gcomparáid le cúig thasc meaisín-aistriúcháin, feicimid go n-athraíonn an comhghaol idir rátálacha samhlacha bunaithe ar chosúlachtaí loga agus BLEU go mór ag brath ar raon na dteaghlach samhlacha a bhfuiltear ag déanamh comparáide orthu. Ar an gcéad dul síos, tá comhghaolmhaireacht mhór idir dóchúlacht logála agus BLEU nuair a bhreithnímid ar shamhlacha laistigh den teaghlach céanna (m.sh. samhlacha uath-aischéimnitheacha, nó samhlacha athróg folaigh a bhfuil an paraiméadarú céanna acu ar an gceann roimhe). Mar sin féin, ní thugaimid faoi deara aon chomhghaol idir rátálacha samhlacha thar theaghlaigh éagsúla: (1) i measc samhlacha athróg folaigh neamh-uathchéimnitheacha, is fearr dáileadh solúbtha roimh ré ag meastachán dlúis ach tugann sé cáilíocht giniúna níos measa ná réamhchéimnitheach simplí, agus (2) samhlacha uath-aischéimnitheacha.
cuireann siad an fheidhmíocht aistriúcháin is fearr ar fáil ar an iomlán, agus tugann samhlacha athróg folaigh le sreabhadh normalaithe roimhe seo an dóchúlacht logála amach is airde thar gach tacar sonraí. Dá bhrí sin, molaimid úsáid a bhaint as réamhshimplí don mhúnla athróg neamh-uath-chéimnitheach folaigh nuair atá luas giniúna gasta ag teastáil.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Suna iya ƙayyade aikin masu ƙaramar-da-sauri, kamar fassarar maɓalli da kuma masu faɗi-zuwa-matsayi, za'a iya iya ƙayyade girma ga matsalar da aka bãyar da cikin cikin akwatin x: p(y).x). Idan an gaskata wannan fassarar, yana da amfani da shiryoyin-durowa zuwa-sequence masu amfani da shiryoyin log-inganci kan wata fitina. Babu kasa, jiyyan kizalin da aka saka (ko kuma an daidaita gaura) shine a gane shi mafi kyaun fitarwa wanda aka bai wa inki, kuma kõwane aikin yana da metric R wanda ke ƙara wata motel da yana sami'a da tsarin misalin y*: R(y, y* * - x). A lokacin da Muke tsammãni da wata misali wanda ya fi ƙaranci ga damƙara, yana aiki mai kyau a kan metric ƙarƙashin ruwa, kuma ba a karanta mazaunin da hakki ba ga aikin dangi na dabam. Ga wannan takardan, a sami da masu ƙidãya masu ƙaranci a kan aikin fassarar mashinaki shan, za'a gane da mazaunin tsakanin da suka yi danganta a kan masu tsari-zane-zane da BLEU yana daidaita mai girma, kuma yana daidaita ga tsarin iyãlanta misãlai da za'a sammenliki. First, log-likelihood is highly correlated with BLEU when we consider models within the same family (e.g. autoregressive models, or latent variable models with the same parameterization of the prior). Babu, ko kuma ba mu ga wata danganci ba a tsakanin danganta masu motsi na daban-jama'a guda: (1) cikin misãlai masu motsi na daban-daban mai nuna ko-kandamati, wani fleksibo ne mafi alhẽri a kwanan rabin rabon nau'in, kuma yana bãyar da nau'in kizafi mafi girma daga birnin gaba ɗaya, da kuma (2) misãlai masu kandamatin fara-regressive
Ka motsa mafi kyaun fassarar aiki a jumla, da kuma motel masu variant na farko da shirin cire-mai normal gabanin ka bãyar da mafi kyauta-log-inganci duk tsarin database. Saboda haka, muna shawarar mu da amfani da wani abu mai sauƙi wa motel na daban-daban mai nuna-regressive, idan an tambaye haraka wa zaɓen tebur.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>משימות רבות של יוצר רצף-לרצף, כולל תרגום מכונת וטקסט-לנאום, יכולות להתייצב כהערכה של צפיפות ההוצאה y בהתחשב בהוצאה x: p( y[UNK]x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. בכל אופן, המטרה של יוצר רצף לרצף (או חזיון מבוסס) היא למצוא את ההוצאה הטובה ביותר y שנתן כניסה x, וכל משימה יש את R המטרי המטרי המאוחר שלה שמוציא תוצאה מודל על ידי השוואה נגד קבוצה של התייחסות y*: R( y, y*  x). למרות שאנחנו מקווים שמודל שמתפתח בערכת צפיפות גם מתבצע היטב במטרית התחתונה, התשובה המדויקת לא נבדקה עבור משימות יוצר רצף. בעיתון הזה, על ידי שיוות מספר מערכי צפיפות על חמישה משימות התרגום מכונות, אנו מוצאים שהקשר בין הדרגות של דוגמנים מבוססים על סבירות לוג-סבירות ולBLEU שונה באופן משמעותי תלוי בטווח של משפחות הדוגמנים שנשוואים. קודם כל, סבירות לוג-קושר מאוד עם BLEU כאשר אנו שוקלים דוגמנים בתוך אותה משפחה (למשל דוגמנים אוטומגרסיבים, או דוגמנים משתנים אוטומטיים עם אותו פרמטריזציה של קודם). בכל אופן, אנחנו שומרים על שום קשר בין הדרגות של דוגמנים ברחבי משפחות שונות: (1) בין דוגמנים לא אוטו-אוטו-אגרסיביים משתנים אוטו-אגרסיביים, פיצוי קודם גמיש הוא טוב יותר בערכת צפיפות, אבל נותן איכות דור גרועה יותר מאשר דוגמנים קודמים פשוטים, ו
מציעים את ביצועי התרגום הטובים ביותר באופן כללי, בעוד מודלים משתנים מוסתרים עם זרם נורמלי קודם נותנים את הסיכוי הגבוה ביותר של היציאה מחזיקה בכל קבוצות נתונים. לכן, אנו ממליצים להשתמש בעבר פשוט למודל השתנה הלא-אוטומגרסיבי השתנה הלא-אוטומגרסיבי כשמהירות הדור המהירה מבוקשת.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>मशीन अनुवाद और टेक्स्ट-टू-स्पीच सहित कई अनुक्रम-से-अनुक्रम पीढ़ी के कार्यों को इनपुट x: p(y|x) को देखते हुए आउटपुट y के घनत्व का अनुमान लगाने के रूप में प्रस्तुत किया जा सकता है। इस व्याख्या को देखते हुए, परीक्षण सेट पर सशर्त लॉग-संभावना का उपयोग करके अनुक्रम-से-अनुक्रम मॉडल का मूल्यांकन करना स्वाभाविक है। हालांकि, अनुक्रम-से-अनुक्रम पीढ़ी (या संरचित भविष्यवाणी) का लक्ष्य एक इनपुट एक्स को देखते हुए सबसे अच्छा आउटपुट वाई ढूंढना है, और प्रत्येक कार्य का अपना डाउनस्ट्रीम मीट्रिक आर होता है जो संदर्भों के एक सेट के खिलाफ तुलना करके एक मॉडल आउटपुट स्कोर करता है y *: R(y, y* | x)। जबकि हम आशा करते हैं कि घनत्व अनुमान में उत्कृष्टता प्राप्त करने वाला एक मॉडल डाउनस्ट्रीम मीट्रिक पर भी अच्छा प्रदर्शन करता है, अनुक्रम पीढ़ी के कार्यों के लिए सटीक सहसंबंध का अध्ययन नहीं किया गया है। इस पेपर में, पांच मशीन अनुवाद कार्यों पर कई घनत्व अनुमानकों की तुलना करके, हम पाते हैं कि लॉग-संभावना और BLEU के आधार पर मॉडल की रैंकिंग के बीच सहसंबंध तुलना किए जा रहे मॉडल परिवारों की सीमा के आधार पर काफी भिन्न होता है। सबसे पहले, लॉग-संभावना BLEU के साथ अत्यधिक सहसंबद्ध है जब हम एक ही परिवार के भीतर मॉडल पर विचार करते हैं (उदाहरण के लिए autoregressive मॉडल, या अव्यक्त चर मॉडल पूर्व के एक ही पैरामीटराइजेशन के साथ)। हालांकि, हम विभिन्न परिवारों में मॉडल की रैंकिंग के बीच कोई संबंध नहीं देखते हैं: (1) गैर-ऑटोरिग्रेसिव अव्यक्त चर मॉडल के बीच, एक लचीला पूर्व वितरण घनत्व अनुमान पर बेहतर है, लेकिन एक साधारण पूर्व की तुलना में बदतर पीढ़ी की गुणवत्ता देता है, और (2) autoregressive मॉडल
समग्र रूप से सबसे अच्छा अनुवाद प्रदर्शन प्रदान करते हैं, जबकि एक सामान्यीकृत प्रवाह के साथ अव्यक्त चर मॉडल सभी डेटासेट में उच्चतम आयोजित लॉग-आउट लॉग-संभावना देते हैं। इसलिए, हम अव्यक्त चर गैर-autoregressive मॉडल के लिए एक सरल पूर्व का उपयोग करने की सलाह देते हैं जब तेजी से पीढ़ी की गति वांछित है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mnogi zadatak generacije sekvence do sekvence, uključujući prevod strojeva i tekst do govora, mogu se postaviti kao procjena gustoće izlaza y s obzirom na ulaz x: p(y.x). S obzirom na ovu interpretaciju, prirodno je procijeniti modele sekvence do sekvence koristeći uvjetnu mogućnost log-vjerojatnosti na testu. Međutim, cilj generacije sekvence-do-sekvence (ili strukturirane predviđanje) je pronaći najbolji izlaz y dajući ulaz x, a svaki zadatak ima svoj vlastiti doljeni metrički R koji rezultira izlaz modela uspoređujući s setom referencija y*: R(y, y*.x). Iako se nadamo da model koji nadmašuje procjenu gustine također dobro izvršava na donjem metriku, tačna korelacija nije ispitivana za zadatke generacije sekvencije. U ovom papiru, uspoređujući nekoliko procjena gustoće na pet zadataka prevoda strojeva, smatramo da se povezanost između redova modela baziranog na log-vjerojatnosti i BLEU-u značajno razlikuje u ovisnosti o rasponu modelnih obitelji koje se uspoređuju. Prvo, vjerojatnost dnevnika je vrlo povezana s BLEU-om kada razmotrimo modele unutar iste obitelji (npr. autoregresivne modele ili latentne varijantne modele s istim parameterizacijom prije). Međutim, mi ne primjećujemo korelaciju između redova modela u različitim obiteljima: 1) između modela ne autoregresivnih latentnih promjena, fleksibilna prethodna distribucija je bolja za procjenu gustosti, ali daje gore kvalitet generacije nego jednostavno ranije, i (2) autoregresivne modele
nude najbolje učinkovito prevoda ukupno, dok latentni varijačni modeli s normalizacijskim tokom prije daju najveću vjerojatnost izvršenog izvještaja u svim podacima. Stoga preporučujemo korištenje jednostavnog ranije za latentnu promjenu ne autoregresivnog model a kada se žele brzina generacije.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Számos szekvencia-szekvencia generálási feladat, beleértve a gépi fordítást és a szöveg-beszédet, úgy állítható fel, mint a kimenet y sűrűségének becslése x: p(y|x bemenet). Ezt az értelmezést figyelembe véve természetes, hogy egy teszthalmazon feltételes naplóvalószínűséggel értékeljük a szekvencia-szekvencia modelleket. A szekvencia-szekvencia generálás (vagy strukturált előrejelzés) célja azonban az, hogy megtalálja a legjobb y kimenetet egy x bemenettel, és minden feladatnak megvan a saját downstream metrika R, amely a modell kimenetét egy y*: R(y, y* | x referenciasorozattal összehasonlítva értékeli. Bár reméljük, hogy a sűrűségbecslésben kiváló modell a downstream metrikán is jól teljesít, a pontos korrelációt nem tanulmányozták a sorozatgyártási feladatok esetében. Ebben a tanulmányban több sűrűségbecslőt összehasonlítva öt gépi fordítási feladatra vonatkozóan azt találjuk, hogy a log-valószínűség és a BLEU kategóriák közötti korreláció jelentősen eltérő az összehasonlított modellcsaládok tartományától függően. Először is, a log-valószínűség nagymértékben korrelálódik a BLEU-val, ha ugyanazon családon belüli modelleket vesszük figyelembe (pl. autoregresszív modelleket, vagy látens változó modelleket, amelyek ugyanazon paraméterezéssel rendelkeznek, mint a korábbi). Nem figyelünk azonban összefüggést a modellek különböző családok közötti rangsorolások között: (1) a nem-autoregresszív látens változó modellek között a rugalmas előzetes eloszlás jobb a sűrűség becslésénél, de rosszabb generációs minőséget biztosít, mint egy egyszerű korábbi modellek, és (2) autoregresszív modellek
Összességében a legjobb fordítási teljesítményt nyújtja, míg a normalizáló áramlást megelőző látens változó modellek a legnagyobb tartós naplózási valószínűséget biztosítják az összes adatkészletben. Ezért javasoljuk, hogy a látens változó nem autoregresszív modellhez egy egyszerű előzetet használjon, ha gyors generációs sebességre van szükség.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Շատ հաջորդականություն-հաջորդականություն ստեղծող առաջադրանքներ, ներառյալ մեքենայի թարգմանությունը և տեքստ-խոսքը, կարելի է դիտարկել որպես արտադրության y խտությունը հաշվարկված x: Եթե հաշվի առնենք այս մեկնաբանությունը, բնական է գնահատել հաջորդականություն հաջորդականության մոդելները՝ օգտագործելով փորձարկման համակարգի պայմանավոր հավանականությունը: Այնուամենայնիվ, հաջորդականության հաջորդականության (կամ կառուցվածված կանխատեսումների) նպատակն է գտնել լավագույն արտադրությունը y-ի, որը տրվում է x-ի, և յուրաքանչյուր խնդիր ունի իր սեփական ներքևի մետրական R-ը, որը գնահատում է մոդելի արտադրությունը համեմատելով y*: R(y, y*՝ x) խորհրդ Մինչդեռ մենք հույս ունենք, որ խտության գնահատման մեդելը լավ է աշխատում նաև հետագա մետրիկայի վրա, ճշգրիտ համեմատությունը չի ուսումնասիրել հաջորդականության սերունդների առաջադրանքների համար: Այս թղթի մեջ, համեմատելով որոշ խտության գնահատողներ հինգ մեքենայի թարգմանման առաջադրանքների վրա, մենք հայտնաբերում ենք, որ մոդելների գնահատականների կապը, որը հիմնված է լոգ-հավանականության և ԲԼԵՎ-ի վրա, նշանակաբար տարբերվում է կախ Առաջինը, լոգ-հավանականությունը շատ կապված է ԲԼԵՎ-ի հետ, երբ մենք դիտարկում ենք նույն ընտանիքի մոդելներ (օրինակ, ինքնագրեսիվ մոդելներ, կամ թաքնված փոփոխական մոդելներ, որոնք ունեն նախկինում նույն պարամետրիզացիան): Այնուամենայնիվ, տարբեր ընտանիքների մոդելների դասակարգումների միջև հարաբերակցություն չենք նկատում: (1) ոչ ինքնաարգեսիվ թաքնված փոփոխականների մոդելների միջև, ճկուն նախկին բաշխվածությունը ավելի լավ է խտության գնահատման ժամանակ, բայց ավելի վատ է սերունդի որակը, քան պարզ նախկին, և (2)
ներկայացնում են ամենալավ թարգմանման արդյունքները ընդհանուր առմամբ, մինչդեռ թաքնված փոփոխականների մոդելները, որոնք նախկինում նորմալ հոսք ունեն, բոլոր տվյալների համակարգերի մեծ հավանականությունն են: Այդ պատճառով, մենք խորհուրդ ենք տալիս օգտագործել պարզ նախօրինակ թաքնված փոփոխականի ոչ ինքնաարգեսիվ մոդելի համար, երբ արագ սերունդ արագություն է ցանկանում:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Banyak tugas generasi urutan-ke-urutan, termasuk terjemahan mesin dan teks-ke-pidato, dapat diposisikan sebagai perhitungan densitas output y diberikan input x: p(yx). Mengingat interpretasi ini, alami untuk mengevaluasi model urutan ke urutan menggunakan kemungkinan log kondisional pada set tes. Namun, tujuan generasi urutan-ke-urutan (atau prediksi strukturasi) adalah untuk menemukan output terbaik y yang diberikan input x, dan setiap tugas memiliki R metrik turun sendiri yang mencetak output model dengan membandingkan dengan set referensi y*: R(y, y*  x). Sementara kita berharap bahwa model yang melebihi dalam penilaian densitas juga bekerja dengan baik pada metrik turun, korelasi tepat belum dipelajari untuk tugas generasi urutan. Dalam kertas ini, dengan membandingkan beberapa penilai ketepatan pada lima tugas terjemahan mesin, kami menemukan bahwa korelasi antara penilaian model berdasarkan kemungkinan log dan BLEU berbeda secara signifikan bergantung pada jangkauan dari keluarga model yang dibandingkan. Pertama, kemungkinan log sangat berkorelasi dengan BLEU ketika kita mempertimbangkan model dalam keluarga yang sama (misalnya model autoregresif, atau model variabel latent dengan parameterisasi yang sama dari sebelumnya). Namun, kita tidak memperhatikan korelasi antara rangkaian model di antara keluarga yang berbeda: (1) diantara model variabel laten bukan-autoregresif, distribusi sebelumnya fleksibel lebih baik pada penilaian densitas tetapi memberikan kualitas generasi yang lebih buruk daripada model sebelumnya sederhana, dan (2) model autoregresif
menawarkan prestasi terjemahan terbaik secara keseluruhan, sementara model variabel latent dengan aliran normalisasi sebelumnya memberikan kemungkinan log-out tertinggi di seluruh set data. Oleh karena itu, kami merekomendasikan menggunakan Sebelumnya sederhana untuk model variabel tidak-autoregresif latent ketika kecepatan generasi cepat diinginkan.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Molte attività di generazione sequenza-sequenza, tra cui traduzione automatica e testo-parola, possono essere poste come stimare la densità dell'output y dato l'input x: p(y|x). Data questa interpretazione, è naturale valutare modelli sequenza-sequenza utilizzando la probabilità di log condizionale su un set di test. Tuttavia, l'obiettivo della generazione sequenza-sequenza (o predizione strutturata) è quello di trovare l'output migliore y dato un input x, e ogni attività ha la sua metrica a valle R che segna un output modello confrontando un insieme di riferimenti y*: R(y, y* | x). Mentre ci auguriamo che un modello che eccelle nella stima della densità funzioni bene anche sulla metrica a valle, l'esatta correlazione non è stata studiata per le attività di generazione di sequenze. In questo articolo, confrontando diversi stimatori di densità su cinque compiti di traduzione automatica, troviamo che la correlazione tra le classifiche dei modelli basate su log-probability e BLEU varia significativamente a seconda della gamma delle famiglie di modelli confrontati. In primo luogo, la probabilità di log è altamente correlata con BLEU quando consideriamo modelli all'interno della stessa famiglia (ad esempio modelli autoregressivi, o modelli variabili latenti con la stessa parametrizzazione del precedente). Tuttavia, non osserviamo alcuna correlazione tra le classifiche dei modelli in diverse famiglie: (1) tra i modelli variabili latenti non autoregressivi, una distribuzione precedente flessibile è migliore alla stima della densità ma dà una qualità di generazione peggiore rispetto a un semplice precedente, e (2) modelli autoregressivi
offrono le migliori prestazioni di traduzione in generale, mentre i modelli variabili latenti con un flusso normalizzante precedente offrono la più alta probabilità di log-out in tutti i set di dati. Pertanto, si consiglia di utilizzare un semplice precedente per il modello non autoregressivo variabile latente quando si desidera una velocità di generazione veloce.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>機械翻訳及びテキスト読み上げを含む多くのシーケンス間生成タスクは、入力ｘ ： ｐ （ ｙ ｜ ｘ ）が与えられたときの出力ｙの密度を推定するものとして提示され得る。 この解釈を考えると、試験セット上の条件付き対数尤度を使用してシーケンス間モデルを評価するのは当然である。 しかしながら、シーケンス間生成（または構造化予測）の目標は、入力xを与えられた最良の出力yを見つけることであり、各タスクは、参照y *: R (y, y *| x)のセットと比較することによってモデル出力をスコア付けする独自の下流メトリックRを有する。 密度推定に優れたモデルが下流のメトリックでも優れたパフォーマンスを発揮することを願っていますが、シーケンス生成タスクについては正確な相関関係は研究されていません。 本論文では， 5つの機械翻訳タスクのいくつかの密度推定子を比較することで，比較対象のモデルファミリーの範囲によって，対数尤度に基づくモデルのランキングとBLEUの相関関係が大きく異なることがわかる． まず、同じファミリー内のモデル（例えば、自己回帰モデル、または先行モデルと同じパラメータを持つ潜在的な変数モデル）を考えると、対数尤度はBLEUと高度に相関します。 しかし、異なるファミリーにわたるモデルのランキング間の相関は観察されません。（ 1 ）非自己回帰的潜在変数モデルのうち、柔軟な事前分布は密度推定に優れていますが、単純な事前分布よりも生成品質が悪く、（ 2 ）自己回帰モデル
は全体的に最高の翻訳パフォーマンスを提供し、正規化フローを持つ潜在的な変数モデルは、すべてのデータセットにわたって最大のホールドアウトログ可能性を提供します。したがって、高速生成が望ましい場合には、潜在変数非自動回帰モデルに単純な事前処理を使用することをお勧めします。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Multiple Nanging titik-titik iki, lak wis rak dadi nggawe model sing sekènsi-to-sekènsi ning gambar condional log-likely nang ujian seten. politenessoffpolite"), and when there is a change ("assertivepoliteness Genjer dhéwé nambah kuwi model sing apik kesempalahan kanggo nggawe kesempalahan ngono nggawe barang kelas Metric, coralation sing diapakan ora bisa digaweh kanggo ngerasahan seneng operasi layarané Nang paper iki, nggawe gerarangke sampulan kanggo sampek dadi tanggal 5 nggo tarjamahan Sampeyan politenessoffpolite"), and when there is a change ("assertive
translation Laptop" and "Desktop</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>მნიშვნელოვანი შემდეგ შემდეგ შემდეგ შემდეგ მოქმედება, რომელიც მაქსინური შემდეგ და ტექსტის შემდეგ სიტყვა, შეიძლება იყოს, როგორც შემდეგ შემდეგ y გადატანა შემდეგ გადატანა x: p(y). ამ ინტერპუქციის შესახებ, შესახებ ტესტის სეტის შესახებ შესახებ შესახებ მოდელების შესახებ შესახებ მოდელების შესახებ. მაგრამ, შემდეგ შემდეგ შემდეგ შემდეგ (ან სტრუქტურული პროგრამების) მიზეზი იქნება, რომ საკეთესო შემდეგ y შემდეგ გადატანა x, და ყოველ დავალება აქვს საკეთესო მეტრიკური R, რომელიც მოდელური გადატანა, რომელიც შემდეგ შე თუმცა ჩვენ გვემედით, რომ მოდელი, რომელიც მცირეობის განსაზღვრებულობაში უფრო კარგი გავაკეთებს მეტრიკში, მარტივი კორელაცია არ მოსწავლია შემდეგების მოქმედებისთვის. ამ დოკუნეში, რამდენიმე მნიშვნელობის განსაზღვრებით ხუთი მანქანის განსაგულისხმებით, ჩვენ აღმოჩნეთ, რომ მოდელის რენექციების რენექციები და BLEU-ის შესაძლებლობაზე მნიშვნელოვანია, რომელიც მოდელის ოჯა პირველი, ჩვენ იგივე ოჯახში მოდელები (მაგალითად, ავტორეგრესიური მოდელები, ან წინასწორი ცვლილებული მოდელები იგივე პარამეტრიზაციით) დავფიქრობთ. მაგრამ, ჩვენ არაფერი მოდელების რენექციების შორის კოლექცია განსხვავებული ოჯახში: (1) არაფერადრეგრესიური ლატენტიური ცვლილების მოდელების შორის, ძალიან წინაფერად წინაფერად განსხვავებული განსხვავება უფრო მეტია, მაგრამ უფ
ყველაზე უკეთესი გაგრძელების გამოყენება, მაგრამ ლატენტი ცვლილების მოდელები, რომლებიც ნორმალიზური გამოყენება წინ უფრო უფრო მეტი გაგრძელებული მონაცემების შესაძ ამიტომ, ჩვენ შევძლებთ გამოყენოთ საუკეთესო წინასწარმოდგენილი არ ავტორეგრესიური მოდელის გამოყენება, როდესაც სწრაფად წინასწარმოდგენილი სიჩქარე</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Бірнеше рет мен рет жасау тапсырмалары, компьютердің аударуы мен мәтіннен сөйлесу дегенде, y келтірілген y- нің жиілігін бағалау үшін болады. Бұл толыққан сияқты, сынақ жиынында тәуелді журнал мүмкіндігін қолданатын sequence- to- sequence үлгілерін бағалауға тәуелді. Бірақ, реттеу мен реттеу құрылғысының (немесе құрылған алдын- алау) мақсаты - y келтірілген x- нің ең жақсы шығысын табу, және әрбір тапсырманың төменгі метрикалық R- сілтемелеріне сәйкес келтірілген үлгі шығысын есептеп, y*: R( y, y Біз тұтықтығының бағалау үлгісін басқа төменгі метрикалық үлгісінде жақсы жұмыс істеу үшін үміттенеміз, дұрыс корелация реттеу тапсырмалары үшін оқылмайды. Бұл қағазда, бес машинаны аудару тапсырмалардың бірнеше жиілік оқиғаларын салыстырып, журнал мүмкіндігіне негізделген модельдердің жолдарының және BLEU әдістеріне негізделген модельдердің аумағына қатысты. Біріншіден, журнал мүмкіндігі бір отбасындағы үлгілер үлгілерін қарастырып тұрғанда, BLEU- мен көп қатынасыз (мысалы, авторегрессиялық үлгілер, немесе алдыңғы параметрлерінің бірдей параметрлері болса Бірақ біз әртүрлі отбасындағы үлгілердің жолдары арасындағы қатынасыз жоқ: (1) авторегрессиялық қатынасыз үлгілер арасында, алдындағы гибсиялық үлестіріміз тыныштық бағалауында жақсы, бірақ қарапайым алдындағы үлгілерден жақсы сапатт
Бүкіл деректер жиындарында ең жоғары жұмыс істеу мүмкіндігін көрсетеді. Сондықтан, жылдам жасау жылдамдығын қалаған кезде, келесі айнымалылығы авторегрессивні емес үлгі үшін қарапайым алдында қолдануға рекомендіреміз.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>기계 번역과 텍스트 음성을 포함한 많은 시퀀스에서 시퀀스 생성 작업은 주어진 입력 x:p(y | x) 상황에서 출력 y의 밀도를 추정하는 것으로 가정할 수 있다.이런 해석을 감안하면 테스트 집합에서 조건 대수를 사용하여 서열 간 모델을 평가하는 것은 자연스러운 것이다.그러나 시퀀스에서 시퀀스 생성(또는 구조화 예측)까지의 목표는 입력 x를 지정한 상황에서 가장 좋은 출력 y를 찾는 것이다. 모든 작업은 자신의 하류 도량 R을 가지고 참고 값 y*:R(y, y*|x)과 비교하여 모델 출력을 평가한다.밀도 평가에서 뛰어난 모델도 하류 도량에서 양호하게 나타나기를 희망하지만 서열 생성 임무의 정확한 관련성은 아직 연구되지 않았다.본고에서 다섯 가지 기계 번역 임무의 몇 가지 밀도 평가를 비교한 결과 대수 유사성과 BLEU의 모델 순위 간의 관련성은 비교된 모델족의 범위에 따라 현저히 다르다는 것을 알 수 있다.먼저, 동일한 패밀리의 모델(예: 회귀 모델, 또는 동일한 선험적 매개 변수가 있는 잠재 변수 모델)을 고려할 때 대수는 BLEU 높이와 관련이 있는 것처럼 보입니다.그러나 우리는 서로 다른 가족의 모델 순위 사이에 관련성이 없다는 것을 관찰했다. (1) 비자귀환 잠재 변수 모델에서 유연한 선험 분포는 밀도 평가에 있어 더 좋지만 간단한 선험 분포보다 질이 떨어진다. (2) 자귀환 모델.
전체적으로 가장 좋은 전환 성능을 제공했고 모든 데이터가 집중되어 규범화 흐름을 가진 잠재적 변수 모델은 가장 높은 보존 대수 가능성을 제시했다.따라서 빠른 생성 속도가 필요할 때 잠재 변수의 비자귀환 모델에 대해 간단한 선험을 사용하는 것을 권장합니다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Daugelis iš eilės į eilę sukuriamų užduočių, įskaitant mašininį vertimą ir tekstą į žodį, gali būti laikoma išėjimo y tankio apskaičiavimu, atsižvelgiant į įvestą x: p(y[UNK]x). Atsižvelgiant į šį aiškinimą, natūralu įvertinti sekos modelius iš eilės į seką naudojant sąlyginę log tikimybę bandymų rinkinyje. Tačiau sekos į seką sukūrimo (arba struktūrizuotos prognozės) tikslas – rasti geriausią išėjimą y, nurodant įvestą x, ir kiekviena užduotis turi savo tolesnę metrinę R, kuri vertina modelio išėjimą palygindama su nuorodų rinkiniu y*: R(y, y*  x). Nors tikimės, kad modelis, kuris geriau vertina tankį, taip pat gerai veikia tolesnės grandinės metrinėmis sąlygomis, tiksli koreliacija nebuvo tirta sekos generavimo užduočių atžvilgiu. Šiame dokumente, palygindami kelis tankio vertinimo rodiklius su penkiomis mašinų vertimo užduotimis, nustatome, kad modelių klasifikacijų, pagrįstų log tikimybe, ir BLEU, koreliacija labai skiriasi priklausomai nuo lyginamų modelių šeimų įvairovės. Pirma, log tikimybė yra labai koreliuojama su BLEU, kai atsižvelgiame į modelius toje pačioje šeimoje (pvz., autoregresinius modelius arba latentinius kintamuosius modelius su ta pačia ankstesne parametrizacija). However, we observe no correlation between rankings of models across different families: (1) among non-autoregressive latent variable models, a flexible prior distribution is better at density estimation but gives worse generation quality than a simple prior, and (2) autoregressive models
siūlo geriausius vertimo rezultatus apskritai, o latentiniai kintamieji modeliai su normalizuojančiu srautu prieš tai suteikia didžiausią laikomo išėjimo iš visų duomenų rinkinių tikimybę. Todėl rekomenduojame naudoti paprastą prielaidą latentiniam kintamajam ne autoregresiniam modeliui, kai norima greitos kartos greičio.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Многу задачи за генерација од секвенца до секвенца, вклучително и машински превод и текст до говор, можат да бидат поставени како проценка на густината на излезот y со влогот x: p( y[UNK]x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. Сепак, целта на генерацијата од секвенца до секвенца (или структурирано предвидување) е да се најде најдобриот излез y даден со влог x, и секоја задача има своја метрична Р на потег која постигнува излез од модел споредувајќи се со множина референции y*: R( y, y*  x). И покрај тоа што се надеваме дека моделот кој е одличен во проценката на густината, исто така, ќе функционира добро и на понатамошната метрика, точната корелација не е проучена за задачите на генерација на секвенца. Во овој весник, споредувајќи неколку проценувачи на густина на пет машински преведувачки задачи, откриваме дека корелацијата помеѓу рангирањата на моделите базирани на лог-веројатноста и БЛЕ се разликува значително во зависност од дометот на моделните семејства што се споредуваат. Прво, веројатноста на логот е високо поврзана со БЛЕУ кога разгледуваме модели во истото семејство (на пример авторегресивни модели или лантни модели со исти параметризации од претходното). Сепак, не забележуваме корелација помеѓу рангирањата на моделите во различните семејства: (1) помеѓу неавторегресивните лантни модели, флексибилната претходна дистрибуција е подобра во проценката на густината, но дава полош квалитет на генерацијата отколку едноставен претходен, и
нудат најдобра резултат на превод вкупно, додека latent variable models with a normalizing flow prior give the highest held-out log-out probability across all datasets. Затоа, препорачуваме да се користи едноставен претход за latent variable non-autoregressive model кога е желба брза генерација брзина.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>മെഷിന്‍ പരിഭാഷവും വാക്കുകളും ചേര്‍ന്നിരിക്കുന്ന പ്രവൃത്തികളില്‍ ഒരുപാട് പ്രവൃത്തികള്‍ക്ക് എണ്ണിപ്പുട്ട് കൊടുത്തിരിക്കുന്ന ഉള്‍പ്പ ഈ വ്യാഖ്യാനം നല്‍കിയാല്‍, പരീക്ഷണസജ്ജീകരണത്തില്‍ സെക്കന്‍സ് മോഡലുകള്‍ പരിഗണിക്കുന്നത് ഉപയോഗിച്ച് സ്വാഭാവികമാണ്. എന്നാലും, സെക്കന്‍സ് തലമുറയുടെ ലക്ഷ്യം (അല്ലെങ്കില്‍ നിര്‍മ്മിക്കപ്പെട്ട പ്രവചനം) ഒരു ഇന്‍പുട്ട് നല്‍കപ്പെട്ട ഏറ്റവും നല്ല ഫലം കണ്ടെത്തുന്നതാണ്, ഓരോ ജോലിയും അതിന്‍റെ സ്വന്തം താഴ്ന് തൂക്കത്തിന്റെ കണക്കിന് മുകളില്‍ ഉയര്‍ന്ന ഒരു മോഡല്‍ പ്രവര്‍ത്തിക്കുന്നത് പോലും താഴ്വരയുടെ മെട്രിക്കിലും നല്ല പ്രവര്‍ത്തിക്കുന്നതാണെ ഈ പത്രത്തില്‍, അഞ്ചു മെഷീന്‍ പരിഭാഷണത്തിന്റെ ജോലികളില്‍ കുറച്ച് കഠിനമായ ഗണിറ്ററികള്‍ തുല്യമാക്കുന്നതിനാല്‍, ലോഗ്-സാധ്യതകള്‍ അടിസ്ഥാനമായി മോഡലുകളുടെ മാതൃ ആദ്യം, ലോഗ്- സാധ്യതകള്‍ ഒരേ കുടുംബത്തിന്റെ ഉള്ളില്‍ മോഡലുകള്‍ കാണുമ്പോള്‍ ബിലൂയുമായി വളരെ ബന്ധപ്പെട്ടിരിക്കുന്നു (ഉദാഹരണത്തിനായി സ്വയമാക എങ്കിലും വ്യത്യസ്ത കുടുംബങ്ങളില്‍ മാതൃകങ്ങളുടെ മാതൃകങ്ങളുടെ മാതൃകങ്ങള്‍ക്കിടയില്‍ നമുക്ക് ബന്ധമൊന്നും കാണാനാവില്ല: (1) സ്വാതന്ത്രികമാക്കുന്ന സാധാരണ മാതൃകങ്
എല്ലാ ഡാറ്റാസറ്റുകളിലും ഏറ്റവും മികച്ച വിവരങ്ങളുടെ പ്രവര്‍ത്തനങ്ങള്‍ മൊത്തം നല്‍കുക, പുതിയ മാറ്റലുകള്‍ സാധാരണമായി നീങ്ങുന്ന അതുകൊണ്ട്, നമ്മള്‍ പുതിയ മുന്‍പ് ഉപയോഗിക്കുന്നത് സ്വയം നിരീക്ഷിക്കാത്ത മോഡലിന് വേഗത്തില്‍ തലമുറതലമുറയുടെ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ихэнх дарааллаас дарааллаар үржүүлэх үйл ажиллагаа, машины хөрөнгө оруулалт, текст-т-хэлбэл, y-н орлуулалтын жинтэй байдлыг тооцоолж чадна. Энэ тодорхойлолтоор, дарааллаас дарааллаар дүрслэх загваруудыг шалгалтын хэлбэрээр ашиглаж байх нь байгалийн юм. Гэхдээ дарааллаас дарааллаар үржүүлэх (эсвэл бүтээгдэхүүний таамаглалт) зорилго нь y-г нэвтрүүлэх хамгийн сайн үржүүлэлтийг олох ба ажил бүр өөрийн доорх метрик R байдаг. Энэ нь загварын үржүүлэлтийг y*: R(y, y*.x-тэй харьцуулахад модель үржүүлэлтийг гаргадаг Хэдийгээр бид жинтэй тооцооллын загвар мөн доорх метрик дээр сайн ажилладаг гэдгийг найдаж байна. Яг тохиромжтой холбоотой нь дарааллын үеийн даалгаврыг судалж чадахгүй. Энэ цаасан дээр олон жинтэй тооцоологчийг таван машины орчуулалтын ажил дээр харьцуулахад бид Log-likelihood болон BLEU-ын загварын харьцуулалтын хоорондын хоорондын хоорондын хоорондын хоорондын хоорондын хоорондын хоорондоо маш чухал өөрчлөгдөж байна. Эхлээд, бид ижил гэр бүлийн дотор загваруудыг бодох үед BLEU-тэй маш холбоотой (жишээ нь автоregressive загваруудын загварууд, эсвэл өмнөх нь ижил параметрийг тодорхойлж байгаа соронзон өөрчлөлтийн загварууд). Гэхдээ бид өөр гэр бүлийн загварын хоорондох холбоотой зүйлийг анзаарч байхгүй: (1) авторегрессийн хувьсалын загваруудын хоорондох холбоотой зүйлийг анзаарч байхгүй. Өмнөх хувьсалын хэвлэл нь жинтэй тооцооллоос илүү сайн байдаг гэхдээ энгийн өмнөх үеэс
Хамгийн сайн орчуулагчийн үйл ажиллагааг дэлгэрүүлэхэд, хамгийн өндөр хувьсагчийн загварууд нь бүх өгөгдлийн санд хамгийн өндөр байрлалт гаргах боломжтой байдаг. Тиймээс бид хурдан үеийн хурдыг хүсч буй хурдтай хурдан автоматжуулах бус өөрчлөлтийн хувьд энгийн өмнө хэрэглэхэд энгийн загварыг зөвлөе.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x). Mengingat interpretasi ini, ia adalah semulajadi untuk menilai model urutan-ke-urutan menggunakan log-kemungkinan syarat pada set ujian. Namun, tujuan generasi jujukan-ke-jujukan (atau ramalan struktur) adalah untuk mencari output terbaik y yang diberikan input x, dan setiap tugas mempunyai R metrik turun sendiri yang mencetak output model dengan membandingkan dengan set rujukan y*: R( y, y * x). Sementara kita berharap bahawa model yang melebihi dalam penilaian ketepatan juga berjalan dengan baik pada metrik turun, korelasi tepat belum dipelajari untuk tugas generasi urutan. Dalam kertas ini, dengan membandingkan beberapa pengiraan densiti pada lima tugas terjemahan mesin, kami mendapati bahawa korelasi antara rangkaian model berdasarkan kemungkinan log dan BLEU berbeza secara signifikan bergantung pada julat keluarga model yang dibandingkan. Pertama, kemungkinan log sangat berkorelasi dengan BLEU apabila kita mempertimbangkan model dalam keluarga yang sama (cth. model autoregresif, atau model pembolehubah latent dengan parameterisasi yang sama dari sebelumnya). Namun, kita tidak memperhatikan korelasi antara rangkaian model melalui keluarga yang berbeza: (1) diantara model pembolehubah yang tersembunyi bukan-autoregressif, distribusi sebelumnya fleksibel lebih baik pada penilaian densiti tetapi memberikan kualiti generasi yang lebih buruk daripada model sebelumnya sederhana, dan (2) model autoregressif
menawarkan prestasi terjemahan terbaik secara keseluruhan, sementara model pembolehubah tersembunyi dengan aliran normalisasi sebelum memberikan kemungkinan log-out tertinggi di seluruh set data. Oleh itu, kami cadangkan menggunakan awal sederhana untuk model pembolehubah tidak-autoregresif yang tersembunyi apabila kelajuan generasi pantas diinginkan.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ħafna kompiti ta’ ġenerazzjoni minn sekwenza għal sekwenza, inklużi t-traduzzjoni tal-magna u t-test-għad-diskors, jistgħu jiġu ppreżentati bħala stima tad-densità tal-output y mogħtija l-input x: p(y[UNK]x). Minħabba din l-interpretazzjoni, huwa naturali li jiġu evalwati mudelli minn sekwenza għal sekwenza bl-użu tal-probabbiltà kondizzjonali ta’ log fuq sett tat-test. Madankollu, l-għan tal-ġenerazzjoni minn sekwenza għal sekwenza (jew tbassir strutturat) huwa li jinstab l-a ħjar output y mogħti input x, u kull kompitu għandu r-R metriku downstream tiegħu stess li jikkalkula output mudell billi jitqabbel ma’ sett ta’ referenzi y*: R(y, y*  x). Filwaqt li nittamaw li mudell li jkun eċċellenti fl-istima tad-densità jwettaq prestazzjoni tajba wkoll fuq il-metrika downstream, il-korrelazzjoni eżatta ma ġietx studjata għal kompiti ta’ ġenerazzjoni ta’ sekwenzi. F’dan id-dokument, billi tqabblu diversi stimaturi tad-densità fuq ħames kompiti ta’ traduzzjoni bil-magni, isibu li l-korrelazzjoni bejn il-klassifikazzjonijiet tal-mudelli bbażati fuq il-probabbiltà tal-ġurnal u l-BLEU tvarja b’mod sinifikanti skont il-firxa tal-familji tal-mudelli li qed jitqabblu. L-ewwel nett, il-probabbiltà ta’ log hija korrelata ħafna mal-BLEU meta nikkunsidraw mudelli fl-istess familja (e ż. mudelli awtoregressivi, jew mudelli varjabbli moħbija bl-istess parametrizzazzjoni ta’ qabel). Madankollu, ma osservajna l-ebda korrelazzjoni bejn il-klassifikazzjonijiet tal-mudelli bejn familji differenti: (1) fost mudelli varjabbli moħbija mhux awtoregressivi, distribuzzjoni flessibbli minn qabel hija a ħjar fl-istima tad-densità iżda tagħti kwalità agħar tal-ġenerazzjoni minn mudelli sempliċi minn qabel, u (2) awtoregressivi
joffru l-a ħjar prestazzjoni tat-traduzzjoni b’mod ġenerali, filwaqt li mudelli varjabbli latenti b’fluss normalizzanti minn qabel jagħtu l-ogħla probabbiltà ta’ log-out miżmuma fis-settijiet tad-dejta kollha. Għalhekk, nirrakkomandaw l-użu ta’ qabel sempliċi għall-mudell varjabbli mhux awtoregressiv moħbi meta tkun mixtieqa veloċità ta’ ġenerazzjoni mgħa ġġla.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Veel sequence-to-sequence generatietaken, waaronder machinevertaling en tekst-to-spraak, kunnen worden gesteld als het schatten van de dichtheid van de uitvoer y gegeven de invoer x: p(y|x). Gezien deze interpretatie is het natuurlijk om sequentiemodellen te evalueren met behulp van voorwaardelijke logwaarschijnlijkheid op een testset. Het doel van opeenvolging-tot-opeenvolging genereren (of gestructureerde voorspelling) is echter om de beste output y te vinden bij een invoer x, en elke taak heeft zijn eigen downstream metric R die een model output scoort door te vergelijken met een reeks referenties y*: R(y, y*, x). Hoewel we hopen dat een model dat uitblinkt in dichtheidsschatting ook goed presteert op de downstream metric, is de exacte correlatie niet bestudeerd voor sequentie generatie taken. In dit artikel, door verschillende dichtheidsschatters te vergelijken op vijf machinevertaaltaken, ontdekken we dat de correlatie tussen rangschikkingen van modellen op basis van log-waarschijnlijkheid en BLEU aanzienlijk varieert afhankelijk van het bereik van de te vergelijken modelfamilien. Ten eerste is log-waarschijnlijkheid sterk gecorreleerd met BLEU wanneer we modellen binnen dezelfde familie bekijken (bijvoorbeeld autoregressieve modellen, of latente variabele modellen met dezelfde parametrisering als de vorige). We zien echter geen correlatie tussen de ranglijsten van modellen over verschillende families: (1) onder niet-autoregressieve latente variabele modellen, een flexibele voorafgaande verdeling is beter in dichtheidsschatting, maar geeft een slechtere generatie kwaliteit dan een eenvoudige voorafgaande, en (2) autoregressieve modellen
bieden de beste vertaalprestaties over het algemeen, terwijl latente variabele modellen met een normaliserende flow voorafgaand de hoogste vastgehouden logwaarschijnlijkheid bieden voor alle datasets. Daarom raden we aan een eenvoudige voorafgaand te gebruiken voor het latente variabele niet-autoregressieve model wanneer een snelle generatie snelheid gewenst is.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mange oppgåver for generering av sekvens-til-sekvens, inkludert maskinsomsetjing og tekst-til-tale, kan plasserast som estimerer tettleiken p å utdata y gitt inn x: p(y.x). Gjennomsikt denne tolkinga, er det naturleg å evaluera sequence-to-sequence-modeller ved hjelp av vilkårleg loggsannsynlighet på eit testsett. Målet for å generera sequence-to-sequence (eller strukturerte forhåndsvising) er imidlertid å finna den beste utdata y gitt ein inndata x, og kvar oppgåve har sitt eigne nedstrømte metrisk R som scorer ein modell utdata ved å sammenligna med eit sett referanser y*: R( y, y *. x). Mens vi håper at eit modell som overstyrer tettheitsestisering også utfører godt på nedstrømmemetrikken, er det nøyaktige korrelasjonen ikkje studiert for sekvensgenerasjonsoppgåver. I denne papiret, ved samanlikning av fleire tettheitsestimatorar på fem maskineoversettelsesoppgåver, finn vi at korrelasjonen mellom rekningane av modeller basert på logsannsynligheten og BLEU varierer betydelig avhengig av området av modellefamiliane som vert samanlikna. Først er loggsannsynligheten svært korrelatert med BLEU når vi forstår modeller i samme familie (f.eks. autoregressiv modeller, eller latent variabel modeller med same parameterisering av førre). Men vi observerer ingen korrelasjon mellom rekningar av modeller i ulike familier: (1) blant ikkje-autoregressiv latent variabel modeller, fleksibel fordeling er bedre ved estimerering av tetthet, men gir dørre genereringskvalitet enn ein enkelt førre, og (2) autoregressiv modeller
tilbyr den beste utviklinga av omsetjinga overalt, mens latente variabel- modeller med ein normalisering av flyt før gjer den høgste likelykken for loggen på alle datasett. Derfor anbefaler vi å bruka ein enkel før for den latente variabelen som ikkje er autoregressiv modellen når rask genereringsfart er ønskt.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Wiele zadań generowania sekwencji na sekwencję, w tym tłumaczenie maszynowe i tekst na mowę, może być postawionych jako oszacowanie gęstości wyjścia y, biorąc pod uwagę wejście x: p(y|x). Biorąc pod uwagę tę interpretację naturalną jest ocena modeli sekwencji do sekwencji przy użyciu warunkowej prawdopodobieństwa logowania na zestawie testowym. Jednakże celem generowania sekwencji do sekwencji (lub strukturalnej predykcji) jest znalezienie najlepszego wyjścia y przy danym wejściu x, a każde zadanie ma własną dolną metrykę R, która ocenia wyjście modelu poprzez porównanie z zestawem odniesień y*: R(y, y*, x). Chociaż mamy nadzieję, że model, który wyróżnia się w szacowaniu gęstości, również sprawdza się dobrze w metryce dolnej, dokładna korelacja nie została zbadana dla zadań generowania sekwencji. W niniejszym artykule, porównując kilka oszacowaczy gęstości na pięciu zadaniach tłumaczenia maszynowego, stwierdzono, że korelacja pomiędzy rankingami modeli opartymi na log-prawdopodobieństwie a BLEU różni się znacząco w zależności od zakresu porównywanych rodzin modeli. Po pierwsze, log-prawdopodobieństwo jest wysoce skorelowane z BLEU, gdy rozważamy modele należące do tej samej rodziny (np. modele autoregresywne lub modele zmienne utajone z taką samą parametryzacją jak poprzednie). Nie obserwujemy jednak korelacji pomiędzy rankingami modeli w różnych rodzinach: (1) wśród modeli zmiennych nieautoregresyjnych latentnych elastyczna dystrybucja poprzednia jest lepsza w szacowaniu gęstości, ale daje gorszą jakość generacji niż zwykły prior oraz (2) modele autoregresyjne
oferują najlepszą ogólną wydajność tłumaczenia, podczas gdy ukryte modele zmiennych z normalizującym przepływem wcześniej dają największe prawdopodobieństwo utrzymywania logu we wszystkich zbiorach danych. Dlatego zalecamy użycie prostego uprzedniego modelu dla zmiennej latentnej nieautoregresywnej, gdy pożądana jest szybka prędkość generowania.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Muitas tarefas de geração de sequência a sequência, incluindo tradução automática e conversão de texto em fala, podem ser colocadas como estimativa da densidade da saída y dada a entrada x: p(y|x). Dada esta interpretação, é natural avaliar modelos de sequência a sequência usando log-verossimilhança condicional em um conjunto de teste. No entanto, o objetivo da geração de sequência a sequência (ou previsão estruturada) é encontrar a melhor saída y dada uma entrada x, e cada tarefa tem sua própria métrica de downstream R que pontua uma saída do modelo comparando com um conjunto de referências y *: R(y, y* | x). Embora esperemos que um modelo que se destaque na estimativa de densidade também tenha um bom desempenho na métrica a jusante, a correlação exata não foi estudada para tarefas de geração de sequência. Neste artigo, comparando vários estimadores de densidade em cinco tarefas de tradução automática, descobrimos que a correlação entre os rankings de modelos baseados em log-likelihood e BLEU varia significativamente dependendo do intervalo das famílias de modelos que estão sendo comparadas. Primeiro, a probabilidade de log é altamente correlacionada com o BLEU quando consideramos modelos dentro da mesma família (por exemplo, modelos autorregressivos ou modelos de variáveis latentes com a mesma parametrização do anterior). No entanto, não observamos correlação entre classificações de modelos em diferentes famílias: (1) entre modelos de variáveis latentes não autorregressivas, uma distribuição a priori flexível é melhor na estimativa de densidade, mas dá pior qualidade de geração do que uma a priori simples e (2) modelos autorregressivos
oferecem o melhor desempenho geral de tradução, enquanto os modelos de variáveis latentes com um fluxo de normalização anterior fornecem a maior probabilidade de log em todos os conjuntos de dados. Portanto, recomendamos o uso de uma prévia simples para o modelo não autorregressivo de variável latente quando a velocidade de geração rápida for desejada.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Multe sarcini de generare secvență-la-secvență, inclusiv traducerea automată și text-la-vorbire, pot fi considerate estimarea densității ieșirii y dată fiind intrarea x: p(y|x). Având în vedere această interpretare, este firesc să se evalueze modelele secvență-la-secvență utilizând probabilitatea logului condiționat pe un set de teste. Cu toate acestea, scopul generarii secvență-la-secvență (sau predicția structurată) este de a găsi cea mai bună ieșire y dată de intrare x, iar fiecare activitate are propria metrică din aval R care scorează o ieșire de model comparând cu un set de referințe y*: R(y, y* | x). Deși sperăm că un model care excelează în estimarea densității funcționează bine și pe metrica din aval, corelația exactă nu a fost studiată pentru sarcinile de generare a secvențelor. În această lucrare, prin compararea mai multor estimatoare de densitate pe cinci sarcini de traducere automată, constatăm că corelația dintre clasamentele modelelor bazate pe log-probabilitate și BLEU variază semnificativ în funcție de gama familiilor de modele comparate. În primul rând, probabilitatea log-ului este foarte corelată cu BLEU atunci când luăm în considerare modele din aceeași familie (de exemplu modele autoregresive, sau modele variabile latente cu același parametrizare a celui anterior). Cu toate acestea, nu observăm nicio corelație între clasamentele modelelor din diferite familii: (1) în rândul modelelor variabile latente non-autoregressive, o distribuție anterioară flexibilă este mai bună la estimarea densității, dar oferă o calitate a generației mai proastă decât un simplu anterior, și (2) modele autoregressive
Oferă cea mai bună performanță de traducere generală, în timp ce modelele variabile latente cu un flux de normalizare anterior oferă cea mai mare probabilitate de logare reținută în toate seturile de date. Prin urmare, vă recomandăm să utilizați un simplu anterior pentru modelul non-autoregresiv variabil latent atunci când se dorește viteza de generare rapidă.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Многие задачи построения последовательности в последовательности, включая машинный перевод и преобразование текста в речь, могут быть поставлены как оценка плотности выходного сигнала y при вводе x: p(y|x). Учитывая эту интерпретацию, естественно оценивать модели последовательности к последовательности, используя условную логарифмическую правдоподобие на тестовом наборе. Однако цель генерации последовательности в последовательность (или структурированного предсказания) заключается в том, чтобы найти лучший выход y при заданном входе x, и каждая задача имеет свою собственную последующую метрику R, которая оценивает выходную модель путем сравнения с набором ссылок y*: R(y, y* | x). Хотя мы надеемся, что модель, которая превосходит в оценке плотности, также хорошо работает по нисходящей метрике, точная корреляция не была изучена для задач генерации последовательности. В этой работе, сравнивая несколько оценщиков плотности по пяти задачам машинного перевода, мы обнаруживаем, что корреляция между ранжированием моделей на основе логарифмической правдоподобия и BLEU значительно варьируется в зависимости от диапазона сравниваемых семейств моделей. Во-первых, логарифмическая правдоподобие в значительной степени коррелируется с BLEU, когда мы рассматриваем модели в пределах одного семейства (например, авторегрессивные модели или модели скрытых переменных с той же параметризацией предшествующего). Тем не менее, мы не наблюдаем корреляции между ранжированиями моделей для различных семейств: (1) среди неавторегрессивных моделей скрытых переменных гибкое предварительное распределение лучше при оценке плотности, но дает худшее качество генерации, чем простые предыдущие, и (2) авторегрессивные модели
обеспечивают наилучшую эффективность трансляции в целом, в то время как латентные переменные модели с нормализующим потоком до дают наибольшую выдержку логарифмической правдоподобия во всех наборах данных. Поэтому мы рекомендуем использовать простой приоритет для неавторегрессивной модели скрытых переменных, когда требуется быстрая скорость генерации.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ගොඩක් පරීක්ෂණ-ට-පරීක්ෂණ පරීක්ෂණ වැඩක්, පරික්ෂණ පරීක්ෂණය සහ පාළු පරික්ෂණය-ට-පරීක්ෂණය සම්බන්ධ වෙනුවෙන්, ප්‍රවේ මේ අභිවාදයක් නිසා, පරීක්ෂණ සෙට් එකේ සාමාන්‍ය ලොක් වර්ගයක් භාවිත කරන්න ප්‍රතිභාවිතයි. හැබැයි, sequence-to-sequence පරීක්ෂණය (නැත්තම් සංස්ථාපිත ප්‍රශ්නයක්) ලක්ෂණය තමයි හොඳම ප්‍රශ්නයක් හොයාගන්න x, හැබැයි වැඩේ වැඩේ ප්‍රශ්නයක් තියෙනවා මෙට්‍රික් R අපි බලාපොරොත්තු වෙන්නේ මොඩේල් එකක් විශාල විශාල විශාල විශාල කරනවා කියලා, පහත් මෙට්‍රික් වලින් හොඳ විශාල කරනවා, සිද මේ පැත්තේ, මැෂින් වාර්ථාව පහක් විතරයි, අපි හොයාගන්නේ ලොක් වර්ගයක් සහ BLUE වර්ගයක් සඳහා මොඩල් වර්ගයක් සම්බන්ධ වෙනුවෙන් සම්බන්ධ වෙන මුලින්ම, ලොග් ප්‍රමාණය බොහොම සම්බන්ධ වෙන්නේ BLUE එක්ක, අපි එකම පවුලේ මොඩේල් හිතන්නේ කියලා (උදාහරණය ස්වයංග්‍රේෂිත මොඩේල්, නැත්ත නමුත්, අපි වෙනස් පවුලෙන් වෙනස් පවුලෙන් ප්‍රමාණයක් අතර කිසිම සම්බන්ධයක් නැහැ: (1) ස්වයංක්‍රියාත්මක වෙනස් මොඩේලන් අතර නැහැ ස්වයංක්‍රියාත්මක විශේෂ
සාමාන්‍ය වෙනස් මොඩල් එක්ක සාමාන්‍ය වෙනස් වෙනුවෙන් හොඳම වාර්ථාව ප්‍රවෘත්තියක් සාමාන්‍ය වෙනුවෙන් ප්‍රවෘත්තිය ස ඉතින්, අපි ඉක්මනින් වේගයේ වේගයක් අවශ්‍ය වෙලාවේ ස්වයංක්‍රියාත්මක වෙන්න පුළුවන් ප්‍රයෝජනයක් ප්‍රයෝ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Številna opravila generiranja zaporedja v zaporedje, vključno s strojnim prevajanjem in besedilom v govor, se lahko predstavljajo kot ocena gostote izhoda y glede na vhod x: p(y|x). Glede na to razlago je naravno oceniti modele zaporedja do zaporedja z uporabo pogojne log verjetnosti na preskusnem nizu. Vendar pa je cilj generiranja zaporedja v zaporedje (ali strukturirane napovedi) poiskati najboljši izhod y ob upoštevanju vhodnega x, vsaka opravila pa ima svojo merilno mero R, ki ocenjuje rezultat modela s primerjavo z nizom referenc y*: R(y, y* | x). Čeprav upamo, da model, ki odlikuje pri ocenjevanju gostote, uspešno deluje tudi pri metričnem merilu, natančna korelacija ni bila raziskana za naloge generiranja zaporedja. V tem prispevku smo s primerjavo več ocenjevalcev gostote pri petih nalogah strojnega prevajanja ugotovili, da se korelacija med razvrstitvami modelov na podlagi log verjetnosti in BLEU bistveno razlikuje glede na obseg primerjanih družin modelov. Prvič, log verjetnost je zelo korelacijska z BLEU, kadar upoštevamo modele znotraj iste družine (npr. avtoregresivni modeli ali latentni variabilni modeli z isto parametrizacijo kot prejšnji). Vendar pa ne opažamo nobene korelacije med razvrstitvami modelov v različnih družinah: (1) pri neavtoregresivnih latentnih variabilnih modelih je fleksibilna predhodna porazdelitev boljša pri ocenjevanju gostote, vendar daje slabšo kakovost generacije kot preprosti predhodni modeli, in (2) avtoregresivni modeli
zagotavljajo najboljšo učinkovitost prevajanja na splošno, medtem ko latentni modeli spremenljivk s pretokom normalizacije pred tem zagotavljajo največjo verjetnost odjave v vseh naborih podatkov. Zato priporočamo uporabo preprostega prejšnjega modela za latentno spremenljivko ne-avtoregresivno, kadar je želena hitra generacijska hitrost.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x). Turjumaankan la siiyo waa dabiicadda in lagu qiimeeyo tusaalaha imtixaanka lagu soo bandhigi karo qoraalka xaaladda ah. Si kastaba ha ahaatee qarniga soo dabaasha (ama la hor dhigi karo) waa in la helo midhaha ugu wanaagsan oo laga soo saaray input, shaqo walbana wuxuu leeyahay metric R oo kooban midhaha hoose ka soo socda, si a y u barbaroorto midhaha y*: R(y* * ;) x). Intaan rajaynayno in model ka sarreeya qiimeynta qiimeynta qiimeynta qarsoon xittaa uu si wanaagsan uga sameeyo metricka hoose, xiriirka saxda ah lama baran shaqooyinka qarniga dabadeed. Qoraalkan waxaa ku qoran warqaddan, iyadoo isbarbardhiga qiyaastii qiimeeya oo ku qoran shan shaqooyin turjuman oo machine ah, waxaynu aragnaa in xiriirka kala duduwanaya qoraalka sameynta iyo habaarka qoraalka iyo BLEU aad bay u kala duwan tahay, waxayna ku xiran tahay tirada qoysaska isbarbardhiga. Marka ugu horeysa, suurtagalka log-gelintu aad buu ugu xiran yahay BLEU markaynu ka fiirsanayno tusaalooyin isku mid ah qoyska dhexdiisa (tusaale ahaan qaababka iskaa-regressive ama modelalka bedelan e e ugu dambeeya oo ku saabsan isku mid is-bedelka hore). Si kastaba ha ahaatee, ma fiirinno xiriir kala duwan oo kala duduwan qoysaska kala duduwan:(1) Mid ka mid ah noocyada aan autoregative-latent bedelan, qaybinta hore waa ka wanaagsan tahay qiimeynta qiimeynta hoose, laakiin waxay siisaa qiimaha qarniga ka sii xumaa qiimo yar tan horaad, iyo (2) qaabab madax u ah
waxay bixisaa muuqashada turjumaadda ugu fiican, isla markaasna tusaalaha ugu dambeeya ee kala duwan oo ay leeyihiin durdurka caadiga ah kahor waxay siisaa suurtagalka ugu sarreeya qoraal-ka-baxa oo dhan. Sidaa darteed waxaan ku talinaynaa inaad isticmaasho tusaale fudud ee ugu dambeeya oo aan iskumarin, marka loo baahdo dhaqso dhaqdhaqaaq.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Shumë detyra të gjenerimit sekuencë-në-sekuencë, duke p ërfshirë përkthimin e makinës dhe tekstin-në-fjalim, mund të përcaktohen si vlerësimin e densitetit të daljes y të dhënë input x: p(y[UNK]x). Duke pasur parasysh këtë interpretim, është natyrore të vlerësohen modelet sekuencë-në-sekuencë duke përdorur logaritmin e kushtueshëm-gjasa në një set testimi. Megjithatë, qëllimi i gjenerimit nga sekuenca në sekuencë (ose parashikimit strukturuar) është të gjejë daljen më të mirë y të dhënë një input x, dhe secila detyrë ka R metrike të vet poshtë që shënon një model dalje duke krahasuar me një sërë referencash y*: R(y, y*  x). While we hope that a model that excels in density estimation also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. Në këtë letër, duke krahasuar disa vlerësime të densitetit në pesë detyra përkthimi makinash, gjejmë se korrelacioni midis renditjeve të modeleve bazuar në rregjistrimin e mundësive të rregjistrimit dhe BLEU ndryshon në mënyrë të konsiderueshme në varësinë e gamës së modeleve që janë krahasuar familjet. Së pari, gjasa e regjistrimit është shumë e korreluar me BLEU kur konsiderojmë modele brenda të njëjtës familje (për shembull modele autoregresive apo modele të ndryshueshme latente me të njëjtin parametrizim të mëparshëm). Megjithatë, ne nuk vëzhgojmë korrelacion midis renditjeve të modeleve nëpër familje të ndryshme: (1) midis modeleve jo-autoregresive të ndryshme të fshehta, një shpërndarje fleksible e mëparshme është më e mirë në vlerësimin e densitetit por jep cilësi më të keqe gjeneratës se një model i thjeshtë të mëparshëm, dhe (2) modele autoregresive
ofrojnë performancën më të mirë të përkthimit përgjithësisht, ndërsa modelet e ndryshueshme të fshehta me një rrjedhje normalizuese përpara japin gjasa më të lartë të regjistrimit të mbajtur në të gjitha grupet e të dhënave. Prandaj, ne rekomandojmë përdorimin e një paraardhjeje të thjeshtë për modelin e ndryshueshëm të fshehtë jo-autoregresiv kur kërkohet shpejtësia e gjeneratës së shpejtë.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mnogi zadatak generacije sekvence do sekvence, uključujući prevod mašine i tekst do govora, mogu se postaviti kao procjenjivanje gustine proizvoda y s obzirom na ulaz x: p(y.x). S obzirom na ovu interpretaciju, prirodno je proceniti modele sekvence do sekvence koristeći uslovnu verovatnoću log a na testu. Međutim, cilj generacije sekvence do sekvence (ili strukturirane predviđanja) je pronaći najbolji izlaz y koji je dao ulaz x, a svaki zadatak ima svoj vlastiti donji metrički R koji rezultira izlaz modela uspoređujući sa setom referencija y*: R(y, y*.x). Iako se nadamo da model koji nadmašuje procjenu gustine takoðe dobro izvodi na metriji dole, taèna korelacija nije proučena za zadatak generacije sekvence. U ovom papiru, uspoređujući nekoliko procjena gustine na pet zadataka za prevod mašine, smatramo da se povezanost između redova modela baziranog na log-verovatnosti i BLEU značajno razlikuje u ovisnosti o rasponu modelnih porodica u usporedbi. Prvo, vjerojatnost dnevnika je vrlo povezana sa BLEU kada razmišljamo o modelima unutar iste porodice (npr. autoregresivne modele, ili latentne varijantne modele sa istom parameterizacijom prethodnog). Međutim, mi ne posmatramo nikakvu korelaciju između redova modela u različitim porodicama: 1) između ne autoregresivnih latentnih modela, fleksibilna prethodna distribucija je bolja za procjenu gustosti, ali daje gore kvalitet generacije nego jednostavno ranije, i (2) autoregresivne modele
ponudite najbolju provedbu prevođenja ukupno, dok latentne varijantne modele sa normalizacijskim tokom prije pružaju najveću verovatnoću izvršenog izveštaja na svim podacima. Stoga preporučujemo korištenje jednostavnog ranije za latentni promjenni ne-autoregresivni model kada se žele brzina generacije.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Många sekvens-till-sekvensgenereringsuppgifter, inklusive maskinöversättning och text-till-tal, kan framställas som att uppskatta densiteten av utmatningen y givet inmatningen x: p(y|x). Med tanke på denna tolkning är det naturligt att utvärdera sekvens-till-sekvensmodeller med villkorad loggsannolikhet på en testuppsättning. Målet med sekvens-till-sekvensgenerering (eller strukturerad förutsägelse) är dock att hitta den bästa utmatningen y givet en inmatning x, och varje uppgift har sin egen nedströms metrisk R som får en modellutmatning genom att jämföra med en uppsättning referenser y*: R(y, y* | x). Även om vi hoppas att en modell som utmärker sig i densitetsberäkning också presterar bra på nedströmsmålet, har den exakta korrelationen inte studerats för sekvensgenereringsuppgifter. I denna uppsats, genom att jämföra flera densitetsestimatorer på fem maskinöversättningsuppgifter, finner vi att korrelationen mellan placeringar av modeller baserat på log-sannolikhet och BLEU varierar avsevärt beroende på intervallet av de modellfamiljer som jämförs. För det första är loggsannolikheten starkt korrelerad med BLEU när vi betraktar modeller inom samma familj (t.ex. autoregressiva modeller, eller latenta variabelmodeller med samma parametrisering som tidigare). Vi observerar dock ingen korrelation mellan placeringar av modeller över olika familjer: (1) bland icke-autoregressiva latenta variabelmodeller är en flexibel tidigare fördelning bättre vid densitetsuppskattning men ger sämre generationskvalitet än en enkel tidigare, och (2) autoregressiva modeller
ger den bästa översättningsprestandan totalt, medan latenta variabelmodeller med normaliserande flödesförinnan ger den högsta hållna loggsannolikheten för alla datauppsättningar. Därför rekommenderar vi att du använder en enkel förkortning för den latenta variabeln icke-autoregressiv modell när snabb genereringshastighet önskas.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kazi nyingi za vizazi vya mfululizo kwa mfululizo, ikiwa ni pamoja na tafsiri ya mashine na hotuba ya maandishi, zinaweza kuchukuliwa kama kutathmini kiwango cha uchungufu wa matokeo yaliyotolewa kwa ajili ya input x: p(y\.x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. Hata hivyo, lengo la kizazi cha mfululizo (au utabiri wa umetengenezwa) ni kutafuta matokeo bora zaidi yaliyotolewa kwa ajili y a input, na kila kazi ina metric yake ya chini ya mto ambayo inachezea matokeo ya mifano kwa kulinganisha mfululizo wa maoni ya y*: R(y* *; x). Wakati tunatumaini kuwa mifano yenye kiwango cha uchunguzi pia kinafanya vizuri katika mito ya chini, mahusiano sahihi hayajafunzwa kwa kazi za vizazi vya mfululizo. Katika karatasi hii, kwa kulinganisha idadi mbalimbali za uchunguzi kuhusu kazi za kutafsiri mashine tano, tunagundua kuwa uhusiano kati ya mabwawa ya mitindo yenye uwezekano wa log na BLEU hutofauti sana na kutegemea kwa kiwango kikubwa cha familia za mifano zinazolinganishwa. Kwanza, uwezekano wa kujilogu umeunganishwa sana na BLEU pale tunapofikiria mifano ndani ya familia hiyo (kwa mfano mifano ya kujikandamiza, au mifano ya mabadiliko ya hivi karibuni yenye kipimo sawa cha kipimo cha awali). Hata hivyo, hatukuona uhusiano kati ya mabadiliko ya mifano katika familia mbalimbali: (1) miongoni mwa mifano ya mabadiliko ya hivi karibuni isiyo na utawala wa kujitegemea, usambazaji wa zamani ni bora zaidi kwa kadiri ya uchungu lakini inatoa viwango vibaya zaidi ya vizazi vibaya kuliko moja kabla, na (2) mifano ya kujidhibiti
kutoa ufanisi bora wa tafsiri kwa ujumla, wakati mifano ya mabadiliko ya hivi karibuni yenye mafuriko ya kawaida kabla ya kutoa uwezekano wa kuzungumzwa zaidi katika seti zote za data. Kwa hiyo, tunapendekeza kutumia kipindi rahisi kwa ajili ya mifano ya hivi karibuni isiyobadilika ya kudhibiti kujitegemea wakati haraka ya kizazi kinahitajika.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>பின்வரும் தலைமுறை பணிகள், இயந்திரம் மொழிபெயர்ப்புகள் மற்றும் உரையில் இருந்து பேச்சுகள் உள்ளீட்டு x: p( y\.x) கொடுக்கப்பட்ட வெளியீட்டின் density இந்த விளக்கம் கொடுத்தால், சோதனையின் அமைப்பில் நிலையான பதிவு சாத்தியமான மாதிரிகளை மதிப்பிட இயல்பாகும். தொடர்ந்து வரும் தலைமுறை (அல்லது கட்டுப்படுத்தப்பட்ட முன்னோட்டம்) உள்ளீட்டு கொடுக்கப்பட்ட சிறந்த வெளியீட்டு வெளியீட்டை கண்டுபிடிக்க, ஒவ்வொரு செயலுக்கும் அதன் சொந்த கீழ்நீர நாம் நம்புகிறோம் அதிக மதிப்பின் மாதிரி அதிகமாக இருக்கும் என்று ஒரு மாதிரி கீழ்நீர் மெட்ரிக்கில் நன்றாக செயல்படுத்துகிறது, தொடர்பு இந்த காகிதத்தில், ஐந்து இயந்திரம் மொழிபெயர்ப்பு பணிகளை ஒப்பிடும் பல தூக்கத்திற்குரிய கணக்கீட்டாளர்களை ஒப்பிடும் மாதிரி குடும்பங்களை ஒப்பிடும் வித்தி முதலில், பதிவு வாய்ப்பு பிலூயுடன் மிகவும் இணைந்து இருக்கிறது நாம் ஒரே குடும்பத்தில் உள்ள மாதிரிகளை கருதும்போது (உ. ம். தானாகவே கட்டுப்பாடு மாத ஆனால், நாம் வேறு குடும்பத்திற்கு முழுவதும் மாதிரிகளின் மாதிரிகளுக்கிடையே உறவு பார்க்கவில்லை: (1) தன்னியக்கமாக கட்டுப்பாட்டு சாதாரண மாதிரிகளில், ஒரு flexible முன் வ
சிறந்த மொழிபெயர்ப்பு செயல்பாட்டை மொத்தமாக கொடுக்கவும், ஆனால் சமாதாயமான மாறிகள் மாதிரிகள் வழங்கும் முன்னால் அனைத்து தரவ அதனால், நாம் ஒரு சுலபமான முன்னால் பயன்படுத்தி தற்போதைய மாறி தானே கட்டுப்பாட்டு மாதிரிக்கு பரிந்துரைக்கிறோம்</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Otomatik terjime we tekst-we-çykyş bilen köp terjime täblikleri, girişinde berilen y girişiniň ýiginligini x: p(y.x). Bu terjime görä, süzme düzümlerinde şartlı log-mümkinçiligi ulanmak tebigdir. Ýöne, sequence-to-sequence döwletleriniň (ýa-da strukturly çaklama) iň gowy çykyş y girdi x'ini tapmak, we her zadyň öz a şak metriýa R'iniň düşürmek üçin bir nusga çykyş mümkin edýän bu nusga y*: R(y, y* .x). Ýagtyklyk hasaplamada ýokary bir nusga a şdyrylýan metriýada gowy gazanýar diýip umyt edýäris. Diňe bir correliýan düzgün dünýä netijesi üçin öwrenmediler. Bu kagyzda, beş maşynyň terjime täbliklerinde birnäçe ýigrenlik pikirimçiliklerini karşılaşdyryp, logg-mümkinçiligine we BLEU düzümlerniň düzümleriniň düzümleriniň derejesinde baglanyşygy örän möhüm döwletlere görä baglanýar. Ilkinji, g ünlük mümkinçiligi bir maşgalada modelleri düşünýän wagtymyz BLEU bilen örän ködleşdirilýär (mysal. otoregressiv modeller, ýa öňki parameteriýasy bilen soňky üýtgeşik modeller). Emma, biz dürli maşgalalaryň arasynda modelleriň a ýratynyň arasynda hiç hili bir görnüşimi ýok etmeýäris: (1) awtomatik-regressiv ýok nusgalaryň arasynda, fleksibil öňki daýratynyň çykyş hasaplamasynda has gowydyr ýöne ýöne jenaýat täsirinden has gowydyr we (2) awtomatik regressiv nusgalarynda
edit-action Şol sebäpli, geçmişi çalt döredijilik tizligi isleýän wagtlar üçin basit bir öňki ulanmagy maslahat berýäris.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>اور بہت سی سی کائنات-سے-کائنات کی نسل کے کام، ماشین ترجمہ اور متن-سے-کلام میں شامل ہوتے ہیں، اس کے گھمنڈ کی گھمنڈی کا مطالبہ کرسکتے ہیں، اِن وروت کی x: p(y) x۔ اس تفسیر کی وجہ سے، ایک تست سٹ پر کانڈیسینل لاگ-likelihood کے استعمال سے سٹ-سے-کٹ-کٹ-کٹ موڈل کا ارزش کرنا طبیعی ہے. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y given an input x, and each task has its own downstream metric R that scores a model output by comparing a set of references y*: R(y, y* .x). حالانکہ ہم امید رکھتے ہیں کہ ایک مدل جو گہرے اندازے میں زیادہ اضافہ کرتا ہے، نیچے سیارے میٹریک پر بھی اچھا کام کرتا ہے، دقیق تعلق کی تعلق کی تعلیم کے لئے نہیں کی گئی ہے۔ اس کاغذ میں، پانچ ماشین ترجمہ کے تابع پر چند گھونٹی کا مصاحبہ مقایسہ کرنے کے ذریعہ، ہم دیکھتے ہیں کہ لوگ-احتمال اور بلیوس کے متعلق مدل خاندان کے مقایسہ پر متفاوت ہے. پہلے، لوگ-احتمال بلیوس کے ساتھ بہت سی تعلق ہے جب ہم ایک خاندان کے اندر موڈل سمجھتے ہیں (جیسے autoregressive models، یا latent variable models with the same parameterization) پہلے سے۔ لیکن ہم مختلف خاندان کے درمیان مدل کے درمیان کوئی تعلق نہیں دیکھتے: (1) غیر autoregressive latent variable models میں، ایک flexible prior distribution density estimation میں بہتر ہے لیکن ایک ساده پہلے سے بدترین نسل کیفیت دیتا ہے، اور (2) autoregressive models
سب سے بہترین ترجمہ کے عملکرد کو پیش کریں، حالانکہ لاٹینٹ ویرائیٹ موڈلوں کو اس سے پہلے ایک سیدھی فلوپ کے ساتھ بہترین ترجمہ کرنا چاہیے کہ تمام ڈیٹ سٹ میں سب سے بالاترین لگ-آئٹ کا احتمال دے۔ لہٰذا، ہم ایک ساده پہلے کے استعمال کرنے کی توصیف دیتے ہیں لٹینٹ ویرئیٹ غیر-autoregressive موڈل کے لئے جب تیز نسل کی سرعت خواہش کی جاتی ہے۔</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ketma- ketlik tarjima va matn tahrirlash vazifalari bilan bir nechta tarjima qilinadi. Foydalanilgan matn tarjima va matn tahrirchi natijasining qiymatini qidirish mumkin. Bu tarjima berilganda, sinov tugmasining imkoniyatini yordamida sezgiruvchi modellarni qiymatish oddiy. Lekin keyingi tarqatish avval (yoki tuzuvchi 预测lik) eng eng yaxshi natijani topish, va har bir vazifaning qismi o'zida metrik R o'zida model natijasini qo'yish mumkin. Ko'rib chiqish usulida o'xshash modeli va quyidagi metrikda yaxshi bajaradi deb umid qilamiz, biz bir xil avval vazifalari uchun haqiqiqiy bogʻlanmagan. Ushbu qogʻozda, besh mashina tarjima vazifalarining bir necha qiyin qiymatlarini kamaytirish bilan o'rganamiz, biz o'ylaymiz, log- imkoniyatlarining orasidagi modellar orasidagi bog'liq va BLEU model oilalarining qismiga bog'liqdir. Birinchi so'zda, log-o'zgarishlar bir xil qoidadagi modellarni tasavvur qilayotganda BLEU bilan juda bog'liq boʻladi (masalan, avto-regressive modellari yoki latent variable modellari bir xil parametrlari bilan birga ega boʻlganda). However, we observe no correlation between rankings of models across different families: (1) among non-autoregressive latent variable models, a flexible prior distribution is better at density estimation but gives worse generation quality than a simple prior, and (2) autoregressive models
@ info: whatsthis Shunday qilib, biz keyingi oʻzgaruvchining avto-regressiv modeli uchun oddiy oldin foydalanishimizni talab qilamiz.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Rất nhiều công việc sản xuất chuỗi-tới-số, bao gồm cả dịch-máy và văn-tới-phát-biểu, có thể được đặt là ước tính mật độ của sản xuất y dựa vào nhập x: p(y). Dựa vào giải pháp này, đánh giá các mô- trình-tới-trình mô-suất dựa trên khả năng đăng ký với một bộ thử là bình thường. Tuy nhiên, mục tiêu của chế tạo chuỗi-tới-trình (hay dự đoán cấu trúc) là tìm nguồn xuất tốt nhất từng được cho một nhập x, và mỗi nhiệm vụ có một khẩu R theo dòng chảy để đạt điểm xuất mô hình bằng cách so sánh với một bộ tham khảo y y y*: R(y, y* (« 124; x). Trong khi chúng tôi hy vọng rằng một mô hình xuất sắc trong khả năng đánh giá mật độ cũng tiến triển tốt theo hệ thống xuôi dòng, thì sự tương quan chính xác chưa được nghiên cứu cho các nhiệm vụ sản xuất chuỗi. Trong tờ giấy này, bằng cách so sánh vài bộ tộc mật độ với năm công việc dịch chuyển máy, chúng tôi thấy sự tương quan giữa phân hạng các mô-đun dựa trên khả năng đăng kí và tiếng bíp khác biệt đáng kể tùy thuộc vào giới hạn các gia đình mô hình được so sánh. Đầu tiên, khả năng đăng ký có mối quan hệ rất cao với tiếng bíp khi chúng ta xem xét các mô hình trong cùng một gia đình (v. d. mô hình tự tụt lại, hoặc các mô hình biến cố tiềm năng với đo giới hạn tương tự của người trước). Tuy nhiên, chúng tôi không quan sát mối tương quan giữa xếp hạng các mô- đun khác nhau: 1) giữa các mô- đun tiềm năng không tự động, phân phối trước có thể thuận theo mật độ tốt hơn, nhưng tạo thế hệ tệ hơn các mô- đun trước, và (2) các mô- đun tự vệ.
cung cấp khả năng dịch chuyển tốt nhất, trong khi các mô- đun tiềm năng với dòng chảy bình thường trước đó cho khả năng vượt trội cao nhất trong tất cả các tập tin dữ liệu. Vì vậy, chúng tôi đề nghị sử dụng một phép trước đơn giản cho mô hình tiềm năng không có tự lực khi nào tốc độ sản xuất nhanh được muốn.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>诸序至序成务,机器翻译文本至语音转换,可为给定输x:p(y|x)度输y密度。 鉴于此说,试集上用之对数似然以质序自然。 然序至(结构化测)者,于给定输 x 得最佳输出 y,而每务有下流指标 R,当指标 R 与一朋引 y*较之对模型输评分:R(y, y* | x)。 虽欲形于密度度表现出色形于下流指标亦见善于上,而于序成之任,未究其相关性。 夫文者,校五机器翻译之密度估计器,见对数似然、BLEU之间相关性显变化,决于系列也。 先虑一家之(,自归于同参数化者在变量),对数似然与BLEU高相关。 然则观其系列列名之间无相关性:(1)在非自归变量中,灵先验布在密度估计,而愈于约先验有更差者,与(2)自反
体上供至善,而有归一化流先验者变量形于所有之数,以至高者存对数似然。 故当速成速度,请于潜变量非自归模用简先验。</span></div></div><dl><dt>Anthology ID:</dt><dd>2020.spnlp-1.10</dd><dt>Volume:</dt><dd><a href=/volumes/2020.spnlp-1/>Proceedings of the Fourth Workshop on Structured Prediction for NLP</a></dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/spnlp/>spnlp</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>84–94</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.spnlp-1.10>https://aclanthology.org/2020.spnlp-1.10</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/2020.spnlp-1.10 title="To the current version of the paper by DOI">10.18653/v1/2020.spnlp-1.10</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">lee-etal-2020-discrepancy</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Jason Lee, Dustin Tran, Orhan Firat, and Kyunghyun Cho. 2020. <a href=https://aclanthology.org/2020.spnlp-1.10>On the Discrepancy between Density Estimation and Sequence Generation</a>. In <i>Proceedings of the Fourth Workshop on Structured Prediction for NLP</i>, pages 84–94, Online. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2020.spnlp-1.10>On the Discrepancy between Density Estimation and Sequence Generation</a> (Lee et al., spnlp 2020)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.spnlp-1.10.pdf>https://aclanthology.org/2020.spnlp-1.10.pdf</a></dd><dt class=acl-button-row>Optional supplementary material:</dt><dd class=acl-button-row><a href=https://aclanthology.org/attachments/2020.spnlp-1.10.OptionalSupplementaryMaterial.tex class="btn btn-attachment btn-sm"><i class="fas fa-file"></i>
&nbsp;2020.spnlp-1.10.OptionalSupplementaryMaterial.tex</a></dd><dt class=acl-button-row>Video:</dt><dd class=acl-button-row><a href=https://slideslive.com/38940144 class="btn btn-attachment btn-sm"><i class="fas fa-video"></i>&nbsp;https://slideslive.com/38940144</a></dd><dt>Code</dt><dd><a href=https://github.com/tensorflow/tensor2tensor><i class="fab fa-github"></i>&nbsp;tensorflow/tensor2tensor</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.spnlp-1.10.pdf title="Open PDF of 'On the Discrepancy between Density Estimation and Sequence Generation'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=On+the+Discrepancy+between+Density+Estimation+and+Sequence+Generation" title="Search for 'On the Discrepancy between Density Estimation and Sequence Generation' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-secondary d-flex flex-wrap justify-content-center" href="https://paperswithcode.com/paper/?acl=2020.spnlp-1.10" title="Code for 'On the Discrepancy between Density Estimation and Sequence Generation' on Papers with Code"><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-big" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg><span class="pl-sm-2 d-none d-sm-inline">Code</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'On the Discrepancy between Density Estimation and Sequence Generation'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a>
<a class="btn btn-attachment d-flex flex-wrap justify-content-center" href=https://aclanthology.org/attachments/2020.spnlp-1.10.OptionalSupplementaryMaterial.tex title="Open optional supplementary material for 'On the Discrepancy between Density Estimation and Sequence Generation'"><span class="align-self-center px-1"><i class="fas fa-file"></i></span>
<span class=px-1>Optional supplementary material</span></a>
<a class="btn btn-attachment d-flex flex-wrap justify-content-center" href=https://slideslive.com/38940144 title="Open video for 'On the Discrepancy between Density Estimation and Sequence Generation'"><span class="align-self-center px-1"><i class="fas fa-video"></i></span>
<span class=px-1>Video</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[On the Discrepancy between Density Estimation and Sequence Generation](https://aclanthology.org/2020.spnlp-1.10) (Lee et al., spnlp 2020)</p><ul class=mt-2><li><a href=https://aclanthology.org/2020.spnlp-1.10>On the Discrepancy between Density Estimation and Sequence Generation</a> (Lee et al., spnlp 2020)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Jason Lee, Dustin Tran, Orhan Firat, and Kyunghyun Cho. 2020. <a href=https://aclanthology.org/2020.spnlp-1.10>On the Discrepancy between Density Estimation and Sequence Generation</a>. In <i>Proceedings of the Fourth Workshop on Structured Prediction for NLP</i>, pages 84–94, Online. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>