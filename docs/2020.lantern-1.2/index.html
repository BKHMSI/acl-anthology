<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Leveraging Visual Question Answering to Improve Text-to-Image Synthesis - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="Leveraging Visual Question Answering to Improve Text-to-Image Synthesis" name=citation_title><meta content="Stanislav Frolov" name=citation_author><meta content="Shailza Jolly" name=citation_author><meta content="Jörn Hees" name=citation_author><meta content="Andreas Dengel" name=citation_author><meta content="Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)" name=citation_conference_title><meta content="2020/12" name=citation_publication_date><meta content="https://aclanthology.org/2020.lantern-1.2.pdf" name=citation_pdf_url><meta content="17" name=citation_firstpage><meta content="22" name=citation_lastpage><meta property="og:title" content="Leveraging Visual Question Answering to Improve Text-to-Image Synthesis"><meta property="og:image" content="https://aclanthology.org/thumb/2020.lantern-1.2.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/2020.lantern-1.2"><meta property="og:description" content="Stanislav Frolov, Shailza Jolly, Jörn Hees, Andreas Dengel. Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN). 2020."><link rel=canonical href=https://aclanthology.org/2020.lantern-1.2></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/2020.lantern-1.2.pdf>Leveraging Visual Question Answering to Improve Text-to-Image Synthesis</a>
<a id=af_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Versterking van visuele vraag antwoord na verbeter teks- na- beeld sintetiseer</a>
<a id=am_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Leveraging Visual Question Answering to Improve Text-to-Image Synthesis</a>
<a id=ar_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>الاستفادة من إجابة الأسئلة المرئية لتحسين توليف تحويل النص إلى صورة</a>
<a id=az_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>M…ôtn-≈ü…ôkill…ôrin Sintezini Improved etm…ôk ΟΦΟßΟΦn Visual sual cavab verm…ôk</a>
<a id=bg_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Използване на визуални отговори на въпроси за подобряване на синтеза текст в изображение</a>
<a id=bn_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>লেভারেজিং দৃশ্য প্রশ্নের উত্তর প্রদান করা হচ্ছে টেক্সট- থেকে ছবি সিন্টিসেস</a>
<a id=bo_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Leveraging Visual Question Answering to Improve Text-to-Image Synthesis</a>
<a id=bs_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Odgovor vizuelnih pitanja za poboljšanje sinteze teksta na slike</a>
<a id=ca_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Ampliar la resposta a preguntes visuals per millorar la síntesi de text a imatge</a>
<a id=cs_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Využití vizuálních odpovědí na otázky ke zlepšení syntézy textu na obrázek</a>
<a id=da_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Udnyttelse af visuel spørgsmål besvarelse for at forbedre tekst-til-billede syntese</a>
<a id=de_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Nutzung visueller Fragebeantworter zur Verbesserung der Text-zu-Bild-Synthese</a>
<a id=el_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Αξιοποίηση οπτικής απάντησης ερωτήσεων για τη βελτίωση της σύνθεσης κειμένου σε εικόνα</a>
<a id=es_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Aprovechamiento de la respuesta visual a preguntas para mejorar la síntesis de texto a imagen</a>
<a id=et_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Visuaalsete küsimustele vastamise võimendamine tekstist pildiks sünteesi parandamiseks</a>
<a id=fa_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>پاسخ سؤال بینایی برای بهترین تنزیس متن به تصویر</a>
<a id=fi_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Visuaalisten kysymysten vastaamisen hyödyntäminen tekstistä kuvaan -synteesin parantamiseksi</a>
<a id=fl_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Tirer parti de la réponse visuelle aux questions pour améliorer la synthèse texte-image</a>
<a id=ga_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Freagairt Amharc-Cheisteanna chun Sintéis Téacs-go-Íomhá a Fheabhsú</a>
<a id=ha_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>KCharselect unicode block name</a>
<a id=he_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>מעלה תשובות על שאלות חזותיות כדי לשפר את סינטזה טקסט לתמונה</a>
<a id=hi_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>पाठ-से-छवि संश्लेषण में सुधार करने के लिए दृश्य प्रश्न का उत्तर देने का लाभ उठाना</a>
<a id=hr_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Odgovor vizualnih pitanja za poboljšanje sinteze teksta na slike</a>
<a id=hu_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>A vizuális kérdésekre adott válaszok kihasználása a szöveg-kép szintézisének javítására</a>
<a id=hy_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Տեսային հարցերի պատասխանը բարելավելու համար</a>
<a id=id_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Meningkatkan Jawaban Pertanyaan Visual untuk meningkatkan Sintesis Teks-ke-Gambar</a>
<a id=is_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Sfruttare la risposta visiva alle domande per migliorare la sintesi da testo a immagine</a>
<a id=ja_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>視覚的な質問への回答を活用してテキストから画像への合成を改善する</a>
<a id=jv_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>politenessoffpolite"), and when there is a change ("assertivepoliteness</a>
<a id=ka_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>ტექსტის გამოსახულებლად გამოსახულებელი სინტეზეზის გარეშე</a>
<a id=kk_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Мәтіннен кескін синтезасын жасау үшін көрінетін сұрақ жауап беру</a>
<a id=ko_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>시각 문답을 이용하여 텍스트와 이미지의 합성을 개선하다</a>
<a id=lt_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Veiksmingesnis atsakymas į vizualius klausimus siekiant pagerinti teksto ir vaizdo sintezę</a>
<a id=mk_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Разголемување на одговорот на визуелните прашања за подобрување на синтезата од текст до слика</a>
<a id=ml_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>പദാവലിയില്‍ നിന്നും ഇമേജിലേക്ക് മെച്ചപ്പെടുത്തുന്നതിന് ഉത്തരം ലഭ്യസിക്കുന്ന കാഴ്ച ചോദ്യം</a>
<a id=mn_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Текст болон зургаас илүү сайжруулахын тулд харагдах хариултын асуулт</a>
<a id=ms_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Menegang Jawapan Pertanyaan Visual untuk meningkatkan Sintesis Teks-ke-Imej</a>
<a id=mt_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>It-tweġiba għall-mistoqsijiet viżwali biex tittejjeb is-Sintesi tat-Test għall-Immaġni</a>
<a id=nl_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Visuele vragen beantwoorden gebruiken om tekst-naar-beeld synthese te verbeteren</a>
<a id=no_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Svar på visuelle spørsmål for å forbedra tekstsyntese til bilete</a>
<a id=pl_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Wykorzystanie wizualnego odpowiedzi na pytania w celu poprawy syntezy tekstu do obrazu</a>
<a id=pt_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Aproveitando a resposta visual de perguntas para melhorar a síntese de texto para imagem</a>
<a id=ro_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Utilizarea răspunsurilor vizuale la întrebări pentru a îmbunătăți sinteza text-în-imagine</a>
<a id=ru_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Использование визуальных ответов на вопросы для улучшения синтеза текста с изображением</a>
<a id=si_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>පැත්තට- පින්තූර සංවිධානය සඳහා ප්‍රශ්න ප්‍රතිච්චාරය ප්‍රතිච්චාරය ප්‍රතිචාරය</a>
<a id=sk_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Izkoriščanje vizualnega odgovarjanja na vprašanja za izboljšanje sinteze besedila v sliko</a>
<a id=so_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Jawaabta su'aalaha aragtida la xiriira horumarinta sawirka</a>
<a id=sq_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Shkatërrimi i përgjigjeve të pyetjeve vizuale për të përmirësuar sintezën tekst-në-imazh</a>
<a id=sr_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Odgovor vizualnih pitanja za poboljšanje sinteze teksta na slike</a>
<a id=sv_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Utnyttja visuella frågesvar för att förbättra text-till-bild syntes</a>
<a id=sw_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Swali la Visual linalojibu kuboresha Synthesis of Text to Image</a>
<a id=ta_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>உரையில் இருந்து பிம்பத்துடன் மேம்படுத்துவதற்கு பதில் காட்சி கேள்வி</a>
<a id=tr_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Metin-t채 Resim Sentezini gowla힊dyrmak 체챌in G철rn철힊 Soragy jogaplamak 체챌in s체첵tge첵채r</a>
<a id=uk_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>ویزول سؤال جواب دینے کے لئے پاکستان سے تصویر سینٹیز کو اچھی طرح کرنا ہے</a>
<a id=uz_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>Đánh giá các câu hỏi ảo để hoàn thiện kết cấu hình ảnh</a>
<a id=zh_title style=display:none href=https://aclanthology.org/2020.lantern-1.2.pdf>因视问答改进文本至图像合成</a></h2><p class=lead><a href=/people/s/stanislav-frolov/>Stanislav Frolov</a>,
<a href=/people/s/shailza-jolly/>Shailza Jolly</a>,
<a href=/people/j/jorn-hees/>Jörn Hees</a>,
<a href=/people/a/andreas-dengel/>Andreas Dengel</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Generating images from textual descriptions has recently attracted a lot of interest. While current models can generate photo-realistic images of individual objects such as <a href=https://en.wikipedia.org/wiki/Bird>birds</a> and <a href=https://en.wikipedia.org/wiki/Face>human faces</a>, synthesising images with multiple objects is still very difficult. In this paper, we propose an effective way to combine Text-to-Image (T2I) synthesis with Visual Question Answering (VQA) to improve the <a href=https://en.wikipedia.org/wiki/Image_quality>image quality</a> and image-text alignment of generated images by leveraging the VQA 2.0 dataset. We create additional training samples by concatenating question and answer (QA) pairs and employ a standard VQA model to provide the T2I model with an auxiliary learning signal. We encourage images generated from QA pairs to look realistic and additionally minimize an external VQA loss. Our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> lowers the FID from 27.84 to 25.38 and increases the R-prec. from 83.82 % to 84.79 % when compared to the baseline, which indicates that T2I synthesis can successfully be improved using a standard VQA model.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Skep beelde van teks beskrywings het onlangs 'n baie belang aantrek. Terwyl huidige modele kan foto- realistiese beelde van individuele voorwerpe genereer soos voëls en menslike gesigte, sintetisering beelde met veelvuldige voorwerpe is nog baie moeilik. In hierdie papier, ons voorstel 'n effektief manier om teks- na- beeld (T2I) sintetiseer te kombinieer met Visuele Fraag Antwoord (VQA) om die beeld kwaliteit en beeld- teks-inligting van gegenereerde beelde te verbeter deur die VQA 2. 0 datastel te verwyder. Ons skep addisionele onderwerp voorbeelde deur samelewing van vraag en antwoord (QA) paar en gebruik 'n standaard VQA model om die T2I model te verskaf met 'n helper onderwerp sein. Ons moedig beelde genereer van QA paars om realistiese en addisionele 'n eksterne VQA verlies te kyk. Ons metode verlaat die FID van 27. 84 tot 25. 38 en verhoog die R- preek. van 83,82% tot 84,79% wanneer vergelyk word met die basislien, wat wys dat T2I sintetes suksesvol met 'n standaard VQA model kan verbeter word.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>የጽሑፍ ትርጉም ምስሎችን መፍጠር በቅርብ ጊዜ እጅግ ተጨማሪ ነው፡፡ የአሁኑ ዓይነቶች ፎቶ-እውነተኛ ምስሎችን እንደ ወፎች እና የሰው ፊቶች በመፍጠር የሚችል ሲሆን፣ ምስሎችን በብዙ አካሄዱ ጋር ማሰናከል ግን እጅግ ችግር ነው፡፡ በዚህ ገጽ ውስጥ የጽሑፍ-ወደ-ምስል (T2I) ሲንሰርዝ እና የVisual ጥያቄ መልስ (VQA) ለጥያቄ ምስል ጥያቄ እና የጽሑፍ ምስል ክፍተቶችን በመጠቀም የVQA 2.0 ዳታሮችን በመጠቀም እና በመጠቀም አቅራቢያ እና ጥያቄ እንዲያበጅል እና ምረጡ አቅራቢያ እና ጥያቄ እና መሠረት We create additional training samples by concatenating question and answer (QA) pairs and employ a standard VQA model to provide the T2I model with an auxiliary learning signal. ከQA ሁለት ዓይነቶች የተወለዱትን ምስሎች እናበረታታለን እና ውጭው የVQA ጉዳይ ማስታወስ እናጎድል፡፡ ሥርዓታችን FID ከ27.84 እስከ 25.38 ድረስ ያዋርዳል፡፡ ከ83.82 በመቶ እስከ 84.79 በመቶ ቢተካክሉ T2I Synthesis በተከታተለ የVQA ሞዴል በመጠቀም ይሻላል፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>جذب توليد الصور من الأوصاف النصية الكثير من الاهتمام مؤخرًا. في حين أن النماذج الحالية يمكن أن تولد صورًا واقعية لكائنات فردية مثل الطيور والوجوه البشرية ، إلا أن تجميع الصور بأشياء متعددة لا يزال صعبًا للغاية. في هذا البحث ، نقترح طريقة فعالة للجمع بين توليف النص إلى صورة (T2I) والإجابة المرئية للأسئلة (VQA) لتحسين جودة الصورة ومحاذاة نص الصورة للصور التي تم إنشاؤها من خلال الاستفادة من مجموعة بيانات VQA 2.0. نقوم بإنشاء عينات تدريب إضافية عن طريق ربط أزواج الأسئلة والأجوبة المتسلسلة واستخدام نموذج VQA القياسي لتزويد نموذج T2I بإشارة تعلم مساعدة. نحن نشجع الصور التي تم إنشاؤها من أزواج ضمان الجودة لتبدو واقعية بالإضافة إلى تقليل خسارة VQA الخارجية. طريقتنا تخفض FID من 27.84 إلى 25.38 وتزيد R-prec. من 83.82٪ إلى 84.79٪ بالمقارنة مع خط الأساس ، مما يشير إلى أنه يمكن تحسين توليف T2I بنجاح باستخدام نموذج VQA القياسي.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mətn tərzlərindən şəkillər yaratması son zamanlarda çox maraqlı oldu. Hazırkı modellər quşlar və insan suratları kimi fotoğraf-reyalistik şəkillərini yarada bilərlər, çoxlu nesmlər ilə şəkilləri sintezləşdirmək hələ də çox çətindir. Bu kağızda, VQA 2.0 veri qurğunu təmizləyib yaratdığı görüntülərin görüntülərin keyfiyyətini və surat-metini düzəltməsini düzəltmək üçün Metin-to-Image (T2I) sintezini Visual Question Response (VQA) ilə birləşdirmək üçün effektiv bir yolu təklif edirik. T2I modelini köməkçi öyrənmə sinyali ilə birləşdirmək üçün Standardlı VQA modeli yaratdıq. QA çiftlərdən yaradılmış görüntüləri reallı görünmək üçün təşkil edirik və həmçinin dış VQA kaybını azaltmaq üçün. Bizim metodumuz FID 27.84-dən 25.38-ə düşürür və R-preci artırır. T2I sintezi standart VQA modeli ilə müvəffəqiyyətlə yaxşılaşdırılabilir.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Генерирането на изображения от текстови описания наскоро привлече голям интерес. Докато настоящите модели могат да генерират фотореалистични изображения на отделни обекти като птици и човешки лица, синтезирането на изображения с множество обекти все още е много трудно. В тази статия предлагаме ефективен начин за комбиниране на синтеза текст в изображение (T2I) с визуално отговаряне на въпроси (ВQA), за да подобрим качеството на изображението и подравняването изображение-текст на генерираните изображения чрез използване на набора от данни ВQA 2.0. Създаваме допълнителни образци за обучение чрез конкатениране на двойки въпроси и отговори и използваме стандартен модел за осигуряване на модела T2I с допълнителен учебен сигнал. Насърчаваме изображенията, генерирани от двойки за контрол на качеството, да изглеждат реалистични и допълнително да минимизират външната загуба на качество. Нашият метод понижава фиД от 27.84 на 25.38 и увеличава Р-прег. от 83, 82% до 84, 79% в сравнение с изходните стойности, което показва, че синтезата на T2I може успешно да бъде подобрена, като се използва стандартен модел за оценка на качеството.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>টেক্সুয়াল বর্ণনা থেকে ছবি তৈরি করা সম্প্রতি অনেক আগ্রহের আকর্ষণ করেছে। While current models can generate photo-realistic images of individual objects such as birds and human faces, synthesising images with multiple objects is still very difficult. এই প্রবন্ধে আমরা ভিকিউএ ২. ০ ডাটাসেট প্রদান করে তৈরি করা ছবির মান এবং ছবির মান এবং টেক্সটের তৈরি করার জন্য একটি কার্যকর উপায় সম্পাদন করার প্রস্তাব করি। আমরা আরো প্রশ্নের উদাহরণ তৈরি করি এবং উত্তর (কিউএ) জোড়ার মাধ্যমে আরো প্রশিক্ষণের নমুনা তৈরি করি এবং টি২I মডেল তৈরি করি যাতে প্রাসঙ্গিক শিক্ষা সিগন্ আমরা কিউএ থেকে উৎসাহিত ছবিগুলো তৈরি করেছি বাস্তবতা দেখতে এবং বাইরের ভিকিয়ার ক্ষতি কমিয়ে দেখতে উৎসাহিত করি। আমাদের পদ্ধতি ২৭. ৮৪ থেকে ২৫. ৩৮ থেকে কমিয়ে আর-পূর্ব বৃদ্ধি করে। ৮৩. ৮২% থেকে ৮৪. ৭৯% পর্যন্ত যখন বেস্ট লাইনের সাথে তুলনা করা হয়, তখন এটি নির্দেশ করে যে টি২I সিন্টিসিস সফল হয়ে যায় একটি স্ট্যান্ডার্ড ভিকি</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ཡིག་གེའི་འགྲེལ་བཤད་ནས་བརྙན་རིས་གསར་བཟོ་བ་མ་ཐག་རིང་ལ་སྐྱེལ་བ་མང་བར་བསམ། While current models can generate photo-realistic images of individual objects such as birds and human faces, synthesising images with multiple objects is still very difficult. In this paper, we propose an effective way to combine text-to-Image (T2I) synthesis with Visual Question Answering (VQA) to improve the image quality and image-text alignment of generated images by leveraging the VQA 2.0 dataset. We create additional training samples by concatenating question and answer (QA) pairs and employ a standard VQA model to provide the T2I model with an auxiliary learning signal. Name ང་ཚོའི་ཐབས་ལམ་ལྟའི་ཕྱོགས་ལམ་ནས་FID 27,84་ལས་25,38་ལ་ཉེ་བར་གནས་ཚད་རྒྱ་བསྐྱེད་བྱེད། སྤྱི་ཚོགས་ཀྱི་ཕྱོགས་དང་མཉམ་དུ་འགྱུར་བའི་སྐབས་ཡོད།</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Stvaranje slika iz tekstualnih opisa nedavno je privuklo mnogo interesa. Iako trenutni modeli mogu stvoriti fotorealistične slike pojedinaca poput ptica i ljudskih lica, sinteziranje slika sa višestrukim objektima još uvijek je teško. U ovom papiru predlažemo efikasan način da kombiniramo sintezu teksta na sliku (T2I) sa odgovorom na vizuelno pitanje (VQA) kako bi poboljšali kvalitet slika i poravnanje teksta slika proizvedenih slika uz primjenu seta podataka VQA 2.0. Mi stvaramo dodatne uzorke obuke povećavajući pitanje i odgovor (QA) par i koristimo standardni model VQA kako bi omogućili model T2I pomoćni signal učenja. Poticamo slike koje su proizvedene iz par QA-a da izgledaju realistično i dodatno minimiziraju vanjski gubitak VQA-a. Naša metoda smanjuje FID od 27,84 do 25,38 i povećava R-prec. od 83,82% do 84,79% u usporedbi s početnom linijom, što ukazuje na to da se sinteza T2I može uspješno poboljšati koristeći standardni model VQA.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Generar imatges a partir de descripcions textuals ha atragit recentment molt interès. Mentre que els models actuals poden generar imatges fotorealistes d'objectes individus com ocells i cares humanes, sintetitzar imatges amb múltiples objectes encara és molt difícil. En aquest article proposem una manera efectiva de combinar la síntesi text-imatge (T2I) amb la Responsa Visual a Preguntes (VQA) per millorar la qualitat de la imatge i l'alliniament imatge-text de les imatges generades utilitzant el conjunt de dades VQA 2.0. Creem mostres adicionals d'entrenament concatenant parells de preguntes i respostes (QA) i utilitzem un model standard de VQA per proporcionar al model T2I un senyal d'aprenentatge auxiliar. Encoragem les imatges generades de parelles QA a semblar realistes i a minimitzar addicionalment una pèrdua externa de VQA. El nostre mètode redueix el FID de 27,84 a 25,38 i augmenta la R-prec. del 83,82% al 84,79% en comparació amb la base de referència, que indica que la síntesi T2I pot millorar amb èxit amb un model VQA standard.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Generování obrázků z textových popisů v poslední době zaujalo velký zájem. Zatímco současné modely mohou generovat fotorealistické snímky jednotlivých objektů, jako jsou ptáci a lidské tváře, syntetizace snímků s více objekty je stále velmi obtížná. V tomto článku navrhujeme efektivní způsob, jak kombinovat syntézu Text-to-Image (T2I) s vizuálním zodpovězením na otázky (VQA) pro zlepšení kvality obrazu a zarovnání obrazu-textu generovaných obrázků využitím datové sady VQA 2.0. Vytváříme další vzorky školení spojením párů otázek a odpovědí (QA) a používáme standardní VQA model, který poskytuje T2I model pomocným učebním signálem. Obrázky generované z párů QA doporučujeme, aby vypadaly realisticky a navíc minimalizovaly externí ztrátu VQA. Naše metoda snižuje FID z 27.84 na 25.38 a zvyšuje R-prec. Od 83,82% do 84,79% ve srovnání s výchozí hodnotou, což naznačuje, že syntézu T2I lze úspěšně zlepšit pomocí standardního VQA modelu.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Generering af billeder fra tekstbeskrivelser har for nylig tiltrukket stor interesse. Mens nuværende modeller kan generere fotorealistiske billeder af enkelte objekter som fugle og menneskelige ansigter, er det stadig meget vanskeligt at syntetisere billeder med flere objekter. I denne artikel foreslår vi en effektiv måde at kombinere tekst-til-billede (T2I) syntese med visuel spørgsmål besvarelse (VQA) for at forbedre billedkvaliteten og billede-tekst justering af genererede billeder ved at udnytte VQA 2.0 datasættet. Vi skaber yderligere træningsprøver ved at sammenkoble spørgsmål og svar (QA) par og anvender en standard VQA model til at give T2I modellen et ekstra læringssignal. Vi opfordrer billeder genereret fra QA par til at se realistiske og desuden minimere et eksternt VQA tab. Vores metode sænker FID fra 27,84 til 25,38 og øger R-prec. fra 83,82% til 84,79% sammenlignet med baseline, hvilket indikerer, at T2I-syntesen med succes kan forbedres ved hjælp af en standard VQA-model.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Das Generieren von Bildern aus Textbeschreibungen hat in letzter Zeit großes Interesse erregt. Während aktuelle Modelle fotorealistische Bilder von einzelnen Objekten wie Vögeln und menschlichen Gesichtern erzeugen können, ist die Synthese von Bildern mit mehreren Objekten immer noch sehr schwierig. In diesem Beitrag schlagen wir eine effektive Möglichkeit vor, Text-to-Image (T2I)-Synthese mit Visual Question Answering (VQA) zu kombinieren, um die Bildqualität und Bild-Text-Ausrichtung generierter Bilder durch Nutzung des VQA 2.0 Datensatzes zu verbessern. Wir erstellen zusätzliche Trainingsmuster durch Verkettung von Frage-Antwort-Paaren (QA) und verwenden ein Standard-VQA-Modell, um das T2I-Modell mit einem zusätzlichen Lernsignal zu versorgen. Aus QA-Paaren generierte Bilder sollten realistisch aussehen und zusätzlich einen externen VQA-Verlust minimieren. Unsere Methode senkt die FID von 27.84 auf 25.38 und erhöht die R-Prec. Von 83,82% bis 84,79% im Vergleich zur Baseline, was darauf hindeutet, dass die T2I-Synthese mit einem Standard-VQA-Modell erfolgreich verbessert werden kann.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Η δημιουργία εικόνων από περιγραφές κειμένου έχει προσελκύσει πρόσφατα μεγάλο ενδιαφέρον. Ενώ τα τρέχοντα μοντέλα μπορούν να δημιουργήσουν φωτορεαλιστικές εικόνες μεμονωμένων αντικειμένων, όπως πουλιά και ανθρώπινα πρόσωπα, η σύνθεση εικόνων με πολλαπλά αντικείμενα εξακολουθεί να είναι πολύ δύσκολη. Στην παρούσα εργασία, προτείνουμε έναν αποτελεσματικό τρόπο συνδυασμού σύνθεσης κειμένου σε εικόνα (με οπτική απάντηση σε ερωτήσεις) για τη βελτίωση της ποιότητας εικόνας και ευθυγράμμισης εικόνας-κειμένου των παραγόμενων εικόνων χρησιμοποιώντας το σύνολο δεδομένων VQA 2.0. Δημιουργούμε πρόσθετα δείγματα εκπαίδευσης συνδέοντας ζεύγη ερωτήσεων και απαντήσεων και χρησιμοποιούμε ένα τυποποιημένο μοντέλο για να παρέχουμε στο μοντέλο ένα βοηθητικό μαθησιακό σήμα. Ενθαρρύνουμε τις εικόνες που παράγονται από ζεύγη για να φαίνονται ρεαλιστικές και επιπλέον να ελαχιστοποιούν μια εξωτερική απώλεια. Η μέθοδος μας μειώνει το FID από 27.84 έως 25.38 και αυξάνει το R-prec. από 83.82% έως 84.79% σε σύγκριση με τη βάση, γεγονός που δείχνει ότι η σύνθεση T2I μπορεί να βελτιωθεί επιτυχώς χρησιμοποιώντας ένα τυποποιημένο μοντέλο VQA.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Generar imágenes a partir de descripciones textuales ha despertado mucho interés recientemente. Si bien los modelos actuales pueden generar imágenes fotorrealistas de objetos individuales, como pájaros y rostros humanos, sintetizar imágenes con múltiples objetos sigue siendo muy difícil. En este artículo, proponemos una forma eficaz de combinar la síntesis de texto a imagen (T2I) con la respuesta visual a preguntas (VQA) para mejorar la calidad de la imagen y la alineación imagen-texto de las imágenes generadas aprovechando el conjunto de datos VQA 2.0. Creamos ejemplos de entrenamiento adicionales mediante la concatenación de pares de preguntas y respuestas (QA) y empleamos un modelo VQA estándar para proporcionar al modelo T2I una señal de aprendizaje auxiliar. Animamos a que las imágenes generadas a partir de pares de control de calidad parezcan realistas y, además, minimizamos la pérdida de Nuestro método reduce el FID de 27,84 a 25,38 y aumenta el R-prec. del 83,82% al 84,79% en comparación con la línea base, lo que indica que la síntesis de T2I se puede mejorar con éxito utilizando un modelo VQA estándar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Piltide genereerimine tekstikirjeldustest on viimasel ajal palju huvi äratanud. Kuigi praegused mudelid suudavad luua fotorealistlikke pilte üksikutest objektidest, näiteks lindudest ja inimnägudest, on piltide sünteesimine mitme objektiga endiselt väga keeruline. Käesolevas töös pakume välja tõhusa viisi kombineerimiseks tekstist pildiks (T2I) sünteesi visuaalse küsimustele vastamisega (VQA), et parandada kujutiste kvaliteeti ja kujutise teksti joondamist, kasutades VQA 2.0 andmekogumit. Koostame täiendavaid koolitusnäiteid küsimuste ja vastuste paaride ühendamisel ning kasutame standardset VQA mudelit, et anda T2I mudelile lisaõppesignaali. Soovitame kvaliteedi tagamise paaridest loodud pilte realistlikuks näha ja lisaks vähendada välise kvaliteedi tagamise kadu. Meie meetod vähendab FID 27,84-lt 25,38-le ja suurendab R-prec-i. 83,82% kuni 84,79% võrreldes lähtetasemega, mis näitab, et T2I sünteesi saab edukalt parandada standardse VQA mudeli abil.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ایجاد تصاویر از توصیف‌های متن اخیراً خیلی علاقه‌ای را جذب کرده است. در حالی که مدل‌های فعلی می‌توانند تصاویر‌های تصاویر واقعی و تصاویر از شیء‌های فردی مثل پرنده‌ها و صورت‌های انسان را ایجاد کند، تصاویر‌سازی با شیء‌های متعدد هنوز بسیار سخت است. در این کاغذ، ما یک راه موثری پیشنهاد می‌کنیم که متن به تصویر (T2I) را با جواب سوال دیده (VQA) ترکیب کند تا کیفیت تصویر و متن تصویر از تصویر تولید شود با استفاده از مجموعه داده‌های VQA 2.0 را بهبود کند. ما نمونه‌های تمرین اضافه‌ای را با جفت‌های مشترک سوال و جواب (QA) ایجاد می‌کنیم و یک مدل استاندارد VQA را استفاده می‌کنیم تا مدل T2I را با سیگنال یادگیری کمک کند. ما تصاویر را از جفت QA تشویق می‌کنیم تا به نظر واقعی بنظر بیاورند و اضافه‌ای از دست دادن VQA خارجی کم کنیم. روش ما FID را از 27.84 تا 25.38 کاهش می‌دهد و پیشنهاد R را افزایش می‌دهد. از 83.82% تا 84.79% در مقایسه با خط بنیادی، که نشان می دهد که استفاده از مدل VQA استاندارد T2I می تواند با موفقیت بهتر شود.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Kuvien luominen tekstikuvauksista on viime aikoina herättänyt paljon kiinnostusta. Vaikka nykyiset mallit voivat luoda valokuvarealistisia kuvia yksittäisistä objekteista, kuten linnuista ja ihmisen kasvoista, kuvien syntetisointi useiden objektien kanssa on edelleen hyvin vaikeaa. Tässä artikkelissa ehdotamme tehokasta tapaa yhdistää tekstistä kuvaan (T2I) synteesi visuaaliseen kysymysvastaukseen (VQA) parantaaksemme luotujen kuvien kuvanlaatua ja kuvan ja tekstin tasausta hyödyntämällä VQA 2.0 -datajoukkoa. Laadimme lisäkoulutusnäytteitä yhdistämällä kysymys- ja vastauspareja (QA) ja käytämme standardia VQA-mallia, joka antaa T2I-mallille apuoppimissignaalin. Kehotamme laadunvarmistuspareista luotuja kuvia näyttämään realistisilta ja minimoimaan ulkoisen laadunvarmistuksen menetyksen. Menetelmämme laskee FID:n 27,84:stä 25,38:een ja lisää R-prec:tä. 83,82 prosentista 84,79 prosenttiin lähtötasoon verrattuna, mikä osoittaa, että T2I-synteesiä voidaan menestyksekkäästi parantaa käyttämällä standardia VQA-mallia.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>La génération d'images à partir de descriptions textuelles a récemment suscité beaucoup d'intérêt. Alors que les modèles actuels peuvent générer des images photoréalistes d'objets individuels tels que des oiseaux et des visages humains, la synthèse d'images avec plusieurs objets reste très difficile. Dans cet article, nous proposons un moyen efficace de combiner la synthèse texte-image (T2I) avec la réponse visuelle aux questions (VQA) afin d'améliorer la qualité de l'image et l'alignement image-texte des images générées en exploitant le jeu de données VQA 2.0. Nous créons des échantillons de formation supplémentaires en concaténant des paires question-réponse (AQ) et utilisons un modèle VQA standard pour fournir au modèle T2I un signal d'apprentissage auxiliaire. Nous encourageons les images générées à partir de paires d'assurance qualité à être réalistes et à minimiser en outre une perte de VQA externe. Notre méthode abaisse le FID de 27,84 à 25,38 et augmente le R-prec. de 83,82 % à 84,79 % par rapport à la base, ce qui indique que la synthèse de T2I peut être améliorée avec succès à l'aide d'un modèle VQA standard.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Mhealladh íomhánna a ghiniúint ó chur síos téacsúil go leor suime le déanaí. Cé gur féidir le samhlacha reatha íomhánna fóta-réadúil a ghiniúint de réada aonair ar nós éin agus aghaidheanna daonna, tá sé an-deacair fós íomhánna a shintéisiú le réada iolracha. Sa pháipéar seo, molaimid bealach éifeachtach chun sintéis Téacs go hÍomhá (T2I) a chomhcheangal le Freagairt Amharc-Cheisteanna (VQA) chun cáilíocht íomhá agus ailíniú íomhá-téacs na n-íomhánna ginte a fheabhsú trí thacair sonraí VQA 2.0 a ghiaráil. Cruthaímid samplaí oiliúna breise trí phéirí ceisteanna agus freagraí (QA) a chomhchaitheamh agus bainimid úsáid as múnla caighdeánach VQA chun comhartha foghlama cúnta a sholáthar don tsamhail T2I. Spreagaimid íomhánna a ghintear ó phéirí QA chun breathnú réalaíoch agus freisin caillteanas seachtrach VQA a íoslaghdú. Íslíonn ár modh an FID ó 27.84 go 25.38 agus méadaíonn sé an R-prec. ó 83.82% go 84.79% i gcomparáid leis an mbunlíne, rud a léiríonn gur féidir feabhas a chur ar shintéis T2I trí mhúnla caighdeánach VQA a úsáid.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Generating images from textual descriptions has recently attracted a lot of interest. A lokacin da misalin yanzu za'a iya ƙiƙira surar-suro-masu gaske na abubuwa masu kama da tsuntsãye da fuskar mutãne, masu haɗin zane da abubuwa masu yawa, sai yana kasancẽwa mai girma. Daga wannan takardar, Munã buɗe wata hanya mai amfani da za'a haɗa matsayin-zuwa-zane (T2I) da Jabu'a na San'a (V QA) dõmin ya canza tsarin zane da mai daidaita matsayin zane da aka gina zane da shi, da kuma ya juma danne-zane na vQA 2.0. Tuna halitta misãlai masu ƙari da za'a samun su sami masu haɗi da tambayi na sami (QA) ko kuma Muke amfani da kwamfyutan misalin ya yi nau'i-nau'in wa'urar da za'a sami misalin T2I da wata sain da za'a sanar da inganci. Munã kwaɗaitar da zanen da aka samar daga QA sauro don su dũba haliki kuma suna ƙaranci da wata hasara na ƙarƙashin QQA. Kayan hanyoyinmu yana ƙara FID daga 27.84 zuwa 25.38 kuma yana ƙara R-pre. daga 83.82% zuwa 84.79% a lokacin da aka sammenliki zuwa asalin, da za'a nuna cewa, za'a ci kyautata sigarin T2I da amfani da wata motel na QA mai daidaita.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>יצירת תמונות מתיאורים טקסטיים משיכה לאחרונה הרבה עניין. למרות שהדוגמנים הנוכחים יכולים ליצור תמונות מציאותיות של חפצים בודדים כמו ציפורים ופרצוף אנושיים, סינטזיה של תמונות עם חפצים רבים עדיין קשה מאוד. בעיתון הזה, אנו מציעים דרך יעילה לשלב את סינטזה טקסט לתמונה (T2I) עם תשובת שאלות חזותית (VQA) כדי לשפר את איכות התמונה ואת התאמת טקסט התמונה של תמונות יוצרות על ידי השימוש במערכת נתונים VQA 2. 0. אנו יוצרים דגימות אימונים נוספות על ידי שאלה ומשובה (QA) זוגות ומשתמשים בדוגמא VQA סטנדרטית כדי לספק לדוגמא T2I אות לימוד נוסף. אנו מעודדים תמונות שנוצרות מזוגי QA להיראות מציאותיים ובנוסף להפחית אובדן VQA חיצוני. Our method lowers the FID from 27.84 to 25.38 and increases the R-prec. from 83.82% to 84.79% when compared to the baseline, which indicates that T2I synthesis can successfully be improved using a standard VQA model.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>पाठ्य विवरण से छवियों को उत्पन्न करना हाल ही में बहुत सारी रुचि को आकर्षित किया है। जबकि वर्तमान मॉडल पक्षियों और मानव चेहरों जैसे व्यक्तिगत वस्तुओं की फोटो-यथार्थवादी छवियां उत्पन्न कर सकते हैं, कई वस्तुओं के साथ छवियों को संश्लेषित करना अभी भी बहुत मुश्किल है। इस पेपर में, हम VQA 2.0 डेटासेट का लाभ उठाकर उत्पन्न छवियों की छवि की गुणवत्ता और छवि-पाठ संरेखण में सुधार करने के लिए दृश्य प्रश्न उत्तर (VQA) के साथ पाठ-से-छवि (T2I) संश्लेषण को संयोजित करने का एक प्रभावी तरीका प्रस्तावित करते हैं। हम प्रश्न और उत्तर (क्यूए) जोड़े को संयोजित करके अतिरिक्त प्रशिक्षण नमूने बनाते हैं और एक सहायक सीखने के संकेत के साथ टी 2 आई मॉडल प्रदान करने के लिए एक मानक वीक्यूए मॉडल को नियोजित करते हैं। हम क्यूए जोड़े से उत्पन्न छवियों को यथार्थवादी दिखने और इसके अलावा एक बाहरी वीक्यूए हानि को कम करने के लिए प्रोत्साहित करते हैं। हमारी विधि 27.84 से 25.38 तक एफआईडी को कम करती है और आर-प्रीक को बढ़ाती है। बेसलाइन की तुलना में 83.82% से 84.79% तक, जो इंगित करता है कि T2I संश्लेषण को एक मानक VQA मॉडल का उपयोग करके सफलतापूर्वक सुधारा जा सकता है।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Stvaranje slika iz tekstualnih opisa nedavno je privuklo mnogo interesa. Iako trenutni modeli mogu stvoriti fotorealne slike pojedinačnih objekata poput ptica i ljudskih lica, sinteziranje slika s višestrukim objektima još uvijek je teško. U ovom papiru predlažemo učinkovit način kombiniranja sinteze teksta na sliku (T2I) s odgovorom na vizuelno pitanje (VQA) kako bi poboljšali kvalitet slika i poravnanje teksta slika proizvedenih slika povećanjem seta podataka VQA 2.0. Napravili smo dodatne uzorke obuke s potvrđivanjem pitanja i odgovora (QA) par i koristili standardni model VQA kako bi omogućili model T2I pomoćni znak učenja. Poticamo slike proizvedene iz QA pare da izgledaju realistično i dodatno minimiziraju vanjski gubitak VQA-a. Naša metoda smanjuje FID od 27,84 do 25,38 i povećava R-prec. od 83,82% do 84,79% u usporedbi s početnom linijom, što ukazuje na to da se T2I sinteza može uspješno poboljšati koristeći standardni model VQA.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A szöveges leírásokból készült képek generálása a közelmúltban nagy érdeklődést vonzott. Míg a jelenlegi modellek képesek fotórealisztikus képeket generálni egyes objektumokról, például madarakról és emberi arcokról, a képek több objektummal való szintetizálása még mindig nagyon nehéz. Jelen tanulmányban hatékony módszert javasolunk arra, hogy kombináljuk a Szöveg-kép (T2I) szintézisét és a Vizuális Kérdések VQA (VQA), hogy javítsuk a generált képek képminőségét és kép-szöveg igazítását a VQA 2.0 adatkészlet hasznosításával. További képzési mintákat készítünk a kérdés és válasz (QA) párok összekapcsolásával, és egy szabványos VQA modellt alkalmazunk, hogy a T2I modell kiegészítő tanulási jelet biztosítsunk. Javasoljuk, hogy a QA párokból készült képek valóságosak legyenek, és minimalizálják a külső VQA veszteséget. Módszerünk 27,84-ről 25,38-ra csökkenti a FID-t és növeli az R-prec-t. a kiindulási értékhez képest 83,82%-ról 84,79%-ra, ami azt jelzi, hogy a T2I szintézise sikeresen javítható egy standard VQA modell használatával.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Տեքստային նկարներից պատկերներ ստեղծելը վերջերս շատ հետաքրքրություն է գրավել: Մինչդեռ ներկայիս մոդելները կարող են ստեղծել առանձին առարկաների, ինչպիսիք են թռչունները և մարդու դեմքերը, լուսանկարներ սինթեզելու համար բազմաթիվ առարկաներով դեռևս շատ դժվար է: Այս թղթի մեջ մենք առաջարկում ենք մի արդյունավետ միջոց համադրելու տեքստի-պատկերի (T2I) սինթեզին տեսողական հարցերի պատասխանի (VQA) հետ, որպեսզի բարելավենք պատկերի որակը և ստեղծված պատկերի-տեքստի համապատասխանը VQA 2.0 տվյալների համակարգի օգնությամբ: Մենք ստեղծում ենք ավելին ուսումնասիրության նմուշներ' հարցի և պատասխանի (QA) զույգերի միջոցով, և օգտագործում ենք ստանդարտ VQA մոդել, որպեսզի T2I մոդելը օգնական ուսումնասիրության ազդանշան տա: Մենք խրախուսում ենք QA զույգերից ստեղծված նկարները իրական տեսք ունենալ և նաև նվազեցնել արտաքին VQA կորստը: Our method lowers the FID from 27.84 to 25.38 and increases the R-prec. Համեմատելով սկզբնական համեմատությունը՝ T2I-ի սինթեզը կարող է հաջողությամբ բարելավվել ստանդարտ VQA մոդելի միջոցով:</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Membuat gambar dari deskripsi teks baru-baru ini menarik banyak minat. Sementara model saat ini dapat menghasilkan gambar foto-realistis dari objek individu seperti burung dan wajah manusia, sintesis gambar dengan banyak objek masih sangat sulit. Dalam kertas ini, kami mengusulkan cara yang efektif untuk menggabungkan sintesis Teks-ke-Gambar (T2I) dengan Jawaban Pertanyaan Visual (VQA) untuk meningkatkan kualitas gambar dan penyesuaian teks-gambar gambar dari gambar yang dihasilkan dengan menggunakan set data VQA 2.0. Kami menciptakan sampel pelatihan tambahan dengan persatuan pertanyaan dan jawaban (QA) pasangan dan menggunakan model VQA standar untuk menyediakan model T2I dengan sinyal pelatihan tambahan. Kami mendorong gambar yang dihasilkan dari pasangan QA untuk terlihat realistis dan tambahan minimalisasi kehilangan VQA luar. Metode kita menurunkan FID dari 27.84 ke 25.38 dan meningkatkan R-prec. dari 83,82% hingga 84,79% ketika dibandingkan dengan dasar dasar, yang menunjukkan bahwa sintesi T2I dapat berjaya diperbaiki dengan menggunakan model VQA standar.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Generare immagini da descrizioni testuali ha recentemente attirato molto interesse. Mentre i modelli attuali possono generare immagini fotorealistiche di singoli oggetti come uccelli e volti umani, sintetizzare immagini con più oggetti è ancora molto difficile. In questo articolo, proponiamo un modo efficace per combinare la sintesi Text-to-Image (T2I) con Visual Question Answering (VQA) per migliorare la qualità dell'immagine e l'allineamento immagine-testo delle immagini generate sfruttando il dataset VQA 2.0. Creiamo ulteriori campioni di formazione concatenando coppie di domande e risposte (QA) e utilizziamo un modello standard VQA per fornire al modello T2I un segnale di apprendimento ausiliario. Incoraggiamo le immagini generate da coppie di QA a sembrare realistiche e ridurre ulteriormente una perdita esterna di VQA. Il nostro metodo abbassa la FID da 27.84 a 25.38 e aumenta la R-prec. dall'83,82% all'84,79% rispetto al basale, il che indica che la sintesi di T2I può essere migliorata con successo utilizzando un modello standard VQA.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>テキストの説明から画像を生成することは、最近大きな関心を集めています。現在のモデルは、鳥や人間の顔などの個々のオブジェクトのフォトリアルな画像を生成することができますが、複数のオブジェクトと画像を合成することは依然として非常に困難です。本稿では， VQA 2.0データセットを活用して生成された画像の画像品質と画像テキストのアライメントを向上させるために， Text - to - Image (T 2 I)合成とVisual Question Answering (VQA)を組み合わせる効果的な方法を提案する．質問と回答（ QA ）のペアを連結して追加のトレーニングサンプルを作成し、標準的なVQAモデルを採用して、T 2 Iモデルに補助的な学習信号を提供します。QAペアから生成された画像をリアルに見せ、さらに外部VQAロスを最小限に抑えることをお勧めします。我々の方法は、ベースラインと比較して、FIDを27.84から25.38に下げ、R -recを83.82%から84.79%に増加させ、これは、標準的なVQAモデルを使用してT 2 I合成を正常に改善することができることを示している。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>text-tool-action Along-ayo model sing iso nggawe gambar foto-réal sistem banjur-uwong karo ngono cah-cah sing sampeyan karo nganggo cah-cah sing sampeyan Nang paper iki, kita suggerusan sistem effectual kanggo sambah seneng Text-to-Image (T 2I) Ngawe Widual question responsing We create additional tutorial samples by concatenation question and responses (qA) and use a Standard VqA model to present the T 2I model with an help Learning Signal. Awak dhéwé éntuk gambar sing gagalé ning kA perusahaan neng iso nggolakake réal lan tambah banter perusahaan VqA ora bisa dianggo. Rasané awakdhéwé nggawe barang-barang nggawe FD mengawe Mulalah, Jaweh tanggal 2, 3 sampek 10 sampek diradirakno ngono nggawe R-prek sugnoi</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ტექსტუალური გამოსახულებიდან გამოსახულების შექმნა ბევრი ინტერესტის შესაძლებელია. მიმდინარე მოდელები შეუძლიათ ინდიველური ობიექტების ფოტორეალური გამოსახულების შექმნა, როგორც ორთულები და ადამიანის ფერები, სინტესტიზება გამოსახულებით მრავალ ამ დომენტში, ჩვენ ვფიქტირებთ ტექსტის გამოსახულება (T2I) სინტესტის გამოსახულება ვიზუალური კითხვების პასუხისთვის (VQA) გამოსახულებლად გამოსახულება და გამოსახულების ტექსტის გამოსახულება, რომელიც გამოსახულებული ჩვენ შევქმნით დამატებული განსწავლების მონაცემები კითხვა და პასუხი (QA) და სტანდარული VQA მოდელის გამოყენება, რომელიც T2I მოდელის შესაძლებელი სწავლების სიგნალის გამოყენება. ჩვენ QA ზოგიდან შექმნილი გამოსახულებების შემდეგ რეალური და დამატებით გარეშე VQA დასრულებას მინუსიზებთ. ჩვენი პროცემი 27.84-დან 25.38-დან FID-ს გადასვლა და R-prec-ს გაზრდება. 83,82%-დან 84,79%-დან, როდესაც მუშაობით, რომელიც აჩვენებს, რომ T2I სინტესტი შეიძლება სტანდარტული VQA მოდელის გამოყენებით გაუკეთება.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Мәтін сипаттамасынан кескіндерді құру жаңа уақытта көп қызықты. Қолданыстағы үлгілер күш және адамдардың беттері секілді жеке нысандардың фото- реалистикалық кескіндерін құруға болады, бірнеше нысандармен кескіндерді синтезация әлі қатты. Бұл қағазда, VQA 2. 0 деректер жинағын өзгертіп, құрылған кескіндердің кескіндердің сапаты мен кескіндердің мәтінді теңдеу үшін мәтінде (T2I) синтезасын көрінетін сұрақтар жауаптары (VQA) мен біріктіру үшін эффективні жолын та Біз қосымша оқыту үлгілерін сұрақ мен жауап (QA) екеуі арқылы құрып, T2I үлгісін көмектесу сигналымен беру үшін стандартты VQA үлгісін қолданамыз. QA екеуінен құрылған кескіндерді реалистикалық және сыртқы VQA жоғалуын шектеу үшін көмектесеміз. Біздің әдіміміз FID-ді 27,84-ден 25,38-ге төмендетеді және R-prec-ды көтереді. Т2I синтезасы стандартты VQA үлгісін қолдану үшін 83,82% мен 84,79% деп белгілейді.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>텍스트 묘사로 그림을 만드는 것이 최근 많은 흥미를 끌고 있다.현재 모델은 조류와 얼굴 같은 단일 대상의 사진과 실제 이미지를 생성할 수 있지만 여러 대상의 이미지를 합성하는 것은 여전히 매우 어렵다.본고에서 우리는 텍스트를 이미지(T2I)로 합성하고 시각문답(VQA)을 결합시키는 효과적인 방법을 제시하여 VQA 2.0 데이터 집합을 이용하여 이미지를 생성하는 이미지의 질과 이미지-텍스트의 정렬을 향상시켰다.우리는 퀴즈(QA) 쌍을 연결하여 추가 훈련 견본을 만들고 표준 VQA 모델을 사용하여 T2I 모델에 보조 학습 신호를 제공한다.Google은 QA에서 생성된 이미지가 사실적으로 보이고 외부 VQA의 손실을 최소화하도록 권장합니다.Dell의 접근 방식은 FID를 27.84에서 25.38로 낮추고 R-prec를 증가시킵니다.기준선과 비교하면 83.82%에서 84.79%로 표준 VQA 모델을 사용하면 T2I 합성을 개선하는 데 성공할 수 있음을 보여준다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Pastaruoju metu labai suinteresuota sukurti vaizdus iš tekstinių aprašų. Nors dabartiniai modeliai gali sukurti fotorealistinius pavienių objektų, pavyzdžiui, paukščių ir žmogaus veidų, vaizdų sintezė su daugeliu objektų vis dar labai sudėtinga. In this paper, we propose an effective way to combine Text-to-Image (T2I) synthesis with Visual Question Answering (VQA) to improve the image quality and image-text alignment of generated images by leveraging the VQA 2.0 dataset. Kuriame papildomus mokymo mėginius sutrumpinant klausimų ir atsakymų (QA) poras ir naudojame standartinį VQA model į, kad T2I modeliui būtų suteiktas papildomas mokymosi signalas. Skatiname iš QA porų sukurtus vaizdus atrodyti realistiškus ir papildomai sumažinti išorinį VQA praradimą. Mūsų metodas sumažina FID nuo 27,84 iki 25,38 ir padidina R-prec. nuo 83,82 % iki 84,79 % palyginti su pradine verte, o tai rodo, kad T2I sintezę galima sėkmingai pagerinti naudojant standartinį VQA model į.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Генерирањето слики од текстуалните описи неодамна привлече многу интерес. Иако актуелните модели можат да генерираат фото-реални слики на индивидуални објекти како што се птиците и човечките лица, синтезирањето на слики со повеќе објекти е сé уште многу тешко. Во овој документ предложуваме ефикасен начин да се комбинира синтезата Текст до слика (T2I) со Визуелниот одговор на прашања (VQA) за подобрување на квалитетот на сликата и израмнувањето на слика до текст на генерираните слики со влијание на датотеката VQA 2. 0. Создаваме дополнителни примероци за обука преку концентрирање на парови прашања и одговори (QA) и употребуваме стандарден VQA модел за да му обезбедиме помошни сигнали за учење на моделот T2I. Ние охрабруваме слики генерирани од парови QA да изгледаат реални и дополнително да минимизираат надворешна загуба на VQA. Нашиот метод го намалува ФИД од 27,84 на 25,38 и го зголемува Р-станицата. од 83,82 отсто на 84,79 отсто во споредба со основната вредност, што покажува дека синтезата на Т2I може успешно да се подобри користејќи стандарден модел на ВКА.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ടെക്സ്ചുള്‍ വിവരണങ്ങളില്‍ നിന്നുള്ള ചിത്രങ്ങള്‍ ഉണ്ടാക്കുന്നത് അടുത്തുതന്നെ വളരെ താല്പര്യമുണ്ട്. ഇപ്പോഴത്തെ മോഡലുകള്‍ക്ക് ഫോട്ടോ ബാക്കിയാക്കാന്‍ സാധിക്കുമ്പോള്‍ പക്ഷികളും മനുഷ്യരുടെ മുഖങ്ങളും പോലുള്ള ചിത്രങ്ങള്‍ ഉണ്ടാ ഈ പത്രത്തില്‍, വിക്യൂ 2. 0 ഡാറ്റാസസെറ്റില്‍ നിര്‍മ്മിച്ച ചിത്രങ്ങളുടെ ചിത്രത്തിന്റെ ചോദ്യങ്ങള്‍ മാറ്റുവാന്‍ വേണ്ടി പദാവലിയില്‍ ടെക്സ്റ്റ് ചേര്‍ക്കുന്നതിനുള കൂടുതല്‍ പരിശീലനത്തിന്റെ ഉപമകള്‍ നമ്മള്‍ സൃഷ്ടിക്കുന്നു. ചോദ്യം കൂട്ടിച്ചേര്‍ക്കുന്നതിനും ഉത്തരം ഉത്തരം (ക്യൂഎ) ജോട്ടികള്‍ ഉണ്ടാക്കുന ക്യൂഎയില്‍ നിന്നുള്ള ഇണകള്‍ ഉണ്ടാക്കിയ ചിത്രങ്ങള്‍ ഞങ്ങള്‍ ആശ്വാസപ്പെടുത്തുന്നു. യഥാര്‍ത്ഥത്തില്‍ നിന്നും പു ഞങ്ങളുടെ രീതി 27.84 ല്‍ നിന്നും 25.38ലേക്കും എഫ്‌ഐഡിയെ കീഴ്പ്പെടുത്തി R-prec വര്‍ദ്ധിപ്പിക്കുന്നു. അടിസ്ഥാനത്തോട് താരതമ്യം ചെയ്യപ്പെടുമ്പോള്‍ 83. 82% വരെ 84. 79% വരെയും, ടി2I സിന്‍റീസിസ് വിസ്കിയാ മോഡല്‍ ഉപയോഗിച്ച് വിജയിച്ച് മെച</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Шинэ тайлбараас зураг бий болгох нь саяхан их сонирхолтой болсон. Хэдийгээр одоогийн загварууд нь шувуу, хүн төрөлхтний зурагнуудын зураг-реалист зураг бий болгодог ч, олон объекттай зурагнуудыг нэгтгэх нь маш хэцүү. Энэ цаасан дээр бид Текст-ээс-зурагт (T2I) синтезийг Visual Question Response (VQA) болон үүсгэн зурагт болон зурагт-текст тэгшитгэлийг сайжруулах боломжтой арга зааж байна. Бид асуулт болон хариулт (QA) хоёр болон T2I загварын загварын тусламжтай суралцах зохиолын стандарт VQA загварыг ашиглаж байна. Бид QA хоёрт гарсан зургуудыг бодит харагдаж, нэмэлтэй нь гадаад VQA-ын алдагдлыг багасгаж байна. Бидний арга нь 27.84-аас 25.38 руу FID-г багасгаж R-prec-ыг нэмэгдүүлнэ. Т2I синтезийг стандарт VQA загвараар амжилттай сайжруулж чадна гэсэн үг.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Menjana imej dari deskripsi teks baru-baru ini menarik banyak minat. Sementara model semasa boleh menghasilkan imej fotorealistik bagi objek individu seperti burung dan muka manusia, penyintesis imej dengan objek berbilang masih sangat sukar. Dalam kertas ini, kami cadangkan cara yang berkesan untuk menggabungkan sintesis Teks-ke-Imej (T2I) dengan Jawapan soalan Visual (VQA) untuk meningkatkan kualiti imej dan penyesuaian teks-imej imej imej yang dijana dengan menggunakan set data VQA 2.0. Kami mencipta sampel latihan tambahan dengan menambah soalan dan jawapan pasangan (QA) dan menggunakan model VQA piawai untuk menyediakan model T2I dengan isyarat pembelajaran tambahan. Kami menyokong imej yang dihasilkan dari pasangan QA untuk kelihatan realistik dan tambahan mengurangkan kerugian VQA luaran. Kaedah kita menurunkan FID dari 27.84 ke 25.38 dan meningkatkan R-prec. Dari 83.82% hingga 84.79% bila dibandingkan dengan asas, yang menunjukkan bahawa sintesis T2I boleh berjaya diperbaiki menggunakan model VQA piawai.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Il-ġenerazzjoni ta’ immaġni minn deskrizzjonijiet testwali reċentement ġibdet ħafna interess. Filwaqt li l-mudelli attwali jistgħu jiġġeneraw immaġni fotorealistiċi ta’ oġġetti individwali bħall-għasafar u l-wiċċ tal-bniedem, is-sinteżi ta’ immaġni b’oġġetti multipli għadha diffiċli ħafna. F’dan id-dokument, qed nipproponu mod effettiv kif nikkombinaw is-sinteżi Test-għal-Immaġni (T2I) mat-tweġiba għall-mistoqsijiet viżwali (VQA) biex itejbu l-kwalità tal-immaġni u l-allinjament tat-test tal-immaġni tal-immaġni ġġenerati billi ninfurzaw is-sett tad-dejta VQA 2.0. Aħna nħolqu kampjuni ta’ taħriġ addizzjonali permezz ta’ pari konċitanti ta’ mistoqsijiet u tweġibiet (QA) u nużaw mudell standard ta’ VQA biex jipprovdu lill-mudell T2I b’sinjal ta’ tagħlim awżiljarju. We encourage images generated from QA pairs to look realistic and additionally minimize an external VQA loss. Il-metodu tagħna jnaqqas il-FID minn 27.84 għal 25.38 u jżid l-R-prec. minn 83.82% sa 84.79% meta mqabbel mal-linja bażi, li jindika li s-sintesi T2I tista’ titjieb b’suċċess bl-użu ta’ mudell VQA standard.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Het genereren van afbeeldingen uit tekstuele beschrijvingen heeft de laatste tijd veel belangstelling getrokken. Terwijl huidige modellen fotorealistische beelden kunnen genereren van individuele objecten zoals vogels en menselijke gezichten, is het synthetiseren van afbeeldingen met meerdere objecten nog steeds erg moeilijk. In dit artikel stellen we een effectieve manier voor om Text-to-Image (T2I) synthese te combineren met Visual Question Answering (VQA) om de beeldkwaliteit en beeld-tekst uitlijning van gegenereerde afbeeldingen te verbeteren door gebruik te maken van de VQA 2.0 dataset. We creëren extra trainingsvoorbeelden door vraag en antwoord (QA) paren aan elkaar te koppelen en gebruiken een standaard VQA model om het T2I model van een hulpleersignaal te voorzien. We moedigen afbeeldingen gegenereerd uit QA-paren aan om er realistisch uit te zien en bovendien een extern VQA-verlies te minimaliseren. Onze methode verlaagt de FID van 27.84 naar 25.38 en verhoogt de R-prec. van 83,82% tot 84,79% in vergelijking met de baseline, wat aangeeft dat T2I synthese met succes kan worden verbeterd met behulp van een standaard VQA model.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Lagring av bilete frå tekstskildringar har nyleg attrakta mykje interesse. Mens gjeldande modeller kan laga fotorealistiske bilete av individuelle objekt, som fugler og menneske ansikter, er det fortsatt vanskeleg å syntisera bilete med fleire objekt. I denne papiret foreslår vi ein effektiv måte å kombinere tekst-til-bilete (T2I) syntese med Visuelt spørsmål-svar (VQA) for å forbetra kvaliteten til biletet og justeringa av biletet med laga bilete ved å levera datasettet VQA 2.0. Vi oppretter ekstra øvingsprøver ved å samsvara spørsmål og svar (QA) og bruke eit standard VQA-modell for å tilbya T2I-modellen med eit hjelpelæringssignal. Vi oppfordrer bilete laga frå QA- par for å sjå realistisk og tillegg minimere ein ekstern tap av VQA. Metoden vårt låser FID frå 27,84 til 25,38 og øker R-prec. frå 83,82% til 84,79% når sammenlignet med baselinja, som viser at T2I-syntese kan forbetrast med standardmodellen VQA.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Generowanie obrazów z opisów tekstowych wzbudziło ostatnio duże zainteresowanie. Podczas gdy obecne modele mogą generować fotorealistyczne obrazy poszczególnych obiektów, takich jak ptaki i ludzkie twarze, synteza obrazów z wieloma obiektami nadal jest bardzo trudna. W niniejszym artykule proponujemy skuteczny sposób połączenia syntezy Text-to-Image (T2I) z Visual Question Respwering (VQA) w celu poprawy jakości obrazu i wyrównania obrazu do tekstu generowanych obrazów poprzez wykorzystanie zbioru danych VQA 2.0. Tworzymy dodatkowe próbki szkoleniowe poprzez łączenie par pytań i odpowiedzi (QA) i wykorzystujemy standardowy model VQA, aby zapewnić model T2I pomocniczy sygnał uczenia się. Zachęcamy obrazy generowane z par QA, aby wyglądały realistycznie i dodatkowo minimalizowały zewnętrzną stratę VQA. Nasza metoda obniża FID z 27.84 do 25.38 i zwiększa R-prec. Od 83,82% do 84,79% w porównaniu do bazowego, co wskazuje, że syntezę T2I można z powodzeniem poprawić za pomocą standardowego modelu VQA.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>A geração de imagens a partir de descrições textuais atraiu recentemente muito interesse. Embora os modelos atuais possam gerar imagens fotorrealistas de objetos individuais, como pássaros e rostos humanos, a síntese de imagens com vários objetos ainda é muito difícil. Neste artigo, propomos uma maneira eficaz de combinar a síntese Text-to-Image (T2I) com o Visual Question Answering (VQA) para melhorar a qualidade da imagem e o alinhamento imagem-texto das imagens geradas, aproveitando o conjunto de dados VQA 2.0. Criamos amostras de treinamento adicionais concatenando pares de perguntas e respostas (QA) e empregamos um modelo VQA padrão para fornecer ao modelo T2I um sinal de aprendizado auxiliar. Incentivamos as imagens geradas a partir de pares de QA a parecerem realistas e, adicionalmente, minimizar uma perda de VQA externa. Nosso método reduz o FID de 27,84 para 25,38 e aumenta o R-prec. de 83,82% para 84,79% quando comparado com a linha de base, o que indica que a síntese de T2I pode ser melhorada com sucesso usando um modelo VQA padrão.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Generarea de imagini din descrieri textuale a atras recent mult interes. În timp ce modelele actuale pot genera imagini foto-realiste ale obiectelor individuale, cum ar fi păsările și fețele umane, sintetizarea imaginilor cu mai multe obiecte este încă foarte dificilă. În această lucrare, propunem o modalitate eficientă de a combina sinteza Text-to-Image (T2I) cu răspunsul la întrebări vizuale (VQA) pentru a îmbunătăți calitatea imaginii și alinierea imagine-text a imaginilor generate prin utilizarea setului de date VQA 2.0. Creăm mostre suplimentare de formare prin concatenarea perechilor de întrebări și răspunsuri (QA) și utilizăm un model standard VQA pentru a furniza modelului T2I un semnal auxiliar de învățare. Încurajăm imaginile generate din perechile QA să arate realist și, în plus, să minimizeze o pierdere externă VQA. Metoda noastră reduce FID de la 27.84 la 25.38 și crește R-prec. de la 83,82% la 84,79% comparativ cu valoarea iniţială, ceea ce indică faptul că sinteza T2I poate fi îmbunătăţită cu succes utilizând un model standard VQA.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Генерация изображений из текстовых описаний недавно вызвала большой интерес. Хотя нынешние модели могут генерировать фотореалистичные изображения отдельных объектов, таких как птицы и человеческие лица, синтезировать изображения с несколькими объектами по-прежнему очень сложно. В этой статье мы предлагаем эффективный способ сочетания синтеза текста с изображением (T2I) с визуальным ответом на вопросы (VQA) для улучшения качества изображения и выравнивания изображения с текстом сгенерированных изображений за счет использования набора данных VQA 2.0. Мы создаем дополнительные обучающие выборки путем объединения пар вопросов и ответов (QA) и используем стандартную модель VQA, чтобы обеспечить модель T2I вспомогательным обучающим сигналом. Мы рекомендуем изображения, созданные из пар QA, выглядеть реалистично и дополнительно минимизировать внешние потери VQA. Наш метод снижает FID с 27,84 до 25,38 и увеличивает R-prec. с 83,82% до 84,79% по сравнению с базовой линией, что указывает на то, что синтез T2I может быть успешно улучшен с использованием стандартной модели VQA.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>පිළිබඳ විස්තරණයෙන් පින්තූර නිර්මාණය කරන්න අවස්ථානයෙන් ගොඩක් විශේෂයක් ඇතුළත් කර ප්‍රස්තූත මොඩේල් එක්ක පින්තූර- ඇත්තම පින්තූර පින්තූර නිර්මාණය කරන්න පුළුවන් විදිහට පින්තූර සහ මින මේ පැත්තේ, අපි ප්‍රශ්නයක් ප්‍රශ්නයක් ප්‍රශ්නයක් ප්‍රශ්නයක් ප්‍රශ්නයක් ප්‍රශ්නයක් වන්න ප්‍රශ්නයක් ප්‍රශ්නයක් සහ පින්තූර පින්තූර පින්තූ අපි ප්‍රශ්නය සහ උත්තර (QA) ජෝඩු වලින් අතර ප්‍රශ්නයක් නිර්මාණය කරනවා ඒ වගේම ප්‍රශ්නයක් VQA මොඩේල් එකක් භාවිත කරනවා T2 අපි QA ජෝඩියෙන් නිර්මාණය කරපු පින්තූරයක් ප්‍රශ්නය කරනවා ඇත්තටම පෙනුම් වගේම ප්‍රශ්නයක් වගේම ප අපේ විධානය 27.84 වල 25.38 වලින් FID එක අඩු කරනවා ඒ වගේම R-pre විශ්වාස කරනවා. 83.82% ඉඳන් 84.79% ඉඳන් ප්‍රමාණික VQA මොඩල් එකක් භාවිත කරන්න පුළුවන් කියලා.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ustvarjanje slik iz besedilnih opisov je v zadnjem času pritegnilo veliko zanimanja. Medtem ko lahko sedanji modeli ustvarijo fotorealistične slike posameznih predmetov, kot so ptice in človeški obrazi, je sintetiziranje slik z več predmeti še vedno zelo težko. V tem prispevku predlagamo učinkovit način kombiniranja sinteze besedila v sliko (T2I) z vizualnim odgovorom na vprašanja (VQA) za izboljšanje kakovosti slike in poravnave slike-besedila ustvarjenih slik z uporabo nabora podatkov VQA 2.0. Ustvarjamo dodatne vzorce usposabljanja s povezovanjem parov vprašanj in odgovorov (QA) in uporabljamo standardni model VQA, da modelu T2I zagotovimo pomožni učni signal. Spodbujamo slike, ustvarjene iz parov kakovosti, da bodo videti realistične in dodatno zmanjšale zunanjo izgubo VQA. Naša metoda zniža FID s 27,84 na 25,38 in poveča R-prec. s 83,82% na 84,79% v primerjavi z izhodiščno vrednostjo, kar kaže, da je sintezo T2I mogoče uspešno izboljšati s standardnim modelom VQA.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Muujinta sawirro ka mid ah qoraalka qoraalka ayaa dhaqdhaqaaqay xiiso badan. Wakhti modellada haatan waxay sameyn karaan sawirro rasmi ah oo sawir rasmi ah, tusaale ahaan shimbirro iyo wejiyada dadku, isla keenista sawirro ay wax badan ku leeyihiin weli waa adag. Qoraalkan waxaynu soo jeedaynaa qaab saamayn ah oo u soo wada ururinta sawir-to-sawir (T2I) iyo jawaabta su'aalka aragga (VQA) si loo hagaajiyo sawirka iyo isbedelka sawirka sawir-sawirka oo lagu sameeyo sawirada lagu soo daabaco sawirada VQA 2.0. We create additional training samples by concatenating question and answer (QA) pairs and employ a standard VQA model to provide the T2I model with an auxiliary learning signal. Waxaynu ku dhiirranaynaa sawirro ka soo dhashay QA labo si ay u eegaan xaqiiqnimo iyo sidoo kale ugu yaraan khasaarada dibadda ee VQA. Waddadayada ayaa FID ka hoosaysiiya 27.84 ilaa 25.38 kadibna u kordhiya R-prec. 83.82% ilaa 84.79% marka la barbardhigo asalka, taasoo looga jeedaa in lagu kordhin karo sameynta qoraalka VQA standardka ah ee T2I.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Gjenerimi i imazheve nga përshkrimet tekstuale ka tërhequr kohët e fundit shumë interes. Ndërsa modelet aktuale mund të gjenerojnë imazhe fotorealiste të objekteve individuale të tilla si zogjtë dhe fytyrat njerëzore, sintetizimi i imazheve me objekte të shumta është ende shumë i vështirë. Në këtë letër, propozojmë një mënyrë efektive për të kombinuar sintezën tekst-imazh (T2I) me përgjigjen vizuale të pyetjeve (VQA) për të përmirësuar cilësinë e imazhit dhe përshtatjen e imazhit-tekstit të imazheve të gjeneruara duke përdorur ngjyrën e të dhënave VQA 2.0. Ne krijojmë kampione shtesë trajnimi duke përmbledhur çiftet e pyetjeve dhe përgjigjeve (QA) dhe përdorim një model standard VQA për të siguruar modelin T2I me një sinjal mësimi ndihmës. Ne inkurajojmë imazhet e gjeneruara nga çiftet QA për të dukur realiste dhe për të minimizuar shtesë një humbje të jashtme VQA. Metoda jonë ulë FID nga 27.84 në 25.38 dhe rrit R-prec. nga 83.82% në 84.79% krahasuar me bazën, që tregon se sinteza T2I mund të përmirësohet me sukses duke përdorur një model VQA standard.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Stvaranje slika iz tekstualnih opisa nedavno je privuklo mnogo interesa. Dok trenutni modeli mogu stvoriti fotorealistične slike individualnih objekata poput ptica i ljudskih lica, sinteziranje slika sa višestrukim objektima je još uvek veoma teško. U ovom papiru predlažemo efikasan naèin da kombiniramo sintezu teksta na sliku (T2I) sa odgovorom na vizuelne pitanja (VQA) kako bi poboljšali kvalitet slika i poravnanje teksta slika proizvedenih slika povećanjem seta podataka VQA 2.0. Napravili smo dodatne uzorke obuke, povećavajući pitanje i odgovor (QA) pare i zapošljavamo standardni model VQA kako bi omogućili model T2I pomoćni signal učenja. Poticamo slike koje su proizvedene iz parova QA da izgledaju realistično i dodatno minimalizuju vanjski gubitak VQA-a. Naša metoda smanjuje FID od 27,84 do 25,38 i povećava R-prec. od 83,82% do 84,79% u usporedbi sa početnom linijom, što ukazuje na to da se sinteza T2I može uspešno poboljšati koristeći standardni model VQA.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Att skapa bilder från textbeskrivningar har nyligen väckt ett stort intresse. Även om nuvarande modeller kan generera fotorealistiska bilder av enskilda objekt som fåglar och mänskliga ansikten, är det fortfarande mycket svårt att syntetisera bilder med flera objekt. I denna uppsats föreslår vi ett effektivt sätt att kombinera text-till-bild (T2I) syntes med Visual Question Answering (VQA) för att förbättra bildkvaliteten och bild-text justering av genererade bilder genom att utnyttja VQA 2.0 datauppsättningen. Vi skapar ytterligare utbildningsprover genom att sammanfoga frågor och svar (QA) par och använder en standard VQA-modell för att ge T2I-modellen en extra inlärningssignal. Vi uppmuntrar bilder som genereras från QA par att se realistiska och dessutom minimera en extern VQA förlust. Vår metod sänker FID från 27,84 till 25,38 och ökar R-prec. från 83,82% till 84,79% jämfört med baslinjen, vilket tyder på att T2I-syntesen framgångsrikt kan förbättras med en standardVQA-modell.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Generating images from textual descriptions has recently attracted a lot of interest. Wakati mifano ya sasa inaweza kutengeneza picha halisi za vitu vya watu kama vile ndege na nyuso za binadamu, kukusanya picha pamoja na vitu kadhaa bado ni vigumu sana. Katika karatasi hii, tunapendekeza njia yenye ufanisi ya kuunganisha mfumo wa simu za mkononi (T2I) na jibu la swali la Visual (VQA) ili kuboresha ubora wa picha na usambazaji wa picha zilizotengenezwa kwa kutumia seti ya data za VQA 2.0. Tunaweza kutengeneza sampuli ya mafunzo zaidi kwa kuunganisha swali na kujibu wawili (QA) na kutumia muundo wa kiwango cha VQA ili kutoa mtindo wa T2I na saini ya kujifunza kwa ushirikiano. Tunamasisha picha zilizotengenezwa kutoka kwa wanaume wa QA ili kuona halisi na kuondoa hasara ya VQA ya nje. Utawala wetu unapunguza FID kutoka 27.84 hadi 25.38 na kuongeza kabla ya R. kutoka asilimia 83.82 hadi asilimia 84.79 ikilinganishwa na mstari wa msingi, ambayo inaonyesha kuwa muungano wa T2I unaweza kuboreshwa kwa kutumia mtindo wa VQA wa kiwango cha kawaida.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>பிம்பங்களை உருவாக்குவது சமீபத்தில் உள்ள வரிசைகளில் இருந்து வட்டி மிகவும் உச்சரிக்கப்பட்டது. தற்போதைய மாதிரிகள் புகைப்படம் உண்மையான பிம்பங்களை உருவாக்க முடியும் போது, பறவைகள் மற்றும் மனித முகங்கள் போன்ற தனிப்பட்ட பொர @ info @ info QA ஜோடி உருவாக்கப்பட்ட பிம்பங்களை நாம் உறுதிப்படுத்துகிறோம் உண்மையாகவும் வெளி விக்குவாய் இழப்பை குறைக்கவு Our method lowers the FID from 27.84 to 25.38 and increases the R-prec. அடிப்படைக்கோட்டிற்கு ஒப்பிடும்போது 83.82% வரை 84. 79% வரை அது T2I ஒத்திசைவு வெற்றிகரமாக முடியும் என்று குறிப்பிடுகிறது ஒரு இயல</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Metin deskriplerinden suratlar döredilmek üçin gaty gyzyklandy. Häzirki modeller birnäçe zadyň suratyny guşlar we ynsan suratlary ýaly foto-realistik suratlaryny döredip biler, birnäçe zadyň bilen suratlaryny synthesize etmek gaty kyn. Bu kagyzda, VQA 2.0 veri setirini etkinleştirmek üçin metin-tä-Resim (T2I) sintezini görkezmek üçin etkinlik bir ýoly teklip berýäris Biz sorag we jogabat(QA) çiftleri bilen daňar okuw öränlerini bejerdik we T2I nusgasyny kömek öwrenmesi sinyali bilen üýtgetmek üçin standart VQA nusgasyny ulanýarys. QA çiftlerden oluşan suratları realistik görünmek üçin teşvik edip, daşary bir VQA ýitişini azaltmak üçin süýtgedik. Bizim yöntemimiz FID'i 27,84'den 25,38'e düşürür ve R-preci arttır. 83.82% we 84.79% baseline we baglanyşykda T2I sintezi standart VQA modelini ullanyp başarıyla gelip biler diýip görkez.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ٹیکسٹل کی توصیف سے تصاویر پیدا کرنے کے لئے اچھے وقت بہت سی علاقه اٹھائی گئی ہے. حاﻻنکہ موجود موڈلز ایک شخصی موجود کی تصاویر پیدا کر سکتے ہیں جیسے پرندے اور انسان کے چہرے، بہت سے موجود موجود کے ساتھ تصاویر سینٹیسی کرنا بھی بہت مشکل ہے اس کاغذ میں ہم ایک مثبت طریقہ پیش کریں گے کہ ایک ٹیکسٹ-سے-تصویر (T2I) سینٹسی کو Visual Question Answering (VQA) کے ساتھ جمع کریں۔ کہ VQA 2.0 ڈیٹ سٹ کے ذریعے ایمزی کیفیت اور ایمزی-ٹیکسٹ سینٹسی کو اضافہ کریں۔ ہم سوال اور جواب (QA) کے ذریعے اضافہ تطارین نمونے بناتے ہیں اور ایک استاندارد VQA نمونڈل کو استعمال کرتے ہیں کہ T2I نمونڈل کو ایک مددگار تعلیم سیگنالہ کے ساتھ دے۔ ہم QA جوڑوں سے پیدا کیے ہوئے تصاویروں کو سفارش دیتے ہیں کہ حقیقت کی طرح بن جائیں اور اضافہ بھی ایک خارجی VQA خسارہ کو کم کریں۔ ہمارا طریقہ 27.84 سے 25.38 سے FID کو کم کرتا ہے اور R-prec کو زیادہ کرتا ہے۔ 83.82% سے 84.79% سے baseline کے مقابلہ میں، جسے نشان دیتا ہے کہ T2I سینٹسی ایک استاندارد VQA موڈل کے مطابق موفقیت سے بہتر کر سکتی ہے.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Name Joriy modellar bir necha obʼekt bilan bir necha obʼektlar sifatida rasmni yaratishi mumkin, bir necha narsa bilan rasmlarni birlashtirish juda qiyin. Bu hujjatda, VQA 2. 0 maʼlumot moslamalarini yordamida yaratilgan rasm sifatini va matnning oʻlchamini oshirish uchun matnning (T2I) tizimini birlashtirish imkoniyatlarini tasavvur qilamiz. Biz qoʻshimcha taʼminlovchi misollarni birlashtirish va javob beramiz va T2I modelini to ʻgʻri o'rganish imkoniyatini yaratish uchun standard VQA modelini ishlaymiz. @ info Our method lowers the FID from 27.84 to 25.38 and increases the R-prec. Andoza VQA modelini ishlatish mumkin.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tạo ảnh từ mô tả kết cấu gần đây thu hút rất nhiều sự chú ý. Trong khi các mô hình hiện tại có thể tạo ảnh chụp thực tế về các vật thể cá nhân như chim và khuôn mặt con người, việc tổng hợp ảnh với nhiều vật thể vẫn rất khó khăn. Trong tờ giấy này, chúng tôi đề nghị một cách hiệu quả để kết hợp kết hợp kết hợp kết cấu văn bản tới ảnh (2.I) với câu trả lời hỏi ảo (kiểu QA) để cải thiện chất lượng ảnh và cấu hình ảnh của các ảnh từ các ảnh đã tạo bằng cách dùng bộ dữ liệu kiểu VQA 2.0. Chúng tôi tạo ra các mẫu huấn luyện bổ sung bằng cách kết nối câu hỏi và trả lời (QA) và sử dụng một mô hình VHA tiêu chuẩn để cung cấp cho mô hình 2 với tín hiệu học phụ. Chúng tôi khuyến khích những hình ảnh tạo ra từ các đội bóng QA để trông thực tế và giảm thiểu rủi ro. Cách của chúng tôi hạ FID từ 27.84 đến 25.38 và nâng R-pre lên. từ 82=.=* tới 84.99=.=sosánh với đường cơ bản, ngụ ý là kết hợp 2-2 có thể tiến bộ thành công nhờ một mô hình VHA chuẩn.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>从文本描述中生成图像近来很多兴趣。 虽形可以成单物(鸟面)照片逼真,合之犹难。 于本文中,设将文本至图像(T2I)合视觉问答(VQA)合者有效,因VQA 2.0数集来以成图像 - 齐之。 接问答(QA)创额外训练样本,并用格VQA为T2I模以助学信。 劝从 QA 生图像逼真,并减损外 VQA 。 吾法降 FID 自 27.84 至 25.38,而增以 R-prec。 比之基线,自83.82%至于84.79%,明用度VQA可以成功改善T2I合也。</span></div></div><dl><dt>Anthology ID:</dt><dd>2020.lantern-1.2</dd><dt>Volume:</dt><dd><a href=/volumes/2020.lantern-1/>Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</a></dd><dt>Month:</dt><dd>December</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Barcelona, Spain</dd><dt>Venues:</dt><dd><a href=/venues/coling/>COLING</a>
| <a href=/venues/lantern/>LANTERN</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>17–22</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.lantern-1.2>https://aclanthology.org/2020.lantern-1.2</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">frolov-etal-2020-leveraging</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Stanislav Frolov, Shailza Jolly, Jörn Hees, and Andreas Dengel. 2020. <a href=https://aclanthology.org/2020.lantern-1.2>Leveraging Visual Question Answering to Improve Text-to-Image Synthesis</a>. In <i>Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</i>, pages 17–22, Barcelona, Spain. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/2020.lantern-1.2>Leveraging Visual Question Answering to Improve Text-to-Image Synthesis</a> (Frolov et al., LANTERN 2020)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2020.lantern-1.2.pdf>https://aclanthology.org/2020.lantern-1.2.pdf</a></dd><dt>Data</dt><dd><a href=https://paperswithcode.com/dataset/coco>COCO</a>,&nbsp;<a href=https://paperswithcode.com/dataset/visual-question-answering>Visual Question Answering</a>,&nbsp;<a href=https://paperswithcode.com/dataset/visual-question-answering-v2-0>Visual Question Answering v2.0</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2020.lantern-1.2.pdf title="Open PDF of 'Leveraging Visual Question Answering to Improve Text-to-Image Synthesis'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Leveraging+Visual+Question+Answering+to+Improve+Text-to-Image+Synthesis" title="Search for 'Leveraging Visual Question Answering to Improve Text-to-Image Synthesis' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'Leveraging Visual Question Answering to Improve Text-to-Image Synthesis'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[Leveraging Visual Question Answering to Improve Text-to-Image Synthesis](https://aclanthology.org/2020.lantern-1.2) (Frolov et al., LANTERN 2020)</p><ul class=mt-2><li><a href=https://aclanthology.org/2020.lantern-1.2>Leveraging Visual Question Answering to Improve Text-to-Image Synthesis</a> (Frolov et al., LANTERN 2020)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Stanislav Frolov, Shailza Jolly, Jörn Hees, and Andreas Dengel. 2020. <a href=https://aclanthology.org/2020.lantern-1.2>Leveraging Visual Question Answering to Improve Text-to-Image Synthesis</a>. In <i>Proceedings of the Second Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)</i>, pages 17–22, Barcelona, Spain. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>