<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css><meta content="NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning" name=citation_title><meta content="Christos Baziotis" name=citation_author><meta content="Athanasiou Nikolaos" name=citation_author><meta content="Alexandra Chronopoulou" name=citation_author><meta content="Athanasia Kolovou" name=citation_author><meta content="Georgios Paraskevopoulos" name=citation_author><meta content="Nikolaos Ellinas" name=citation_author><meta content="Shrikanth Narayanan" name=citation_author><meta content="Alexandros Potamianos" name=citation_author><meta content="Proceedings of The 12th International Workshop on Semantic Evaluation" name=citation_conference_title><meta content="2018/6" name=citation_publication_date><meta content="https://aclanthology.org/S18-1037.pdf" name=citation_pdf_url><meta content="245" name=citation_firstpage><meta content="255" name=citation_lastpage><meta content="10.18653/v1/S18-1037" name=citation_doi><meta property="og:title" content="NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning"><meta property="og:image" content="https://aclanthology.org/thumb/S18-1037.jpg"><meta property="og:image:alt" content="First page of paper PDF."><meta property="og:type" content="article"><meta property="og:site_name" content="ACL Anthology"><meta property="og:url" content="https://aclanthology.org/S18-1037"><meta property="og:description" content="Christos Baziotis, Athanasiou Nikolaos, Alexandra Chronopoulou, Athanasia Kolovou, Georgios Paraskevopoulos, Nikolaos Ellinas, Shrikanth Narayanan, Alexandros Potamianos. Proceedings of The 12th International Workshop on Semantic Evaluation. 2018."><link rel=canonical href=https://aclanthology.org/S18-1037></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><div><h2 id=title><a id=en_title href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning<span class=acl-fixed-case>NTUA</span>-<span class=acl-fixed-case>SLP</span> at <span class=acl-fixed-case>S</span>em<span class=acl-fixed-case>E</span>val-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive <span class=acl-fixed-case>RNN</span>s and Transfer Learning</a>
<a id=af_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA- SLP by SemEval- 2018 Taak 1: Voorskou Affektiewe Inhoud in Tweets met Deep Aangaande RNN en Oordrag Leer</a>
<a id=am_title style=display:none href=https://aclanthology.org/S18-1037.pdf>ስራ 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learn</a>
<a id=ar_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP في SemEval-2018 المهمة 1: توقع المحتوى المؤثر في التغريدات باستخدام RNNs اليقظة للغاية ونقل التعلم</a>
<a id=az_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP at SemEval-2018 Task 1: Deep Attentive RNN and Transfer Learning in Tweets</a>
<a id=bg_title style=display:none href=https://aclanthology.org/S18-1037.pdf>Задача 1: Прогнозиране на афективното съдържание в Tweets с дълбоко внимание и трансферно обучение</a>
<a id=bn_title style=display:none href=https://aclanthology.org/S18-1037.pdf>সেমইভাল-২০১৮ কাজ ১-এ NTUA-SLP: গভীর অ্যাটেন্টিভিভ আর ট্রান্সফার্নার শিক্ষা দিয়ে টুইটে আফ্রিকেটিভ বিষয়বস্তু প্রস্তুত করছে</a>
<a id=bo_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning</a>
<a id=bs_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP na semiEval-2018 zadatku 1: predviđanje efektivnog sadržaja u Tweets sa dubokim pozornim RNN-ima i učenjem prijenosa</a>
<a id=ca_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP a SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNN and Transfer Learning</a>
<a id=cs_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP na SemEval-2018 Úkol 1: Predikce efektivního obsahu ve tweetech s hlubokou pozorností RNN a transfer učení</a>
<a id=da_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP på SemEval-2018 Opgave 1: Forudsigelse af påvirket indhold i Tweets med dybt opmærksomme RNN'er og Transfer Learning</a>
<a id=de_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP bei SemEval-2018 Aufgabe 1: Vorhersage von affektiven Inhalten in Tweets mit Deep Attentive RNNs und Transfer Learning</a>
<a id=el_title style=display:none href=https://aclanthology.org/S18-1037.pdf>Εργασία 1: Προβλέποντας Επιθετικό Περιεχόμενο σε Τουίτερ με Βαθιά Προσοχή και Μάθηση Μεταφοράς</a>
<a id=es_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP en la Tarea 1 de SemEval-2018: Predecir el contenido afectivo en los tuits con RNN profundamente atentos y transferir el aprendizaje</a>
<a id=et_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP SemEval-2018 Ülesanne 1: Affektiivse sisu prognoosimine Tweetides sügava tähelepanuväärse RNN-iga ja siirdeõppega</a>
<a id=fa_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA</a>
<a id=fi_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP SemEval-2018 Tehtävä 1: Affektiivisen sisällön ennustaminen tweeteissä syvän tarkkaavaisen RNN:n ja siirtooppimisen avulla</a>
<a id=fl_title style=display:none href=https://aclanthology.org/S18-1037.pdf></a>
<a id=fr_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP à SEMEVAL-2018 Tâche 1 : Prédire le contenu affectif dans les tweets avec des RNN attentifs et un apprentissage par transfert</a>
<a id=ga_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP ag SemEval-2018 Tasc 1: Ábhar Taitneamhach a Thuar in Tweetanna le RNNanna Aireach domhain agus Foghlaim Aistrigh</a>
<a id=ha_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP at SemEal-2018 Tafiyar 1: Predicting Affective Content in Twitter with Depth AttAttAttent RNNs and Transverse Learn</a>
<a id=he_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP ב SemEval-2018 משימה 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning</a>
<a id=hi_title style=display:none href=https://aclanthology.org/S18-1037.pdf>SEMEval-2018 कार्य 1 पर NTUA-SLP: गहरी चौकस RNNs और स्थानांतरण सीखने के साथ Tweets में भावात्मक सामग्री की भविष्यवाणी</a>
<a id=hr_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP na pola Evala-2018 zadatku 1: predviđanje efektivnog sadržaja u Tweets sa dubokim pozornim RNN-ima i učenjem prijenosa</a>
<a id=hu_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP a SemEval-2018 1. feladat: Az érintett tartalom előrejelzése Tweetekben mélyreható RNN-ekkel és transzfertanulással</a>
<a id=hy_title style=display:none href=https://aclanthology.org/S18-1037.pdf>ՆԹՈԱ-ՍԼՊ-ը, 2018 թվականի կիսագնդի առաջին հանձնարարում. Անֆեկտիվ պարունակության կանխատեսումը թվիթերում, որոնք ունեն խորը ուշադրություն դարձնող ՌՆՆ-ներ և փոխանցման ուսումնասիրություն</a>
<a id=id_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning</a>
<a id=is_title style=display:none href=https://aclanthology.org/S18-1037.pdf></a>
<a id=it_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP a SemEval-2018 Task 1: Prevedere contenuti affettivi nei tweet con RNN profondi e Transfer Learning</a>
<a id=ja_title style=display:none href=https://aclanthology.org/S18-1037.pdf>SemEval -2018のNTUA - SLPタスク1 ：深い注意を払ったRNNと転移学習を備えたツイートにおける感情的なコンテンツの予測</a>
<a id=jv_title style=display:none href=https://aclanthology.org/S18-1037.pdf>For Word Wrap Around</a>
<a id=ka_title style=display:none href=https://aclanthology.org/S18-1037.pdf>Comment</a>
<a id=kk_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA- SLP жарым- 2018 тапсырманың 1- тапсырмасында: Твиттерде эффективті мазмұнын қарау үшін түсті нақты RNN және трансферт оқыту</a>
<a id=ko_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP가SemEval-18에서의 임무1: RNN에 대한 깊은 관심과 이동 학습을 통해 트위터의 감정 내용을 예측한다</a>
<a id=lt_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP per 2018 m. pusmetį 1 užduotis: Poveikio turinio Tweetuose su giliai aktyviais RNR ir perkėlimo mokymosi prognozė</a>
<a id=mk_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP на SemEval-2018 задача 1: Предвидување на влијателна содржина на твитови со длабоко пристрасни РНН и пренесување</a>
<a id=ml_title style=display:none href=https://aclanthology.org/S18-1037.pdf>സെമ്എവാല്‍- 2018 ടാസ്ക് 1- ല്‍ NTUA- SLP: ആഴത്തിലുള്ള ഉള്ളടക്കത്തില്‍ ആഫ്ഫിക്റ്റീവ് ഉള്ളടക്കത്തിന്റെ മുന്‍കൂട്ടുന്നു</a>
<a id=mn_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP at SemEval-2018 Task 1: Predicting Effective Content in Tweets with Deep Attentive RNN and Transfer Learning</a>
<a id=ms_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP pada SemEval-2018 Tugas 1: Prediksi Kandungan Afektif dalam Tweets dengan RNN Attentive Deep and Transfer Learning</a>
<a id=mt_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP f’SemEval-2018 Kompitu 1: Tbassir ta’ Kontenut Affettiv fit-Tweets b’RNNs Attentivi Profondi u Tagħlim ta’ Trasferiment</a>
<a id=nl_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP bij SemEval-2018 Taak 1: Het voorspellen van Affectieve Content in Tweets met Deep Attentieve RNN's en Transfer Learning</a>
<a id=no_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP på semiEval-2018 oppgåve 1: Foreventar effektiv innhald i tweeter med dypt attentive RNN og overføringslæring</a>
<a id=pl_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP w SemEval-2018 Zadanie 1: Przewidywanie treści afektywnych w tweetach z głębokimi uważnymi RNN i transfer learning</a>
<a id=pt_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP na SemEval-2018 Tarefa 1: Prevendo Conteúdo Afetivo em Tweets com RNNs Atentos e Transferência de Aprendizagem</a>
<a id=ro_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP la SemEval-2018 Sarcina 1: Predicția conținutului afectiv în Tweets cu RNN-uri atente profunde și transfer de învățare</a>
<a id=ru_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP на SemEval-2018 Задача 1: Прогнозирование аффективного контента в твитах с глубоким вниманием RNN и передачей обучения</a>
<a id=si_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP at Halfeval-2018 Job 1: ප්‍රශ්නයක් ප්‍රශ්නයක් තියෙන්නේ ට්විට්ස් වල ගොඩක් අවධාන RNN සහ Transfer Training</a>
<a id=sk_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP na SemEval-2018 Naloga 1: Napovedovanje afektivne vsebine v tweetih z globokimi pozornimi RNN-ji in prenosnim učenjem</a>
<a id=so_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning</a>
<a id=sq_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP në SemEval-2018 Task 1: Prediktimi i përmbajtjes ndikuese në Tweets me RNN të thella dhe mësim transferimi</a>
<a id=sr_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP na semiEval-2018 zadatku 1: predviđanje efektivnog sadržaja u Tweets sa dubokim pozornim RNN-ima i učenjem prijenosa</a>
<a id=sv_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP på SemEval-2018 Uppgift 1: Att förutsäga affektivt innehåll i Tweets med djupt uppmärksam RNN och Transfer Learning</a>
<a id=sw_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP katika kazi ya SemEval-2018 1: Kujiandaa Maudhui yenye ufanisi katika Twita na Ujumbe wa NNN na Ufunzi wa Uhamiaji</a>
<a id=ta_title style=display:none href=https://aclanthology.org/S18-1037.pdf>செம்Eval- 2018 பணியில் NTUA- SLP 1: ஆழம் Attention RNNs மற்றும் மாற்று கற்றுக்கொண்டு கட்டுப்படுத்தல் ஆழமான உள்ளடக்கத்தை முன்னேற்றுகிறது</a>
<a id=tr_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP at SemEval-2018 Task 1: Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning</a>
<a id=uk_title style=display:none href=https://aclanthology.org/S18-1037.pdf></a>
<a id=ur_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP نصف-2018 ٹاکس ۱ میں: ٹویٹوں میں اثرات منصوبت کی پیش بینی کرتا ہے جو عمیق اثرات RNN اور ترنسفور سیکھنے کے ساتھ</a>
<a id=uz_title style=display:none href=https://aclanthology.org/S18-1037.pdf>Name</a>
<a id=vi_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP ở Nhiệm vụ Semkhai-thẩm 8: định giá nội dung tác động trong Tweet bằng RNN Deep Attentive và truyền giáo</a>
<a id=zh_title style=display:none href=https://aclanthology.org/S18-1037.pdf>NTUA-SLP在SemEval-2018务1:用深RNN迁学推文之情</a></h2><p class=lead><a href=/people/c/christos-baziotis/>Christos Baziotis</a>,
<a href=/people/a/athanasiou-nikolaos/>Athanasiou Nikolaos</a>,
<a href=/people/a/alexandra-chronopoulou/>Alexandra Chronopoulou</a>,
<a href=/people/a/athanasia-kolovou/>Athanasia Kolovou</a>,
<a href=/people/g/georgios-paraskevopoulos/>Georgios Paraskevopoulos</a>,
<a href=/people/n/nikolaos-ellinas/>Nikolaos Ellinas</a>,
<a href=/people/s/shrikanth-narayanan/>Shrikanth Narayanan</a>,
<a href=/people/a/alexandros-potamianos/>Alexandros Potamianos</a></p></div><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><div class="card bg-light mb-2 mb-lg-3" id=en_abstract><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition : Affect in Tweets. We participated in all subtasks for <a href=https://en.wikipedia.org/wiki/Twitter>English tweets</a>. We propose a Bi-LSTM architecture equipped with a multi-layer self attention mechanism. The attention mechanism improves the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> performance and allows us to identify salient words in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>, as well as gain insight into the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> making them more interpretable. Our model utilizes a set of word2vec word embeddings trained on a large collection of 550 million Twitter messages, augmented by a set of word affective features. Due to the limited amount of task-specific training data, we opted for a transfer learning approach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A. The proposed approach ranked 1st in Subtask E Multi-Label Emotion Classification, 2nd in Subtask A Emotion Intensity Regression and achieved competitive results in other subtasks.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=af_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In hierdie papier laat ons diep-leer modele voorsien wat aan die semiEval-2018 taak 1-samenskap voorsien het: 'Afteken in Tweets'. Ons het gedeel in alle subtaske vir Engels tweets. Ons voorstel 'n Bi-LSTM-arkitektuur wat met 'n multi-laag self-aandag mekanisme beskikbaar is. Die aandagmekanisme verbeter die model prestasie en laat ons toe om salient woorde in tweets te identifiseer, en ook aandag te verkry in die modele wat hulle meer vertaling maak. Ons model gebruik 'n stel woorde 2vec woord inbêdings wat op 'n groot versameling van 550 miljoen Twitter boodskappe opgelei is, wat deur 'n stel woord effektiewe funksies vergroot is. Dus die beperkte hoeveelheid van taak-spesifieke onderwerking data, het ons opgekies vir 'n oordrag onderwerp onderwerp deur die Bi-LSTMs op die datastel van semievaal 2017, Opdrag 4A te trek. Die voorgestelde toegang het 1st in Subtask E 'Multi-Label Emotification Classification', 2nd in Subtask A 'Emotion Intensity Regression' gemaak en gemaak gemaakte resultate in ander subtaske.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=am_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>በዚህ ገጾች ውስጥ ለሳምEval-2018 ስራ 1 ተቃውሞ የሰጡት የጥልቅ ትምህርት ሞዴላዎችን እናቀርባታለን ‹በትዊተሮች ላይ ተግባር›› እንግሊዘኛ በትዊተሮች ላይ ሁሉን ደብዳቤዎችን ተጋጠመን:: በብዙ ደረጃዎች የራሳቸውን ማስታወቂያ አካባቢ-ኤልስቴም መሠረት እናሳልጋለን፡፡ የጥያቄ አካሄዱ ምሳሌ ድምፅን ያሳድጋል እና በጣቢያ ቃሎችን እና በዓይነቶች ላይ የሚተረጉማቸውን ማግኘት ይችላል፡፡ ሞዴሌያችን የቃላት 2vec ቃላት በቁጥጥር በሚያሳየው የ550 ሚሊዮን ትዊተር መልእክቶች ላይ የተሰራረበ ቃላት የሚታወቀውን የቃላት ቃላትን ይጠቅማል፡፡ በሥርዓት-ምርጫው ስርዓት አካባቢ-LSTMs በሳምቨርስቲ 2017 ዳታ ማድረጊያውን በመዘርጋት ለመለወጥ ጥያቄን መረጥነው፡፡ የተዘጋጀው ሥርዓት በ(Submission E) 'Multi-Label Emotion Classification', ሁለተኛው በጉዳዩ A 'Emotion Intensity Regression' እና ለሌሎች ደብዳቤዎችን አግኝተዋል፡፡</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ar_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>نقدم في هذه الورقة نماذج التعلم العميق التي تم تقديمها إلى مسابقة SemEval-2018 Task 1: "Affect in Twitter". شاركنا في جميع المهام الفرعية للتغريدات الإنجليزية. نقترح بنية Bi-LSTM مجهزة بآلية متعددة الطبقات للانتباه الذاتي. تعمل آلية الانتباه على تحسين أداء النموذج وتسمح لنا بتحديد الكلمات البارزة في التغريدات ، فضلاً عن اكتساب نظرة ثاقبة للنماذج مما يجعلها أكثر قابلية للتفسير. يستخدم نموذجنا مجموعة من حفلات الزفاف بالكلمة word2vec المُدرَّبة على مجموعة كبيرة من 550 مليون رسالة Twitter ، مدعومة بمجموعة من السمات المؤثرة بالكلمات. نظرًا للكمية المحدودة من بيانات التدريب الخاصة بالمهمة ، اخترنا نهج تعلم النقل من خلال التدريب المسبق لـ Bi-LSTMs على مجموعة بيانات Semeval 2017 ، المهمة 4A. حصل النهج المقترح على المرتبة الأولى في المهمة الفرعية E "تصنيف العاطفة متعدد الملصقات" ، والمرتبة الثانية في المهمة الفرعية أ "انحدار كثافة العاطفة" وحققت نتائج تنافسية في المهام الفرعية الأخرى.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=az_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bu kağıtda hal-2018 Öyrənməsi üçün "Tövtlərə müvafiq" göstərilən derin öyrənmə modellərini göstəririk. İngilizce tweets üçün bütün subtasklara katıldıq. Biz bir çoxlu-katlı self-attention mehanizmi ilə birləşdirilmiş bi-LSTM arhitektarını təklif edirik. Dikkat mehanizmisi modellərin performansını yaxşılaşdırır və bizə twetlərdə süslü sözləri tanıtmağı və modellərə daha çox çəkinməyə imkan verir. Bizim modellərimiz 550 milyon Twitter mesajlarının böyük koleksiyonunda təhsil edilmiş sözləri 2vec sözlərini istifadə edir. Bu sözləri təsirli fəaliyyətlərlə artırır. Gözəl müəyyən təhsil məlumatlarının sınırlı qiyməti üzündən, 2017-ci yarı əsrlərin, 4.A Gözəl təhsil qurmağı ilə bi-LSTMs'lərin təhsil qurmasını təhsil etmək üçün seçdik. Önülləşdirilmiş tərzim, Subtask E 'Multi-Label Emotion Classification', Subtask A 'Emotion Intensity Regression' və başqa subtasklarda müharibə sonuçlarını başa düşdü.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bg_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>В настоящата статия представяме модели на дълбоко обучение, които се представиха в конкурса Задача 1: "Въздействие в туитове". Участвахме във всички подзадачи за английски туитове. Предлагаме архитектура, оборудвана с многослоен механизъм за самовнимание. Механизмът за внимание подобрява производителността на модела и ни позволява да идентифицираме видни думи в туитове, както и да придобием представа за моделите, което ги прави по-разбираеми. Нашият модел използва набор от вграждания на думи обучени върху голяма колекция от 550 милиона съобщения в Туитър, допълнени с набор от думи афективни функции. Поради ограниченото количество специфични за задачата данни за обучение, ние избрахме подход за трансферно обучение чрез предобучение на Би-ЛТМ върху набора от данни на задача 4А. Предложеният подход заема първо място в подзадача Е "Класификация на емоциите с много етикети", второ място в подзадача А "Регресия на интензивността на емоциите" и постига конкурентни резултати в други подзадачи.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>এই কাগজটিতে আমরা গভীর শিক্ষার মডেল উপস্থাপন করছি যা সেমভাল-২০১৮ কাজের প্রতিযোগিতায় জমা দিয়েছে “টুইটারে প্রভাব ফেলেছে ইংরেজি টুইটের জন্য আমরা সব সাবট্যাকাজে অংশগ্রহণ করেছি। আমরা একটি বি-এলস্টিএম কাঠামোর প্রস্তাব করছি যা বহুস্তরের আত্মমনোযোগের মাধ্যমে। এই মনোযোগের মাধ্যমে মডেল প্রদর্শনের উন্নতি প্রদান করে এবং টুইটে আমাদের বেদনাদায়ক শব্দ চিহ্নিত করার সুযোগ দেয় এবং মডেলের দৃষ্টিভঙ্গ আমাদের মডেল ২ভেক শব্দ ব্যবহার করে বিশাল ৫৫০ মিলিয়ন টুইটার বার্তার সংগ্রহে প্রশিক্ষণ প্রদান করা হয়েছে, যা কিছু শব্দ প্রভাবিত বৈশিষ্ট্য দ্ কাজের নির্দিষ্ট প্রশিক্ষণের পরিমাণ সীমিত তথ্যের কারণে আমরা বিএলএসএমএস-এর ডাটাসেট ২০১৭ সেমেভেল ২০১৭ তারিখে কাজের ৪এ কাজে বিএলস্টিমে প্রস্তাবিত পদক্ষেপ ইউ 'বহুল লেবেলের Emotion Classication', সাবটাবাস এ 'Emotion Intentioty Regression' দ্বিতীয় ভাবে প্রতিযোগিতার ফলাফল অর্জন করেছে।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bo_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ང་ཚོས་ཤོག་བྱང་འདིའི་ནང་གི་རྣམ་པ་ལྡན་གྱི་དཔེ་དབྱིབས་བཀོད་པ་དེ་ནི་SemEval-2018 དྲ་རྒྱའི་སྤྱིར་བཏང་བའི་དོན་ལྟར་དང་པོ་ ང་ཚོས་དབྱིན་ཡིག་གི་འགོད་ཐེངས་ཕྱི་རྗེས་སུ་འཇུག་སྟེ། ང་ཚོས་Bi-LSTM་སྒྲིག་གཞུང་དང་སྣ་མང་ཆེ་བའི་རང་ཉིད་ཀྱི་ཞལ་འཛུགས་སྒྲིག་ཆས་ཤིག་གི་སྤྲོད་ཡོད། དམིགས་འཛུགས་ཀྱི་ཐབས་ལམ་དེ་སྟངས་འཛིན་གྱི་ལས་འགན་འགྱུར་བ་དང་། ང་ཚོར་དྲ་བར་ནང་གི་salient གཟུགས་རིས་གསལ་བཤད་ཀྱི་ཡོད་པ་དང་། མ་དཔེ་གཞི་ཚོ ང་ཚོའི་མ་དབྱིབས་སྔོན་གྱིས་དྲ་རྒྱ་སྟངས་ལ་གཏོང་གི་ཐབས་ལམ་དུ་བརྗོད་པའི་ཐ་སྙད་ཅིག་སྟོན་པ། ང་ཚོས་བྱ་ཚུལ་གསལ་བཤད་ཀྱི་ལས་འགན་སྤྲོད་ཀྱི་ཚད་ལྡན་པའི་ལས་འགན་སྐྱོང་གི་ཡིག་ཆ་ལ་བསྟེན་ནས་ སྐྱེལ་སྒྲིག་གི་ཐབས་ལམ་སྤྲོད་རྒྱུ་དང་། The proposed approach ranked 1 in Subtask E 'Multi-Label Emotion Classification', 2 in Subtask A 'Emotion Intensity Regression' and achieved competitive results in other subtasks.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=bs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>U ovom papiru predstavljamo modele dubokog učenja koji su predali natjecanju prvog zadatka u polovini Evala-2018: 'utjecaja na Tweets'. Mi smo sudjelovali u svim podupiranjima za engleske tweets. Predlažemo arhitekturu bi-LSTM opremljenu mehanizam samopouzdanja više slojeva. Mehanizam pažnje poboljšava modelnu funkciju i omogućava nam da identificiramo salične riječi u tweetima, kao i da dobijemo uvid u modele koji ih čine interpretabilnijim. Naš model koristi skup rečenica 2vec rečenica obučenih na velikoj kolekciji 550 milijuna tviterskih poruka, povećanom skupom rečenih utjecaja. Zbog ograničene količine podataka o obuci određenih zadataka, odabrali smo pristup prijenosnom učenju, pretvarajući bi-LSTMs na setu podataka poluvekova 2017., zadatak 4A. Predloženi pristup je prvi u podzadatku E 'klasifikacija emocija multi-etiketa', drugi u podzadatku A 'Regresija intenzitete emocija' i ostvario konkurentne rezultate u drugim podizanjima.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ca_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition: 'Affect in Tweets'. We participated in all subtasks for English tweets. Proposem una arquitectura Bi-LSTM equipada amb un mecanisme d'auto-atenció multicapa. The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable. El nostre model utilitza un conjunt d'incorporacions de paraules word2vec entrenats en una gran col·lecció de 550 milions de missatges Twitter, augmentats per un conjunt de característiques afectives de paraules. Gràcies a la quantitat limitada de dades d'entrenament específics per a les tasques, vam optar per un enfocament d'aprenentatge de transfer ència pré-entrenant els Bi-LSTMs en el conjunt de dades de Semeval 2017, tasca 4A. L'enfocament proposat es va classificar en primer lloc a la Subtasca E "Classificació d'emocions multietiquetades", en segon lloc a la Subtasca A "Regressió d'intensitat emocional" i va aconseguir resultats competitius en altres subtaskes.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=cs_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>V tomto článku představujeme modely hlubokého učení, které byly předloženy do soutěže SemEval-2018 Task 1: 'Affect in Tweets'. Podíleli jsme se na všech podúkolech pro anglické tweety. Navrhujeme architekturu Bi-LSTM vybavenou vícevrstvým mechanismem sebepozornosti. Mechanismus pozornosti zlepšuje výkon modelu a umožňuje identifikovat významná slova ve tweetech, stejně jako získat vhled do modelů, čímž jsou interpretovatelnější. Náš model využívá sadu vložení slov Word2vec trénované na velké sbírce 550 milionů Twitter zpráv, rozšířené o sadu slovních afektivních funkcí. Vzhledem k omezenému množství úkolově specifických tréninkových dat jsme se rozhodli pro přenosový přístup učení předškolením Bi-LSTMs na datové sadě Semeval 2017, Úkol 4A. Navržený přístup se zařadil na první místo v podúkolu E "Multi-Label Emotion Classification", na druhé v podúkolu A "Emotion Intensity Regression" a dosáhl konkurenčních výsledků v dalších podúkolech.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=da_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>I denne artikel præsenterer vi deep learning modeller, der blev indsendt til SemEval-2018 Task 1 konkurrencen: 'Påfør i tweets'. Vi deltog i alle underopgaver til engelske tweets. Vi foreslår en Bi-LSTM arkitektur udstyret med en multi-lags selvopmærksomhedsmekanisme. Opmærksomhedsmekanismen forbedrer modellens ydeevne og giver os mulighed for at identificere fremtrædende ord i tweets, samt få indsigt i modellerne, der gør dem mere fortolkelige. Vores model bruger et sæt word2vec ord embeddings trænet på en stor samling af 550 millioner Twitter-beskeder, forstærket med et sæt ord affektive funktioner. På grund af den begrænsede mængde opgavespecifikke træningsdata valgte vi en transfer learning tilgang ved at forudtræne Bi-LSTMs på datasættet i Semeval 2017, Task 4A. Den foreslåede fremgangsmåde rangerede 1. i underopgave E 'Multi-Label Emotion Classification', 2. i underopgave A 'Emotion Intensity Regression' og opnåede konkurrencedygtige resultater i andre underopgaver.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=de_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In diesem Beitrag stellen wir Deep-Learning-Modelle vor, die beim SemEval-2018 Task 1 Wettbewerb eingereicht wurden: 'Affect in Tweets'. Wir haben an allen Teilaufgaben für englische Tweets teilgenommen. Wir schlagen eine Bi-LSTM Architektur vor, die mit einem mehrschichtigen Selbstaufmerksamkeitsmechanismus ausgestattet ist. Der Aufmerksamkeitsmechanismus verbessert die Modellleistung und ermöglicht es uns, wichtige Wörter in Tweets zu identifizieren, sowie Einblicke in die Modelle zu gewinnen, die sie besser interpretierbar machen. Unser Modell verwendet eine Reihe von Word2vec Wort Einbettungen trainiert auf einer großen Sammlung von 550 Millionen Twitter Nachrichten, ergänzt durch eine Reihe von Wort affektiven Funktionen. Aufgrund der begrenzten Menge an aufgabenspezifischen Trainingsdaten haben wir uns für einen Transfer Learning Ansatz entschieden, indem wir die Bi-LSTMs auf dem Datensatz von Semeval 2017, Task 4A vortrainierten. Der vorgeschlagene Ansatz erreichte den ersten Platz in Teilaufgabe E 'Multi-Label Emotion Classification', zweiten Platz in Teilaufgabe A 'Emotion Intensity Regression' und erzielte wettbewerbsfähige Ergebnisse in anderen Teilaufgaben.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=el_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Στην παρούσα εργασία παρουσιάζουμε μοντέλα βαθιάς μάθησης που υποβλήθηκαν στον διαγωνισμό εργασίας 1: "Επίδραση στα tweets". Συμμετείχαμε σε όλες τις δευτερεύουσες εργασίες για τα Αγγλικά tweets. Προτείνουμε μια αρχιτεκτονική εξοπλισμένη με πολυστρωματικό μηχανισμό αυτοπροσοχής. Ο μηχανισμός προσοχής βελτιώνει την απόδοση του μοντέλου και μας επιτρέπει να εντοπίσουμε σημαντικές λέξεις στα tweets, καθώς και να αποκτήσουμε γνώση των μοντέλων καθιστώντας τα πιο ερμηνευτικά. Το μοντέλο μας χρησιμοποιεί ένα σύνολο ενσωμάτωσης λέξεων που εκπαιδεύονται σε μια μεγάλη συλλογή 550 εκατομμυρίων μηνυμάτων που ενισχύονται από ένα σύνολο συναισθηματικών χαρακτηριστικών λέξεων. Λόγω του περιορισμένου ποσού των δεδομένων κατάρτισης που αφορούν συγκεκριμένες εργασίες, επιλέξαμε μια προσέγγιση μάθησης μεταφοράς με προεπιλογή των Bi-LSTMs στο σύνολο δεδομένων του Semeval 2017, Task 4A. Η προτεινόμενη προσέγγιση κατατάχθηκε 1η στην Υποεργασία Ε "Ταξινόμηση Συναισθηματικών Πολλαπλών ετικετών", 2η στην Υποεργασία Α "Μεταστροφή Έντασης Συναισθηματισμού" και πέτυχε ανταγωνιστικά αποτελέσματα σε άλλες δευτερεύουσες εργασίες.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=es_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>En este artículo presentamos los modelos de aprendizaje profundo que se presentaron a la competencia Tarea 1 de SemeVal-2018: «Affect in Tweets». Participamos en todas las subtareas de los tuits en inglés. Proponemos una arquitectura Bi-LSTM equipada con un mecanismo de autoatención multicapa. El mecanismo de atención mejora el rendimiento del modelo y nos permite identificar las palabras más destacadas en los tuits, así como obtener información sobre los modelos, haciéndolos más interpretables. Nuestro modelo utiliza un conjunto de incrustaciones de palabras word2vec entrenadas en una gran colección de 550 millones de mensajes de Twitter, aumentados por un conjunto de características afectivas de palabras. Debido a la cantidad limitada de datos de capacitación específicos de la tarea, optamos por un enfoque de aprendizaje de transferencia mediante la capacitación previa de los BI-LSTMS en el conjunto de datos de Semeval 2017, Tarea 4A. El enfoque propuesto ocupó el primer lugar en la subtarea E «Clasificación de emociones multietiqueta», el segundo en la subtarea A «Regresión de la intensidad de las emociones» y logró resultados competitivos en otras subtareas.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=et_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Käesolevas dokumendis tutvustame sügavõppe mudeleid, mis esitasid SemEval-2018 Task 1 konkursile "Affect in Tweets". Osalesime kõigis inglise keele säutsude alamülesannetes. Pakume välja Bi-LSTM arhitektuuri, mis on varustatud mitmekihilise enesetähelepanu mehhanismiga. Tähelepanumehhanism parandab mudeli jõudlust ja võimaldab meil tuvastada säutsudes silmapaistvaid sõnu ning saada ülevaadet mudelitest, muutes need tõlgendatavamaks. Meie mudel kasutab Word2vec sõna manustamise komplekti, mis on koolitatud suure kogumi 550 miljonit Twitteri sõnumit, mida täiendavad sõna afektiivsed funktsioonid. Ülesandepõhiste koolitusandmete piiratud hulga tõttu valisime siirdeõppe lähenemisviisi, koolitades Bi-LSTMd Semeval 2017 ülesande 4A andmekogumil. Kavandatud lähenemisviis oli alaülesandes E "Multi-Label Emotion Classification" esimene, alaülesandes A "Emotion Intensity Regression" teine ja saavutas teistes alaülesannetes konkurentsivõimelised tulemused.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fa_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>در این کاغذ ما مدل یادگیری عمیق را پیشنهاد می‌کنیم که به مسابقه‌ی مسابقه‌ی نیم‌Eval-2018 ارائه می‌شود: اثر در تویتها. ما در تمامی زیر خواسته‌های tweets انگلیسی شرکت کردیم. ما یک معماری BILSTM را پیشنهاد می‌کنیم که با یک مکانیسم توجه به خودش بسیار لایه‌های زیادی آماده شده است. مکانیسم توجه عملکرد مدل را بهتر می کند و به ما اجازه می دهد کلمات تسلیم را در تویت شناسایی کنیم، همچنین به مدل توجه کنیم که آنها را ترجمه بیشتری می کند. مدل ما یک مجموعه کلمه ۲vek کلمه استفاده می‌کند که در مجموعه‌ی بزرگ ۵۵۰ میلیون پیام توئیتر آموزش داده شده است، که توسط مجموعه کلمه ویژه‌های تاثیرگذاری شده است. به خاطر مقدار محدودیت داده های آموزش مشخص کار، ما برای یک روش آموزش انتقال انتقال انتقال را انتخاب کردیم با تحریک کردن دو-LSTMs در مجموعه داده های سنتی ۲۰۱۷، Task 4A. این دستور پیشنهاد اولین درجه در کلاس‌شناسی Emotion Multi-Label E است، دومین درجه در Subtask A «تغییر فشار احساسات» و نتیجه‌های مسابقه‌ای را در دیگر تحقیق یافت.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Tässä artikkelissa esitellään SemEval-2018 Task 1 -kilpailuun osallistuneita syväoppimisen malleja: "Affect in Tweets". Osallistuimme kaikkiin englanninkielisten twiittien alatehtäviin. Ehdotamme Bi-LSTM-arkkitehtuuria, jossa on monikerroksinen itsehoitomekanismi. Huomiomekanismi parantaa mallin suorituskykyä ja mahdollistaa sen, että voimme tunnistaa näkyviä sanoja tweeteissä sekä saada tietoa malleista tehden niistä ymmärrettävämpiä. Mallimme hyödyntää joukon word2vec-sanaupotuksia, jotka on koulutettu suurelle 550 miljoonan Twitter-viestin kokoelmalle, jota täydentää joukko sanaafektiivisia ominaisuuksia. Tehtäväkohtaisten koulutustietojen rajallisen määrän vuoksi valitsimme siirtooppimisen lähestymistavan esikouluttamalla Bi-LSTMs-harjoituksia Semeval 2017, Task 4A -aineistossa. Ehdotettu lähestymistapa sijoittui ensimmäiseksi osatehtävässä E "Multi-Label Emotion Classification", toiseksi osatehtävässä A "Emotion Intensity Regression" ja saavutti kilpailukykyisiä tuloksia muissa osatehtävissä.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=fr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dans cet article, nous présentons des modèles d'apprentissage profond soumis au concours SEMEVAL-2018 Task 1 : « Affect in Tweets ». Nous avons participé à toutes les tâches subordonnées pour les tweets en anglais. Nous proposons une architecture bi-LSTM équipée d'un mécanisme d'auto-attention multicouche. Le mécanisme d'attention améliore les performances du modèle et nous permet d'identifier les mots saillants dans les tweets, ainsi que d'obtenir un aperçu des modèles qui les rendent plus interprétables. Notre modèle utilise un ensemble d'intégrations de mots word2vec formées sur une vaste collection de 550 millions de messages Twitter, augmentées par un ensemble de fonctionnalités affectives de mots. En raison de la quantité limitée de données de formation spécifiques aux tâches, nous avons opté pour une approche d'apprentissage par transfert en préentraînant les Bi-LSTMS sur l'ensemble de données de Semeval 2017, tâche 4A. L'approche proposée s'est classée première dans la sous-tâche E « Classification des émotions en plusieurs étiquettes », 2e dans la sous-tâche A « Régression de l'intensité des émotions » et a obtenu des résultats compétitifs dans d'autres sous-tâches.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ga_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Sa pháipéar seo cuirimid i láthair múnlaí domhainfhoghlama a chuir isteach ar chomórtas Tasc 1 SemEval-2018: “Affect in Tweets”. Ghlacamar páirt i ngach fothasc le haghaidh tweets Béarla. Molaimid ailtireacht Dé-LSTM atá feistithe le meicníocht féinairdithe ilchiseal. Feabhsaíonn an mheicníocht aird ar fheidhmíocht na samhla agus ligeann dúinn focail shuntasacha a aithint i dtvuíteanna, chomh maith le léargas a fháil ar na samhlacha, rud a fhágann go mbeidh siad níos inmhínithe. Úsáideann ár múnla sraith de leabú focal word2vec oilte ar bhailiúchán mór de 550 milliún teachtaireachtaí Twitter, méadaithe ag sraith de ghnéithe mothúcháin focal. Mar gheall ar an méid teoranta sonraí oiliúna a bhaineann go sonrach le tasc, roghnaigh muid cur chuige foghlama aistrithe trí réamhoiliúint a chur ar na Bi-LSTManna ar thacar sonraí Semeval 2017, Tasc 4A. Tháinig an cur chuige molta sa 1ú háit i bhFothasc E “Aicmiú Mothúcháin Illipéid”, 2ú háit i bhFothasc A “Aischéimniú ar Dhéine Mothúcháin” agus bhain sé amach torthaí iomaíocha i bhfothascanna eile.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ha_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ga wannan takarda, Munã halatar da misãlai masu sanar da kewayi da aka saka zuwa al'amarin na Semeval-2018: "Ina amfani da shi a cikin Twitter." Mun yi mataimaki ga dukkan taskõki na Ingiriya. Tuna goyyar da wani matsayin Bi-LSM wanda yana da wani matsayin muhalli na multi-daraja. Akamatarar muhalli na ƙari mafarin mazaɓa misalin kuma yana yarda mu iya gane magana masu sali a cikin Twitter, da kuma ka sami gane na misãlai da za'a fassarar da su. Ana amfani da tsarin maganar 2viec da aka sanar da shi ga mai haɗa jumui masu juma 550 millions na Twitter, an ƙara wani set of words masu yin hushi. Ga da yakin tsarin aikin da aka ƙayyade data masu yin amfani da shi, muka zãɓe wa wata hanyarwa ta shige da wa'azi a kanana kafin ta kafin ta bi-LSM kan danne-tsarin shekara 2017, Taimar 4A. Tsarin da aka buƙata ta ranked na farko a cikin Subaikin Eki 'Cirari ga Label Kuwo-Label', na biyu a cikin Subaikin A 'Regression Intense Sura'</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=he_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>בעיתון הזה אנחנו מציגים דוגמנים ללימודים עמוקים ששלחו לתחרות משימה 1 SemEval-2018: "השפעה בטוויטס". השתתפנו בכל התשואלות לטוויטים אנגליים. אנו מציעים ארכיטקטורה ביי-LSTM מוכשרת במנגנון תשומת לב עצמית במספר שכבות. מנגנון תשומת לב משפר את ההופעה של המודל ומאפשר לנו לזהות מילים מפורסמות בטוויטים, כמו גם להשיג תובנה לדוגמנים שהופכים אותם יותר ניתן להפריע. המודל שלנו משתמש בסט של מילים מילים 2vec מאומנים על אוסף גדול של 550 מיליון הודעות טוויטר, מוגדלים על ידי סט של תכונות מילים חיוביות. Due to the limited amount of task-specific training data, we opted for a transfer learning approach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A. הגישה הציעה התייצבה ראשונה בתפקיד E 'מסווג רגשות רבות-תגים', השנייה בתפקיד A 'חזרה חשמלית רגשות' ושיגה תוצאות תחרותיות בתפקידים אחרים.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>इस पेपर में हम गहरी सीखने के मॉडल प्रस्तुत करते हैं जो SemEval-2018 टास्क 1 प्रतियोगिता में प्रस्तुत किए गए हैं: "ट्वीट्स में प्रभाव"। हम अंग्रेजी tweets के लिए सभी subtasks में भाग लिया. हम एक द्वि-एलएसटीएम आर्किटेक्चर का प्रस्ताव करते हैं जो एक बहु-परत आत्म-ध्यान तंत्र से सुसज्जित है। ध्यान तंत्र मॉडल के प्रदर्शन में सुधार करता है और हमें ट्वीट्स में मुख्य शब्दों की पहचान करने की अनुमति देता है, साथ ही साथ मॉडल में अंतर्दृष्टि प्राप्त करता है जिससे उन्हें अधिक व्याख्यायोग्य बनाया जा सकता है। हमारा मॉडल word2vec शब्द embeddings के एक सेट का उपयोग करता है जो 550 मिलियन ट्विटर संदेशों के एक बड़े संग्रह पर प्रशिक्षित होता है, जो शब्द भावात्मक सुविधाओं के एक सेट द्वारा संवर्धित होता है। कार्य-विशिष्ट प्रशिक्षण डेटा की सीमित मात्रा के कारण, हमने सेमेवल 2017, टास्क 4 ए के डेटासेट पर द्वि-एलएसटीएम को प्रीट्रेन करके एक हस्तांतरण सीखने के दृष्टिकोण का विकल्प चुना। प्रस्तावित दृष्टिकोण Subtask E "Multi-Label Emotion Classification" में 1 स्थान पर है, Subtask A "Emotion Intensity Regression" में 2nd है और अन्य subtasks में प्रतिस्पर्धी परिणाम प्राप्त किए हैं।</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>U ovom papiru predstavljamo modele dubokog učenja koji su predali natjecanju za zadatak 1 iz polovine Evala-2018: 'utjecaja na Tweets'. Učestvovali smo u svim podpitanjima za engleske tweets. Predlažemo arhitekturu bi-LSTM-a opremljenu mehanizam višeslojnih samopouzdanja. Mehanizam pažnje poboljšava uspješnost modela i omogućava nam identificirati salične riječi u tweetima, kao i dobiti uvid u modele koji ih čine interpretabilnijim. Naš model koristi skup rečenica 2vec riječi obučenih na velikoj kolekciji 550 milijuna tviterskih poruka, povećanom skupom rečenih utjecaja. Zbog ograničene količine podataka o obuci određenih zadataka, odabrali smo pristup prijenosnom učenju pretvarajući bi-LSTMs na skupu podataka poluvječnog 2017. godine, zadatak 4A. Predloženi pristup je prvi u podzadatku E 'klasifikacija emocija višeoznačenih etiketa', drugi u podzadatku A 'Regresija intenzitete emocija' i ostvario konkurentne rezultate u drugim podizacima.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hu_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Ebben a tanulmányban bemutatjuk azokat a mélytanulási modelleket, amelyeket a SemEval-2018 Task 1 pályázatra jelentettek be: "Affect in Tweets". Részt vettünk az angol tweetek összes részfeladatában. A Bi-LSTM architektúra többrétegű önfigyelő mechanizmussal felszerelt. A figyelem mechanizmusa javítja a modell teljesítményét és lehetővé teszi számunkra, hogy felismerjük a kiemelkedő szavakat a tweetekben, valamint betekintést nyerjünk a modellekbe, amelyek értelmezhetőbbek legyenek. Modellünk egy sor word2vec szóbeágyazást használ, amelyet 550 millió Twitter üzenetből álló nagy gyűjteményre készítettünk, és kiegészítve szó affektív funkciókkal. A feladatspecifikus képzési adatok korlátozott mennyisége miatt transzfertanulási megközelítést választottunk a Semeval 2017, feladat 4A adatkészletén található Bi-LSTM-ek előkészítésével. A javasolt megközelítés az "E" alcsoportban az első helyezett, az "A" alcsoportban a második helyezett, és versenyképes eredményeket ért el más alcsoportban.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=hy_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition: 'Affect in Tweets'. Մենք մասնակցեցինք անգլերեն թվիթերի բոլոր ենթախնդիրներին: Մենք առաջարկում ենք երկու-ԼՍԹՄ ճարտարապետություն, որն ունի բազմաշերտ եսի ուշադրության մեխանիզմ: Ուշադրության մեխանիզմը բարելավում է մոդելի արտադրողականությունը և թույլ է տալիս մեզ հնարավորություն տալ հնարավոր բառեր թվիթերում, ինչպես նաև հասկանալ մոդելները, որոնք դարձնում են դրանք ավելի թարգմանելի: Մեր մոդելը օգտագործում է մի շարք բառեր 2veք, որոնք պատրաստված են 550 միլիոն Թվիթերի հաղորդագրությունների մեծ հավաքածուի վրա, ավելացված բառերի ազդեցություն ունեցող հատկանիշներով: Որոշ խնդիրների մասնավոր ուսուցման տվյալների սահմանափակ քանակի պատճառով, մենք ընտրեցինք փոխանցման ուսուցման մոտեցում' նախապատրաստելով ԲիԼՍԹՄ-ները 2017 թվականի կիսադարյան, 4Ա-ի տվյալների համակարգի վրա: The proposed approach ranked 1st in Subtask E 'Multi-Label Emotion Classification', 2nd in Subtask A 'Emotion Intensity Regression' and achieved competitive results in other subtasks.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=id_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dalam kertas ini kami mempersembahkan model belajar dalam yang diserahkan ke kompetisi SemEval-2018 Task 1: 'Affect in Tweets'. We participated in all subtasks for English tweets. Kami mengusulkan arsitektur Bi-LSTM yang disediakan dengan mekanisme perhatian diri multi-lapisan. The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable. Model kami memanfaatkan set kata word2vec embedding dilatih pada koleksi besar 550 juta pesan Twitter, ditambah oleh set karakteristik kata yang affectif. Karena jumlah data latihan spesifik tugas terbatas, kami memilih pendekatan belajar transfer dengan melatih Bi-LSTM pada set data Semeval 2017, Task 4A. Pendekatan yang diusulkan rangka pertama dalam Subtask E 'Multi-Label Emotion Classification', kedua dalam Subtask A 'Emotion Intensity Regression' dan mencapai hasil kompetitif dalam Subtask lain.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=it_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In questo articolo presentiamo modelli di deep learning presentati al concorso SemEval-2018 Task 1: 'Affect in Tweets'. Abbiamo partecipato a tutte le sottorichieste per i tweet in inglese. Proponiamo un'architettura Bi-LSTM dotata di un meccanismo di auto attenzione multistrato. Il meccanismo di attenzione migliora le prestazioni del modello e ci permette di identificare parole salienti nei tweet, così come di acquisire una visione dei modelli rendendoli più interpretabili. Il nostro modello utilizza un insieme di embedding word2vec di parole addestrati su una vasta collezione di 550 milioni di messaggi Twitter, aumentati da una serie di funzioni affettive di parola. A causa della quantità limitata di dati formativi specifici per attività, abbiamo optato per un approccio di transfer learning predisponendo i Bi-LSTMs sul set di dati di Semeval 2017, Task 4A. L'approccio proposto si è classificato 1 ° nella sottomissione E 'Multi-Label Emotion Classification', 2 ° nella sottomissione A 'Emotion Intensity Regression' e ha ottenuto risultati competitivi in altre sottoattività.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ja_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>この論文では、SemEval -2018タスク1コンペティション「ツイートに影響を与える」に提出されたディープラーニングモデルを紹介します。私たちは英語のツイートのすべてのサブタスクに参加しました。多層自己注目機構を備えたBi - LSTMアーキテクチャを提案します。注意メカニズムは、モデルのパフォーマンスを向上させ、ツイートの中の顕著な単語を特定し、モデルをより解釈しやすくするための洞察を得ることができます。当社のモデルは、5億5000万のTwitterメッセージの大規模なコレクションでトレーニングされたWord 2 vecワード埋め込みのセットを利用しており、一連のワードの感情的な機能によって拡張されています。タスク固有のトレーニングデータの量が限られているため、私たちはSemeval 2017, Task 4 Aのデータセット上のBi - LSTMを事前にトレーニングすることにより、転送学習アプローチを選択しました。提案されたアプローチは、サブタスクE「マルチラベル感情分類」で1位、サブタスクA「感情強度回帰」で2位となり、他のサブタスクで競争的な結果を達成した。</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=jv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Nang pepul iki, awak dhéwé iso nggawe model sing luwih-luwih dumaten sing isin mi-ne soko semebal-2013 task 1 dumatenan: 'AfEffect in Two'. Awak dhéwé ngejaraké kabèh basa kanggo tuwit Inggris We proposal a bi-SLT architecture device with a multi-layer Self Mind mehanical. Mehanistik perbudhakan langgar aturan macem sing bisa awak dhéwé nggawe gerakan kelas sing luwih dumadhi lan tuytir, lan uga bantuan ingkang sampek modèl sing bisa nguasai iki bakal terus-terusahaan. Kernel Tungkin nggawe Perintah sing paling nggawe Daftar nggawe Perintah sing berarti dadi podho nggawe nyimpen kuwi, nggawe nyimpen mulai nggawe Daftar nggawe Bih-LTT M seneng dataset ng semi-hekal, task 4A . Nalika sing pergunakake sing wis ambang tanggal saben 1 ing Subtask E 'Multi-Label emuation CLSS', 2 ing Subtask A 'emuation Intensity Regration' lan arno sing wis dipolehasno sakjane kapan tanggal sing wis ana.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ka_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ამ დოკუნეში ჩვენ ჩვენ ჩვენ აჩვენებთ ძალიან სწავლებელი მოდელები, რომლებიც SemEval-2018-ის კონტენეციაში გადატანა 1-ი დავალების კონტენეციაში: "შემდეგ ჩვენ ყველა ინგლისური ტივიტების საკუთარი საკუთარი საკუთარი საკუთარი საკუთარი საკუთარი საკუთარი დავწყებდით. ჩვენ მინდომათ ბი-LSTM არქიტექტიკას, რომელიც მრავალური თავისუფლიო მექანიზმისთვის გამოყენებული. მოდინარების მექანსიმა მოდელის გამოსახულებას უფრო მეტადებს და ჩვენ შეგვიძლია გავიდენოთ სულიან სიტყვები ტივიტებში, და მოდელის შესახებ უფრო მეტად განახულება. ჩვენი მოდელი გამოიყენება სიტყვას 2vec სიტყვას სიტყვას, რომლებიც 550 მილიონის Twitter შეტყობინებების დიდი კოლექციაში განაკეთებულია, რომლებიც სიტყვების შესაძლებლობებით აზ სამუშაო მონაცემების განსაკუთრებული მონაცემების შესახებ, ჩვენ მონიშნეთ მონაცემების გასწავლებელი გასწავლებელი პროგრამის შესახებ, რომელიც BI-LSTMs მონაცემების გასწავლა პროგრამების პროგრამა 1-ში "მრავალური ელემოციის კლასიფიკაცია", 2-ში "ელემოციის ინტენსტიური რეგრესია" და სხვა ქვემოსტაციის კონპექტიური შედეგები მიიღეთ.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=kk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Бұл қағазда біз біз бірінші тапсырма "Твиттерге әсер ету" деген жарықтық оқыту үлгілерін таңдаймыз. Біз ағылшынша tweets үшін барлық суреттерге қатынасыз. Біз бірнеше қабатты өзіңіздің өзіңіздің өзіңіздің бірнеше қабатты архитектурасының Bi-LSTM архитектурасын ұсынамыз. Бақылау механизмі үлгіліктерді жақсартып, тейтеттерде теңіздік сөздерді анықтауға мүмкіндік береді, сондай-ақ үлгілерді қолдануға болады. Біздің үлгіміз 550 миллион Твиттер хаттардың үлкен жинақтағы сөздерді 2vec сөздерді ендіру жинағын қолданады. Бұл сөздерді әсер етпейтін мүмкіндіктері бойынша көптеген. Тапсырманың ерекше оқыту деректерінің шектелген мөлшерлері себебі, біз 2017 жылдың жарым-жылдық тапсырма 4A тапсырмасындағы Bi-LSTMs деректер жиынына ауыстыру жағдайын таңдадық. Бұл тапсырма "Көп жарлық күй- жарлық күй- жарлық күй- жарлық классификациясы" 1- інде, 2- тапсырма "Күй- жарлық күй- жарлық регрессиясы" 2- інде, басқа жарлық күй- жарлы</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ko_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>본고에서 우리는SemEval-18미션1대회에 제출한 심도 있는 학습 모델인'트위터의 영향'을 보여 주었다.우리는 영어 추문의 모든 하위 임무에 참여했다.다중 자가 주의 메커니즘을 갖춘 이중 LSTM 아키텍처를 제시했습니다.주의 메커니즘은 모델의 성능을 향상시켜 우리가 추문 중의 현저한 단어를 식별하고 모델을 깊이 있게 이해하여 이해하기 쉽게 한다.우리의 모델은 5억 5000만 개의 트위터 메시지를 포함하는 큰 집합에 삽입된word2vec 단어를 활용하여 하나의 단어의 감정적 특징을 강화했다.임무에 특정된 교육 데이터가 제한되어 있기 때문에 우리는 Semeval 2017 임무 4A의 데이터 집합에서 Bi LSTM을 미리 교육함으로써 이전 학습 방법을 선택했다.이 방법은 하위 퀘스트 E'다라벨 정서 분류'에서 1위, 하위 퀘스트 A'정서 강도 회귀'에서 2위를 차지하며 다른 하위 퀘스트에서 경쟁력 있는 결과를 얻었다.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=lt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Šiame dokumente pristatome išsamaus mokymosi modelius, kurie buvo pateikti konkursui „Poveikis Tweetuose“ 1 užduoties puslapyje 2018 m. Mes dalyvavome visuose paklausimuose apie anglų tweetus. Siūlome dviejų sluoksnių savarankiško dėmesio mechanizmą. The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable. Mūsų modelis naudoja žodžių 2vec įdėjimų rinkinį, apmokytą didelėje 550 milijonų Twitter pranešimų rinkinyje, papildytą žodžių įtakingų savybių rinkiniu. Atsižvelgdami į ribotą konkrečioms užduotims skirtų mokymo duomenų kiekį, nusprendėme taikyti mokymosi perdavimu metodą iš anksto parengdami dviejų LSTM programų 2017 m. pusmečio duomenų rinkinį, 4A užduotis. Siūlomas metodas pirmą kartą priskirtas E pakopos „Daugiaženklių emocijų klasifikavimui“, antrą – A pakopos „Emocijų intensyvumo regresijai“ ir pasiekė konkurencinius rezultatus kitose pakopose.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Во овој документ претставуваме модели за длабоко учење кои се поднесоа на натпреварот SemEval-2018 Task 1: „Влијание на твитови“. Учествувавме на сите потпрашања за англиски твитови. Предложуваме архитектура на Би-ЛСТМ опремена со механизам на повеќе слоеви за себе внимание. Механизмот на внимание ја подобрува резултатот на моделот и ни овозможува да идентификуваме значајни зборови на твитовите, како и да добиеме информација за моделите што ги прави поинтерпретабилни. Нашиот модел користи сет на зборови вградени во зборови word2vec тренирани на голема колекција од 550 милиони Твитер пораки, зголемени од сет на зборови влијателни карактеристики. Поради ограничената количина на податоци за обука специфични за задачите, избравме пристап на трансферентно учење со претренирање на Би-ЛСТМ на податоците од Северниот 2017 година, задача 4А. Предложениот пристап се рангираше на првата во подзадачата Е „Класификација на емоции со повеќе ознаки“, на втората во подзадачата А „Регресија на емоционска интензитет“ и постигна конкурентни резултати во другите подзадачи.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ml_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>ഈ പത്രത്തില്‍ ഞങ്ങള്‍ ആഴത്തില്‍ പഠിക്കുന്ന മോഡലുകള്‍ കൊണ്ടുവരുന്നു. സെമ്എവാല്‍-2018 ടാസ്ക് 1 പ്രതിയോഗിക്കുന്നു: “ടൂട ഇംഗ്ലീഷ് ടൂട്ടുകള്‍ക്ക് ഞങ്ങള്‍ എല്ലാ സബ്ടേജുകളിലും പങ്കുചേര്‍ന്നു. നമ്മള്‍ ഒരു ബി-എല്‍സ്റ്റം ആര്‍ക്ടിക്കറ്റര്‍ പ്രൊദ്ദേശിപ്പിക്കുന്നു. ഒരു multi-layer സ്വയം ശ്രദ്ധിക്കുന്ന ഒരു സ് ശ്രദ്ധിക്കുന്ന മെനിസ്റ്റം മാതൃകയുടെ പ്രകടനത്തില്‍ മെച്ചപ്പെടുത്തുന്നു. ടൂട്ടില്‍ കഷ്ടപ്പെട്ട വാക്കുകള്‍ തിരിച്ചറി നമ്മുടെ മോഡല്‍ ഒരു വാര്‍ഡ്2വെക്ക് വാക്കുകള്‍ ഉപയോഗിക്കുന്നു. 550 മില്ല്യന്‍ ടൂട്ടര്‍ സന്ദേശങ്ങളില്‍ പരിശീലിക്കപ്പെട്ട വാക്കുകള്‍ ഉപയോഗ ജോലിക്കുള്ള പ്രത്യേക പരിശീലന വിവരങ്ങളുടെ കാരണം ഞങ്ങള്‍ ഒരു മാറ്റം പഠിക്കാനുള്ള വഴിയില്‍ തെരഞ്ഞെടുത്തു. സെമെവാല്‍ 2017-ലെ ഡാറ്റാസെറ്റ് പ്രൊദ്ദേശിക്കപ്പെട്ട ആദ്യത്തെ സുബ്ബട്ടില്‍ 'Multi-Label Emotion Classification', രണ്ടാമത്തെ സുബ്ബട്ടില്‍ 'Emotion Intensity Regression' നിര്‍ബന്ധപ്പെടുത്തി മറ്റു സബ്ട്ടേഷ</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mn_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Энэ цаасан дээр бид SemEval-2018 Task 1-ийн өрсөлдөөнд "Tweets-д нөлөөлөх нөлөөлөх" загварыг суралцах загваруудыг тайлбарлаж байна. Бид Англи хэлний tweets дээр оролцсон. Бид Би-ЛСТМ архитектурыг олон давхар өөрийн анхаарлын механизмтай зохион байгуулсан. Анхаарлын механизм загварын үйл ажиллагааг сайжруулж, бидэнд tweets дээр сайхан үгсийг тодорхойлох боломжтой болгодог. Мөн загваруудыг илүү ойлгох боломжтой болгодог. Бидний загварын загвар нь 550 сая Твиттерийн хуудас дээр сургалтын хэмжээний хэмжээний хэмжээний хэлбэрийг ашиглаж байна. Бид ажлын тодорхой сургалт өгөгдлийн хязгаарлагдмал хэмжээний учраас бид 2017 оны дундаж зуун зуун зуун үеийн Би-ЛСТМ-г, Task 4A-ын өгөгдлийн сан дээр шилжүүлэх арга замыг сонгосон. Тайлбарласан арга баримт 1-р "Олон-найралтын Хүйцэтгэл Хүйцэтгэл Классификацийн Хүйцэтгэл Хүйцэтгэл Хүйцэтгэл Хүйцэтгэл Хүйцэтгэл Хүйцэтгэл Хүйцэтгэл Хүйцэтгэл Хүйцэтгэл Хүйцэтгэл" 2</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ms_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Dalam kertas ini kami memperkenalkan model belajar dalam yang dihantar kepada persaingan SemEval-2018 Task 1: 'Affect in Tweets'. Kami berpartisipasi dalam semua sub-tanya untuk tweet Inggeris. Kami cadangkan arkitektur Bi-LSTM yang dilengkapi dengan mekanisme perhatian diri berbilang lapisan. Mekanisme perhatian meningkatkan prestasi model dan membolehkan kita mengenalpasti perkataan yang penting dalam tweet, serta mendapatkan pandangan ke dalam model yang membuat mereka lebih boleh diterjemahkan. Model kami menggunakan set pembenaman kata word2vec dilatih pada koleksi besar 550 juta mesej Twitter, ditambah oleh set ciri-ciri perkataan yang affectif. Kerana jumlah data latihan khusus tugas, kami memilih pendekatan pembelajaran pemindahan dengan melatih Bi-LSTM pada set data Semeval 2017, Tugas 4A. Pendekatan yang diusulkan ditangkap pertama dalam Subtugas E 'Klasifikasi Emosi Label-berbilang', kedua dalam Subtugas A 'Regresi Intensiti Emosi' dan mencapai keputusan kompetitif dalam subtugas lain.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=mt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>F'dan id-dokument nippreżentaw mudelli ta' tagħlim profond li tressqu lill-kompetizzjoni SemEval-2018 Task 1: 'Affett in Tweets'. Parteċipajna fis-sottotalbiet kollha għal tweets bl-Ingliż. Aħna nipproponu arkitettura Bi-LSTM mgħammra b’mekkaniżmu ta’ awtonomija b’diversi saffi. Il-mekkaniżmu ta’ attenzjoni jtejjeb il-prestazzjoni tal-mudell u jippermettilna nidentifikaw kliem importanti fit-tweets, kif ukoll jiksbu għarfien dwar il-mudelli li jagħmluhom aktar interpretabbli. Il-mudell tagħna juża sett ta’ inkorporazzjonijiet tal-kliem word2vec imħarrġa fuq ġabra kbira ta’ 550 miljun messaġġ Twitter, miżjuda b’sett ta’ karatteristiċi affetttivi tal-kliem. Minħabba l-ammont limitat ta’ dejta dwar it-taħriġ speċifiku għall-kompiti, għa żlna approċċ ta’ tagħlim tat-trasferiment billi nħarrġu minn qabel il-Bi-LSTMs fuq is-sett ta’ dejta tas-Semeval 2017, Kompitu 4A. L-approċċ propost kellu l-ewwel klassifikazzjoni fis-Subkompitu E 'Klassifikazzjoni tal-Emożizzjoni b'Tikketti Multi', it-tieni fis-Subkompitu A 'Regressjoni tal-Intensità tal-Emożizzjoni' u kiseb riżultati kompetittivi f'sottokompiti oħra.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=nl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In dit artikel presenteren we deep-learning modellen die zijn ingediend voor de SemEval-2018 Taak 1 competitie: 'Affect in Tweets'. We hebben deelgenomen aan alle subtaken voor Engelse tweets. We stellen een Bi-LSTM architectuur voor die is uitgerust met een meerlaags zelfattentiemechanisme. Het aandachtsmechanisme verbetert de prestaties van het model en stelt ons in staat om opvallende woorden in tweets te identificeren, evenals inzicht te krijgen in de modellen waardoor ze beter interpreteerbaar worden. Ons model maakt gebruik van een set word2vec woord embeddings getraind op een grote verzameling van 550 miljoen Twitter berichten, aangevuld met een reeks woord affectieve functies. Vanwege de beperkte hoeveelheid taakspecifieke trainingsdata hebben we gekozen voor een transfer learning aanpak door de Bi-LSTMs vooraf te trainen op de dataset van Semeval 2017, Taak 4A. De voorgestelde aanpak scoorde 1e in Subtaak E 'Multi-Label Emotion Classification', 2e in Subtaak A 'Emotie Intensity Regressie' en behaalde concurrerende resultaten in andere subtaken.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=no_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>I denne papiret presenterer vi dype læringsmodeller som sendte til sammenheng med semiEval-2018 Oppgåve 1: "Etter tweets". Vi delta i alle underspørjingar om engelske tweets. Vi foreslår ein bi-LSTM-arkitektur med ein fleire lag selvstyrkemekanisme. Merknadsmekanismenyen forbedrar modellen og tillater oss å identifisera salient ord i tweets, og få innsyning i modelane som gjer dei meir tolkbare. Modellen vårt bruker eit sett med ord 2vec- innbygging som treng på eit stor samling av 550 millioner Twitter- meldingar, som blir auka av eit sett ord- effektiv funksjonar. På grunn av begrenset mengda oppgåvespesifikke opplæringsdata, valte vi ein tilnærming for å lære overføringar ved å trekke Bi-LSTMs på datasettet i halvåld 2017, oppgåve 4A. Den foreslåde tilnærminga rangerte 1 i underoppgåva E «Multi-Label Emotion Classification», 2 i underoppgåva A «Emotion Intensity Regression», og oppnådd konkurrentiv resultat i andre underoppgåver.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pl_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>W artykule przedstawiamy modele głębokiego uczenia, które zgłosiły się do konkursu SemEval-2018 Task 1: "Affect in Tweets". Uczestniczyliśmy we wszystkich podzadaniach dotyczących angielskich tweetów. Proponujemy architekturę Bi-LSTM wyposażoną w wielowarstwowy mechanizm samoobserwacji. Mechanizm uwagi poprawia wydajność modelu i pozwala nam zidentyfikować istotne słowa w tweetach, a także uzyskać wgląd w modele, dzięki czemu są one bardziej interpretowalne. Nasz model wykorzystuje zestaw osadzeń słów Word2vec przeszkolonych na dużej kolekcji 550 milionów wiadomości na Twitterze, rozszerzonych o zestaw funkcji afektywnych słów. Ze względu na ograniczoną ilość danych szkoleniowych specyficznych dla zadania, zdecydowaliśmy się na transfer learning poprzez wstępne szkolenie Bi-LSTMs na zbiorze danych Semeval 2017, Zadanie 4A. Proponowane podejście zajęło pierwsze miejsce w podzadaniu E "Klasyfikacja emocji wieloznakowych", drugie w podzadaniu A "Regresja intensywności emocji" i osiągnęło konkurencyjne wyniki w innych podzadaniach.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=pt_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Neste artigo apresentamos modelos de deep learning submetidos ao concurso SemEval-2018 Task 1: “Affect in Tweets”. Participamos de todas as subtarefas de tweets em inglês. Propomos uma arquitetura Bi-LSTM equipada com um mecanismo de autoatenção multicamadas. O mecanismo de atenção melhora o desempenho do modelo e nos permite identificar palavras salientes em tweets, bem como obter insights sobre os modelos tornando-os mais interpretáveis. Nosso modelo utiliza um conjunto de incorporações de palavras word2vec treinadas em uma grande coleção de 550 milhões de mensagens do Twitter, aumentadas por um conjunto de recursos afetivos de palavras. Devido à quantidade limitada de dados de treinamento específicos de tarefas, optamos por uma abordagem de aprendizado de transferência pré-treinando os Bi-LSTMs no conjunto de dados de Semeval 2017, Tarefa 4A. A abordagem proposta ficou em 1º lugar na Subtarefa E “Multi-Label Emotion Classification”, 2º na Subtarefa A “Emotion Intensity Regression” e obteve resultados competitivos em outras subtarefas.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ro_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>În această lucrare prezentăm modele de învățare profundă care au participat la concursul SemEval-2018 Task 1: "Afect in Tweets". Am participat la toate subactivitățile pentru tweeturile în limba engleză. Propunem o arhitectură Bi-LSTM echipată cu un mecanism de auto-atenție multistrat. Mecanismul de atenție îmbunătățește performanța modelului și ne permite să identificăm cuvinte importante în tweet-uri, precum și să obținem o perspectivă asupra modelelor făcându-le mai interpretabile. Modelul nostru utilizează un set de încorporări word2vec de cuvinte instruite pe o colecție mare de 550 de milioane de mesaje Twitter, amplificate de un set de caracteristici afective de cuvinte. Datorită cantității limitate de date de instruire specifice sarcinilor, am optat pentru o abordare de transfer learning prin pregătirea Bi-LSTMs pe setul de date Semeval 2017, Task 4A. Abordarea propusă s-a clasat pe locul 1 în Subsarcina E "Clasificarea emoțiilor cu etichete multiple", pe locul 2 în Subsarcina A "Regresia intensității emoționale" și a obținut rezultate competitive în alte subsarcini.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ru_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>В этой статье мы представляем модели глубокого обучения, которые были представлены на конкурс Задача 1 SemEval-2018: «Влияние в твитах». Мы участвовали во всех подзадачах для английских твитов. Мы предлагаем архитектуру Bi-LSTM, оснащенную многослойным механизмом самовнимания. Механизм внимания улучшает производительность модели и позволяет идентифицировать характерные слова в твитах, а также получить представление о моделях, делая их более интерпретируемыми. Наша модель использует набор словесных вложений word2vec, обученных на большой коллекции из 550 миллионов сообщений Twitter, дополненных набором словесных аффективных функций. В связи с ограниченным объемом данных обучения по конкретным задачам, мы выбрали подход к обучению переносу, предварительно обучив Bi-LSTMs на наборе данных Semeval 2017, Task 4A. Предлагаемый подход занял 1-е место в подзадаче Е «Многоуровневая классификация эмоций», 2-е место в подзадаче А «Регрессия интенсивности эмоций» и достиг конкурентных результатов в других подзадачах.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=si_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>මේ පත්තරේ අපි ගොඩක් ඉගෙනගන්න ප්‍රමාණයක් පෙන්වන්නේ, ඒ වගේම සේම්වල් 2018 කාර්ය 1 ප්‍රයෝජනයෙන් පෙන්වන්නේ: 'ට අපි ඉංග්‍රීසි ට්විට් එක්ක සියළුම් ප්‍රශ්නයක් කරනවා. අපි බී-ල්ස්ටිම් ස්ථාපනයක් ප්‍රසිද්ධා කරනවා වඩා ස්ථාපනයක් තියෙන්නේ. අවධානය පද්ධතිය ප්‍රමාණය වැඩි කරනවා ඒ වගේම අපිට ට්විට් වලින් ප්‍රමාණයක් තියෙන්න පුළුවන් වෙනවා, ඒ වගේම මොඩේල් ව අපේ මොඩල් භාවිතා කරනවා වචන 2වෙක් වචන ඇම්බෙන්ඩින්ග් සූදානයක් තියෙනවා ට්විටර් මිලියන 550 සංකේතනයේ ලොකු සංකේතනයක් වැඩක් විශේෂ ප්‍රධාන දත්තේ සීමාවිත විශේෂ විශේෂ විදිහට, අපි තෝරාගත්තා බී-LSTMs විදිහට ඉගෙන ගන්න ප්‍රධානයක් විදි ප්‍රශ්න විදිහට ප්‍රශ්න විදිහට පළවෙනි විදිහට සුබ් විදිහට E 'Multi-Label Emotion Classication', 2 විදිහට සුබ් විදිහට A 'Emotion Intensty regression' සහ අනිත් විදිහට</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sk_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>V prispevku predstavljamo modele globokega učenja, ki so se predložili natečaju SemEval-2018 Task 1: "Affect in Tweets". Sodelovali smo pri vseh podnalogah za angleške tweete. Predlagamo Bi-LSTM arhitekturo, opremljeno z večplastnim mehanizmom samopozornosti. Mehanizem pozornosti izboljša učinkovitost modela in nam omogoča prepoznavanje pomembnih besed v tweetih ter vpogled v modele, zaradi česar so bolj razložljive. Naš model uporablja nabor besednih vdelav word2vec, usposobljenih na veliki zbirki 550 milijonov Twitterjevih sporočil, dopolnjenih z naborom besednih afektivnih funkcij. Zaradi omejene količine podatkov o usposabljanju za posamezne naloge smo se odločili za pristop prenosnega učenja s predusposabljanjem Bi-LSTMs na podatkovnem naboru Semeval 2017, naloga 4A. Predlagani pristop se je uvrstil na prvo mesto v podnalogo E "Razvrstitev več znakov čustev", drugo mesto v podnalogo A "Regresija intenzivnosti čustev" in dosegel konkurenčne rezultate v drugih podnalogah.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=so_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Warqadan waxaynu ku qornaa modelal waxbarasho dheer ah oo loo dhiibay tartanka 1-shaqo ee SemEval-2018: "Wax saameyn ku leh Tweets". Waxaannu ka qeybqaadanay shaqooyin kasta oo ku saabsan tweetka Ingiriiska. Bi-LSTM waxaan soo jeedaynaa dhismo ay leedahay meymisyo aad u fiirsan karto. Meherka daryeelka ayaa horumarinaya sameynta modelalka, wuxuuna inagu ogolaan karaa inaannu aqoonsanno hadalka sameynta ee tweetka, sidoo kale aragtida sameynta oo ka sameynaya in ay turjumaan. Tusaale ahaan wuxuu isticmaalaa hadal 2vec ah oo lagu baray urur badan oo farriin badan oo 550 milyan oo Twitter ah, oo lagu kordhiyey xarumo hadal saameyn ah. Sababta aad u xadan waxbarashada shaqo oo gaar ah ayaannu u doorannay habka waxbarashada la wareejiyo, iyadoo aan ka hor dhignay Bi-LSTMs oo ku qoran taariikhda danbiyada Semeval 2017, shaqo 4A. Dhaqdhaqaaqa la soo jeeday waxay ka soo baxay 1st ee Sub-task E 'Fiisashada qalabka badan', 2nd in Subtask A 'Emotion Intensity Regression' waxayna gaadheen sabab tartank ah oo ay ka helaan shaqaalaha kale.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sq_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition: 'Affect in Tweets'. Ne morëm pjesë në të gjitha nënkërkesat për tweetet angleze. Ne propozojmë një arkitekturë Bi-LSTM të pajisur me një mekanizëm shumë-shtresa vetëvëmendje. Mekanizmi i vëmendjes përmirëson performancën e modelit dhe na lejon të identifikojmë fjalë të rëndësishme në tweet si dhe të fitojmë kuptim në modelet që i bëjnë ato më të interpretueshme. Modeli ynë përdor një sërë fjalë2vec të përfshira të stërvitura në një koleksion të madh prej 550 milion mesazhe Twitter, të shtuar nga një sërë fjalëafektive karakteristika. Për shkak të sasisë së kufizuar të të dhënave të trajnimit specifik për detyrat, zgjodhëm një metodë transferimi mësimi duke paratrajnuar Bi-LSTMs në grupin e të dhënave të Semeval 2017, Task 4A. Përqasja e propozuar u rendit e para në nëndetyrën E 'Klasifikimi i Emocioneve Multi-Label', e dyta në nëndetyrën A 'Regresioni i Intensitetit Emocional' dhe arriti rezultate konkurruese në nëndetyra të tjera.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>U ovom papiru predstavljamo modele dubokog učenja koji su predali natjecanju na prvom zadatku u polu Evala-2018: 'Posluživanje Tweets'. Mi smo sudjelovali u svim podpitanjima za engleske tweets. Predlažemo arhitekturu bi-LSTM opremljenu mehanizam pažnje na višeslojevima. Mehanizam pažnje poboljšava modelnu funkciju i omogućava nam da identifikujemo saliène reèi u tweetovima, kao i da dobijemo uvid u modele koji ih čine interpretabilnijim. Naš model koristi setu rečenica 2vec rečenica obučenih na velikoj kolekciji 550 miliona tviterskih poruka, povećanoj sa setom rečenih utjecaja. Zbog ograničene količine podataka o obuci određenih zadataka, odabrali smo pristup prijenosnoj učenju, pretvarajući bi-LSTMs na setu podataka poluvekova 2017., zadatak 4A. Predloženi pristup je bio prvi u subtask E 'Multi-Label Emotion Classification', drugi u podzadatku A 'Emotion Intensity Regression' i ostvario konkurentne rezultate u drugim podkazama.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sv_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>I denna uppsats presenterar vi djupinlärningsmodeller som anmälts till SemEval-2018 Task 1-tävlingen: "Påverkan i tweets". Vi deltog i alla underuppdrag för engelska tweets. Vi föreslår en Bi-LSTM arkitektur utrustad med en flerlagers självuppmärksamhetsmekanism. Uppmärksamhetsmekanismen förbättrar modellens prestanda och gör det möjligt för oss att identifiera viktiga ord i tweets, samt få insikt i modellerna som gör dem mer tolkningsbara. Vår modell använder en uppsättning word2vec ord inbäddningar utbildade på en stor samling av 550 miljoner Twitter-meddelanden, förstärkta av en uppsättning ord affektiva funktioner. På grund av den begränsade mängden uppgiftsspecifika utbildningsdata valde vi en överföringsinlärningsmetod genom att förbereda Bi-LSTMs på datauppsättningen för Semeval 2017, Task 4A. Det föreslagna tillvägagångssättet rankades 1:a i underuppgift E 'Multi-Label Emotion Classification', 2:a i underuppgift A 'Emotion Intensity Regression' och uppnådde konkurrenskraftiga resultat i andra underuppgifter.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=sw_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Katika karatasi hii tunaweka modeli za kujifunza kwa kina ambazo zilitoa kwenye mashindano ya SemEval-2018 ya kazi 1: “Inaathiri kwenye twita”. Tumeshiriki katika kazi zote za twita za Kiingereza. Tunazipendekeza ujenzi wa Bi-LSTM wenye uwezo wa mfumo wa kujisikiliza kwa ngazi nyingi. Mfumo wa ufuatiliaji unaboresha utendaji wa mifano na unaruhusu kututambua maneno mazungumzo katika twiti, pamoja na kupata uelewa wa mifano ili kuwafanya kuwa na ufafanuzi zaidi. Mfano wetu unatumia seti ya neno la Word2vec ambalo limefundishwa kwenye mkusanyiko mkubwa wa jumbe za Twita milioni 550, uliongezeka na seti ya maneno yanayoathiri. Kutokana na kiwango kikubwa cha taarifa za mafunzo maalum za kazi, tulichagua njia ya kuhamisha elimu kwa kuchelea Bi-LST kwenye seti ya taarifa za Semeval 2017, Kazi 4A. Mpango huu wa mapendekezo ulikuwa wa kwanza katika kazi ya Ujumbe wa E 'Kutangaza hisia za Kialama Multi-Label', wa pili katika kazi ya A 'Kuzuia Ujasiri wa Tamko' na kupata matokeo ya ushindani katika majukumu mengine.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ta_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>இந்த காகிதத்தில் நாம் ஆழமான கற்றுக் கொள்ளும் மாதிரிகளை கொண்டுள்ளோம் செம்வால்-2018 செயல் 1 வேலைக்கு கொடுக்கப்பட் நாங்கள் ஆங்கிலத்திற்கு அனைத்து உப பணிகளிலும் பங்கிட்டோம். நாம் பல அடுக்கு தன்னை கவனத்திற்கு உருவாக்கப்பட்ட ஒரு பி எல்ஸ்டிஎம் அமைப்பை பரிந்துரைக்கிறோம். கவனம் முறைமை மாதிரி செயல்பாட்டை மேம்படுத்துகிறது மற்றும் தேவையான வார்த்தைகளை தெரிந்து கொள்ள அனுமதிக்கிறது, மாதிரிகளில் எங்கள் மாதிரி வார்த்தை 2வெக் வார்த்தையை பயன்படுத்துகிறது, 550 மில்லியன் தொடர்பு செய்திகளில் பயிற்சிக்கப்பட்டுள்ளது, சொல்லும் பாத செயல் குறிப்பிட்ட பயிற்சியின் குறிப்பிட்ட அளவு காரணத்தால், நாங்கள் மாற்று கற்றுக்கொள்ள முறையை தேர்ந்தெடுத்தோம் செம்வால் 2017-ல் துணை செயலில் முதல் முறை வரையறுக்கப்பட்டது 'பல- சிட்டி உணர்வு வகைப்படுத்தல்', உப செயலில் A 'உணர்வு உணர்வு உணர்வு கடினம்' மற்றும் பொருத்தும் முடிவுகள் மற</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=tr_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Bu kagyzda çukur öwrenmek nusgalaryny SemEval-2018 Görev 1-nji ýaryşy tarapyna gönderýäris: 'Tweets içinde etkiler'. Biz iňlisçe tweets üçin ähli Subasklara goşuldyk. Biz birnäçe gaty bir özi üns meýdançasynda bi-LSTM arhitektura teklip edip görýäris. Diňleşme mekanismi nusgasyny gowurap ýöredýär we tweetlerde çykyş sözleri tanamagymyza mümkin edýär, we nusgalary has gowurak ýagdaýa düşürýär. Biziň nusgamyz 550 milyon Twitter mesajlarynda bilinmeli sözler guralýar. Beýik bir şekilde laýyk guralýan sözler bilen üýtgedýär Görevleriň azalty taýýarlama maglumaty üçin biz Bi-LSTMsleriň 2017-nji ýylyň yary sany çykyşynda, Görev 4A-nyň üstüne süýtgetmesi üçin bir hereket etdik. Subtask E 'Multi-Label Emotion Classification', 2-nji Subtask A 'Emotion Intensity Regression' tarafından birinci tarafından berildi ve diğer subtasklarda rekabetçi neticeleri başardı.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=ur_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>اس کاغذ میں ہم عمیق سیکھنے کی موڈل پیش کرتے ہیں جو نصف Eval-2018 ٹاکس ۱ کی مسابقه پر پیش کیا گیا تھا: Tweets میں اثر ہے۔ ہم انگلیسی ٹویٹ کے لئے تمام سپٹسٹوں میں شریک ہوئے۔ ہم ایک Bi-LSTM معماری پیشنهاد کریں جو ایک بہت سی لائر کی اظہار مکانیزم کے ساتھ مطابق ہے. توئیٹوں میں صاف کلمات پہچان دینے کی اجازت دیتا ہے اور مدلکوں میں بھی زیادہ تفسیر کرنے کی اجازت دیتا ہے۔ ہمارا مدل ایک زبان 2vec word embeddings کا مجموعہ استعمال کرتا ہے جو 550 میلیون ٹویٹر پیغام کے بڑے مجموعہ پر آموزش کی گئی ہے، جو ایک زبان کے ذریعہ اضافہ کرتی ہے۔ ہم نے دنیا کی تعلیم کے مقدار کے باعث ایک ترنس سیکھنے کی طریقہ کے لئے انتخاب کیا تھا، اس کے ذریعہ ہم نے ۲۰۱۷، ٹاکس 4A کے ڈاٹ سٹ پر Bi-LSTMs کو ترنس سیکھنے کے لئے انتخاب کیا تھا. پیغمبر کی تقریبا ایک مرتبہ ہے Subtask E 'Multi-Label Emotion Classification' میں، دوسرے مطابق A 'Emotion Intensity Regression' میں اور دوسرے مطابق میں مقابلہ کے نتائج پہنچ گئے۔</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=uz_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition: 'Affect in Tweets'. Biz ingliz tilida hamma sub-vazifalarga murojaat qildik. Biz ko'plab-darajada o'zimni o'zimga tayyorlangan Bi-LSTM struktureni tasavvur qilamiz. Muhimlik mekaniya model bajarishni o'zgartiradi va bizni Twitterda soʻzlarni aniqlashga ruxsat beradi, va modellarga ko'rsatish mumkin. Bizning modelimiz 550 million Twitter xabarlarining katta тўпланган soʻzlardan foydalanadi. Bu so'zlar tashkilotga qo'llangan imkoniyatlarni qo'yish mumkin. Vazifaning qanchalik taʼminlovchi maʼlumotning chegarasi sabab, biz Vazifa 4A (Semeval 2017) haqida Bi-LSTMs maʼlumotlar tarkibini o'zgartirish muvaffaqiyatlarini o'zgartirish usulini tanlashimiz mumkin. Tashkilot E 'Multi-lab xabarlarning birinchi darajasi' bilan birinchi darajaga kiritildi, ikkinchi vazifani 'Emotion Intensity Regression' bilan boshqa vazifaning rivojlanish natijalariga yetardi.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=vi_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>Trong tờ giấy này chúng tôi giới thiệu các mô hình học sâu được gửi đến cuộc thi Nhiệm vụ SemEvol-208: "tác động trên Tweet". Chúng tôi tham gia tất cả các thư ngầm về tiếng Anh. Chúng tôi đề nghị một kiến trúc hai-LSTM được trang bị với cơ chế tự chú ý nhiều lớp. Các cơ chế chú ý cải thiện hiệu ứng mô hình và cho phép chúng ta xác định những từ nổi tiếng trên Twitter, cũng như có được sự hiểu biết về các mô hình khiến chúng dễ hiểu hơn. Người mẫu của chúng tôi sử dụng một tập hợp từ ngữ 2vector từ mới được huấn luyện trên một tập hợp gồm một triệu tập tin Twitter, được tăng thêm bởi một loạt các tính năng tác động từ. Do lượng nhỏ các dữ liệu về huấn luyện đặc nhiệm, chúng tôi đã chọn phương pháp học chuyển nhượng, bằng cách đoán trước các tập tin Bi-LSTM trên tập tin thuộc Semeval Des7, Task 4A. Cách tiếp cận được đề xuất thứ nhất theo phân loại E'đa nhãn để mê hoặc', thứ hai theo nền trừ A'Thuyết phục độ mạnh cảm xúc'và đạt được kết quả cạnh tranh trong các yêu cầu khác.</span></div></div><div class="card bg-light mb-2 mb-lg-3" id=zh_abstract style=display:none><div class="card-body acl-abstract"><h5 class=card-title>Abstract</h5><span>在本文中,我们介绍了提交给SemEval-2018 Task 1竞赛的深度学习模样:"推文中的影响"。 吾与英语推文之所有也。 请备数自机Bi-LSTM架构。 谨察其能,使知推文之奇单词,深知其更具可解释性。 吾侪因word2vec以销之,其词嵌于5.5亿条Twitter消息之大集,因单词情而增之。 特定数有限,择迁徙之法,法在Semeval 2017,Task 4A之数集上预习Bi-LSTM。 在子职E多标情类为第一,在子职A"情强归"为第二,而于诸子取竞争性成。</span></div></div><dl><dt>Anthology ID:</dt><dd>S18-1037</dd><dt>Volume:</dt><dd><a href=/volumes/S18-1/>Proceedings of The 12th International Workshop on Semantic Evaluation</a></dd><dt>Month:</dt><dd>June</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>New Orleans, Louisiana</dd><dt>Venue:</dt><dd><a href=/venues/semeval/>SemEval</a></dd><dt>SIGs:</dt><dd><a href=/sigs/siglex/>SIGLEX</a>
|
<a href=/sigs/sigsem/>SIGSEM</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>245–255</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href=https://aclanthology.org/S18-1037>https://aclanthology.org/S18-1037</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/S18-1037 title="To the current version of the paper by DOI">10.18653/v1/S18-1037</a></dd><dt class=acl-button-row>Bibkey:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citePaperBibkey><i class="far fa-clipboard"></i><span id=citePaperBibkey class="pl-2 text-monospace">baziotis-etal-2018-ntua</span></button></dd><dt>Cite (ACL):</dt><dd><span id=citeACL>Christos Baziotis, Athanasiou Nikolaos, Alexandra Chronopoulou, Athanasia Kolovou, Georgios Paraskevopoulos, Nikolaos Ellinas, Shrikanth Narayanan, and Alexandros Potamianos. 2018. <a href=https://aclanthology.org/S18-1037>NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning</a>. In <i>Proceedings of The 12th International Workshop on Semantic Evaluation</i>, pages 245–255, New Orleans, Louisiana. Association for Computational Linguistics.</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeACL><i class="far fa-clipboard"></i></button></dd><dt>Cite (Informal):</dt><dd><span id=citeRichText><a href=https://aclanthology.org/S18-1037>NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning</a> (Baziotis et al., SemEval 2018)</span><button type=button class="btn btn-clipboard btn-secondary btn-sm d-none ml-2" data-clipboard-target=#citeRichText><i class="far fa-clipboard"></i></button></dd><dt class=acl-button-row>Copy Citation:</dt><dd class=acl-button-row><button type=button class="btn btn-clipboard-outside btn-secondary btn-sm d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Markdown</button>
<button type=button class="btn btn-secondary btn-sm" data-toggle=modal data-target=#citeModal>More options…</button></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/S18-1037.pdf>https://aclanthology.org/S18-1037.pdf</a></dd><dt>Code</dt><dd><a href="https://paperswithcode.com/paper/?acl=S18-1037"><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg>&nbsp;additional community code</a></dd><dt>Terminologies:</dt><dd id=terms></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/S18-1037.pdf title="Open PDF of 'NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF</span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=NTUA-SLP+at+SemEval-2018+Task+1+%3A+Predicting+Affective+Content+in+Tweets+with+Deep+Attentive+RNNs+and+Transfer+LearningNTUA-SLP+at+SemEval-2018+Task+1%3A+Predicting+Affective+Content+in+Tweets+with+Deep+Attentive+RNNs+and+Transfer+Learning" title="Search for 'NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a>
<a class="btn btn-secondary d-flex flex-wrap justify-content-center" href="https://paperswithcode.com/paper/?acl=S18-1037" title="Code for 'NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning' on Papers with Code"><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-big" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg><span class="pl-sm-2 d-none d-sm-inline">Code</span></a>
<a class="btn btn-dark" data-toggle=modal data-target=#translateModal title="Translate for 'NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning'" style=color:#fff><i class="fas fa-language"></i><span class=pl-2>Translate</span></a></div></div><hr><div class="modal fade" id=citeModal tabindex=-1 role=dialog aria-labelledby=citeModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel>Export citation</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><ul class="nav nav-tabs mb-2" id=citeFormats role=tablist><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeBibtex role=tab aria-controls=citeBibtex aria-selected=false>BibTeX</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeMods role=tab aria-controls=citeMods aria-selected=false>MODS XML</a></li><li class=nav-item><a class="nav-link disabled" data-toggle=list href=#citeEndnote role=tab aria-controls=citeEndnote aria-selected=false>Endnote</a></li><li class=nav-item><a class="nav-link active" data-toggle=list href=#citeMarkdown role=tab aria-controls=citeMarkdown aria-selected=true>Preformatted</a></li></ul><div class=tab-content id=citeFormatsContent><div class="tab-pane active" id=citeBibtex role=tabpanel></div><div class=tab-pane id=citeMods role=tabpanel></div><div class=tab-pane id=citeEndnote role=tabpanel></div><div class=tab-pane id=citeMarkdown role=tabpanel><h5>Markdown (Informal)</h5><p id=citeMarkdownContent class="text-monospace small bg-light border p-2">[NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning](https://aclanthology.org/S18-1037) (Baziotis et al., SemEval 2018)</p><ul class=mt-2><li><a href=https://aclanthology.org/S18-1037>NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning</a> (Baziotis et al., SemEval 2018)</li></ul><h5>ACL</h5><ul class=mt-2><li id=citeACLstyleContent>Christos Baziotis, Athanasiou Nikolaos, Alexandra Chronopoulou, Athanasia Kolovou, Georgios Paraskevopoulos, Nikolaos Ellinas, Shrikanth Narayanan, and Alexandros Potamianos. 2018. <a href=https://aclanthology.org/S18-1037>NTUA-SLP at SemEval-2018 Task 1 : Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer LearningNTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning</a>. In <i>Proceedings of The 12th International Workshop on Semantic Evaluation</i>, pages 245–255, New Orleans, Louisiana. Association for Computational Linguistics.</li></ul><div class="modal-footer pb-1"><button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeMarkdownContent><i class="far fa-clipboard pr-2"></i>Copy Markdown to Clipboard</button>
<button type=button class="btn btn-clipboard btn-primary d-none" data-clipboard-target=#citeACLstyleContent><i class="far fa-clipboard pr-2"></i>Copy ACL to Clipboard</button></div></div></div></div></div></div></div><div class="modal fade" id=translateModal tabindex=-1 role=dialog aria-labelledby=translateModalLabel aria-hidden=true><div class="modal-dialog modal-lg" role=document><div class=modal-content><div class=modal-header><h5 class=modal-title id=citeModalLabel><i class="fas fa-language"></i> Translate</h5><button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body style=text-align:center><input id=lang_query type=text class="form-control mr-sm-2" style="width:50%;margin:0 auto!important" name=language placeholder=Search...><br><div id=buttons></div></div></div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script><script src=/js/clipboard.min.js></script>
<script>let lang_codes=["af","sq","am","ar","hy","az","bn","bs","bg","ca","zh","hr","cs","da","nl","et","fl","fi","fr","ka","de","el","ha","he","hi","hu","is","id","ga","it","ja","jv","kk","ko","lt","mk","ms","ml","mt","mn","no","fa","pl","pt","ro","ru","sr","si","sk","so","es","sw","sv","ta","bo","tr","uk","ur","uz","vi","en"],languages=["Afrikaans","Albanian","Amharic","Arabic","Armenian","Azerbaijani","Bengali","Bosnian","Bulgarian","Catalan","Chinese","Croatian","Czech","Danish","Dutch","Estonian","Filipino","Finnish","French","Georgian","German","Greek","Hausa","Hebrew","Hindi","Hungarian","Icelandic","Indonesian","Irish","Italian","Japanese","Javanese","Kazakh","Korean","Lithuanian","Macedonian","Malay","Malayalam","Maltese","Mongolian","Norwegian","Persian","Polish","Portuguese","Romanian","Russian","Serbian","Sinhala","Slovak","Somali","Spanish","Swahili","Swedish","Tamil","Tibetan","Turkish","Ukranian","Urdu","Uzbek","Vietnamese","English"];$(document).ready(function(){if(create_buttons(),ClipboardJS.isSupported()){success_fn=function(t){var e=$(t.trigger);e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check"),t.clearSelection(),setTimeout(function(){e.toggleClass("btn-success"),e.children("i").toggleClass("far fa-clipboard fas fa-clipboard-check")},2e3)};var e,t=new ClipboardJS(".btn-clipboard");t.on("success",success_fn),$(".btn-clipboard").removeClass("d-none"),e=new ClipboardJS(".btn-clipboard-outside",{text:function(e){var t=e.getAttribute("data-clipboard-target");return $(t).text()}}),e.on("success",success_fn),$(".btn-clipboard-outside").removeClass("d-none")}}),$("#lang_query").on("input",function(){var e=$(this),t=e.val();let n=document.getElementById("buttons");if(n.innerHTML="",e.data("lastval")!=t){e.data("lastval",t);for(let e in languages){let s=languages[e],o=lang_codes[e];s.includes(t)&&(n.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${o}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${s}</span></button>`)}}});function create_buttons(){let e=document.getElementById("buttons");for(let t in languages){let n=languages[t],s=lang_codes[t];e.innerHTML+=`<button class='btn btn-secondary' onclick="show_lang('${s}')" data-dismiss='modal' style='margin:10px; width:120px; text-align: center;'><span class='pl-2'>${n}</span></button>`}}function show_lang(e){hide_all(),console.log(e),$("#"+e+"_abstract").show(),$("#"+e+"_title").show()}function hide_all(){for(let t in lang_codes){let e=lang_codes[t];$("#"+e+"_abstract").hide(),$("#"+e+"_title").hide()}}</script></body></html>