<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W18-44.pdf>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (<span class=acl-fixed-case>TRAC</span>-2018)</a></h2><p class=lead><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>,
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>,
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>,
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W18-44</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Santa Fe, New Mexico, USA</dd><dt>Venues:</dt><dd><a href=/venues/coling/>COLING</a>
| <a href=/venues/trac/>TRAC</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W18-44>https://aclanthology.org/W18-44</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W18-44.pdf>https://aclanthology.org/W18-44.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W18-44.pdf title="Open PDF of 'Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+First+Workshop+on+Trolling%2C+Aggression+and+Cyberbullying+%28TRAC-2018%29" title="Search for 'Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4400/>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (<span class=acl-fixed-case>TRAC</span>-2018)</a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4401 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4401/>Benchmarking Aggression Identification in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a></a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/a/atul-kr-ojha/>Atul Kr. Ojha</a>
|
<a href=/people/s/shervin-malmasi/>Shervin Malmasi</a>
|
<a href=/people/m/marcos-zampieri/>Marcos Zampieri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4401><div class="card-body p-3 small">In this paper, we present the report and findings of the Shared Task on Aggression Identification organised as part of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-1) at COLING 2018. The task was to develop a <a href=https://en.wikipedia.org/wiki/Social_class>classifier</a> that could discriminate between Overtly Aggressive, Covertly Aggressive, and Non-aggressive texts. For this task, the participants were provided with a dataset of 15,000 aggression-annotated Facebook Posts and Comments each in <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> (in both Roman and Devanagari script) and <a href=https://en.wikipedia.org/wiki/English_language>English</a> for training and validation. For testing, two different sets-one from <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and another from a different social media-were provided. A total of 130 teams registered to participate in the task, 30 teams submitted their test runs, and finally 20 teams also sent their system description paper which are included in the TRAC workshop proceedings. The best system obtained a weighted F-score of 0.64 for both <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> on the Facebook test sets, while the best scores on the surprise set were 0.60 and 0.50 for <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> respectively. The results presented in this report depict how challenging the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is. The positive response from the community and the great levels of participation in the first edition of this shared task also highlights the interest in this topic.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4402.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4402 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4402 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4402/>RiTUAL-UH at TRAC 2018 Shared Task : Aggression Identification<span class=acl-fixed-case>R</span>i<span class=acl-fixed-case>TUAL</span>-<span class=acl-fixed-case>UH</span> at <span class=acl-fixed-case>TRAC</span> 2018 Shared Task: Aggression Identification</a></strong><br><a href=/people/n/niloofar-safi-samghabadi/>Niloofar Safi Samghabadi</a>
|
<a href=/people/d/deepthi-mave/>Deepthi Mave</a>
|
<a href=/people/s/sudipta-kar/>Sudipta Kar</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4402><div class="card-body p-3 small">This paper presents our <a href=https://en.wikipedia.org/wiki/System>system</a> for TRAC 2018 Shared Task on Aggression Identification. Our best systems for the English dataset use a combination of lexical and semantic features. However, for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi data</a> using only lexical features gave us the best results. We obtained weighted F1-measures of 0.5921 for the English Facebook task (ranked 12th), 0.5663 for the English Social Media task (ranked 6th), 0.6292 for the Hindi Facebook task (ranked 1st), and 0.4853 for the Hindi Social Media task (ranked 2nd).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4405.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4405 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4405 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4405/>Cyberbullying Intervention Based on <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/q/qianjia-huang/>Qianjia Huang</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a>
|
<a href=/people/j/jianhong-zhang/>Jianhong Zhang</a>
|
<a href=/people/d/david-van-bruwaene/>David Van Bruwaene</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4405><div class="card-body p-3 small">This paper describes the process of building a cyberbullying intervention interface driven by a machine-learning based text-classification service. We make two main contributions. First, we show that <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> can be identified in real-time before it takes place, with available machine learning and natural language processing tools. Second, we present a mechanism that provides individuals with early feedback about how other people would feel about wording choices in their messages before they are sent out. This <a href=https://en.wikipedia.org/wiki/User_interface>interface</a> not only gives a chance for the user to revise the text, but also provides a system-level flagging / intervention in a situation related to <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4406.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4406 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4406 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4406/>LSTMs with Attention for Aggression Detection<span class=acl-fixed-case>LSTM</span>s with Attention for Aggression Detection</a></strong><br><a href=/people/n/nishant-nikhil/>Nishant Nikhil</a>
|
<a href=/people/r/ramit-pahwa/>Ramit Pahwa</a>
|
<a href=/people/m/mehul-kumar-nirala/>Mehul Kumar Nirala</a>
|
<a href=/people/r/rohan-khilnani/>Rohan Khilnani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4406><div class="card-body p-3 small">In this paper, we describe the <a href=https://en.wikipedia.org/wiki/System>system</a> submitted for the shared task on Aggression Identification in <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook posts</a> and comments by the team Nishnik. Previous works demonstrate that <a href=https://en.wikipedia.org/wiki/Linear_time-invariant_system>LSTMs</a> have achieved remarkable performance in natural language processing tasks. We deploy an LSTM model with an attention unit over it. Our system ranks 6th and 4th in the Hindi subtask for Facebook comments and subtask for generalized social media data respectively. And <a href=https://en.wikipedia.org/wiki/It_(2017_film)>it</a> ranks 17th and 10th in the corresponding English subtasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4407.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4407 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4407 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4407/>TRAC-1 Shared Task on Aggression Identification : IIT(ISM)@COLING’18<span class=acl-fixed-case>TRAC</span>-1 Shared Task on Aggression Identification: <span class=acl-fixed-case>IIT</span>(<span class=acl-fixed-case>ISM</span>)@<span class=acl-fixed-case>COLING</span>’18</a></strong><br><a href=/people/r/ritesh-kumar/>Ritesh Kumar</a>
|
<a href=/people/g/guggilla-bhanodai/>Guggilla Bhanodai</a>
|
<a href=/people/r/rajendra-pamula/>Rajendra Pamula</a>
|
<a href=/people/m/maheshwar-reddy-chennuru/>Maheshwar Reddy Chennuru</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4407><div class="card-body p-3 small">This paper describes the work that our team bhanodaig did at Indian Institute of Technology (ISM) towards TRAC-1 Shared Task on Aggression Identification in Social Media for COLING 2018. In this paper we label aggression identification into three categories : Overtly Aggressive, Covertly Aggressive and Non-aggressive. We train a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to differentiate between these categories and then analyze the results in order to better understand how we can distinguish between them. We participated in two different <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> named as English (Facebook) task and English (Social Media) task. For English (Facebook) task System 05 was our best run (i.e. 0.3572) above the Random Baseline (i.e. 0.3535). For English (Social Media) task our <a href=https://en.wikipedia.org/wiki/System>system</a> 02 got the value (i.e. 0.1960) below the Random Bseline (i.e. 0.3477). For all of our runs we used Long Short-Term Memory model. Overall, our performance is not satisfactory. However, as new entrant to the field, our scores are encouraging enough to work for better results in future.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4408 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4408/>An Ensemble Approach for Aggression Identification in English and Hindi Text<span class=acl-fixed-case>E</span>nglish and <span class=acl-fixed-case>H</span>indi Text</a></strong><br><a href=/people/a/arjun-roy/>Arjun Roy</a>
|
<a href=/people/p/prashant-kapil/>Prashant Kapil</a>
|
<a href=/people/k/kingshuk-basak/>Kingshuk Basak</a>
|
<a href=/people/a/asif-ekbal/>Asif Ekbal</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4408><div class="card-body p-3 small">This paper describes our <a href=https://en.wikipedia.org/wiki/System>system</a> submitted in the shared task at COLING 2018 TRAC-1 : Aggression Identification. The objective of this task was to predict online aggression spread through online textual post or comment. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> was released in two languages, <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a>. We submitted a single <a href=https://en.wikipedia.org/wiki/System>system</a> for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and a single <a href=https://en.wikipedia.org/wiki/System>system</a> for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Both the systems are based on an <a href=https://en.wikipedia.org/wiki/Ensemble_learning>ensemble architecture</a> where the individual models are based on <a href=https://en.wikipedia.org/wiki/Convoluted_neural_network>Convoluted Neural Network</a> and <a href=https://en.wikipedia.org/wiki/Support_vector_machine>Support Vector Machine</a>. Evaluation shows promising results for both the languages. The total submission for <a href=https://en.wikipedia.org/wiki/English_language>English</a> was 30 and <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> was 15. Our system on <a href=https://en.wikipedia.org/wiki/Facebook>English facebook</a> and social media obtained F1 score of 0.5151 and 0.5099 respectively where <a href=https://en.wikipedia.org/wiki/Facebook>Hindi facebook</a> and <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> obtained F1 score of 0.5599 and 0.3790 respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4409 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4409/>Aggression Identification and Multi Lingual Word Embeddings</a></strong><br><a href=/people/t/thiago-galery/>Thiago Galery</a>
|
<a href=/people/e/efstathios-charitos/>Efstathios Charitos</a>
|
<a href=/people/y/ye-tian/>Ye Tian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4409><div class="card-body p-3 small">The system presented here took part in the 2018 Trolling, Aggression and Cyberbullying shared task (Forest and Trees team) and uses a Gated Recurrent Neural Network architecture (Cho et al., 2014) in an attempt to assess whether combining pre-trained English and Hindi fastText (Mikolov et al., 2018) word embeddings as a representation of the sequence input would improve classification performance. The motivation for this comes from the fact that the shared task data for <a href=https://en.wikipedia.org/wiki/English_language>English</a> contained many Hindi tokens and therefore some users might be doing <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a> : the alternation between two or more languages in communication. To test this hypothesis, we also aligned Hindi and English vectors using pre-computed SVD matrices that pulls representations from different languages into a common space (Smith et al., 2017). Two conditions were tested : (i) one with standard pre-trained fastText word embeddings where each <a href=https://en.wikipedia.org/wiki/Hindi>Hindi word</a> is treated as an OOV token, and (ii) another where word embeddings for <a href=https://en.wikipedia.org/wiki/Hindi>Hindi</a> and <a href=https://en.wikipedia.org/wiki/English_language>English</a> are loaded in a common vector space, so <a href=https://en.wikipedia.org/wiki/Hindi>Hindi tokens</a> can be assigned a meaningful representation. We submitted the second (i.e., multilingual) <a href=https://en.wikipedia.org/wiki/System>system</a> and obtained the scores of 0.531 <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F1</a> for the EN-FB dataset and 0.438 <a href=https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>weighted F1</a> for the EN-TW dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4410 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4410/>A K-Competitive Autoencoder for Aggression Detection in Social Media Text</a></strong><br><a href=/people/p/promita-maitra/>Promita Maitra</a>
|
<a href=/people/r/ritesh-sarkhel/>Ritesh Sarkhel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4410><div class="card-body p-3 small">We present an approach to detect <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a> from social media text in this work. A winner-takes-all autoencoder, called Emoti-KATE is proposed for this purpose. Using a log-normalized, weighted word-count vector at input dimensions, the <a href=https://en.wikipedia.org/wiki/Autoencoder>autoencoder</a> simulates a competition between neurons in the hidden layer to minimize the reconstruction loss between the input and final output layers. We have evaluated the performance of our <a href=https://en.wikipedia.org/wiki/System>system</a> on the <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> provided by the organizers of TRAC workshop, 2018. Using the <a href=https://en.wikipedia.org/wiki/Code>encoding</a> generated by Emoti-KATE, a 3-way classification is performed for every social media text in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. Each data point is classified as &#8216;Overtly Aggressive&#8217;, &#8216;Covertly Aggressive&#8217; or &#8216;Non-aggressive&#8217;. Results show that our (team name : PMRS) proposed method is able to achieve promising results on some of these <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>. In this paper, we have described the effects of introducing an winner-takes-all autoencoder for the task of aggression detection, reported its performance on four different datasets, analyzed some of its limitations and how to improve its performance in future works.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4413 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4413/>Degree based Classification of Harmful Speech using Twitter Data<span class=acl-fixed-case>T</span>witter Data</a></strong><br><a href=/people/s/sanjana-sharma/>Sanjana Sharma</a>
|
<a href=/people/s/saksham-agrawal/>Saksham Agrawal</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4413><div class="card-body p-3 small">Harmful speech has various forms and it has been plaguing the <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> in different ways. If we need to crackdown different degrees of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> and abusive behavior amongst it, the classification needs to be based on complex ramifications which needs to be defined and hold accountable for, other than racist, sexist or against some particular group and community. This paper primarily describes how we created an ontological classification of harmful speech based on degree of hateful intent and used it to annotate twitter data accordingly. The key contribution of this paper is the new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of tweets we created based on <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontological classes</a> and degrees of harmful speech found in the text. We also propose supervised classification system for recognizing these respective harmful speech classes in the texts hence. This serves as a preliminary work to lay down foundation on defining different classes of harmful speech and subsequent work will be done in making it&#8217;s automatic detection more robust and efficient.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4414 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4414" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4414/>Aggressive Language Identification Using Word Embeddings and Sentiment Features</a></strong><br><a href=/people/c/constantin-orasan/>Constantin Orăsan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4414><div class="card-body p-3 small">This paper describes our participation in the First Shared Task on Aggression Identification. The method proposed relies on <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> to identify social media texts which contain <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a>. The main <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> employed by our method are information extracted from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and the output of a <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analyser</a>. Several <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning methods</a> and different combinations of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> were tried. The official submissions used <a href=https://en.wikipedia.org/wiki/Support_vector_machine>Support Vector Machines</a> and <a href=https://en.wikipedia.org/wiki/Random_forest>Random Forests</a>. The official evaluation showed that for texts similar to the ones in the training dataset Random Forests work best, whilst for texts which are different SVMs are a better choice. The evaluation also showed that despite its simplicity the <a href=https://en.wikipedia.org/wiki/Methodology>method</a> performs well when compared with more elaborated <a href=https://en.wikipedia.org/wiki/Methodology>methods</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4415.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4415 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4415 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4415/>Aggression Detection in <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> using Deep Neural Networks</a></strong><br><a href=/people/s/sreekanth-madisetty/>Sreekanth Madisetty</a>
|
<a href=/people/m/maunendra-sankar-desarkar/>Maunendra Sankar Desarkar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4415><div class="card-body p-3 small">With the rise of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a> in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> coupled with almost non-existent <a href=https://en.wikipedia.org/wiki/Moderation_system>moderation</a> in many such systems, aggressive contents have been observed to rise in such <a href=https://en.wikipedia.org/wiki/Internet_forum>forums</a>. In this paper, we work on the problem of aggression detection in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. Aggression can sometimes be expressed directly or overtly or it can be hidden or covert in the text. On the other hand, most of the content in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is non-aggressive in nature. We propose an ensemble based system to classify an input post to into one of three classes, namely, Overtly Aggressive, Covertly Aggressive, and Non-aggressive. Our approach uses three deep learning methods, namely, Convolutional Neural Networks (CNN) with five layers (input, convolution, pooling, hidden, and output), Long Short Term Memory networks (LSTM), and Bi-directional Long Short Term Memory networks (Bi-LSTM). A majority voting based ensemble method is used to combine these classifiers (CNN, LSTM, and Bi-LSTM). We trained our method on Facebook comments dataset and tested on Facebook comments (in-domain) and other social media posts (cross-domain). Our system achieves the F1-score (weighted) of 0.604 for <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>Facebook posts</a> and 0.508 for <a href=https://en.wikipedia.org/wiki/List_of_Facebook_features>social media posts</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4416.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4416 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4416 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4416/>Merging Datasets for Aggressive Text Identification</a></strong><br><a href=/people/p/paula-fortuna/>Paula Fortuna</a>
|
<a href=/people/j/jose-ferreira/>José Ferreira</a>
|
<a href=/people/l/luiz-pires/>Luiz Pires</a>
|
<a href=/people/g/guilherme-routar/>Guilherme Routar</a>
|
<a href=/people/s/sergio-nunes/>Sérgio Nunes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4416><div class="card-body p-3 small">This paper presents the approach of the team groutar to the shared task on Aggression Identification, considering the test sets in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, both from <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and general Social Media. This experiment aims to test the effect of merging new <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in the performance of <a href=https://en.wikipedia.org/wiki/Statistical_model>classification models</a>. We followed a standard machine learning approach with training, validation, and testing phases, and considered features such as part-of-speech, frequencies of insults, <a href=https://en.wikipedia.org/wiki/Punctuation>punctuation</a>, <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment</a>, and <a href=https://en.wikipedia.org/wiki/Capitalization>capitalization</a>. In terms of algorithms, we experimented with Boosted Logistic Regression, Multi-Layer Perceptron, Parallel Random Forest and eXtreme Gradient Boosting. One question appearing was how to merge datasets using different classification systems (e.g. aggression vs. toxicity). Other issue concerns the possibility to generalize <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> and apply <a href=https://en.wikipedia.org/wiki/Mathematical_model>them</a> to data from different <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. Regarding these, we merged two datasets, and the results showed that training with similar data is an advantage in the classification of social networks data. However, adding data from different platforms, allowed slightly better results in both <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a>, indicating that more generalized models can be an advantage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4417.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4417 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4417 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4417/>Cyberbullying Detection Task : the EBSI-LIA-UNAM System (ELU) at COLING’18 TRAC-1<span class=acl-fixed-case>EBSI</span>-<span class=acl-fixed-case>LIA</span>-<span class=acl-fixed-case>UNAM</span> System (<span class=acl-fixed-case>ELU</span>) at <span class=acl-fixed-case>COLING</span>’18 <span class=acl-fixed-case>TRAC</span>-1</a></strong><br><a href=/people/i/ignacio-arroyo-fernandez/>Ignacio Arroyo-Fernández</a>
|
<a href=/people/d/dominic-forest/>Dominic Forest</a>
|
<a href=/people/j/juan-manuel-torres-moreno/>Juan-Manuel Torres-Moreno</a>
|
<a href=/people/m/mauricio-carrasco-ruiz/>Mauricio Carrasco-Ruiz</a>
|
<a href=/people/t/thomas-legeleux/>Thomas Legeleux</a>
|
<a href=/people/k/karen-joannette/>Karen Joannette</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4417><div class="card-body p-3 small">The phenomenon of <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> has growing in worrying proportions with the development of <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. Forums and <a href=https://en.wikipedia.org/wiki/Chat_room>chat rooms</a> are spaces where serious damage can now be done to others, while the tools for avoiding on-line spills are still limited. This study aims to assess the ability that both classical and state-of-the-art vector space modeling methods provide to well known <a href=https://en.wikipedia.org/wiki/Machine_learning>learning machines</a> to identify aggression levels in social network cyberbullying (i.e. social network posts manually labeled as Overtly Aggressive, Covertly Aggressive and Non-aggressive). To this end, an exploratory stage was performed first in order to find relevant settings to test, i.e. by using training and development samples, we trained multiple learning machines using multiple vector space modeling methods and discarded the less informative configurations. Finally, we selected the two best settings and their <a href=https://en.wikipedia.org/wiki/Electoral_system>voting combination</a> to form three competing <a href=https://en.wikipedia.org/wiki/Electoral_system>systems</a>. These systems were submitted to the competition of the TRACK-1 task of the Workshop on Trolling, Aggression and Cyberbullying. Our voting combination system resulted second place in predicting Aggression levels on a test set of untagged social network posts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4418.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4418 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4418 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W18-4418" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W18-4418/>Aggression Identification Using <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> and Data Augmentation</a></strong><br><a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4418><div class="card-body p-3 small">Social media platforms allow users to share and discuss their opinions online. However, a minority of user posts is aggressive, thereby hinders respectful discussion, and at an extreme level is liable to prosecution. The automatic identification of such harmful posts is important, because <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> can support the costly manual moderation of online discussions. Further, the <a href=https://en.wikipedia.org/wiki/Automation>automation</a> allows unprecedented analyses of discussion datasets that contain millions of posts. This system description paper presents our submission to the First Shared Task on Aggression Identification. We propose to augment the provided <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> to increase the number of labeled comments from 15,000 to 60,000. Thereby, we introduce <a href=https://en.wikipedia.org/wiki/Variety_(linguistics)>linguistic variety</a> into the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>. As a consequence of the larger amount of training data, we are able to train a special <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural net</a>, which generalizes especially well to unseen data. To further boost the performance, we combine this <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural net</a> with three logistic regression classifiers trained on character and word n-grams, and hand-picked syntactic features. This <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> is more robust than the individual single models. Our team named Julian achieves an F1-score of 60 % on both English datasets, 63 % on the Hindi Facebook dataset, and 38 % on the Hindi Twitter dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4419.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4419 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4419 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4419/>Cyber-aggression Detection using Cross Segment-and-Concatenate Multi-Task Learning from Text</a></strong><br><a href=/people/a/ahmed-husseini-orabi/>Ahmed Husseini Orabi</a>
|
<a href=/people/m/mahmoud-husseini-orabi/>Mahmoud Husseini Orabi</a>
|
<a href=/people/q/qianjia-huang/>Qianjia Huang</a>
|
<a href=/people/d/diana-inkpen/>Diana Inkpen</a>
|
<a href=/people/d/david-van-bruwaene/>David Van Bruwaene</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4419><div class="card-body p-3 small">In this paper, we propose a novel deep-learning architecture for text classification, named cross segment-and-concatenate multi-task learning (CSC-MTL). We use CSC-MTL to improve the performance of cyber-aggression detection from text. Our approach provides a robust shared feature representation for <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> by detecting contrasts and similarities among polarity and neutral classes. We participated in the cyber-aggression shared task under the team name uOttawa. We report 59.74 % F1 performance for the Facebook test set and 56.9 % for the Twitter test set, for detecting <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a> from text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4420.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4420 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4420 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4420/>Delete or not Delete? Semi-Automatic Comment Moderation for the Newsroom</a></strong><br><a href=/people/j/julian-risch/>Julian Risch</a>
|
<a href=/people/r/ralf-krestel/>Ralf Krestel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4420><div class="card-body p-3 small">Comment sections of online news providers have enabled millions to share and discuss their opinions on news topics. Today, moderators ensure respectful and informative discussions by deleting not only insults, <a href=https://en.wikipedia.org/wiki/Defamation>defamation</a>, and hate speech, but also unverifiable facts. This process has to be transparent and comprehensive in order to keep the community engaged. Further, news providers have to make sure to not give the impression of <a href=https://en.wikipedia.org/wiki/Censorship>censorship</a> or dissemination of fake news. Yet <a href=https://en.wikipedia.org/wiki/Moderation_system>manual moderation</a> is very expensive and becomes more and more unfeasible with the increasing amount of comments. Hence, we propose a semi-automatic, holistic approach, which includes comment features but also their context, such as information about users and articles. For evaluation, we present experiments on a novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 3 million news comments annotated by a team of professional moderators.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4421.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4421 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4421 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4421/>Textual Aggression Detection through <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a></a></strong><br><a href=/people/a/antonela-tommasel/>Antonela Tommasel</a>
|
<a href=/people/j/juan-manuel-rodriguez/>Juan Manuel Rodriguez</a>
|
<a href=/people/d/daniela-godoy/>Daniela Godoy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4421><div class="card-body p-3 small">Cyberbullying and cyberaggression are serious and widespread issues increasingly affecting Internet users. With the widespread of <a href=https://en.wikipedia.org/wiki/Social_media>social media networks</a>, <a href=https://en.wikipedia.org/wiki/Bullying>bullying</a>, once limited to particular places, can now occur anytime and anywhere. Cyberaggression refers to aggressive online behaviour that aims at harming other individuals, and involves rude, insulting, offensive, teasing or demoralising comments through <a href=https://en.wikipedia.org/wiki/Social_media>online social media</a>. Considering the dangerous consequences that cyberaggression has on its victims and its rapid spread amongst internet users (specially kids and teens), it is crucial to understand how <a href=https://en.wikipedia.org/wiki/Cyberbullying>cyberbullying</a> occurs to prevent <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> from escalating. Given the massive information overload on the <a href=https://en.wikipedia.org/wiki/World_Wide_Web>Web</a>, there is an imperious need to develop intelligent techniques to automatically detect harmful content, which would allow the large-scale social media monitoring and early detection of undesired situations. This paper presents the Isistanitos&#8217;s approach for detecting aggressive content in multiple social media sites. The approach is based on combining Support Vector Machines and Recurrent Neural Network models for analysing a wide-range of <a href=https://en.wikipedia.org/wiki/Character_(symbol)>character</a>, <a href=https://en.wikipedia.org/wiki/Word>word</a>, word embeddings, sentiment and irony features. Results confirmed the difficulty of the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> (particularly for detecting covert aggressions), showing the limitations of traditionally used <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4422.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4422 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4422 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4422/>Combining Shallow and Deep Learning for Aggressive Text Detection</a></strong><br><a href=/people/v/viktor-golem/>Viktor Golem</a>
|
<a href=/people/m/mladen-karan/>Mladen Karan</a>
|
<a href=/people/j/jan-snajder/>Jan Šnajder</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4422><div class="card-body p-3 small">We describe the participation of team TakeLab in the aggression detection shared task at the TRAC1 workshop for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Aggression manifests in a variety of ways. Unlike some forms of <a href=https://en.wikipedia.org/wiki/Aggression>aggression</a> that are impossible to prevent in day-to-day life, aggressive speech abounding on <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a> could in principle be prevented or at least reduced by simply disabling users that post aggressively worded messages. The first step in achieving this is to detect such <a href=https://en.wikipedia.org/wiki/Message>messages</a>. The task, however, is far from being trivial, as what is considered as aggressive speech can be quite subjective, and the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> is further complicated by the noisy nature of <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated text</a> on <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>social networks</a>. Our system learns to distinguish between <a href=https://en.wikipedia.org/wiki/Aggression>open aggression</a>, <a href=https://en.wikipedia.org/wiki/Non-aggression_principle>covert aggression</a>, and <a href=https://en.wikipedia.org/wiki/Non-aggression_principle>non-aggression</a> in social media texts. We tried different machine learning approaches, including traditional (shallow) machine learning models, <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning models</a>, and a combination of both. We achieved respectable results, ranking 4th and 8th out of 31 submissions on the Facebook and Twitter test sets, respectively.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W18-4423.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W18-4423 data-toggle=collapse aria-expanded=false aria-controls=abstract-W18-4423 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W18-4423/>Filtering Aggression from the Multilingual Social Media Feed</a></strong><br><a href=/people/s/sandip-modha/>Sandip Modha</a>
|
<a href=/people/p/prasenjit-majumder/>Prasenjit Majumder</a>
|
<a href=/people/t/thomas-mandl/>Thomas Mandl</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W18-4423><div class="card-body p-3 small">This paper describes the participation of team DA-LD-Hildesheim from the Information Retrieval Lab(IRLAB) at DA-IICT Gandhinagar, India in collaboration with the University of Hildesheim, Germany and LDRP-ITR, Gandhinagar, India in a shared task on Aggression Identification workshop in COLING 2018. The objective of the shared task is to identify the level of aggression from the User-Generated contents within <a href=https://en.wikipedia.org/wiki/Social_media>Social media</a> written in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Devanagari>Devnagiri Hindi</a> and Romanized Hindi. Aggression levels are categorized into three predefined classes namely : &#8216;Overtly Aggressive &#8216;, &#8216;Covertly Aggressive &#8216;and &#8216;Non-aggressive &#8216;. The participating teams are required to develop a multi-class classifier which classifies <a href=https://en.wikipedia.org/wiki/User-generated_content>User-generated content</a> into these pre-defined classes. Instead of relying on a <a href=https://en.wikipedia.org/wiki/Bag-of-words_model>bag-of-words model</a>, we have used pre-trained vectors for <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a>. We have performed experiments with standard <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning classifiers</a>. In addition, we have developed various deep learning models for the multi-class classification problem. Using the <a href=https://en.wikipedia.org/wiki/Data_validation>validation data</a>, we found that validation accuracy of our deep learning models outperform all standard <a href=https://en.wikipedia.org/wiki/Statistical_classification>machine learning classifiers</a> and voting based ensemble techniques and results on <a href=https://en.wikipedia.org/wiki/Test_data>test data</a> support these findings. We have also found that <a href=https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)>hyper-parameters</a> of the <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> are the keys to improve the results.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>