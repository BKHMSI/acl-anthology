<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Fourth Workshop on Online Abuse and Harms - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title>Proceedings of the Fourth Workshop on Online Abuse and Harms</h2><p class=lead><a href=/people/s/seyi-akiwowo/>Seyi Akiwowo</a>,
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>,
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>,
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2020.alw-1</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2020</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/alw/>ALW</a>
| <a href=/venues/emnlp/>EMNLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2020.alw-1>https://aclanthology.org/2020.alw-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Fourth+Workshop+on+Online+Abuse+and+Harms" title="Search for 'Proceedings of the Fourth Workshop on Online Abuse and Harms' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2020.alw-1.0/>Proceedings of the Fourth Workshop on Online Abuse and Harms</a></strong><br><a href=/people/s/seyi-akiwowo/>Seyi Akiwowo</a>
|
<a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/v/vinodkumar-prabhakaran/>Vinodkumar Prabhakaran</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.4.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--4 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.4 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939522 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.4/>Fine-tuning for multi-domain and multi-label uncivil language detection</a></strong><br><a href=/people/k/kadir-bulut-ozler/>Kadir Bulut Ozler</a>
|
<a href=/people/k/kate-kenski/>Kate Kenski</a>
|
<a href=/people/s/steve-rains/>Steve Rains</a>
|
<a href=/people/y/yotam-shmargad/>Yotam Shmargad</a>
|
<a href=/people/k/kevin-coe/>Kevin Coe</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--4><div class="card-body p-3 small">Incivility is a problem on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>, and it comes in many forms (name-calling, <a href=https://en.wikipedia.org/wiki/Vulgarity>vulgarity</a>, <a href=https://en.wikipedia.org/wiki/Threat>threats</a>, etc.) and domains (microblog posts, <a href=https://en.wikipedia.org/wiki/Online_newspaper>online news comments</a>, <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia edits</a>, etc.). Training <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> to detect such <a href=https://en.wikipedia.org/wiki/Incivility>incivility</a> must handle the multi-label and multi-domain nature of the problem. We present a BERT-based model for incivility detection and propose several approaches for training it for multi-label and multi-domain datasets. We find that individual binary classifiers outperform a joint multi-label classifier, and that simply combining multiple domains of training data outperforms other recently-proposed fine tuning strategies. We also establish new state-of-the-art performance on several incivility detection datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939530 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.5/>HurtBERT : Incorporating Lexical Features with BERT for the Detection of Abusive Language<span class=acl-fixed-case>H</span>urt<span class=acl-fixed-case>BERT</span>: Incorporating Lexical Features with <span class=acl-fixed-case>BERT</span> for the Detection of Abusive Language</a></strong><br><a href=/people/a/anna-koufakou/>Anna Koufakou</a>
|
<a href=/people/e/endang-wahyu-pamungkas/>Endang Wahyu Pamungkas</a>
|
<a href=/people/v/valerio-basile/>Valerio Basile</a>
|
<a href=/people/v/viviana-patti/>Viviana Patti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--5><div class="card-body p-3 small">The detection of abusive or offensive remarks in <a href=https://en.wikipedia.org/wiki/Social_text>social texts</a> has received significant attention in research. In several related <a href=https://en.wikipedia.org/wiki/Task_(computing)>shared tasks</a>, BERT has been shown to be the state-of-the-art. In this paper, we propose to utilize lexical features derived from a hate lexicon towards improving the performance of BERT in such tasks. We explore different ways to utilize the <a href=https://en.wikipedia.org/wiki/Lexicon>lexical features</a> in the form of lexicon-based encodings at the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level</a> or embeddings at the <a href=https://en.wikipedia.org/wiki/Word_(linguistics)>word level</a>. We provide an extensive dataset evaluation that addresses in-domain as well as cross-domain detection of abusive content to render a complete picture. Our results indicate that our proposed models combining BERT with lexical features help improve over a baseline BERT model in many of our in-domain and cross-domain experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939534 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.10/>Attending the Emotions to Detect Online Abusive Language</a></strong><br><a href=/people/n/niloofar-safi-samghabadi/>Niloofar Safi Samghabadi</a>
|
<a href=/people/a/afsheen-hatami/>Afsheen Hatami</a>
|
<a href=/people/m/mahsa-shafaei/>Mahsa Shafaei</a>
|
<a href=/people/s/sudipta-kar/>Sudipta Kar</a>
|
<a href=/people/t/thamar-solorio/>Thamar Solorio</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--10><div class="card-body p-3 small">In recent years, <a href=https://en.wikipedia.org/wiki/Abuse>abusive behavior</a> has become a serious issue in <a href=https://en.wikipedia.org/wiki/List_of_social_networking_websites>online social networks</a>. In this paper, we present a new <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for the task of abusive language detection that is collected from a semi-anonymous online platform, and unlike the majority of other available resources, is not created based on a specific list of bad words. We also develop <a href=https://en.wikipedia.org/wiki/Computational_model>computational models</a> to incorporate <a href=https://en.wikipedia.org/wiki/Emotion>emotions</a> into <a href=https://en.wikipedia.org/wiki/Sensory_cue>textual cues</a> to improve <a href=https://en.wikipedia.org/wiki/Aggression>aggression identification</a>. We evaluate our proposed <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> on a set of corpora related to the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> and show promising results with respect to abusive language detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.13.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--13 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.13 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.alw-1.13.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939518 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.13/>Countering hate on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> : Large scale classification of hate and counter speech</a></strong><br><a href=/people/j/joshua-garland/>Joshua Garland</a>
|
<a href=/people/k/keyan-ghazi-zahedi/>Keyan Ghazi-Zahedi</a>
|
<a href=/people/j/jean-gabriel-young/>Jean-Gabriel Young</a>
|
<a href=/people/l/laurent-hebert-dufresne/>Laurent Hébert-Dufresne</a>
|
<a href=/people/m/mirta-galesic/>Mirta Galesic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--13><div class="card-body p-3 small">Hateful rhetoric is plaguing <a href=https://en.wikipedia.org/wiki/Online_discourse>online discourse</a>, fostering <a href=https://en.wikipedia.org/wiki/Extremism>extreme societal movements</a> and possibly giving rise to <a href=https://en.wikipedia.org/wiki/Violence>real-world violence</a>. A potential solution to this growing global problem is citizen-generated counter speech where citizens actively engage with <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> to restore <a href=https://en.wikipedia.org/wiki/Civil_discourse>civil non-polarized discourse</a>. However, its actual effectiveness in curbing the spread of <a href=https://en.wikipedia.org/wiki/Hatred>hatred</a> is unknown and hard to quantify. One major obstacle to researching this question is a lack of large labeled data sets for training <a href=https://en.wikipedia.org/wiki/Statistical_classification>automated classifiers</a> to identify counter speech. Here we use a unique situation in <a href=https://en.wikipedia.org/wiki/Germany>Germany</a> where self-labeling groups engaged in organized online hate and counter speech. We use an ensemble learning algorithm which pairs a variety of paragraph embeddings with regularized logistic regression functions to classify both hate and counter speech in a corpus of millions of relevant tweets from these two groups. Our <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a> achieves macro F1 scores on out of sample balanced test sets ranging from 0.76 to 0.97accuracy in line and even exceeding the state of the art. We then use the classifier to discover hate and counter speech in more than 135,000 fully-resolved Twitter conversations occurring from 2013 to 2018 and study their frequency and interaction. Altogether, our results highlight the potential of automated methods to evaluate the impact of coordinated counter speech in stabilizing conversations on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939516 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.14/>Moderating Our (Dis)Content : Renewing the Regulatory Approach</a></strong><br><a href=/people/c/claire-pershan/>Claire Pershan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--14><div class="card-body p-3 small">As online platforms become central to our democracies, the problem of toxic content threatens the free flow of information and the enjoyment of fundamental rights. But effective policy response to toxic content must grasp the idiosyncrasies and interconnectedness of <a href=https://en.wikipedia.org/wiki/Content-control_software>content moderation</a> across a fragmented online landscape. This report urges regulators and legislators to consider a range of <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a> and moderation approaches in the <a href=https://en.wikipedia.org/wiki/Regulation>regulation</a>. In particular, it calls for a holistic, process-oriented regulatory approach that accounts for actors beyond the handful of dominant platforms that currently shape public debate.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.alw-1.19.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939526 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.19/>Detecting East Asian Prejudice on Social Media<span class=acl-fixed-case>E</span>ast <span class=acl-fixed-case>A</span>sian Prejudice on Social Media</a></strong><br><a href=/people/b/bertie-vidgen/>Bertie Vidgen</a>
|
<a href=/people/s/scott-hale/>Scott Hale</a>
|
<a href=/people/e/ella-guest/>Ella Guest</a>
|
<a href=/people/h/helen-margetts/>Helen Margetts</a>
|
<a href=/people/d/david-broniatowski/>David Broniatowski</a>
|
<a href=/people/z/zeerak-waseem/>Zeerak Waseem</a>
|
<a href=/people/a/austin-botelho/>Austin Botelho</a>
|
<a href=/people/m/matthew-hall/>Matthew Hall</a>
|
<a href=/people/r/rebekah-tromble/>Rebekah Tromble</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--19><div class="card-body p-3 small">During COVID-19 concerns have heightened about the spread of aggressive and hateful language online, especially hostility directed against East Asia and East Asian people. We report on a new dataset and the creation of a machine learning classifier that categorizes social media posts from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> into four classes : Hostility against East Asia, Criticism of East Asia, Meta-discussions of East Asian prejudice, and a neutral class. The <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> achieves a macro-F1 score of 0.83. We then conduct an in-depth ground-up error analysis and show that the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> struggles with <a href=https://en.wikipedia.org/wiki/Edge_case>edge cases</a> and <a href=https://en.wikipedia.org/wiki/Ambiguity>ambiguous content</a>. We provide the 20,000 tweet training dataset (annotated by experienced analysts), which also contains several secondary categories and additional flags. We also provide the 40,000 original annotations (before adjudication), the full codebook, annotations for COVID-19 relevance and East Asian relevance and stance for 1,000 hashtags, and the final model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.20.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--20 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.20 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.alw-1.20.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939537 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.20/>On Cross-Dataset Generalization in Automatic Detection of Online Abuse</a></strong><br><a href=/people/i/isar-nejadgholi/>Isar Nejadgholi</a>
|
<a href=/people/s/svetlana-kiritchenko/>Svetlana Kiritchenko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--20><div class="card-body p-3 small">NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and test datasets</a> are usually obtained from similar data samples, in practice systems are often applied on data that are different from the training set in topic and class distributions. Also, the ambiguity in class definitions inherited in this <a href=https://en.wikipedia.org/wiki/Task_(computing)>task</a> aggravates the discrepancies between source and target datasets. We explore the topic bias and the task formulation bias in cross-dataset generalization. We show that the benign examples in the Wikipedia Detox dataset are biased towards platform-specific topics. We identify these examples using unsupervised topic modeling and manual inspection of topics&#8217; keywords. Removing these topics increases cross-dataset generalization, without reducing in-domain classification performance. For a robust dataset design, we suggest applying inexpensive <a href=https://en.wikipedia.org/wiki/Unsupervised_learning>unsupervised methods</a> to inspect the collected data and downsize the non-generalizable content before manually annotating for class labels.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2020.alw-1.22.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2020--alw-1--22 data-toggle=collapse aria-expanded=false aria-controls=abstract-2020.alw-1.22 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2020.alw-1.22.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://slideslive.com/38939539 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2020.alw-1.22" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span>
<span class=d-block><strong><a class=align-middle href=/2020.alw-1.22/>Investigating Annotator Bias with a Graph-Based Approach</a></strong><br><a href=/people/m/maximilian-wich/>Maximilian Wich</a>
|
<a href=/people/h/hala-al-kuwatly/>Hala Al Kuwatly</a>
|
<a href=/people/g/georg-groh/>Georg Groh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2020--alw-1--22><div class="card-body p-3 small">A challenge that many online platforms face is <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> or any other form of online abuse. To cope with this, hate speech detection systems are developed based on <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> to reduce manual work for monitoring these <a href=https://en.wikipedia.org/wiki/Computing_platform>platforms</a>. Unfortunately, <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> is vulnerable to unintended bias in training data, which could have severe consequences, such as a decrease in <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification performance</a> or unfair behavior (e.g., discriminating minorities). In the scope of this study, we want to investigate annotator bias a form of <a href=https://en.wikipedia.org/wiki/Bias>bias</a> that annotators cause due to different knowledge in regards to the task and their subjective perception. Our goal is to identify annotation bias based on similarities in the <a href=https://en.wikipedia.org/wiki/Annotation>annotation behavior</a> from annotators. To do so, we build a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a> based on the annotations from the different annotators, apply a community detection algorithm to group the annotators, and train for each group classifiers whose performances we compare. By doing so, we are able to identify annotator bias within a <a href=https://en.wikipedia.org/wiki/Data_set>data set</a>. The proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> and collected insights can contribute to developing fairer and more reliable hate speech classification models.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>