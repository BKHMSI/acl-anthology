<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/I17-1.pdf>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></h2><p class=lead><a href=/people/g/greg-kondrak/>Greg Kondrak</a>,
<a href=/people/t/taro-watanabe/>Taro Watanabe</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>I17-1</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Taipei, Taiwan</dd><dt>Venue:</dt><dd><a href=/venues/ijcnlp/>IJCNLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Asian Federation of Natural Language Processing</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/I17-1>https://aclanthology.org/I17-1</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/I17-1.pdf>https://aclanthology.org/I17-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/I17-1.pdf title="Open PDF of 'Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+Eighth+International+Joint+Conference+on+Natural+Language+Processing+%28Volume+1%3A+Long+Papers%29" title="Search for 'Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1000/>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</a></strong><br><a href=/people/g/greg-kondrak/>Greg Kondrak</a>
|
<a href=/people/t/taro-watanabe/>Taro Watanabe</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1001/>Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks</a></strong><br><a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/l/lluis-marquez/>Lluís Màrquez</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/j/james-glass/>James Glass</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1001><div class="card-body p-3 small">While neural machine translation (NMT) models provide improved translation quality in an elegant framework, it is less clear what they learn about language. Recent work has started evaluating the quality of <a href=https://en.wikipedia.org/wiki/Vector_space>vector representations</a> learned by NMT models on morphological and syntactic tasks. In this paper, we investigate the <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a> learned at different layers of NMT encoders. We train NMT systems on parallel data and use the models to extract <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> for training a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> on two tasks : part-of-speech and semantic tagging. We then measure the performance of the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> as a proxy to the quality of the original NMT model for the given task. Our quantitative analysis yields interesting insights regarding <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> in NMT models. For instance, we find that higher layers are better at learning <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> while lower layers tend to be better for <a href=https://en.wikipedia.org/wiki/Part-of-speech_tagging>part-of-speech tagging</a>. We also observe little effect of the target language on source-side representations, especially in higher quality models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1002/>Context-Aware Smoothing for <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a></a></strong><br><a href=/people/k/kehai-chen/>Kehai Chen</a>
|
<a href=/people/r/rui-wang/>Rui Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a>
|
<a href=/people/t/tiejun-zhao/>Tiejun Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1002><div class="card-body p-3 small">In Neural Machine Translation (NMT), each word is represented as a low-dimension, real-value vector for encoding its syntax and semantic information. This means that even if the word is in a different sentence context, it is represented as the fixed vector to learn source representation. Moreover, a large number of Out-Of-Vocabulary (OOV) words, which have different syntax and semantic information, are represented as the same vector representation of unk. To alleviate this problem, we propose a novel context-aware smoothing method to dynamically learn a sentence-specific vector for each word (including OOV words) depending on its local context words in a sentence. The learned context-aware representation is integrated into the NMT to improve the <a href=https://en.wikipedia.org/wiki/Translation_(biology)>translation</a> performance. Empirical results on NIST Chinese-to-English translation task show that the proposed <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>approach</a> achieves 1.78 BLEU improvements on average over a strong attentional NMT, and outperforms some existing <a href=https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations>systems</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1004/>What does Attention in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> Pay Attention to?</a></strong><br><a href=/people/h/hamidreza-ghader/>Hamidreza Ghader</a>
|
<a href=/people/c/christof-monz/>Christof Monz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1004><div class="card-body p-3 small">Attention in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> provides the possibility to encode relevant parts of the source sentence at each translation step. As a result, <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is considered to be an alignment model as well. However, there is no work that specifically studies <a href=https://en.wikipedia.org/wiki/Attention>attention</a> and provides analysis of what is being learned by <a href=https://en.wikipedia.org/wiki/Attention>attention models</a>. Thus, the question still remains that how <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is similar or different from the traditional alignment. In this paper, we provide detailed analysis of <a href=https://en.wikipedia.org/wiki/Attention>attention</a> and compare it to traditional alignment. We answer the question of whether <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is only capable of modelling translational equivalent or it captures more information. We show that <a href=https://en.wikipedia.org/wiki/Attention>attention</a> is different from alignment in some cases and is capturing useful information other than alignments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1007/>Neural Probabilistic Model for Non-projective MST Parsing<span class=acl-fixed-case>MST</span> Parsing</a></strong><br><a href=/people/x/xuezhe-ma/>Xuezhe Ma</a>
|
<a href=/people/e/eduard-hovy/>Eduard Hovy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1007><div class="card-body p-3 small">In this paper, we propose a probabilistic parsing model that defines a proper <a href=https://en.wikipedia.org/wiki/Conditional_probability_distribution>conditional probability distribution</a> over non-projective dependency trees for a given sentence, using neural representations as inputs. The neural network architecture is based on bi-directional LSTMCNNs, which automatically benefits from both word- and character-level representations, by using a combination of bidirectional LSTMs and CNNs. On top of the <a href=https://en.wikipedia.org/wiki/Neural_network>neural network</a>, we introduce a probabilistic structured layer, defining a conditional log-linear model over non-projective trees. By exploiting Kirchhoff&#8217;s Matrix-Tree Theorem (Tutte, 1984), the partition functions and marginals can be computed efficiently, leading to a straightforward end-to-end model training procedure via <a href=https://en.wikipedia.org/wiki/Backpropagation>back-propagation</a>. We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on 17 different datasets, across 14 different languages. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> achieves state-of-the-art <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> performance on nine datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1009" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1009/>MIPA : Mutual Information Based Paraphrase Acquisition via Bilingual Pivoting<span class=acl-fixed-case>MIPA</span>: Mutual Information Based Paraphrase Acquisition via Bilingual Pivoting</a></strong><br><a href=/people/t/tomoyuki-kajiwara/>Tomoyuki Kajiwara</a>
|
<a href=/people/m/mamoru-komachi/>Mamoru Komachi</a>
|
<a href=/people/d/daichi-mochihashi/>Daichi Mochihashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1009><div class="card-body p-3 small">We present a pointwise mutual information (PMI)-based approach to formalize paraphrasability and propose a variant of PMI, called MIPA, for the paraphrase acquisition. Our paraphrase acquisition method first acquires lexical paraphrase pairs by bilingual pivoting and then reranks them by PMI and distributional similarity. The complementary nature of information from bilingual corpora and from monolingual corpora makes the proposed method robust. Experimental results show that the proposed method substantially outperforms bilingual pivoting and distributional similarity themselves in terms of metrics such as MRR, MAP, coverage, and Spearman&#8217;s correlation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1010/>Improving Implicit Semantic Role Labeling by Predicting Semantic Frame Arguments</a></strong><br><a href=/people/q/quynh-ngoc-thi-do/>Quynh Ngoc Thi Do</a>
|
<a href=/people/s/steven-bethard/>Steven Bethard</a>
|
<a href=/people/m/marie-francine-moens/>Marie-Francine Moens</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1010><div class="card-body p-3 small">Implicit semantic role labeling (iSRL) is the task of predicting the semantic roles of a predicate that do not appear as explicit arguments, but rather regard common sense knowledge or are mentioned earlier in the <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a>. We introduce an approach to iSRL based on a predictive recurrent neural semantic frame model (PRNSFM) that uses a large unannotated corpus to learn the probability of a sequence of semantic arguments given a predicate. We leverage the sequence probabilities predicted by the PRNSFM to estimate selectional preferences for predicates and their arguments. On the NomBank iSRL test set, our approach improves state-of-the-art performance on implicit semantic role labeling with less reliance than prior work on manually constructed language resources.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1012/>Enabling Transitivity for <a href=https://en.wikipedia.org/wiki/Lexical_analysis>Lexical Inference</a> on <a href=https://en.wikipedia.org/wiki/Varieties_of_Chinese>Chinese Verbs</a> Using Probabilistic Soft Logic<span class=acl-fixed-case>C</span>hinese Verbs Using Probabilistic Soft Logic</a></strong><br><a href=/people/w/wei-chung-wang/>Wei-Chung Wang</a>
|
<a href=/people/l/lun-wei-ku/>Lun-Wei Ku</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1012><div class="card-body p-3 small">To learn more knowledge, enabling transitivity is a vital step for lexical inference. However, most of the lexical inference models with good performance are for <a href=https://en.wikipedia.org/wiki/Noun>nouns</a> or <a href=https://en.wikipedia.org/wiki/Noun_phrase>noun phrases</a>, which can not be directly applied to the <a href=https://en.wikipedia.org/wiki/Inference>inference</a> on events or states. In this paper, we construct the largest Chinese verb lexical inference dataset containing 18,029 verb pairs, where for each pair one of four inference relations are annotated. We further build a probabilistic soft logic (PSL) model to infer verb lexicons using the <a href=https://en.wikipedia.org/wiki/Logic_language>logic language</a>. With PSL, we easily enable transitivity in two layers, the observed layer and the feature layer, which are included in the <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a>. We further discuss the effect of <a href=https://en.wikipedia.org/wiki/Transient_(oscillation)>transitives</a> within and between these <a href=https://en.wikipedia.org/wiki/Lattice_model_(physics)>layers</a>. Results show the performance of the proposed <a href=https://en.wikipedia.org/wiki/Partial_differential_equation>PSL model</a> can be improved at least 3.5 % (relative) when the transitivity is enabled. Furthermore, experiments show that enabling transitivity in the observed layer benefits the most.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1013 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1013/>An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing</a></strong><br><a href=/people/m/marcin-junczys-dowmunt/>Marcin Junczys-Dowmunt</a>
|
<a href=/people/r/roman-grundkiewicz/>Roman Grundkiewicz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1013><div class="card-body p-3 small">In this work, we explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling pe directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT-2016 shared task on automatic post-editing and can demonstrate that dual-attention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Dual-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input.<tex-math>mt</tex-math> (raw MT output) and <tex-math>src</tex-math> (source language input) in a single neural architecture, modeling <tex-math>\\{mt, src\\} \\rightarrow pe</tex-math> directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT-2016 shared task on automatic post-editing and can demonstrate that dual-attention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Dual-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1014 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1014/>Imagination Improves Multimodal Translation</a></strong><br><a href=/people/d/desmond-elliott/>Desmond Elliott</a>
|
<a href=/people/a/akos-kadar/>Ákos Kádár</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1014><div class="card-body p-3 small">We decompose multimodal translation into two sub-tasks : learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoder-decoder, and grounded representations are learned through image representation prediction. Our approach improves <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance compared to the state of the art on the Multi30 K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1015/>Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder</a></strong><br><a href=/people/f/fahim-dalvi/>Fahim Dalvi</a>
|
<a href=/people/n/nadir-durrani/>Nadir Durrani</a>
|
<a href=/people/h/hassan-sajjad/>Hassan Sajjad</a>
|
<a href=/people/y/yonatan-belinkov/>Yonatan Belinkov</a>
|
<a href=/people/s/stephan-vogel/>Stephan Vogel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1015><div class="card-body p-3 small">End-to-end training makes the neural machine translation (NMT) architecture simpler, yet elegant compared to traditional statistical machine translation (SMT). However, little is known about linguistic patterns of morphology, <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a> and <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a> learned during the training of NMT systems, and more importantly, which parts of the <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> are responsible for learning each of these phenomenon. In this paper we i) analyze how much <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> an NMT decoder learns, and ii) investigate whether injecting target morphology in the <a href=https://en.wikipedia.org/wiki/Code>decoder</a> helps it to produce better translations. To this end we present three methods : i) <a href=https://en.wikipedia.org/wiki/Simultaneous_translation>simultaneous translation</a>, ii) joint-data learning, and iii) <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a>. Our results show that explicit <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological information</a> helps the <a href=https://en.wikipedia.org/wiki/Machine_learning>decoder</a> learn target language morphology and improves the translation quality by 0.20.6 <a href=https://en.wikipedia.org/wiki/Linguistic_description>BLEU points</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1016/>Improving <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> through Phrase-based Forced Decoding</a></strong><br><a href=/people/j/jingyi-zhang/>Jingyi Zhang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichro Sumita</a>
|
<a href=/people/g/graham-neubig/>Graham Neubig</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1016><div class="card-body p-3 small">Compared to traditional <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>statistical machine translation (SMT)</a>, <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation (NMT)</a> often sacrifices adequacy for the sake of fluency. We propose a method to combine the advantages of traditional SMT and NMT by exploiting an existing phrase-based SMT model to compute the phrase-based decoding cost for an NMT output and then using the phrase-based decoding cost to rerank the n-best NMT outputs. The main challenge in implementing this approach is that NMT outputs may not be in the search space of the standard phrase-based decoding algorithm, because the search space of phrase-based SMT is limited by the phrase-based translation rule table. We propose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1017" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1017/>Convolutional Neural Network with Word Embeddings for Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/c/chunqi-wang/>Chunqi Wang</a>
|
<a href=/people/b/bo-xu/>Bo Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1017><div class="card-body p-3 small">Character-based sequence labeling framework is flexible and efficient for Chinese word segmentation (CWS). Recently, many character-based neural models have been applied to <a href=https://en.wikipedia.org/wiki/Computational_fluid_dynamics>CWS</a>. While they obtain good performance, they have two obvious weaknesses. The first is that they heavily rely on manually designed bigram feature, i.e. they are not good at capturing n-gram features automatically. The second is that <a href=https://en.wikipedia.org/wiki/They>they</a> make no use of full word information. For the first weakness, we propose a convolutional neural model, which is able to capture rich n-gram features without any <a href=https://en.wikipedia.org/wiki/Feature_engineering>feature engineering</a>. For the second one, we propose an effective approach to integrate the proposed <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>. We evaluate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two benchmark datasets : PKU and MSR. Without any <a href=https://en.wikipedia.org/wiki/Software_feature>feature engineering</a>, the model obtains competitive performance 95.7 % on PKU and 97.3 % on MSR. Armed with <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves state-of-the-art performance on both <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> 96.5 % on <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>PKU</a> and 98.0 % on <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>MSR</a>, without using any external labeled resource.<tex-math>n</tex-math>-gram features automatically. The second is that they make no use of full word information. For the first weakness, we propose a convolutional neural model, which is able to capture rich <tex-math>n</tex-math>-gram features without any feature engineering. For the second one, we propose an effective approach to integrate the proposed model with word embeddings. We evaluate the model on two benchmark datasets: PKU and MSR. Without any feature engineering, the model obtains competitive performance &#8212; 95.7% on PKU and 97.3% on MSR. Armed with word embeddings, the model achieves state-of-the-art performance on both datasets &#8212; 96.5% on PKU and 98.0% on MSR, without using any external labeled resource.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1018.Software.zip data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1018" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1018/>Character-based Joint Segmentation and POS Tagging for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> using Bidirectional RNN-CRF<span class=acl-fixed-case>POS</span> Tagging for <span class=acl-fixed-case>C</span>hinese using Bidirectional <span class=acl-fixed-case>RNN</span>-<span class=acl-fixed-case>CRF</span></a></strong><br><a href=/people/y/yan-shao/>Yan Shao</a>
|
<a href=/people/c/christian-hardmeier/>Christian Hardmeier</a>
|
<a href=/people/j/jorg-tiedemann/>Jörg Tiedemann</a>
|
<a href=/people/j/joakim-nivre/>Joakim Nivre</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1018><div class="card-body p-3 small">We present a character-based model for joint segmentation and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a> for <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. The bidirectional RNN-CRF architecture for general sequence tagging is adapted and applied with novel vector representations of Chinese characters that capture rich contextual information and lower-than-character level features. The proposed model is extensively evaluated and compared with a state-of-the-art tagger respectively on CTB5, CTB9 and UD Chinese. The experimental results indicate that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is accurate and robust across <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> in different sizes, genres and <a href=https://en.wikipedia.org/wiki/Annotation>annotation schemes</a>. We obtain state-of-the-art performance on CTB5, achieving 94.38 F1-score for joint segmentation and <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>POS tagging</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1019 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1019/>Addressing Domain Adaptation for Chinese Word Segmentation with Global Recurrent Structure<span class=acl-fixed-case>C</span>hinese Word Segmentation with Global Recurrent Structure</a></strong><br><a href=/people/s/shen-huang/>Shen Huang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1019><div class="card-body p-3 small">Boundary features are widely used in traditional Chinese Word Segmentation (CWS) methods as they can utilize unlabeled data to help improve the Out-of-Vocabulary (OOV) word recognition performance. Although various neural network methods for CWS have achieved performance competitive with state-of-the-art systems, these methods, constrained by the domain and size of the training corpus, do not work well in domain adaptation. In this paper, we propose a novel BLSTM-based neural network model which incorporates a global recurrent structure designed for modeling <a href=https://en.wikipedia.org/wiki/Boundary_value_problem>boundary features</a> dynamically. Experiments show that the proposed structure can effectively boost the performance of Chinese Word Segmentation, especially OOV-Recall, which brings benefits to <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a>. We achieved state-of-the-art results on 6 domains of CNKI articles, and competitive results to the best reported on the 4 domains of SIGHAN Bakeoff 2010 data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1020 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1020.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1020/>Information Bottleneck Inspired Method For Chat Text Segmentation</a></strong><br><a href=/people/s/s-vishal/>S Vishal</a>
|
<a href=/people/m/mohit-yadav/>Mohit Yadav</a>
|
<a href=/people/l/lovekesh-vig/>Lovekesh Vig</a>
|
<a href=/people/g/gautam-shroff/>Gautam Shroff</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1020><div class="card-body p-3 small">We present a novel technique for segmenting chat conversations using the information bottleneck method (Tishby et al., 2000), augmented with sequential continuity constraints. Furthermore, we utilize critical non-textual clues such as time between two consecutive posts and people mentions within the posts. To ascertain the effectiveness of the proposed method, we have collected data from public Slack conversations and Fresco, a proprietary platform deployed inside our organization. Experiments demonstrate that the proposed method yields an absolute (relative) improvement of as high as 3.23 % (11.25 %). To facilitate future research, we are releasing manual annotations for segmentation on public Slack conversations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1021/>Distributional Modeling on a Diet : One-shot Word Learning from Text Only</a></strong><br><a href=/people/s/su-wang/>Su Wang</a>
|
<a href=/people/s/stephen-roller/>Stephen Roller</a>
|
<a href=/people/k/katrin-erk/>Katrin Erk</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1021><div class="card-body p-3 small">We test whether <a href=https://en.wikipedia.org/wiki/Distribution_(mathematics)>distributional models</a> can do one-shot learning of definitional properties from text only. Using <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian models</a>, we find that first learning overarching structure in the known data, regularities in textual contexts and in <a href=https://en.wikipedia.org/wiki/Property_(philosophy)>properties</a>, helps <a href=https://en.wikipedia.org/wiki/One-shot_learning>one-shot learning</a>, and that individual context items can be highly informative.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1022/>A Computational Study on Word Meanings and Their Distributed Representations via Polymodal Embedding</a></strong><br><a href=/people/j/joohee-park/>Joohee Park</a>
|
<a href=/people/s/sung-hyon-myaeng/>Sung-hyon Myaeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1022><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representation</a> has become a popular approach to capturing a <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>word meaning</a>. Besides its success and practical value, however, questions arise about the relationships between a true word meaning and its <a href=https://en.wikipedia.org/wiki/Distributed_representation>distributed representation</a>. In this paper, we examine such a relationship via polymodal embedding approach inspired by the theory that humans tend to use diverse sources in developing a word meaning. The result suggests that the existing <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> lack in capturing certain aspects of <a href=https://en.wikipedia.org/wiki/Semantics>word meanings</a> which can be significantly improved by the polymodal approach. Also, we show distinct characteristics of different types of words (e.g. concreteness) via <a href=https://en.wikipedia.org/wiki/Computational_neuroscience>computational studies</a>. Finally, we show our proposed embedding method outperforms the baselines in the word similarity measure tasks and the hypernym prediction tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1023 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1023/>Geographical Evaluation of Word Embeddings</a></strong><br><a href=/people/m/michal-konkol/>Michal Konkol</a>
|
<a href=/people/t/tomas-brychcin/>Tomáš Brychcín</a>
|
<a href=/people/m/michal-nykl/>Michal Nykl</a>
|
<a href=/people/t/tomas-hercig/>Tomáš Hercig</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1023><div class="card-body p-3 small">Word embeddings are commonly compared either with human-annotated word similarities or through improvements in natural language processing tasks. We propose a novel principle which compares the information from <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> with reality. We implement this <a href=https://en.wikipedia.org/wiki/Principle>principle</a> by comparing the information in the <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> with geographical positions of cities. Our evaluation linearly transforms the <a href=https://en.wikipedia.org/wiki/Semantic_space>semantic space</a> to optimally fit the real positions of cities and measures the deviation between the position given by word embeddings and the real position. A set of well-known <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> with state-of-the-art results were evaluated. We also introduce a <a href=https://en.wikipedia.org/wiki/Visualization_(graphics)>visualization</a> that helps with error analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1025 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1025/>Unsupervised Segmentation of Phoneme Sequences based on Pitman-Yor Semi-Markov Model using Phoneme Length Context<span class=acl-fixed-case>P</span>itman-<span class=acl-fixed-case>Y</span>or Semi-<span class=acl-fixed-case>M</span>arkov Model using Phoneme Length Context</a></strong><br><a href=/people/r/ryu-takeda/>Ryu Takeda</a>
|
<a href=/people/k/kazunori-komatani/>Kazunori Komatani</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1025><div class="card-body p-3 small">Unsupervised segmentation of phoneme sequences is an essential process to obtain unknown words during <a href=https://en.wikipedia.org/wiki/Dialogue>spoken dialogues</a>. In this segmentation, an input <a href=https://en.wikipedia.org/wiki/Phoneme>phoneme sequence</a> without delimiters is converted into segmented sub-sequences corresponding to words. The Pitman-Yor semi-Markov model (PYSMM) is promising for this problem, but its performance degrades when it is applied to phoneme-level word segmentation. This is because of insufficient cues for the segmentation, e.g., <a href=https://en.wikipedia.org/wiki/Homophone>homophones</a> are improperly treated as single entries and their different contexts are also confused. We propose a phoneme-length context model for PYSMM to give a helpful cue at the <a href=https://en.wikipedia.org/wiki/Phoneme>phoneme-level</a> and to predict succeeding segments more accurately. Our experiments showed that the peak performance with our <a href=https://en.wikipedia.org/wiki/Context_model>context model</a> outperformed those without such a <a href=https://en.wikipedia.org/wiki/Context_model>context model</a> by 0.045 at most in terms of <a href=https://en.wikipedia.org/wiki/F-number>F-measures</a> of estimated segmentation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1026/>A Sensitivity Analysis of (and Practitioners’ Guide to) <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a> for Sentence Classification</a></strong><br><a href=/people/y/ye-zhang/>Ye Zhang</a>
|
<a href=/people/b/byron-c-wallace/>Byron Wallace</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1026><div class="card-body p-3 small">Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (Kim, 2014 ; Kalchbrenner et al., 2014 ; Johnson and Zhang, 2014 ; Zhang et al., 2016). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization parameters</a>, and so on. It is currently unknown how sensitive <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> performance is to changes in these configurations for the task of sentence classification. We thus conduct a <a href=https://en.wikipedia.org/wiki/Sensitivity_and_specificity>sensitivity analysis</a> of one-layer CNNs to explore the effect of architecture components on model performance ; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance, which makes it a modern standard baseline method akin to Support Vector Machine (SVMs) and <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>. We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification in real world settings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1028 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1028/>Turning Distributional Thesauri into Word Vectors for Synonym Extraction and Expansion</a></strong><br><a href=/people/o/olivier-ferret/>Olivier Ferret</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1028><div class="card-body p-3 small">In this article, we propose to investigate a new <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a> consisting in turning a distributional thesaurus into dense word vectors. We propose more precisely a method for performing such task by associating <a href=https://en.wikipedia.org/wiki/Graph_embedding>graph embedding</a> and distributed representation adaptation. We have applied and evaluated it for <a href=https://en.wikipedia.org/wiki/English_nouns>English nouns</a> at a large scale about its ability to retrieve <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a>. In this context, we have also illustrated the interest of the developed method for three different tasks : the improvement of already existing <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a>, the fusion of heterogeneous representations and the expansion of synsets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1030.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1030/>Learning How to Simplify From Explicit Labeling of Complex-Simplified Text Pairs</a></strong><br><a href=/people/f/fernando-alva-manchego/>Fernando Alva-Manchego</a>
|
<a href=/people/j/joachim-bingel/>Joachim Bingel</a>
|
<a href=/people/g/gustavo-paetzold/>Gustavo Paetzold</a>
|
<a href=/people/c/carolina-scarton/>Carolina Scarton</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1030><div class="card-body p-3 small">Current research in <a href=https://en.wikipedia.org/wiki/Text_simplification>text simplification</a> has been hampered by two central problems : (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sentence pairs often differ to such a high degree that <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> becomes difficult. End-to-end models also make it hard to interpret what is actually learned from data. We propose a <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> that decomposes the task of TS into its sub-problems. We devise a way to automatically identify operations in a parallel corpus and introduce a sequence-labeling approach based on these annotations. Finally, we provide insights on the types of <a href=https://en.wikipedia.org/wiki/Transformation_(function)>transformations</a> that different <a href=https://en.wikipedia.org/wiki/Interpretation_(logic)>approaches</a> can model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1031/>Domain-Adaptable Hybrid Generation of RDF Entity Descriptions<span class=acl-fixed-case>RDF</span> Entity Descriptions</a></strong><br><a href=/people/o/or-biran/>Or Biran</a>
|
<a href=/people/k/kathleen-mckeown/>Kathleen McKeown</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1031><div class="card-body p-3 small">RDF ontologies provide <a href=https://en.wikipedia.org/wiki/Structured_data>structured data</a> on entities in many domains and continue to grow in size and diversity. While they can be useful as a starting point for generating descriptions of entities, they often miss important information about an entity that can not be captured as simple relations. In addition, generic approaches to generation from <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF</a> can not capture the unique style and content of specific domains. We describe a framework for hybrid generation of entity descriptions, which combines generation from <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>RDF data</a> with text extracted from a corpus, and extracts unique aspects of the domain from the <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> to create domain-specific generation systems. We show that each component of our approach significantly increases the satisfaction of readers with the text across multiple applications and domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1032.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1032 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1032 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1032/>ES-LDA : Entity Summarization using Knowledge-based Topic Modeling<span class=acl-fixed-case>ES</span>-<span class=acl-fixed-case>LDA</span>: Entity Summarization using Knowledge-based Topic Modeling</a></strong><br><a href=/people/s/seyedamin-pouriyeh/>Seyedamin Pouriyeh</a>
|
<a href=/people/m/mehdi-allahyari/>Mehdi Allahyari</a>
|
<a href=/people/k/krzysztof-kochut/>Krzysztof Kochut</a>
|
<a href=/people/g/gong-cheng/>Gong Cheng</a>
|
<a href=/people/h/hamid-reza-arabnia/>Hamid Reza Arabnia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1032><div class="card-body p-3 small">With the advent of the Internet, the amount of Semantic Web documents that describe real-world entities and their inter-links as a set of statements have grown considerably. These descriptions are usually lengthy, which makes the utilization of the underlying entities a difficult task. Entity summarization, which aims to create summaries for real-world entities, has gained increasing attention in recent years. In this paper, we propose a probabilistic topic model, ES-LDA, that combines prior knowledge with statistical learning techniques within a single framework to create more reliable and representative summaries for entities. We demonstrate the effectiveness of our approach by conducting extensive experiments and show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the state-of-the-art techniques and enhances the quality of the entity summaries.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1033/>Procedural Text Generation from an Execution Video</a></strong><br><a href=/people/a/atsushi-ushiku/>Atsushi Ushiku</a>
|
<a href=/people/h/hayato-hashimoto/>Hayato Hashimoto</a>
|
<a href=/people/a/atsushi-hashimoto/>Atsushi Hashimoto</a>
|
<a href=/people/s/shinsuke-mori/>Shinsuke Mori</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1033><div class="card-body p-3 small">In recent years, there has been a surge of interest in automatically describing images or videos in a <a href=https://en.wikipedia.org/wiki/Natural_language>natural language</a>. These descriptions are useful for image / video search, etc. In this paper, we focus on procedure execution videos, in which a human makes or repairs something and propose a method for generating procedural texts from them. Since video / text pairs available are limited in size, the direct application of end-to-end deep learning is not feasible. Thus we propose to train Faster R-CNN network for <a href=https://en.wikipedia.org/wiki/Outline_of_object_recognition>object recognition</a> and LSTM for text generation and combine them at run time. We took pairs of recipe and cooking video, generated a <a href=https://en.wikipedia.org/wiki/Recipe>recipe</a> from a video, and compared it with the original <a href=https://en.wikipedia.org/wiki/Recipe>recipe</a>. The experimental results showed that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> can produce a <a href=https://en.wikipedia.org/wiki/Recipe>recipe</a> as accurate as the state-of-the-art scene descriptions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1034/>Text Sentiment Analysis based on Fusion of Structural Information and Serialization Information</a></strong><br><a href=/people/l/ling-gan/>Ling Gan</a>
|
<a href=/people/h/houyu-gong/>Houyu Gong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1034><div class="card-body p-3 small">Tree-structured Long Short-Term Memory (Tree-LSTM) has been proved to be an effective method in the sentiment analysis task. It extracts structural information on text, and uses Long Short-Term Memory (LSTM) cell to prevent gradient vanish. However, though combining the LSTM cell, it is still a kind of <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> that extracts the structural information and almost not extracts serialization information. In this paper, we propose three new models in order to combine those two kinds of <a href=https://en.wikipedia.org/wiki/Information>information</a> : the structural information generated by the Constituency Tree-LSTM and the serialization information generated by Long-Short Term Memory neural network. Our experiments show that combining those two kinds of information can give contributes to the performance of the sentiment analysis task compared with the single Constituency Tree-LSTM model and the LSTM model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1035/>Length, <a href=https://en.wikipedia.org/wiki/Interchangeability>Interchangeability</a>, and External Knowledge : Observations from Predicting Argument Convincingness</a></strong><br><a href=/people/p/peter-potash/>Peter Potash</a>
|
<a href=/people/r/robin-bhattacharya/>Robin Bhattacharya</a>
|
<a href=/people/a/anna-rumshisky/>Anna Rumshisky</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1035><div class="card-body p-3 small">In this work, we provide insight into three key aspects related to predicting argument convincingness. First, we explicitly display the power that text length possesses for predicting convincingness in an unsupervised setting. Second, we show that a bag-of-words embedding model posts state-of-the-art on a dataset of arguments annotated for convincingness, outperforming an SVM with numerous hand-crafted features as well as recurrent neural network models that attempt to capture semantic composition. Finally, we assess the feasibility of integrating external knowledge when predicting convincingness, as arguments are often more convincing when they contain abundant information and facts. We finish by analyzing the correlations between the various <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> we propose.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1036/>Exploiting Document Level Information to Improve Event Detection via Recurrent Neural Networks</a></strong><br><a href=/people/s/shaoyang-duan/>Shaoyang Duan</a>
|
<a href=/people/r/ruifang-he/>Ruifang He</a>
|
<a href=/people/w/wenli-zhao/>Wenli Zhao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1036><div class="card-body p-3 small">This paper tackles the task of event detection, which involves identifying and categorizing events. The previous work mainly exist two problems : (1) the traditional feature-based methods apply cross-sentence information, yet need taking a large amount of human effort to design complicated feature sets and inference rules ; (2) the representation-based methods though overcome the problem of manually extracting features, while just depend on local sentence representation. Considering local sentence context is insufficient to resolve ambiguities in identifying particular event types, therefore, we propose a novel document level Recurrent Neural Networks (DLRNN) model, which can automatically extract cross-sentence clues to improve sentence level event detection without designing complex reasoning rules. Experiment results show that our approach outperforms other state-of-the-art methods on ACE 2005 dataset without external knowledge base.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1037.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1037 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1037 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1037/>Embracing Non-Traditional Linguistic Resources for Low-resource Language Name Tagging</a></strong><br><a href=/people/b/boliang-zhang/>Boliang Zhang</a>
|
<a href=/people/d/di-lu/>Di Lu</a>
|
<a href=/people/x/xiaoman-pan/>Xiaoman Pan</a>
|
<a href=/people/y/ying-lin/>Ying Lin</a>
|
<a href=/people/h/halidanmu-abudukelimu/>Halidanmu Abudukelimu</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a>
|
<a href=/people/k/kevin-knight/>Kevin Knight</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1037><div class="card-body p-3 small">Current supervised name tagging approaches are inadequate for most low-resource languages due to the lack of annotated data and actionable linguistic knowledge. All supervised learning methods (including deep neural networks (DNN)) are sensitive to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a> and thus they are not quite portable without massive clean annotations. We found that the <a href=https://en.wikipedia.org/wiki/F-number>F-scores</a> of DNN-based name taggers drop rapidly (20%-30 %) when we replace clean manual annotations with noisy annotations in the training data. We propose a new solution to incorporate many non-traditional language universal resources that are readily available but rarely explored in the Natural Language Processing (NLP) community, such as the World Atlas of Linguistic Structure, CIA names, PanLex and survival guides. We acquire and encode various types of non-traditional linguistic resources into a DNN name tagger. Experiments on three low-resource languages show that feeding linguistic knowledge can make DNN significantly more robust to <a href=https://en.wikipedia.org/wiki/Noise_(signal_processing)>noise</a>, achieving 8%-22 % absolute <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> gains on <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>name tagging</a> without using any human annotation</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1038/>NMT or SMT : Case Study of a Narrow-domain English-Latvian Post-editing Project<span class=acl-fixed-case>NMT</span> or <span class=acl-fixed-case>SMT</span>: Case Study of a Narrow-domain <span class=acl-fixed-case>E</span>nglish-<span class=acl-fixed-case>L</span>atvian Post-editing Project</a></strong><br><a href=/people/i/inguna-skadina/>Inguna Skadiņa</a>
|
<a href=/people/m/marcis-pinnis/>Mārcis Pinnis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1038><div class="card-body p-3 small">The recent technological shift in <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> from statistical machine translation (SMT) to neural machine translation (NMT) raises the question of the strengths and weaknesses of NMT. In this paper, we present an analysis of NMT and SMT systems&#8217; outputs from narrow domain English-Latvian MT systems that were trained on a rather small amount of data. We analyze <a href=https://en.wikipedia.org/wiki/Post-editing>post-edits</a> produced by <a href=https://en.wikipedia.org/wiki/Translation>professional translators</a> and manually annotated errors in these outputs. Analysis of post-edits allowed us to conclude that both approaches are comparably successful, allowing for an increase in translators&#8217; productivity, with the NMT system showing slightly worse results. Through the analysis of annotated errors, we found that NMT translations are more fluent than SMT translations. However, <a href=https://en.wikipedia.org/wiki/Error>errors</a> related to <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, especially, mistranslation and omission errors, occur more often in NMT outputs. The word form errors, that characterize the morphological richness of Latvian, are frequent for both systems, but slightly fewer in NMT outputs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1039/>Towards <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a> with Partially Aligned Corpora</a></strong><br><a href=/people/y/yining-wang/>Yining Wang</a>
|
<a href=/people/y/yang-zhao/>Yang Zhao</a>
|
<a href=/people/j/jiajun-zhang/>Jiajun Zhang</a>
|
<a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/z/zhengshan-xue/>Zhengshan Xue</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1039><div class="card-body p-3 small">While neural machine translation (NMT) has become the new paradigm, the parameter optimization requires large-scale parallel data which is scarce in many domains and language pairs. In this paper, we address a new translation scenario in which there only exists monolingual corpora and phrase pairs. We propose a new method towards <a href=https://en.wikipedia.org/wiki/Translation>translation</a> with partially aligned sentence pairs which are derived from the phrase pairs and <a href=https://en.wikipedia.org/wiki/Text_corpus>monolingual corpora</a>. To make full use of the partially aligned corpora, we adapt the conventional NMT training method in two aspects. On one hand, different generation strategies are designed for aligned and unaligned target words. On the other hand, a different <a href=https://en.wikipedia.org/wiki/Loss_function>objective function</a> is designed to model the partially aligned parts. The experiments demonstrate that our method can achieve a relatively good result in such a translation scenario, and tiny bitexts can boost translation quality to a large extent.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1040/>Identifying Usage Expression Sentences in Consumer Product Reviews</a></strong><br><a href=/people/s/shibamouli-lahiri/>Shibamouli Lahiri</a>
|
<a href=/people/v/v-g-vinod-vydiswaran/>V.G.Vinod Vydiswaran</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1040><div class="card-body p-3 small">In this paper we introduce the problem of identifying usage expression sentences in a consumer product review. We create a human-annotated gold standard dataset of 565 reviews spanning five distinct product categories. Our <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> consists of more than 3,000 annotated sentences. We further introduce a classification system to label sentences according to whether or not they describe some usage. The system combines lexical, syntactic, and semantic features in a product-agnostic fashion to yield good <a href=https://en.wikipedia.org/wiki/Categorization>classification</a> performance. We show the effectiveness of our approach using importance ranking of features, error analysis, and cross-product classification experiments.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1041/>Between Reading Time and Syntactic / Semantic Categories</a></strong><br><a href=/people/m/masayuki-asahara/>Masayuki Asahara</a>
|
<a href=/people/s/sachi-kato/>Sachi Kato</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1041><div class="card-body p-3 small">This article presents a contrastive analysis between reading time and syntactic / semantic categories in <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a>. We overlaid the reading time annotation of BCCWJ-EyeTrack and a syntactic / semantic category information annotation on the &#8216;Balanced Corpus of Contemporary Written Japanese&#8217;. Statistical analysis based on a <a href=https://en.wikipedia.org/wiki/Mixed_linear_model>mixed linear model</a> showed that verbal phrases tend to have shorter reading times than <a href=https://en.wikipedia.org/wiki/Adjective>adjectives</a>, <a href=https://en.wikipedia.org/wiki/Adverbial_phrase>adverbial phrases</a>, or nominal phrases. The results suggest that the preceding phrases associated with the presenting phrases promote the reading process to shorten the gazing time.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1044/>Local Monotonic Attention Mechanism for End-to-End Speech And Language Processing</a></strong><br><a href=/people/a/andros-tjandra/>Andros Tjandra</a>
|
<a href=/people/s/sakriani-sakti/>Sakriani Sakti</a>
|
<a href=/people/s/satoshi-nakamura/>Satoshi Nakamura</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1044><div class="card-body p-3 small">Recently, encoder-decoder neural networks have shown impressive performance on many sequence-related tasks. The architecture commonly uses an <a href=https://en.wikipedia.org/wiki/Attentional_control>attentional mechanism</a> which allows the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to learn <a href=https://en.wikipedia.org/wiki/Sequence_alignment>alignments</a> between the source and the target sequence. Most attentional mechanisms used today is based on a global attention property which requires a computation of a weighted summarization of the whole input sequence generated by encoder states. However, it is computationally expensive and often produces <a href=https://en.wikipedia.org/wiki/Sequence_alignment>misalignment</a> on the longer input sequence. Furthermore, it does not fit with monotonous or left-to-right nature in several tasks, such as automatic speech recognition (ASR), grapheme-to-phoneme (G2P), etc. In this paper, we propose a novel <a href=https://en.wikipedia.org/wiki/Attentional_control>attention mechanism</a> that has local and monotonic properties. Various ways to control those properties are also explored. Experimental results on ASR, G2P and <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> between two languages with similar sentence structures, demonstrate that the proposed encoder-decoder model with local monotonic attention could achieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1046.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1046 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1046 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1046.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1046/>Diachrony-aware Induction of Binary Latent Representations from Typological Features</a></strong><br><a href=/people/y/yugo-murawaki/>Yugo Murawaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1046><div class="card-body p-3 small">Although features of linguistic typology are a promising alternative to lexical evidence for tracing evolutionary history of languages, a large number of missing values in the dataset pose serious difficulties for <a href=https://en.wikipedia.org/wiki/Statistical_model>statistical modeling</a>. In this paper, we combine two existing approaches to the problem : (1) the <a href=https://en.wikipedia.org/wiki/Synchrony_and_diachrony>synchronic approach</a> that focuses on interdependencies between features and (2) the <a href=https://en.wikipedia.org/wiki/Synchrony_and_diachrony>diachronic approach</a> that exploits phylogenetically- and/or spatially-related languages. Specifically, we propose a <a href=https://en.wikipedia.org/wiki/Bayesian_inference>Bayesian model</a> that (1) represents each language as a sequence of binary latent parameters encoding inter-feature dependencies and (2) relates a language&#8217;s parameters to those of its phylogenetic and spatial neighbors. Experiments show that the proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> recovers missing values more accurately than others and that <a href=https://en.wikipedia.org/wiki/Induced_representations>induced representations</a> retain phylogenetic and spatial signals observed for surface features.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1047.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1047 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1047 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1047.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1047/>Image-Grounded Conversations : Multimodal Context for Natural Question and Response Generation</a></strong><br><a href=/people/n/nasrin-mostafazadeh/>Nasrin Mostafazadeh</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a>
|
<a href=/people/m/michel-galley/>Michel Galley</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/g/georgios-spithourakis/>Georgios Spithourakis</a>
|
<a href=/people/l/lucy-vanderwende/>Lucy Vanderwende</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1047><div class="card-body p-3 small">The popularity of <a href=https://en.wikipedia.org/wiki/Image_sharing>image sharing</a> on <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> and the engagement it creates between users reflect the important role that <a href=https://en.wikipedia.org/wiki/Context_(language_use)>visual context</a> plays in <a href=https://en.wikipedia.org/wiki/Conversation>everyday conversations</a>. We present a novel task, Image Grounded Conversations (IGC), in which natural-sounding conversations are generated about a shared image. To benchmark progress, we introduce a new multiple reference dataset of crowd-sourced, event-centric conversations on images. IGC falls on the continuum between <a href=https://en.wikipedia.org/wiki/Chit-chat>chit-chat</a> and goal-directed conversation models, where visual grounding constrains the topic of conversation to event-driven utterances. Experiments with models trained on social media data show that the combination of visual and textual context enhances the quality of generated conversational turns. In human evaluation, the gap between human performance and that of both neural and retrieval architectures suggests that multi-modal IGC presents an interesting challenge for dialog research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1048.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1048 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1048 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1048" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1048/>A Neural Language Model for Dynamically Representing the Meanings of Unknown Words and Entities in a Discourse</a></strong><br><a href=/people/s/sosuke-kobayashi/>Sosuke Kobayashi</a>
|
<a href=/people/n/naoaki-okazaki/>Naoaki Okazaki</a>
|
<a href=/people/k/kentaro-inui/>Kentaro Inui</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1048><div class="card-body p-3 small">This study addresses the problem of identifying the meaning of unknown words or entities in a <a href=https://en.wikipedia.org/wiki/Discourse>discourse</a> with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al. (2016) and incorporates a copy mechanism proposed independently by Gu et al. (2016) and Gulcehre et al. In addition, we construct a new task and <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> called Anonymized Language Modeling for evaluating the ability to capture <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>word meanings</a> while reading. Experiments conducted using our novel <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> show that the proposed variant of RNN language model outperformed the <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baseline model</a>. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1049.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1049 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1049 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1049/>Using Explicit Discourse Connectives in <a href=https://en.wikipedia.org/wiki/Translation>Translation</a> for Implicit Discourse Relation Classification</a></strong><br><a href=/people/w/wei-shi/>Wei Shi</a>
|
<a href=/people/f/frances-yung/>Frances Yung</a>
|
<a href=/people/r/raphael-rubino/>Raphael Rubino</a>
|
<a href=/people/v/vera-demberg/>Vera Demberg</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1049><div class="card-body p-3 small">Implicit discourse relation recognition is an extremely challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> due to the lack of indicative connectives. Various neural network architectures have been proposed for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> recently, but most of them suffer from the shortage of <a href=https://en.wikipedia.org/wiki/Labeled_data>labeled data</a>. In this paper, we address this problem by procuring additional training data from parallel corpora : When humans translate a text, they sometimes add connectives (a process known as explicitation). We automatically back-translate it into an English connective and use <a href=https://en.wikipedia.org/wiki/Information_technology>it</a> to infer a label with high confidence. We show that a training set several times larger than the original training set can be generated this way. With the extra labeled instances, we show that even a simple bidirectional Long Short-Term Memory Network can outperform the current <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a>.<i>explicitation</i>). We automatically back-translate it into an English connective and use it to infer a label with high confidence. We show that a training set several times larger than the original training set can be generated this way. With the extra labeled instances, we show that even a simple bidirectional Long Short-Term Memory Network can outperform the current state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1050.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1050 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1050 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1050/>Tag-Enhanced Tree-Structured Neural Networks for Implicit Discourse Relation Classification</a></strong><br><a href=/people/y/yizhong-wang/>Yizhong Wang</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a>
|
<a href=/people/j/jingfeng-yang/>Jingfeng Yang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1050><div class="card-body p-3 small">Identifying implicit discourse relations between text spans is a challenging task because it requires understanding the meaning of the text. To tackle this task, recent studies have tried several <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning methods</a> but few of them exploited the <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a>. In this work, we explore the idea of incorporating syntactic parse tree into <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a>. Specifically, we employ the Tree-LSTM model and Tree-GRU model, which is based on the <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree structure</a>, to encode the arguments in a relation. And we further leverage the constituent tags to control the semantic composition process in these tree-structured neural networks. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves state-of-the-art performance on PDTB corpus.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1051.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1051 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1051 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1051/>Cross-Lingual Sentiment Analysis Without (Good) Translation</a></strong><br><a href=/people/m/mohamed-abdalla/>Mohamed Abdalla</a>
|
<a href=/people/g/graeme-hirst/>Graeme Hirst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1051><div class="card-body p-3 small">Current approaches to cross-lingual sentiment analysis try to leverage the wealth of labeled English data using bilingual lexicons, bilingual vector space embeddings, or machine translation systems. Here we show that it is possible to use a single <a href=https://en.wikipedia.org/wiki/Linear_map>linear transformation</a>, with as few as 2000 word pairs, to capture fine-grained sentiment relationships between words in a cross-lingual setting. We apply these cross-lingual sentiment models to a diverse set of <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> to demonstrate their functionality in a non-English context. By effectively leveraging English sentiment knowledge without the need for accurate <a href=https://en.wikipedia.org/wiki/Translation>translation</a>, we can analyze and extract features from other languages with scarce data at a very low cost, thus making sentiment and related analyses for many languages inexpensive.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1052.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1052 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1052 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1052/>Implicit Syntactic Features for Target-dependent Sentiment Analysis</a></strong><br><a href=/people/y/yuze-gao/>Yuze Gao</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/t/tong-xiao/>Tong Xiao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1052><div class="card-body p-3 small">Targeted sentiment analysis investigates the sentiment polarities on given target mentions from input texts. Different from <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence level sentiment</a>, it offers more fine-grained knowledge on each entity mention. While early work leveraged syntactic information, recent research has used neural representation learning to induce features automatically, thereby avoiding error propagation of syntactic parsers, which are particularly severe on social media texts. We study a method to leverage syntactic information without explicitly building the parser outputs, by training an encoder-decoder structure parser model on standard syntactic treebanks, and then leveraging its hidden encoder layers when analysing tweets. Such hidden vectors do not contain explicit syntactic outputs, yet encode rich syntactic features. We use them to augment the inputs to a baseline state-of-the-art targeted sentiment classifier, observing significant improvements on various <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. We obtain the best accuracies on all test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1053.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1053 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1053 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1053/>Graph Based Sentiment Aggregation using ConceptNet Ontology<span class=acl-fixed-case>C</span>oncept<span class=acl-fixed-case>N</span>et Ontology</a></strong><br><a href=/people/s/srikanth-tamilselvam/>Srikanth Tamilselvam</a>
|
<a href=/people/s/seema-nagar/>Seema Nagar</a>
|
<a href=/people/a/abhijit-mishra/>Abhijit Mishra</a>
|
<a href=/people/k/kuntal-dey/>Kuntal Dey</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1053><div class="card-body p-3 small">The sentiment aggregation problem accounts for analyzing the sentiment of a user towards various aspects / features of a product, and meaningfully assimilating the pragmatic significance of these features / aspects from an opinionated text. The current paper addresses the sentiment aggregation problem, by assigning weights to each aspect appearing in the <a href=https://en.wikipedia.org/wiki/User-generated_content>user-generated content</a>, that are proportionate to the strategic importance of the aspect in the pragmatic domain. The novelty of this paper is in computing the pragmatic significance (weight) of each aspect, using graph centrality measures (applied on domain specific ontology-graphs extracted from ConceptNet), and deeply ingraining these weights while aggregating the sentiments from opinionated text. We experiment over multiple real-life product review data. Our <a href=https://en.wikipedia.org/wiki/System>system</a> consistently outperforms the state of the art-by as much as a F-score of 20.39 % in one case.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1054.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1054 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1054 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1054/>Sentence Modeling with Deep Neural Architecture using Lexicon and Character Attention Mechanism for Sentiment Classification</a></strong><br><a href=/people/h/huy-thanh-nguyen/>Huy Thanh Nguyen</a>
|
<a href=/people/m/minh-le-nguyen/>Minh Le Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1054><div class="card-body p-3 small">Tweet-level sentiment classification in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter social networking</a> has many challenges : exploiting <a href=https://en.wikipedia.org/wiki/Syntax>syntax</a>, <a href=https://en.wikipedia.org/wiki/Semantics>semantic</a>, sentiment, and context in <a href=https://en.wikipedia.org/wiki/Twitter>tweets</a>. To address these problems, we propose a novel approach to <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> that uses lexicon features for building lexicon embeddings (LexW2Vs) and generates character attention vectors (CharAVs) by using a Deep Convolutional Neural Network (DeepCNN). Our approach integrates LexW2Vs and CharAVs with continuous word embeddings (ContinuousW2Vs) and dependency-based word embeddings (DependencyW2Vs) simultaneously in order to increase information for each word into a Bidirectional Contextual Gated Recurrent Neural Network (Bi-CGRNN). We evaluate our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two Twitter sentiment classification datasets. Experimental results show that our model can improve the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> of <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentence-level sentiment analysis</a> in <a href=https://en.wikipedia.org/wiki/Twitter>Twitter social networking</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1055.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1055 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1055 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1055/>Combining Lightly-Supervised Text Classification Models for Accurate <a href=https://en.wikipedia.org/wiki/Contextual_advertising>Contextual Advertising</a></a></strong><br><a href=/people/y/yiping-jin/>Yiping Jin</a>
|
<a href=/people/d/dittaya-wanvarie/>Dittaya Wanvarie</a>
|
<a href=/people/p/phu-le/>Phu Le</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1055><div class="card-body p-3 small">In this paper we propose a lightly-supervised framework to rapidly build <a href=https://en.wikipedia.org/wiki/Text_classification>text classifiers</a> for <a href=https://en.wikipedia.org/wiki/Contextual_advertising>contextual advertising</a>. Traditionally text classification techniques require labeled training documents for each predefined class. In the scenario of <a href=https://en.wikipedia.org/wiki/Contextual_advertising>contextual advertising</a>, advertisers often want to target to a specific class of webpages most relevant to their product or service, which may not be covered by a pre-trained classifier. Moreover, the advertisers are interested in whether a webpage is relevant or irrelevant. It is time-consuming to solicit the advertisers for reliable training signals for the negative class. Therefore, it is more suitable to model the problem as a one-class classification problem, in contrast to traditional classification problems where disjoint classes are defined a priori. We first apply two state-of-the-art lightly-supervised classification models, generalized expectation (GE) criteria (Druck et al., 2008) and multinomial naive Bayes (MNB) with priors (Settles, 2011) to one-class classification where the user only needs to provide a small list of labeled words for the target class. To combine the strengths of the two <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>, we fuse them together by using MNB to automatically enrich the constraints for GE training. We also explore ensemble method to combine classifiers. On a corpus of webpages from real-time bidding requests, the proposed <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> achieves the highest average F1 of 0.69 and closes more than half of the gap between previous state-of-the-art lightly-supervised models to a fully-supervised MaxEnt model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1056.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1056 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1056 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1056" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1056/>Capturing Long-range Contextual Dependencies with Memory-enhanced Conditional Random Fields</a></strong><br><a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a>
|
<a href=/people/t/trevor-cohn/>Trevor Cohn</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1056><div class="card-body p-3 small">Despite successful applications across a broad range of NLP tasks, conditional random fields (CRFs), in particular the linear-chain variant, are only able to model local features. While this has important benefits in terms of inference tractability, it limits the ability of the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to capture long-range dependencies between items. Attempts to extend CRFs to capture long-range dependencies have largely come at the cost of <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>computational complexity</a> and <a href=https://en.wikipedia.org/wiki/Approximate_inference>approximate inference</a>. In this work, we propose an extension to CRFs by integrating <a href=https://en.wikipedia.org/wiki/External_memory>external memory</a>, taking inspiration from memory networks, thereby allowing CRFs to incorporate information far beyond neighbouring steps. Experiments across two <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> show substantial improvements over strong CRF and LSTM baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1057.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1057 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1057 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1057/>Named Entity Recognition with Stack Residual LSTM and Trainable Bias Decoding<span class=acl-fixed-case>LSTM</span> and Trainable Bias Decoding</a></strong><br><a href=/people/q/quan-hung-tran/>Quan Tran</a>
|
<a href=/people/a/andrew-mackinlay/>Andrew MacKinlay</a>
|
<a href=/people/a/antonio-jimeno-yepes/>Antonio Jimeno Yepes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1057><div class="card-body p-3 small">Recurrent Neural Network models are the state-of-the-art for Named Entity Recognition (NER). We present two innovations to improve the performance of these <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. The first innovation is the introduction of residual connections between the Stacked Recurrent Neural Network model to address the degradation problem of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. The second innovation is a bias decoding mechanism that allows the trained <a href=https://en.wikipedia.org/wiki/System>system</a> to adapt to non-differentiable and externally computed objectives, such as the entity-based F-measure. Our work improves the state-of-the-art results for both Spanish and English languages on the standard train / development / test split of the CoNLL 2003 Shared Task NER dataset.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1058.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1058 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1058 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1058.Notes.pdf data-toggle=tooltip data-placement=top title=Note><i class="fas fa-file-alt"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1058/>Neuramanteau : A Neural Network Ensemble Model for Lexical Blends<span class=acl-fixed-case>N</span>euramanteau: A Neural Network Ensemble Model for Lexical Blends</a></strong><br><a href=/people/k/kollol-das/>Kollol Das</a>
|
<a href=/people/s/shaona-ghosh/>Shaona Ghosh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1058><div class="card-body p-3 small">The problem of blend formation in <a href=https://en.wikipedia.org/wiki/Generative_linguistics>generative linguistics</a> is interesting in the context of <a href=https://en.wikipedia.org/wiki/Neologism>neologism</a>, their quick adoption in modern life and the creative generative process guiding their formation. Blend quality depends on multitude of factors with high degrees of uncertainty. In this work, we investigate if the modern <a href=https://en.wikipedia.org/wiki/Neural_circuit>neural network models</a> can sufficiently capture and recognize the creative blend composition process. We propose recurrent neural network sequence-to-sequence models, that are evaluated on multiple blend datasets available in the literature. We propose an ensemble neural and hybrid model that outperforms most of the baselines and <a href=https://en.wikipedia.org/wiki/Heuristic_(computer_science)>heuristic models</a> upon evaluation on test data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1059.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1059 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1059 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1059.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1059" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1059/>Leveraging <a href=https://en.wikipedia.org/wiki/Discourse>Discourse Information</a> Effectively for Authorship Attribution</a></strong><br><a href=/people/e/elisa-ferracane/>Elisa Ferracane</a>
|
<a href=/people/s/su-wang/>Su Wang</a>
|
<a href=/people/r/raymond-mooney/>Raymond Mooney</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1059><div class="card-body p-3 small">We explore techniques to maximize the effectiveness of <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse information</a> in the task of <a href=https://en.wikipedia.org/wiki/Attribution_(psychology)>authorship attribution</a>. We present a novel method to embed discourse features in a Convolutional Neural Network text classifier, which achieves a state-of-the-art result by a significant margin. We empirically investigate several featurization methods to understand the conditions under which discourse features contribute non-trivial performance gains, and analyze discourse embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1060.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1060 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1060 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1060/>Lightly-Supervised Modeling of Argument Persuasiveness</a></strong><br><a href=/people/i/isaac-persing/>Isaac Persing</a>
|
<a href=/people/v/vincent-ng/>Vincent Ng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1060><div class="card-body p-3 small">We propose the first lightly-supervised approach to scoring an argument&#8217;s persuasiveness. Key to our approach is the novel hypothesis that lightly-supervised persuasiveness scoring is possible by explicitly modeling the major errors that negatively impact <a href=https://en.wikipedia.org/wiki/Persuasion>persuasiveness</a>. In an evaluation on a new annotated corpus of online debate arguments, our approach rivals its fully-supervised counterparts in performance by four scoring metrics when using only 10 % of the available training instances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1061.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1061 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1061 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1061/>Multi-Task Learning for Speaker-Role Adaptation in Neural Conversation Models</a></strong><br><a href=/people/y/yi-luan/>Yi Luan</a>
|
<a href=/people/c/chris-brockett/>Chris Brockett</a>
|
<a href=/people/w/william-b-dolan/>Bill Dolan</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/m/michel-galley/>Michel Galley</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1061><div class="card-body p-3 small">Building a persona-based conversation agent is challenging owing to the lack of large amounts of speaker-specific conversation data for model training. This paper addresses the problem by proposing a multi-task learning approach to training neural conversation models that leverages both conversation data across speakers and other types of <a href=https://en.wikipedia.org/wiki/Data>data</a> pertaining to the speaker and speaker roles to be modeled. Experiments show that our approach leads to significant improvements over baseline model quality, generating responses that capture more precisely speakers&#8217; traits and <a href=https://en.wikipedia.org/wiki/Style_(sociolinguistics)>speaking styles</a>. The <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1062.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1062 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1062 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1062.Datasets.tgz data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1062/>Chat Disentanglement : Identifying Semantic Reply Relationships with Random Forests and <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a></a></strong><br><a href=/people/s/shikib-mehri/>Shikib Mehri</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1062><div class="card-body p-3 small">Thread disentanglement is a precursor to any high-level analysis of multiparticipant chats. Existing research approaches the problem by calculating the likelihood of two messages belonging in the same thread. Our approach leverages a newly annotated dataset to identify reply relationships. Furthermore, we explore the usage of an RNN, along with large quantities of unlabeled data, to learn semantic relationships between messages. Our proposed <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>pipeline</a>, which utilizes a reply classifier and an RNN to generate a set of disentangled threads, is novel and performs well against previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1063.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1063 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1063 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1063.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1063/>Towards Bootstrapping a Polarity Shifter Lexicon using <a href=https://en.wikipedia.org/wiki/Linguistic_feature>Linguistic Features</a></a></strong><br><a href=/people/m/marc-schulder/>Marc Schulder</a>
|
<a href=/people/m/michael-wiegand/>Michael Wiegand</a>
|
<a href=/people/j/josef-ruppenhofer/>Josef Ruppenhofer</a>
|
<a href=/people/b/benjamin-roth/>Benjamin Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1063><div class="card-body p-3 small">We present a major step towards the creation of the first high-coverage lexicon of polarity shifters. In this work, we bootstrap a lexicon of verbs by exploiting various <a href=https://en.wikipedia.org/wiki/Linguistic_feature>linguistic features</a>. Polarity shifters, such as abandon, are similar to negations (e.g. not) in that they move the polarity of a phrase towards its inverse, as in abandon all hope. While there exist lists of <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation words</a>, creating comprehensive lists of polarity shifters is far more challenging due to their sheer number. On a sample of manually annotated verbs we examine a variety of <a href=https://en.wikipedia.org/wiki/Feature_(linguistics)>linguistic features</a> for this task. Then we build a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised classifier</a> to increase <a href=https://en.wikipedia.org/wiki/Coverage_(statistics)>coverage</a>. We show that this approach drastically reduces the annotation effort while ensuring a high-precision lexicon. We also show that our acquired knowledge of verbal polarity shifters improves phrase-level sentiment analysis.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1064.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1064 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1064 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1064/>Cascading Multiway Attentions for Document-level Sentiment Classification</a></strong><br><a href=/people/d/dehong-ma/>Dehong Ma</a>
|
<a href=/people/s/sujian-li/>Sujian Li</a>
|
<a href=/people/x/xiaodong-zhang/>Xiaodong Zhang</a>
|
<a href=/people/h/houfeng-wang/>Houfeng Wang</a>
|
<a href=/people/x/xu-sun/>Xu Sun</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1064><div class="card-body p-3 small">Document-level sentiment classification aims to assign the user reviews a sentiment polarity. Previous methods either just utilized the document content without consideration of user and product information, or did not comprehensively consider what roles the three kinds of information play in text modeling. In this paper, to reasonably use all the information, we present the idea that <a href=https://en.wikipedia.org/wiki/User_(computing)>user</a>, product and their combination can all influence the generation of attentions to words and sentences, when judging the <a href=https://en.wikipedia.org/wiki/Sentimentality>sentiment</a> of a document. With this idea, we propose a cascading multiway attention (CMA) model, where multiple ways of using user and product information are cascaded to influence the generation of attentions on the word and sentence layers. Then, sentences and documents are well modeled by multiple representation vectors, which provide rich information for sentiment classification. Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1066.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1066 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1066 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1066/>Leveraging Auxiliary Tasks for Document-Level Cross-Domain Sentiment Classification</a></strong><br><a href=/people/j/jianfei-yu/>Jianfei Yu</a>
|
<a href=/people/j/jing-jiang/>Jing Jiang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1066><div class="card-body p-3 small">In this paper, we study <a href=https://en.wikipedia.org/wiki/Domain_adaptation>domain adaptation</a> with a state-of-the-art hierarchical neural network for document-level sentiment classification. We first design a new <a href=https://en.wikipedia.org/wiki/Task_(computing)>auxiliary task</a> based on sentiment scores of domain-independent words. We then propose two neural network architectures to respectively induce document embeddings and <a href=https://en.wikipedia.org/wiki/Sentence_embedding>sentence embeddings</a> that work well for different domains. When these document and sentence embeddings are used for sentiment classification, we find that with both pseudo and external sentiment lexicons, our proposed methods can perform similarly to or better than several highly competitive domain adaptation methods on a benchmark dataset of product reviews.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1067.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1067 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1067 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1067/>Measuring Semantic Relations between Human Activities</a></strong><br><a href=/people/s/steven-wilson/>Steven Wilson</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1067><div class="card-body p-3 small">The things people do in their daily lives can provide valuable insights into their <a href=https://en.wikipedia.org/wiki/Personality_psychology>personality</a>, values, and interests. Unstructured text data on <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> are rich in behavioral content, and automated systems can be deployed to learn about human activity on a broad scale if these systems are able to reason about the content of interest. In order to aid in the evaluation of such <a href=https://en.wikipedia.org/wiki/System>systems</a>, we introduce a new phrase-level semantic textual similarity dataset comprised of human activity phrases, providing a testbed for automated systems that analyze relationships between phrasal descriptions of people&#8217;s actions. Our set of 1,000 pairs of activities is annotated by human judges across four relational dimensions including <a href=https://en.wikipedia.org/wiki/Similarity_(psychology)>similarity</a>, <a href=https://en.wikipedia.org/wiki/Coefficient_of_relationship>relatedness</a>, motivational alignment, and perceived actor congruence. We evaluate a set of strong baselines for the task of generating scores that correlate highly with human ratings, and we introduce several new approaches to the phrase-level similarity task in the domain of human activities.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1068.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1068 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1068 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1068/>Learning Transferable Representation for Bilingual Relation Extraction via <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>Convolutional Neural Networks</a></a></strong><br><a href=/people/b/bonan-min/>Bonan Min</a>
|
<a href=/people/z/zhuolin-jiang/>Zhuolin Jiang</a>
|
<a href=/people/m/marjorie-freedman/>Marjorie Freedman</a>
|
<a href=/people/r/ralph-weischedel/>Ralph Weischedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1068><div class="card-body p-3 small">Typically, relation extraction models are trained to extract instances of a <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>relation ontology</a> using only training data from a single language. However, the concepts represented by the <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>relation ontology</a> (e.g. ResidesIn, EmployeeOf) are language independent. The numbers of annotated examples available for a given <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>ontology</a> vary between languages. For example, there are far fewer annotated examples in <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> and <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> than <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>. Furthermore, using only language-specific training data results in the need to manually annotate equivalently large amounts of training for each new language a system encounters. We propose a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> to learn transferable, discriminative bilingual representation. Experiments on the ACE 2005 multilingual training corpus demonstrate that the joint training process results in significant improvement in relation classification performance over the monolingual counterparts. The learnt representation is discriminative and transferable between languages. When using 10 % (25 K English words, or 30 K Chinese characters) of the training data, our approach results in doubling F1 compared to a monolingual baseline. We achieve comparable performance to the monolingual system trained with 250 K English words (or 300 K Chinese characters) With 50 % of training data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1071.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1071 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1071 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1071/>Joint Learning of Dialog Act Segmentation and Recognition in Spoken Dialog Using <a href=https://en.wikipedia.org/wiki/Neural_network>Neural Networks</a></a></strong><br><a href=/people/t/tianyu-zhao/>Tianyu Zhao</a>
|
<a href=/people/t/tatsuya-kawahara/>Tatsuya Kawahara</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1071><div class="card-body p-3 small">Dialog act segmentation and recognition are basic natural language understanding tasks in <a href=https://en.wikipedia.org/wiki/Spoken_dialog_systems>spoken dialog systems</a>. This paper investigates a unified architecture for these two <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>, which aims to improve the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s performance on both of the <a href=https://en.wikipedia.org/wiki/Task_(computing)>tasks</a>. Compared with past joint models, the proposed architecture can (1) incorporate <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> in dialog act recognition, and (2) integrate <a href=https://en.wikipedia.org/wiki/Computer_simulation>models</a> for tasks of different levels as a whole, i.e. dialog act segmentation on the word level and dialog act recognition on the <a href=https://en.wikipedia.org/wiki/Segment_(linguistics)>segment level</a>. Experimental results show that the joint training system outperforms the simple cascading system and the joint coding system on both dialog act segmentation and recognition tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1072.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1072 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1072 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1072/>Predicting Users’ Negative Feedbacks in Multi-Turn Human-Computer Dialogues</a></strong><br><a href=/people/x/xin-wang/>Xin Wang</a>
|
<a href=/people/j/jianan-wang/>Jianan Wang</a>
|
<a href=/people/y/yuanchao-liu/>Yuanchao Liu</a>
|
<a href=/people/x/xiaolong-wang/>Xiaolong Wang</a>
|
<a href=/people/z/zhuoran-wang/>Zhuoran Wang</a>
|
<a href=/people/b/baoxun-wang/>Baoxun Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1072><div class="card-body p-3 small">User experience is essential for <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-computer dialogue systems</a>. However, it is impractical to ask users to provide explicit feedbacks when the agents&#8217; responses displease them. Therefore, in this paper, we explore to predict users&#8217; imminent dissatisfactions caused by <a href=https://en.wikipedia.org/wiki/Intelligent_agent>intelligent agents</a> by analysing the existing utterances in the dialogue sessions. To our knowledge, this is the first work focusing on this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>. Several possible factors that trigger <a href=https://en.wikipedia.org/wiki/Emotion>negative emotions</a> are modelled. A relation sequence model (RSM) is proposed to encode the sequence of appropriateness of current response with respect to the earlier utterances. The experimental results show that the proposed structure is effective in modelling emotional risk (possibility of negative feedback) than existing conversation modelling approaches. Besides, strategies of obtaining distance supervision data for pre-training are also discussed in this work. Balanced sampling with respect to the last response in the distance supervision data are shown to be reliable for <a href=https://en.wikipedia.org/wiki/Data_augmentation>data augmentation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1073.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1073 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1073 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1073/>Finding Dominant User Utterances And System Responses in Conversations</a></strong><br><a href=/people/d/dhiraj-madan/>Dhiraj Madan</a>
|
<a href=/people/s/sachindra-joshi/>Sachindra Joshi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1073><div class="card-body p-3 small">There are several dialog frameworks which allow manual specification of intents and rule based dialog flow. The rule based framework provides good control to dialog designers at the expense of being more time consuming and laborious. The job of a dialog designer can be reduced if we could identify pairs of user intents and corresponding responses automatically from prior conversations between users and agents. In this paper we propose an approach to find these frequent user utterances (which serve as examples for intents) and corresponding agent responses. We propose a novel SimCluster algorithm that extends standard <a href=https://en.wikipedia.org/wiki/K-means_clustering>K-means algorithm</a> to simultaneously cluster user utterances and agent utterances by taking their adjacency information into account. The method also aligns these <a href=https://en.wikipedia.org/wiki/Cluster_analysis>clusters</a> to provide pairs of <a href=https://en.wikipedia.org/wiki/Intention>intents</a> and response groups. We compare our results with those produced by using simple Kmeans clustering on a real dataset and observe upto 10 % absolute improvement in <a href=https://en.wikipedia.org/wiki/F-number>F1-scores</a>. Through our experiments on synthetic dataset, we show that our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> gains more advantage over K-means algorithm when the data has large variance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1074.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1074 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1074 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1074" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1074/>End-to-End Task-Completion Neural Dialogue Systems</a></strong><br><a href=/people/x/xiujun-li/>Xiujun Li</a>
|
<a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/l/lihong-li/>Lihong Li</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1074><div class="card-body p-3 small">One of the major drawbacks of modularized task-completion dialogue systems is that each <a href=https://en.wikipedia.org/wiki/Modular_programming>module</a> is trained individually, which presents several challenges. For example, downstream modules are affected by earlier <a href=https://en.wikipedia.org/wiki/Modular_programming>modules</a>, and the performance of the entire <a href=https://en.wikipedia.org/wiki/System>system</a> is not robust to the accumulated errors. This paper presents a novel end-to-end learning framework for task-completion dialogue systems to tackle such issues. Our neural dialogue system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks. The reinforcement learning based dialogue manager offers robust capabilities to handle <a href=https://en.wikipedia.org/wiki/Noise>noises</a> caused by other components of the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a>. Our experiments in a movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines for both objective and subjective evaluation, but also is robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to the language understanding module.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1077.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1077 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1077 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1077/>Domain Adaptation from User-level Facebook Models to County-level Twitter Predictions<span class=acl-fixed-case>F</span>acebook Models to County-level <span class=acl-fixed-case>T</span>witter Predictions</a></strong><br><a href=/people/d/daniel-rieman/>Daniel Rieman</a>
|
<a href=/people/k/kokil-jaidka/>Kokil Jaidka</a>
|
<a href=/people/h/h-andrew-schwartz/>H. Andrew Schwartz</a>
|
<a href=/people/l/lyle-ungar/>Lyle Ungar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1077><div class="card-body p-3 small">Several studies have demonstrated how language models of user attributes, such as <a href=https://en.wikipedia.org/wiki/Personality>personality</a>, can be built by using the Facebook language of social media users in conjunction with their responses to psychology questionnaires. It is challenging to apply these models to make general predictions about attributes of communities, such as personality distributions across US counties, because it requires 1. the potentially inavailability of the original training data because of privacy and ethical regulations, 2. adapting Facebook language models to Twitter language without retraining the model, and 3. adapting from users to county-level collections of tweets. We propose a two-step algorithm, Target Side Domain Adaptation (TSDA) for such domain adaptation when no labeled Twitter / county data is available. TSDA corrects for the different word distributions between <a href=https://en.wikipedia.org/wiki/Facebook>Facebook</a> and <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> and for the varying word distributions across counties by adjusting target side word frequencies ; no changes to the trained model are made. In the case of predicting the <a href=https://en.wikipedia.org/wiki/Big_Five_personality_traits>Big Five county-level personality traits</a>, TSDA outperforms a state-of-the-art domain adaptation method, gives county-level predictions that have fewer extreme outliers, higher year-to-year stability, and higher correlation with county-level outcomes.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1078.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1078 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1078 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1078/>Recognizing Explicit and Implicit Hate Speech Using a Weakly Supervised Two-path Bootstrapping Approach</a></strong><br><a href=/people/l/lei-gao/>Lei Gao</a>
|
<a href=/people/a/alexis-kuppersmith/>Alexis Kuppersmith</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1078><div class="card-body p-3 small">In the wake of a polarizing election, <a href=https://en.wikipedia.org/wiki/Social_media>social media</a> is laden with hateful content. To address various limitations of supervised hate speech classification methods including corpus bias and huge cost of annotation, we propose a weakly supervised two-path bootstrapping approach for an online hate speech detection model leveraging large-scale unlabeled data. This system significantly outperforms hate speech detection systems that are trained in a <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised manner</a> using manually annotated data. Applying this model on a large quantity of tweets collected before, after, and on election day reveals motivations and patterns of inflammatory language.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1081.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1081 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1081 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1081/>Concept-Map-Based Multi-Document Summarization using Concept Coreference Resolution and Global Importance Optimization</a></strong><br><a href=/people/t/tobias-falke/>Tobias Falke</a>
|
<a href=/people/c/christian-m-meyer/>Christian M. Meyer</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1081><div class="card-body p-3 small">Concept-map-based multi-document summarization is a variant of traditional <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarization</a> that produces structured summaries in the form of <a href=https://en.wikipedia.org/wiki/Concept_map>concept maps</a>. In this work, we propose a new <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a> that addresses several issues in previous methods. It learns to identify and merge coreferent concepts to reduce redundancy, determines their importance with a strong <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised model</a> and finds an optimal summary concept map via <a href=https://en.wikipedia.org/wiki/Integer_linear_programming>integer linear programming</a>. It is also computationally more efficient than previous methods, allowing us to summarize larger document sets. We evaluate the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> on two datasets, finding that it outperforms several approaches from previous work.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1082.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1082 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1082 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1082/>Abstractive Multi-document Summarization by Partial Tree Extraction, Recombination and Linearization</a></strong><br><a href=/people/l/litton-j-kurisinkel/>Litton J Kurisinkel</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1082><div class="card-body p-3 small">Existing work for abstractive multidocument summarization utilise existing <a href=https://en.wikipedia.org/wiki/Phrase_structure>phrase structures</a> directly extracted from input documents to generate summary sentences. These methods can suffer from lack of consistence and coherence in merging phrases. We introduce a novel approach for abstractive multidocument summarization through partial dependency tree extraction, <a href=https://en.wikipedia.org/wiki/Genetic_recombination>recombination</a> and <a href=https://en.wikipedia.org/wiki/Linearization>linearization</a>. The method entrusts the <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarizer</a> to generate its own topically coherent sequential structures from scratch for effective communication. Results on TAC 2011, DUC-2004 and 2005 show that our system gives competitive results compared with state of the art abstractive summarization approaches in the literature. We also achieve competitive results in linguistic quality assessed by <a href=https://en.wikipedia.org/wiki/Evaluation>human evaluators</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1083.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1083 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1083 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1083/>Event Argument Identification on Dependency Graphs with Bidirectional LSTMs<span class=acl-fixed-case>LSTM</span>s</a></strong><br><a href=/people/a/alex-judea/>Alex Judea</a>
|
<a href=/people/m/michael-strube/>Michael Strube</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1083><div class="card-body p-3 small">In this paper we investigate the performance of event argument identification. We show that the performance is tied to syntactic complexity. Based on this finding, we propose a novel and effective <a href=https://en.wikipedia.org/wiki/System>system</a> for event argument identification. Recurrent Neural Networks learn to produce meaningful representations of long and short dependency paths. Convolutional Neural Networks learn to decompose the lexical context of argument candidates. They are combined into a simple system which outperforms a feature-based, state-of-the-art event argument identifier without any manual feature engineering.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1084.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1084 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1084 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1084/>Selective Decoding for Cross-lingual Open Information Extraction</a></strong><br><a href=/people/s/sheng-zhang/>Sheng Zhang</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1084><div class="card-body p-3 small">Cross-lingual open information extraction is the task of distilling facts from the source language into representations in the target language. We propose a novel encoder-decoder model for this <a href=https://en.wikipedia.org/wiki/Problem_solving>problem</a>. It employs a novel selective decoding mechanism, which explicitly models the sequence labeling process as well as the sequence generation process on the decoder side. Compared to a standard encoder-decoder model, selective decoding significantly increases the performance on a Chinese-English cross-lingual open IE dataset by 3.87-4.49 BLEU and 1.91-5.92 <a href=https://en.wikipedia.org/wiki/F-number>F1</a>. We also extend our approach to low-resource scenarios, and gain promising improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1085.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1085 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1085 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1085/>Event Ordering with a Generalized Model for Sieve Prediction Ranking</a></strong><br><a href=/people/b/bill-mcdowell/>Bill McDowell</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/a/alexander-ororbia-ii/>Alexander Ororbia II</a>
|
<a href=/people/d/david-reitter/>David Reitter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1085><div class="card-body p-3 small">This paper improves on several aspects of a sieve-based event ordering architecture, CAEVO (Chambers et al., 2014), which creates globally consistent temporal relations between events and time expressions. First, we examine the usage of <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> and semantic role features. With the incorporation of these new features, we demonstrate a 5 % relative F1 gain over our replicated version of CAEVO. Second, we reformulate the architecture&#8217;s sieve-based inference algorithm as a prediction reranking method that approximately optimizes a scoring function computed using classifier precisions. Within this prediction reranking framework, we propose an alternative scoring function, showing an 8.8 % relative gain over the original CAEVO. We further include an in-depth analysis of one of the main datasets that is used to evaluate temporal classifiers, and we show how despite using the densest corpus, there is still a danger of <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. While this paper focuses on temporal ordering, its results are applicable to other areas that use sieve-based architectures.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1086.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1086 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1086 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1086/>Open Relation Extraction and Grounding</a></strong><br><a href=/people/d/dian-yu/>Dian Yu</a>
|
<a href=/people/l/lifu-huang/>Lifu Huang</a>
|
<a href=/people/h/heng-ji/>Heng Ji</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1086><div class="card-body p-3 small">Previous open Relation Extraction (open RE) approaches mainly rely on linguistic patterns and constraints to extract important relational triples from large-scale corpora. However, they lack of abilities to cover diverse relation expressions or measure the relative importance of candidate triples within a sentence. It is also challenging to name the relation type of a relational triple merely based on context words, which could limit the usefulness of open RE in downstream applications. We propose a novel importance-based open RE approach by exploiting the global structure of a dependency tree to extract salient triples. We design an unsupervised relation type naming method by grounding relational triples to a large-scale Knowledge Base (KB) schema, leveraging KB triples and weighted context words associated with relational triples. Experiments on the English Slot Filling 2013 dataset demonstrate that our <a href=https://en.wikipedia.org/wiki/Methodology>approach</a> achieves 8.1 % higher <a href=https://en.wikipedia.org/wiki/F-score>F-score</a> over state-of-the-art open RE methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1087.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1087 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1087 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1087/>Extraction of Gene-Environment Interaction from the Biomedical Literature</a></strong><br><a href=/people/j/jinseon-you/>Jinseon You</a>
|
<a href=/people/j/jin-woo-chung/>Jin-Woo Chung</a>
|
<a href=/people/w/wonsuk-yang/>Wonsuk Yang</a>
|
<a href=/people/j/jong-c-park/>Jong C. Park</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1087><div class="card-body p-3 small">Genetic information in the literature has been extensively looked into for the purpose of discovering the etiology of a disease. As the gene-disease relation is sensitive to external factors, their identification is important to study a disease. Environmental influences, which are usually called Gene-Environment interaction (GxE), have been considered as important factors and have extensively been researched in <a href=https://en.wikipedia.org/wiki/Biology>biology</a>. Nevertheless, there is still a lack of systems for automatic GxE extraction from the biomedical literature due to new challenges : (1) there are no preprocessing tools and corpora for GxE, (2) expressions of GxE are often quite implicit, and (3) document-level comprehension is usually required. We propose to overcome these challenges with neural network models and show that a modified sequence-to-sequence model with a static RNN decoder produces a good performance in GxE recognition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1088.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1088 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1088 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1088/>Course Concept Extraction in MOOCs via Embedding-Based Graph Propagation<span class=acl-fixed-case>MOOC</span>s via Embedding-Based Graph Propagation</a></strong><br><a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/x/xiaochen-wang/>Xiaochen Wang</a>
|
<a href=/people/c/chengjiang-li/>Chengjiang Li</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a>
|
<a href=/people/j/jie-tang/>Jie Tang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1088><div class="card-body p-3 small">Massive Open Online Courses (MOOCs), offering a new way to study online, are revolutionizing education. One challenging issue in <a href=https://en.wikipedia.org/wiki/Massive_open_online_course>MOOCs</a> is how to design effective and fine-grained course concepts such that students with different backgrounds can grasp the essence of the course. In this paper, we conduct a systematic investigation of the problem of course concept extraction for <a href=https://en.wikipedia.org/wiki/Massive_open_online_course>MOOCs</a>. We propose to learn latent representations for <a href=https://en.wikipedia.org/wiki/Feasible_region>candidate concepts</a> via an embedding-based method. Moreover, we develop a graph-based propagation algorithm to rank the candidate concepts based on the learned <a href=https://en.wikipedia.org/wiki/Representation_(mathematics)>representations</a>. We evaluate the proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> using different <a href=https://en.wikipedia.org/wiki/Course_(education)>courses</a> from XuetangX and <a href=https://en.wikipedia.org/wiki/Coursera>Coursera</a>. Experimental results show that our method significantly outperforms all the alternative methods (+0.013-0.318 in terms of R-precision ; p0.01, t-test).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1089.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1089 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1089 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1089/>Identity Deception Detection</a></strong><br><a href=/people/v/veronica-perez-rosas/>Verónica Pérez-Rosas</a>
|
<a href=/people/q/quincy-davenport/>Quincy Davenport</a>
|
<a href=/people/a/anna-mengdan-dai/>Anna Mengdan Dai</a>
|
<a href=/people/m/mohamed-abouelenien/>Mohamed Abouelenien</a>
|
<a href=/people/r/rada-mihalcea/>Rada Mihalcea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1089><div class="card-body p-3 small">This paper addresses the task of detecting identity deception in <a href=https://en.wikipedia.org/wiki/Language>language</a>. Using a novel identity deception dataset, consisting of real and portrayed identities from 600 individuals, we show that we can build accurate identity detectors targeting both age and gender, with accuracies of up to 88. We also perform an analysis of the <a href=https://en.wikipedia.org/wiki/Pattern>linguistic patterns</a> used in <a href=https://en.wikipedia.org/wiki/Identity_deception>identity deception</a>, which lead to interesting insights into identity portrayers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1091.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1091 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1091 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1091.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/I17-1091/>Dataset for a Neural Natural Language Interface for Databases (NNLIDB)<span class=acl-fixed-case>NNLIDB</span>)</a></strong><br><a href=/people/f/florin-brad/>Florin Brad</a>
|
<a href=/people/r/radu-cristian-alexandru-iacob/>Radu Cristian Alexandru Iacob</a>
|
<a href=/people/i/ionel-alexandru-hosu/>Ionel Alexandru Hosu</a>
|
<a href=/people/t/traian-rebedea/>Traian Rebedea</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1091><div class="card-body p-3 small">Progress in natural language interfaces to databases (NLIDB) has been slow mainly due to linguistic issues (such as language ambiguity) and domain portability. Moreover, the lack of a large corpus to be used as a standard benchmark has made data-driven approaches difficult to develop and compare. In this paper, we revisit the problem of NLIDBs and recast it as a sequence translation problem. To this end, we introduce a large dataset extracted from the Stack Exchange Data Explorer website, which can be used for training neural natural language interfaces for databases. We also report encouraging baseline results on a smaller manually annotated test corpus, obtained using an attention-based sequence-to-sequence neural network.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1093.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1093 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1093 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1093/>Demographic Word Embeddings for Racism Detection on Twitter<span class=acl-fixed-case>T</span>witter</a></strong><br><a href=/people/m/mohammed-hasanuzzaman/>Mohammed Hasanuzzaman</a>
|
<a href=/people/g/gael-dias/>Gaël Dias</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1093><div class="card-body p-3 small">Most <a href=https://en.wikipedia.org/wiki/Social_media>social media platforms</a> grant users freedom of speech by allowing them to freely express their thoughts, beliefs, and opinions. Although this represents incredible and unique communication opportunities, it also presents important challenges. Online racism is such an example. In this study, we present a supervised learning strategy to detect racist language on <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a> based on <a href=https://en.wikipedia.org/wiki/Word_embedding>word embedding</a> that incorporate demographic (Age, Gender, and Location) information. Our <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> achieves reasonable <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification accuracy</a> over a gold standard dataset (F1=76.3 %) and significantly improves over the <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance of demographic-agnostic models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1095.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1095 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1095 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1095.Software.txt data-toggle=tooltip data-placement=top title=Software><i class="fas fa-file-code"></i>
</a><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1095.Datasets.txt data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1095" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1095/>Semantic Document Distance Measures and Unsupervised Document Revision Detection</a></strong><br><a href=/people/x/xiaofeng-zhu/>Xiaofeng Zhu</a>
|
<a href=/people/d/diego-klabjan/>Diego Klabjan</a>
|
<a href=/people/p/patrick-bless/>Patrick Bless</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1095><div class="card-body p-3 small">In this paper, we model the document revision detection problem as a minimum cost branching problem that relies on computing document distances. Furthermore, we propose two new document distance measures, word vector-based Dynamic Time Warping (wDTW) and word vector-based Tree Edit Distance (wTED). Our revision detection system is designed for a large scale corpus and implemented in <a href=https://en.wikipedia.org/wiki/Apache_Spark>Apache Spark</a>. We demonstrate that our system can more precisely detect revisions than state-of-the-art methods by utilizing the Wikipedia revision dumps and simulated data sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1096.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1096 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1096 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1096/>An Empirical Analysis of Multiple-Turn Reasoning Strategies in Reading Comprehension Tasks</a></strong><br><a href=/people/y/yelong-shen/>Yelong Shen</a>
|
<a href=/people/x/xiaodong-liu/>Xiaodong Liu</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/j/jianfeng-gao/>Jianfeng Gao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1096><div class="card-body p-3 small">Reading comprehension (RC) is a challenging task that requires synthesis of information across sentences and multiple turns of reasoning. Using a state-of-the-art RC model, we empirically investigate the performance of single-turn and multiple-turn reasoning on the SQuAD and MS MARCO datasets. The <a href=https://en.wikipedia.org/wiki/RC_model>RC model</a> is an end-to-end neural network with iterative attention, and uses <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> to dynamically control the number of turns. We find that multiple-turn reasoning outperforms single-turn reasoning for all question and answer types ; further, we observe that enabling a flexible number of turns generally improves upon a fixed multiple-turn strategy. % across all question types, and is particularly beneficial to questions with lengthy, descriptive answers. We achieve results competitive to the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on these two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1097.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1097 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1097 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1097/>Automated Historical Fact-Checking by Passage Retrieval, Word Statistics, and Virtual Question-Answering</a></strong><br><a href=/people/m/mio-kobayashi/>Mio Kobayashi</a>
|
<a href=/people/a/ai-ishii/>Ai Ishii</a>
|
<a href=/people/c/chikara-hoshino/>Chikara Hoshino</a>
|
<a href=/people/h/hiroshi-miyashita/>Hiroshi Miyashita</a>
|
<a href=/people/t/takuya-matsuzaki/>Takuya Matsuzaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1097><div class="card-body p-3 small">This paper presents a hybrid approach to the verification of statements about historical facts. The test data was collected from the world history examinations in a standardized achievement test for high school students. The <a href=https://en.wikipedia.org/wiki/Data>data</a> includes various kinds of false statements that were carefully written so as to deceive the students while they can be disproven on the basis of the teaching materials. Our system predicts the truth or falsehood of a statement based on text search, word cooccurrence statistics, factoid-style question answering, and temporal relation recognition. These <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> contribute to the judgement complementarily and achieved the state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1098.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1098 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1098 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1098/>Integrating Subject, Type, and Property Identification for Simple <a href=https://en.wikipedia.org/wiki/Question_answering>Question Answering</a> over Knowledge Base</a></strong><br><a href=/people/w/wei-chuan-hsiao/>Wei-Chuan Hsiao</a>
|
<a href=/people/h/hen-hsen-huang/>Hen-Hsen Huang</a>
|
<a href=/people/h/hsin-hsi-chen/>Hsin-Hsi Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1098><div class="card-body p-3 small">This paper presents an approach to identify subject, type and property from knowledge base (KB) for answering simple questions. We propose new <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> to rank entity candidates in KB. Besides, we split a relation in <a href=https://en.wikipedia.org/wiki/Kibibyte>KB</a> into type and property. Each of <a href=https://en.wikipedia.org/wiki/Matrix_(mathematics)>them</a> is modeled by a bi-directional LSTM. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> achieves the state-of-the-art performance on the SimpleQuestions dataset. The hard questions in the experiments are also analyzed in detail.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1099.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1099 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1099 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/I17-1099.Datasets.zip data-toggle=tooltip data-placement=top title=Dataset><i class="fas fa-file-archive"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1099" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1099/>DailyDialog : A Manually Labelled Multi-turn Dialogue Dataset<span class=acl-fixed-case>D</span>aily<span class=acl-fixed-case>D</span>ialog: A Manually Labelled Multi-turn Dialogue Dataset</a></strong><br><a href=/people/y/yanran-li/>Yanran Li</a>
|
<a href=/people/h/hui-su/>Hui Su</a>
|
<a href=/people/x/xiaoyu-shen/>Xiaoyu Shen</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/z/ziqiang-cao/>Ziqiang Cao</a>
|
<a href=/people/s/shuzi-niu/>Shuzi Niu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1099><div class="card-body p-3 small">We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects. The <a href=https://en.wikipedia.org/wiki/Language>language</a> is human-written and less noisy. The dialogues in the <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> reflect our daily communication way and cover various topics about our daily life. We also manually label the developed <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with <a href=https://en.wikipedia.org/wiki/Intentionality>communication intention</a> and <a href=https://en.wikipedia.org/wiki/Emotion>emotion information</a>. Then, we evaluate existing approaches on DailyDialog dataset and hope it benefit the research field of dialog systems. The <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> is available on<b>DailyDialog</b>, which is intriguing in several aspects. The language is human-written and less noisy. The dialogues in the dataset reflect our daily communication way and cover various topics about our daily life. We also manually label the developed dataset with communication intention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it benefit the research field of dialog systems. The dataset is available on <url>http://yanran.li/dailydialog</url>\n</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1100 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/I17-1100/>Inference is Everything : Recasting Semantic Resources into a Unified Evaluation Framework</a></strong><br><a href=/people/a/aaron-steven-white/>Aaron Steven White</a>
|
<a href=/people/p/pushpendre-rastogi/>Pushpendre Rastogi</a>
|
<a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/b/benjamin-van-durme/>Benjamin Van Durme</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1100><div class="card-body p-3 small">We propose to unify a variety of existing semantic classification tasks, such as <a href=https://en.wikipedia.org/wiki/Semantic_role_labeling>semantic role labeling</a>, <a href=https://en.wikipedia.org/wiki/Anaphora_(linguistics)>anaphora resolution</a>, and <a href=https://en.wikipedia.org/wiki/Paraphrase_detection>paraphrase detection</a>, under the heading of Recognizing Textual Entailment (RTE). We present a general <a href=https://en.wikipedia.org/wiki/Strategy>strategy</a> to automatically generate one or more sentential hypotheses based on an input sentence and pre-existing manual semantic annotations. The resulting suite of <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> enables us to probe a statistical RTE model&#8217;s performance on different aspects of <a href=https://en.wikipedia.org/wiki/Semantics>semantics</a>. We demonstrate the value of this approach by investigating the behavior of a popular neural network RTE model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/I17-1102.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-I17-1102 data-toggle=collapse aria-expanded=false aria-controls=abstract-I17-1102 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=I17-1102" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/I17-1102/>Multilingual Hierarchical Attention Networks for Document Classification</a></strong><br><a href=/people/n/nikolaos-pappas/>Nikolaos Pappas</a>
|
<a href=/people/a/andrei-popescu-belis/>Andrei Popescu-Belis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-I17-1102><div class="card-body p-3 small">Hierarchical attention networks have recently achieved remarkable performance for <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a> in a given language. However, when multilingual document collections are considered, training such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> separately for each language entails linear parameter growth and lack of cross-language transfer. Learning a single multilingual model with fewer parameters is therefore a challenging but potentially beneficial objective. To this end, we propose multilingual hierarchical attention networks for learning document structures, with shared encoders and/or shared attention mechanisms across languages, using <a href=https://en.wikipedia.org/wiki/Multi-task_learning>multi-task learning</a> and an aligned semantic space as input. We evaluate the proposed models on multilingual document classification with disjoint label sets, on a large dataset which we provide, with 600k news documents in 8 languages, and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of <a href=https://en.wikipedia.org/wiki/Language_transfer>cross-language transfer</a>.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>