<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/D18-2.pdf>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></h2><p class=lead><a href=/people/e/eduardo-blanco/>Eduardo Blanco</a>,
<a href=/people/w/wei-lu/>Wei Lu</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>D18-2</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2018</dd><dt>Address:</dt><dd>Brussels, Belgium</dd><dt>Venue:</dt><dd><a href=/venues/emnlp/>EMNLP</a></dd><dt>SIG:</dt><dd><a href=/sigs/sigdat/>SIGDAT</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/D18-2>https://aclanthology.org/D18-2</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/D18-2.pdf>https://aclanthology.org/D18-2.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/D18-2.pdf title="Open PDF of 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2018+Conference+on+Empirical+Methods+in+Natural+Language+Processing%3A+System+Demonstrations" title="Search for 'Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2000/>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</a></strong><br><a href=/people/e/eduardo-blanco/>Eduardo Blanco</a>
|
<a href=/people/w/wei-lu/>Wei Lu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2001 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2001" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2001/>SyntaViz : Visualizing Voice Queries through a Syntax-Driven Hierarchical Ontology<span class=acl-fixed-case>S</span>ynta<span class=acl-fixed-case>V</span>iz: Visualizing Voice Queries through a Syntax-Driven Hierarchical Ontology</a></strong><br><a href=/people/m/md-iftekhar-tanveer/>Md Iftekhar Tanveer</a>
|
<a href=/people/f/ferhan-ture/>Ferhan Ture</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2001><div class="card-body p-3 small">This paper describes SyntaViz, a visualization interface specifically designed for analyzing natural-language queries that were created by users of a voice-enabled product. SyntaViz provides a platform for browsing the ontology of user queries from a syntax-driven perspective, providing quick access to high-impact failure points of the existing intent understanding system and evidence for data-driven decisions in the development cycle. A case study on Xfinity X1 (a voice-enabled entertainment platform from Comcast) reveals that SyntaViz helps developers identify multiple action items in a short amount of time without any special training. SyntaViz has been open-sourced for the benefit of the community.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2005 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2005/>MorAz : an Open-source Morphological Analyzer for Azerbaijani Turkish<span class=acl-fixed-case>M</span>or<span class=acl-fixed-case>A</span>z: an Open-source Morphological Analyzer for <span class=acl-fixed-case>A</span>zerbaijani <span class=acl-fixed-case>T</span>urkish</a></strong><br><a href=/people/b/berke-ozenc/>Berke Özenç</a>
|
<a href=/people/r/razieh-ehsani/>Razieh Ehsani</a>
|
<a href=/people/e/ercan-solak/>Ercan Solak</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2005><div class="card-body p-3 small">MorAz is an open-source <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyzer</a> for <a href=https://en.wikipedia.org/wiki/Azerbaijani_language>Azerbaijani Turkish</a>. The analyzer is available through both as a website for interactive exploration and as a <a href=https://en.wikipedia.org/wiki/Representational_state_transfer>RESTful web service</a> for integration into a <a href=https://en.wikipedia.org/wiki/Pipeline_(software)>natural language processing pipeline</a>. MorAz implements the <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphology</a> of <a href=https://en.wikipedia.org/wiki/Azerbaijani_language>Azerbaijani Turkish</a> in two-level using Helsinki finite-state transducer and wraps the analyzer with python scripts in a Django instance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2007 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2007/>Visual Interrogation of Attention-Based Models for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>Natural Language Inference</a> and Machine Comprehension</a></strong><br><a href=/people/s/shusen-liu/>Shusen Liu</a>
|
<a href=/people/t/tao-li/>Tao Li</a>
|
<a href=/people/z/zhimin-li/>Zhimin Li</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a>
|
<a href=/people/v/valerio-pascucci/>Valerio Pascucci</a>
|
<a href=/people/p/peer-timo-bremer/>Peer-Timo Bremer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2007><div class="card-body p-3 small">Neural networks models have gained unprecedented popularity in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> due to their state-of-the-art performance and the flexible end-to-end training scheme. Despite their advantages, the lack of interpretability hinders the deployment and refinement of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>. In this work, we present a flexible visualization library for creating customized visual analytic environments, in which the user can investigate and interrogate the relationships among the input, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model internals</a> (i.e., <a href=https://en.wikipedia.org/wiki/Attentional_control>attention</a>), and the output predictions, which in turn shed light on the <a href=https://en.wikipedia.org/wiki/Decision-making_software>model decision-making process</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2008.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2008 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2008 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2008/>DERE : A Task and Domain-Independent Slot Filling Framework for Declarative Relation Extraction<span class=acl-fixed-case>DERE</span>: A Task and Domain-Independent Slot Filling Framework for Declarative Relation Extraction</a></strong><br><a href=/people/h/heike-adel/>Heike Adel</a>
|
<a href=/people/l/laura-ana-maria-oberlander/>Laura Ana Maria Bostan</a>
|
<a href=/people/s/sean-papay/>Sean Papay</a>
|
<a href=/people/s/sebastian-pado/>Sebastian Padó</a>
|
<a href=/people/r/roman-klinger/>Roman Klinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2008><div class="card-body p-3 small">Most <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning systems</a> for <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> are tailored to specific tasks. As a result, comparability of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> across tasks is missing and their applicability to new tasks is limited. This affects end users without machine learning experience as well as model developers. To address these limitations, we present DERE, a novel <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a> for declarative specification and compilation of template-based information extraction. It uses a generic <a href=https://en.wikipedia.org/wiki/Specification_language>specification language</a> for the task and for data annotations in terms of spans and <a href=https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)>frames</a>. This formalism enables the representation of a large variety of natural language processing challenges. The <a href=https://en.wikipedia.org/wiki/Front_and_back_ends>backend</a> can be instantiated by different <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, following different paradigms. The clear separation of frame specification and <a href=https://en.wikipedia.org/wiki/Modeling_language>model backend</a> will ease the implementation of new <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and the evaluation of different <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> across different tasks. Furthermore, it simplifies <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a>, joint learning across tasks and/or domains as well as the assessment of model generalizability. DERE is available as open-source software.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2009 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2009/>Demonstrating Par4Sem-A Semantic Writing Aid with Adaptive Paraphrasing<span class=acl-fixed-case>P</span>ar4<span class=acl-fixed-case>S</span>em - A Semantic Writing Aid with Adaptive Paraphrasing</a></strong><br><a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2009><div class="card-body p-3 small">In this paper, we present Par4Sem, a semantic writing aid tool based on adaptive paraphrasing. Unlike many annotation tools that are primarily used to collect training examples, Par4Sem is integrated into a real word application, in this case a writing aid tool, in order to collect training examples from usage data. Par4Sem is a tool, which supports an adaptive, iterative, and interactive process where the underlying <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning models</a> are updated for each iteration using new training examples from usage data. After motivating the use of ever-learning tools in NLP applications, we evaluate Par4Sem by adopting it to a text simplification task through mere usage.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2010 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2010/>Juman++ : A Morphological Analysis Toolkit for Scriptio Continua<span class=acl-fixed-case>J</span>uman++: A Morphological Analysis Toolkit for Scriptio Continua</a></strong><br><a href=/people/a/arseny-tolmachev/>Arseny Tolmachev</a>
|
<a href=/people/d/daisuke-kawahara/>Daisuke Kawahara</a>
|
<a href=/people/s/sadao-kurohashi/>Sadao Kurohashi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2010><div class="card-body p-3 small">We present a three-part toolkit for developing <a href=https://en.wikipedia.org/wiki/Morphology_(linguistics)>morphological analyzers</a> for languages without natural word boundaries. The first part is a C++11/14 lattice-based morphological analysis library that uses a combination of linear and recurrent neural net language models for analysis. The other parts are a tool for exposing problems in the trained <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> and a partial annotation tool. Our morphological analyzer of <a href=https://en.wikipedia.org/wiki/Japanese_language>Japanese</a> achieves new SOTA on Jumandic-based corpora while being 250 times faster than the previous one. We also perform a small experiment and quantitive analysis and experience of using <a href=https://en.wikipedia.org/wiki/Programming_tool>development tools</a>. All <a href=https://en.wikipedia.org/wiki/Component-based_software_engineering>components</a> of the <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is open source and available under a permissive Apache 2 License.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2012 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2012/>SentencePiece : A simple and language independent subword tokenizer and detokenizer for Neural Text Processing<span class=acl-fixed-case>S</span>entence<span class=acl-fixed-case>P</span>iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a></strong><br><a href=/people/t/taku-kudo/>Taku Kudo</a>
|
<a href=/people/j/john-richardson/>John Richardson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2012><div class="card-body p-3 small">This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>Neural Machine Translation</a>. It provides open-source <a href=https://en.wikipedia.org/wiki/C++>C++</a> and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at.<url>https://github.com/google/sentencepiece</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2013.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2013 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2013 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2013/>CogCompTime : A Tool for Understanding Time in <a href=https://en.wikipedia.org/wiki/Natural_language>Natural Language</a><span class=acl-fixed-case>C</span>og<span class=acl-fixed-case>C</span>omp<span class=acl-fixed-case>T</span>ime: A Tool for Understanding Time in Natural Language</a></strong><br><a href=/people/q/qiang-ning/>Qiang Ning</a>
|
<a href=/people/b/ben-zhou/>Ben Zhou</a>
|
<a href=/people/z/zhili-feng/>Zhili Feng</a>
|
<a href=/people/h/haoruo-peng/>Haoruo Peng</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2013><div class="card-body p-3 small">Automatic extraction of temporal information is important for <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language understanding</a>. It involves two basic <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a> : (1) Understanding time expressions that are mentioned explicitly in text (e.g., February 27, 1998 or tomorrow), and (2) Understanding <a href=https://en.wikipedia.org/wiki/Time>temporal information</a> that is conveyed implicitly via <a href=https://en.wikipedia.org/wiki/Binary_relation>relations</a>. This paper introduces CogCompTime, a <a href=https://en.wikipedia.org/wiki/System>system</a> that has these two important functionalities. It incorporates the most recent progress, achieves state-of-the-art performance, and is publicly available at.<url>http://cogcomp.org/page/publication_view/844</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2014.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2014 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2014 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2014/>A Multilingual Information Extraction Pipeline for Investigative Journalism</a></strong><br><a href=/people/g/gregor-wiedemann/>Gregor Wiedemann</a>
|
<a href=/people/s/seid-muhie-yimam/>Seid Muhie Yimam</a>
|
<a href=/people/c/chris-biemann/>Chris Biemann</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2014><div class="card-body p-3 small">We introduce an advanced information extraction pipeline to automatically process very large collections of <a href=https://en.wikipedia.org/wiki/Unstructured_data>unstructured textual data</a> for the purpose of <a href=https://en.wikipedia.org/wiki/Investigative_journalism>investigative journalism</a>. The <a href=https://en.wikipedia.org/wiki/Pipeline_(computing)>pipeline</a> serves as a new input processor for the upcoming major release of our New / s / leak 2.0 software, which we develop in cooperation with a large German news organization. The use case is that journalists receive a large collection of files up to several Gigabytes containing unknown contents. Collections may originate either from official disclosures of documents, e.g. Freedom of Information Act requests, or unofficial data leaks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2016 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2016" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2016/>KT-Speech-Crawler : Automatic Dataset Construction for <a href=https://en.wikipedia.org/wiki/Speech_recognition>Speech Recognition</a> from YouTube Videos<span class=acl-fixed-case>KT</span>-Speech-Crawler: Automatic Dataset Construction for Speech Recognition from <span class=acl-fixed-case>Y</span>ou<span class=acl-fixed-case>T</span>ube Videos</a></strong><br><a href=/people/e/egor-lakomkin/>Egor Lakomkin</a>
|
<a href=/people/s/sven-magg/>Sven Magg</a>
|
<a href=/people/c/cornelius-weber/>Cornelius Weber</a>
|
<a href=/people/s/stefan-wermter/>Stefan Wermter</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2016><div class="card-body p-3 small">We describe KT-Speech-Crawler : an approach for automatic dataset construction for <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a> by crawling <a href=https://en.wikipedia.org/wiki/YouTube>YouTube videos</a>. We outline several filtering and post-processing steps, which extract samples that can be used for training end-to-end neural speech recognition systems. In our experiments, we demonstrate that a single-core version of the <a href=https://en.wikipedia.org/wiki/Web_crawler>crawler</a> can obtain around 150 hours of <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcribed speech</a> within a day, containing an estimated 3.5 % <a href=https://en.wikipedia.org/wiki/Word_error_rate>word error rate</a> in the <a href=https://en.wikipedia.org/wiki/Transcription_(linguistics)>transcriptions</a>. Automatically collected samples contain reading and spontaneous speech recorded in various conditions including background noise and music, distant microphone recordings, and a variety of <a href=https://en.wikipedia.org/wiki/Accent_(sociolinguistics)>accents</a> and <a href=https://en.wikipedia.org/wiki/Reverberation>reverberation</a>. When training a <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural network</a> on <a href=https://en.wikipedia.org/wiki/Speech_recognition>speech recognition</a>, we observed around 40 % word error rate reduction on the Wall Street Journal dataset by integrating 200 hours of the collected samples into the training set.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2018 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2018/>An Interface for Annotating Science Questions</a></strong><br><a href=/people/m/michael-boratko/>Michael Boratko</a>
|
<a href=/people/h/harshit-padigela/>Harshit Padigela</a>
|
<a href=/people/d/divyendra-mikkilineni/>Divyendra Mikkilineni</a>
|
<a href=/people/p/pritish-yuvraj/>Pritish Yuvraj</a>
|
<a href=/people/r/rajarshi-das/>Rajarshi Das</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a>
|
<a href=/people/m/maria-chang/>Maria Chang</a>
|
<a href=/people/a/achille-fokoue-nkoutche/>Achille Fokoue</a>
|
<a href=/people/p/pavan-kapanipathi/>Pavan Kapanipathi</a>
|
<a href=/people/n/nicholas-mattei/>Nicholas Mattei</a>
|
<a href=/people/r/ryan-musa/>Ryan Musa</a>
|
<a href=/people/k/kartik-talamadupula/>Kartik Talamadupula</a>
|
<a href=/people/m/michael-j-witbrock/>Michael Witbrock</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2018><div class="card-body p-3 small">Recent work introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into an Easy Set and a Challenge Set. That work includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them. However, it does not include clear definitions of these types, nor does it offer information about the quality of the labels or the annotation process used. In this paper, we introduce a novel interface for human annotation of science question-answer pairs with their respective knowledge and reasoning types, in order that the classification of new questions may be improved. We build on the classification schema proposed by prior work on the ARC dataset, and evaluate the effectiveness of our interface with a preliminary study involving 10 participants.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2019.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2019 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2019 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2019/>APLenty : annotation tool for creating high-quality datasets using active and proactive learning<span class=acl-fixed-case>APL</span>enty: annotation tool for creating high-quality datasets using active and proactive learning</a></strong><br><a href=/people/m/minh-quoc-nghiem/>Minh-Quoc Nghiem</a>
|
<a href=/people/s/sophia-ananiadou/>Sophia Ananiadou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2019><div class="card-body p-3 small">In this paper, we present APLenty, an annotation tool for creating high-quality sequence labeling datasets using active and proactive learning. A major innovation of our tool is the integration of automatic annotation with <a href=https://en.wikipedia.org/wiki/Active_learning>active learning</a> and <a href=https://en.wikipedia.org/wiki/Proactive_learning>proactive learning</a>. This makes the task of creating labeled datasets easier, less time-consuming and requiring less human effort. APLenty is highly flexible and can be adapted to various other <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2020.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2020 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2020 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2020" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2020/>Interactive Instance-based Evaluation of Knowledge Base Question Answering</a></strong><br><a href=/people/d/daniil-sorokin/>Daniil Sorokin</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2020><div class="card-body p-3 small">Most approaches to Knowledge Base Question Answering are based on <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. In this paper, we present a tool that aids in debugging of question answering systems that construct a structured semantic representation for the input question. Previous work has largely focused on building <a href=https://en.wikipedia.org/wiki/Question_answering>question answering interfaces</a> or evaluation frameworks that unify multiple data sets. The primary objective of our <a href=https://en.wikipedia.org/wiki/System>system</a> is to enable interactive debugging of model predictions on individual instances (questions) and to simplify manual error analysis. Our interactive interface helps researchers to understand the shortcomings of a particular <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>, qualitatively analyze the complete <a href=https://en.wikipedia.org/wiki/Pipeline_transport>pipeline</a> and compare different <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. A set of sit-by sessions was used to validate our <a href=https://en.wikipedia.org/wiki/User_interface_design>interface design</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2021 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2021/>Magnitude : A Fast, Efficient Universal Vector Embedding Utility Package<span class=acl-fixed-case>M</span>agnitude: A Fast, Efficient Universal Vector Embedding Utility Package</a></strong><br><a href=/people/a/ajay-patel/>Ajay Patel</a>
|
<a href=/people/a/alexander-sands/>Alexander Sands</a>
|
<a href=/people/c/chris-callison-burch/>Chris Callison-Burch</a>
|
<a href=/people/m/marianna-apidianaki/>Marianna Apidianaki</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2021><div class="card-body p-3 small">Vector space embedding models like <a href=https://en.wikipedia.org/wiki/Word2vec>word2vec</a>, <a href=https://en.wikipedia.org/wiki/GloVe_(machine_learning)>GloVe</a>, and <a href=https://en.wikipedia.org/wiki/FastText>fastText</a> are extremely popular representations in natural language processing (NLP) applications. We present Magnitude, a fast, lightweight tool for utilizing and processing <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. Magnitude is an open source <a href=https://en.wikipedia.org/wiki/Python_(programming_language)>Python package</a> with a compact vector storage file format that allows for efficient manipulation of huge numbers of <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a>. Magnitude performs common <a href=https://en.wikipedia.org/wiki/Instruction_set_architecture>operations</a> up to 60 to 6,000 times faster than <a href=https://en.wikipedia.org/wiki/Gensim>Gensim</a>. Magnitude introduces several novel <a href=https://en.wikipedia.org/wiki/Software_feature>features</a> for improved <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> like out-of-vocabulary lookups.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2022 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2022/>Integrating Knowledge-Supported Search into the INCEpTION Annotation Platform<span class=acl-fixed-case>INCE</span>p<span class=acl-fixed-case>TION</span> Annotation Platform</a></strong><br><a href=/people/b/beto-boullosa/>Beto Boullosa</a>
|
<a href=/people/r/richard-eckart-de-castilho/>Richard Eckart de Castilho</a>
|
<a href=/people/n/naveen-kumar-laskari/>Naveen Kumar</a>
|
<a href=/people/j/jan-christoph-klie/>Jan-Christoph Klie</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2022><div class="card-body p-3 small">Annotating entity mentions and linking them to a knowledge resource are essential tasks in many domains. It disambiguates mentions, introduces cross-document coreferences, and the resources contribute extra information, e.g. taxonomic relations. Such tasks benefit from <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text annotation tools</a> that integrate a <a href=https://en.wikipedia.org/wiki/Search_engine_technology>search</a> which covers the <a href=https://en.wikipedia.org/wiki/Text_(literary_theory)>text</a>, the <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a>, as well as the knowledge resource. However, to the best of our knowledge, no current tools integrate knowledge-supported search as well as entity linking support. We address this gap by introducing knowledge-supported search functionality into the INCEpTION text annotation platform. In our approach, cross-document references are created by linking entity mentions to a <a href=https://en.wikipedia.org/wiki/Knowledge_base>knowledge base</a> in the form of a structured hierarchical vocabulary. The resulting <a href=https://en.wikipedia.org/wiki/Annotation>annotations</a> are then indexed to enable fast and yet complex <a href=https://en.wikipedia.org/wiki/Information_retrieval>queries</a> taking into account the text, the annotations, and the vocabulary structure.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2023.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2023 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2023 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2023" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2023/>CytonMT : an Efficient Neural Machine Translation Open-source Toolkit Implemented in C++<span class=acl-fixed-case>C</span>yton<span class=acl-fixed-case>MT</span>: an Efficient Neural Machine Translation Open-source Toolkit Implemented in <span class=acl-fixed-case>C</span>++</a></strong><br><a href=/people/x/xiaolin-wang/>Xiaolin Wang</a>
|
<a href=/people/m/masao-utiyama/>Masao Utiyama</a>
|
<a href=/people/e/eiichiro-sumita/>Eiichiro Sumita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2023><div class="card-body p-3 small">This paper presents an open-source neural machine translation toolkit named CytonMT. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> is built from scratch only using <a href=https://en.wikipedia.org/wiki/C++>C++</a> and NVIDIA&#8217;s GPU-accelerated libraries. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a> features training efficiency, code simplicity and translation quality. Benchmarks show that cytonMT accelerates the training speed by 64.5 % to 110.8 % on <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> of various sizes, and achieves competitive translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2024 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=D18-2024" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/D18-2024/>OpenKE : An Open Toolkit for Knowledge Embedding<span class=acl-fixed-case>O</span>pen<span class=acl-fixed-case>KE</span>: An Open Toolkit for Knowledge Embedding</a></strong><br><a href=/people/x/xu-han/>Xu Han</a>
|
<a href=/people/s/shulin-cao/>Shulin Cao</a>
|
<a href=/people/x/xin-lv/>Xin Lv</a>
|
<a href=/people/y/yankai-lin/>Yankai Lin</a>
|
<a href=/people/z/zhiyuan-liu/>Zhiyuan Liu</a>
|
<a href=/people/m/maosong-sun/>Maosong Sun</a>
|
<a href=/people/j/juanzi-li/>Juanzi Li</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2024><div class="card-body p-3 small">We release an open toolkit for knowledge embedding (OpenKE), which provides a unified framework and various fundamental models to embed knowledge graphs into a continuous low-dimensional space. OpenKE prioritizes <a href=https://en.wikipedia.org/wiki/Operational_efficiency>operational efficiency</a> to support quick model validation and large-scale knowledge representation learning. Meanwhile, OpenKE maintains sufficient modularity and extensibility to easily incorporate new <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> into the <a href=https://en.wikipedia.org/wiki/Software_framework>framework</a>. Besides the toolkit, the embeddings of some existing large-scale knowledge graphs pre-trained by OpenKE are also available, which can be directly applied for many applications including <a href=https://en.wikipedia.org/wiki/Information_retrieval>information retrieval</a>, personalized recommendation and <a href=https://en.wikipedia.org/wiki/Question_answering>question answering</a>. The <a href=https://en.wikipedia.org/wiki/List_of_toolkits>toolkit</a>, documentation, and pre-trained embeddings are all released on.<url>http://openke.thunlp.org/</url>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2025.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2025 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2025 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2025/>LIA : A Natural Language Programmable Personal Assistant<span class=acl-fixed-case>LIA</span>: A Natural Language Programmable Personal Assistant</a></strong><br><a href=/people/i/igor-labutov/>Igor Labutov</a>
|
<a href=/people/s/shashank-srivastava/>Shashank Srivastava</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2025><div class="card-body p-3 small">We present LIA, an intelligent personal assistant that can be programmed using <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language</a>. Our <a href=https://en.wikipedia.org/wiki/System>system</a> demonstrates multiple competencies towards learning from <a href=https://en.wikipedia.org/wiki/Human&#8211;computer_interaction>human-like interactions</a>. These include the ability to be taught reusable conditional procedures, the ability to be taught new knowledge about the world (concepts in an ontology) and the ability to be taught how to ground that knowledge in a set of <a href=https://en.wikipedia.org/wiki/Sensor>sensors</a> and effectors. Building such a system highlights design questions regarding the overall architecture that such an <a href=https://en.wikipedia.org/wiki/Intelligent_agent>agent</a> should have, as well as questions about <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a> and grounding language in situational contexts. We outline key properties of this <a href=https://en.wikipedia.org/wiki/Architecture>architecture</a>, and demonstrate a <a href=https://en.wikipedia.org/wiki/Prototype>prototype</a> that embodies them in the form of a <a href=https://en.wikipedia.org/wiki/Personal_assistant>personal assistant</a> on an <a href=https://en.wikipedia.org/wiki/Android_(operating_system)>Android device</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2026 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2026/>PizzaPal : Conversational Pizza Ordering using a High-Density Conversational AI Platform<span class=acl-fixed-case>P</span>izza<span class=acl-fixed-case>P</span>al: Conversational Pizza Ordering using a High-Density Conversational <span class=acl-fixed-case>AI</span> Platform</a></strong><br><a href=/people/a/antoine-raux/>Antoine Raux</a>
|
<a href=/people/y/yi-ma/>Yi Ma</a>
|
<a href=/people/p/paul-yang/>Paul Yang</a>
|
<a href=/people/f/felicia-wong/>Felicia Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2026><div class="card-body p-3 small">This paper describes PizzaPal, a voice-only agent for ordering pizza, as well as the Conversational AI architecture built at b4.ai. Based on the principles of high-density conversational AI, it supports natural and flexible interactions through neural conversational language understanding, robust dialog state tracking, and hierarchical task decomposition.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2027 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2027/>Developing Production-Level Conversational Interfaces with Shallow Semantic Parsing</a></strong><br><a href=/people/a/arushi-raghuvanshi/>Arushi Raghuvanshi</a>
|
<a href=/people/l/lucien-carroll/>Lucien Carroll</a>
|
<a href=/people/k/karthik-raghunathan/>Karthik Raghunathan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2027><div class="card-body p-3 small">We demonstrate an end-to-end approach for building conversational interfaces from prototype to production that has proven to work well for a number of applications across diverse verticals. Our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> improves on the standard domain-intent-entity classification hierarchy and dialogue management architecture by leveraging shallow semantic parsing. We observe that NLU systems for industry applications often require more structured representations of entity relations than provided by the standard <a href=https://en.wikipedia.org/wiki/Hierarchy>hierarchy</a>, yet without requiring full semantic parses which are often inaccurate on real-world conversational data. We distinguish two kinds of semantic properties that can be provided through shallow semantic parsing : <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity groups</a> and <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity roles</a>. We also provide live demos of conversational apps built for two different <a href=https://en.wikipedia.org/wiki/Use_case>use cases</a> : <a href=https://en.wikipedia.org/wiki/Online_food_ordering>food ordering</a> and meeting control.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2028.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2028 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2028 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2028/>When <a href=https://en.wikipedia.org/wiki/Science_journalism>science journalism</a> meets <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a> : An interactive demonstration</a></strong><br><a href=/people/r/raghuram-vadapalli/>Raghuram Vadapalli</a>
|
<a href=/people/b/bakhtiyar-syed/>Bakhtiyar Syed</a>
|
<a href=/people/n/nishant-prabhu/>Nishant Prabhu</a>
|
<a href=/people/b/balaji-vasan-srinivasan/>Balaji Vasan Srinivasan</a>
|
<a href=/people/v/vasudeva-varma/>Vasudeva Varma</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2028><div class="card-body p-3 small">We present an online interactive tool that generates titles of blog titles and thus take the first step toward automating <a href=https://en.wikipedia.org/wiki/Science_journalism>science journalism</a>. Science journalism aims to transform jargon-laden scientific articles into a form that the common reader can comprehend while ensuring that the underlying meaning of the article is retained. In this work, we present a tool, which, given the title and abstract of a research paper will generate a blog title by mimicking a human science journalist. The tool makes use of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> trained on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> of 87,328 pairs of research papers and their corresponding <a href=https://en.wikipedia.org/wiki/Blog>blogs</a>, built from two <a href=https://en.wikipedia.org/wiki/News_aggregator>science news aggregators</a>. The architecture of the <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is a two-stage mechanism which generates <a href=https://en.wikipedia.org/wiki/Blog>blog titles</a>. Evaluation using standard <a href=https://en.wikipedia.org/wiki/Metric_(mathematics)>metrics</a> indicate the viability of the proposed <a href=https://en.wikipedia.org/wiki/System>system</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D18-2029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D18-2029 data-toggle=collapse aria-expanded=false aria-controls=abstract-D18-2029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D18-2029/>Universal Sentence Encoder for English<span class=acl-fixed-case>E</span>nglish</a></strong><br><a href=/people/d/daniel-cer/>Daniel Cer</a>
|
<a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/s/sheng-yi-kong/>Sheng-yi Kong</a>
|
<a href=/people/n/nan-hua/>Nan Hua</a>
|
<a href=/people/n/nicole-limtiaco/>Nicole Limtiaco</a>
|
<a href=/people/r/rhomni-st-john/>Rhomni St. John</a>
|
<a href=/people/n/noah-constant/>Noah Constant</a>
|
<a href=/people/m/mario-guajardo-cespedes/>Mario Guajardo-Cespedes</a>
|
<a href=/people/s/steve-yuan/>Steve Yuan</a>
|
<a href=/people/c/chris-tar/>Chris Tar</a>
|
<a href=/people/b/brian-strope/>Brian Strope</a>
|
<a href=/people/r/ray-kurzweil/>Ray Kurzweil</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D18-2029><div class="card-body p-3 small">We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> and <a href=https://en.wikipedia.org/wiki/Computational_resource>compute resources</a>. We report the relationship between model complexity, <a href=https://en.wikipedia.org/wiki/Resource_(computer_science)>resources</a>, and transfer performance. Comparisons are made with <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> without <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and to <a href=https://en.wikipedia.org/wiki/Baseline_(configuration_management)>baselines</a> that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without <a href=https://en.wikipedia.org/wiki/Transfer_learning>transfer learning</a> and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>