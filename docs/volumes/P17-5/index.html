<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/P17-5.pdf>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></h2><p class=lead><a href=/people/m/maja-popovic/>Maja Popović</a>,
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>P17-5</dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Vancouver, Canada</dd><dt>Venue:</dt><dd><a href=/venues/acl/>ACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/P17-5>https://aclanthology.org/P17-5</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/P17-5.pdf>https://aclanthology.org/P17-5.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/P17-5.pdf title="Open PDF of 'Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+55th+Annual+Meeting+of+the+Association+for+Computational+Linguistics%3A+Tutorial+Abstracts" title="Search for 'Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-5000/>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</a></strong><br><a href=/people/m/maja-popovic/>Maja Popović</a>
|
<a href=/people/j/jordan-boyd-graber/>Jordan Boyd-Graber</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-5001 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-5001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-5001/>NLP for Precision Medicine<span class=acl-fixed-case>NLP</span> for Precision Medicine</a></strong><br><a href=/people/h/hoifung-poon/>Hoifung Poon</a>
|
<a href=/people/c/chris-quirk/>Chris Quirk</a>
|
<a href=/people/k/kristina-toutanova/>Kristina Toutanova</a>
|
<a href=/people/w/wen-tau-yih/>Wen-tau Yih</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-5001><div class="card-body p-3 small">We will introduce <a href=https://en.wikipedia.org/wiki/Precision_medicine>precision medicine</a> and showcase the vast opportunities for <a href=https://en.wikipedia.org/wiki/Neuro-linguistic_programming>NLP</a> in this burgeoning field with great societal impact. We will review pressing NLP problems, state-of-the art methods, and important applications, as well as <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a>, medical resources, and practical issues. The tutorial will provide an accessible overview of <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedicine</a>, and does not presume knowledge in <a href=https://en.wikipedia.org/wiki/Biology>biology</a> or <a href=https://en.wikipedia.org/wiki/Health_care>healthcare</a>. The ultimate goal is to reduce the entry barrier for NLP researchers to contribute to this exciting domain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-5002 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-5002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-5002/>Multimodal Machine Learning : Integrating Language, <a href=https://en.wikipedia.org/wiki/Visual_perception>Vision</a> and Speech</a></strong><br><a href=/people/l/louis-philippe-morency/>Louis-Philippe Morency</a>
|
<a href=/people/t/tadas-baltrusaitis/>Tadas Baltrušaitis</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-5002><div class="card-body p-3 small">Multimodal machine learning is a vibrant multi-disciplinary research field which addresses some of the original goals of <a href=https://en.wikipedia.org/wiki/Artificial_intelligence>artificial intelligence</a> by integrating and modeling multiple communicative modalities, including linguistic, acoustic and visual messages. With the initial research on audio-visual speech recognition and more recently with image and video captioning projects, this research field brings some unique challenges for multimodal researchers given the heterogeneity of the data and the contingency often found between modalities. This tutorial builds upon a recent course taught at Carnegie Mellon University during the Spring 2016 semester (CMU course 11-777) and two tutorials presented at CVPR 2016 and ICMI 2016. The present tutorial will review fundamental concepts of <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and deep neural networks before describing the five main challenges in multimodal machine learning : (1) multimodal representation learning, (2) translation & mapping, (3) modality alignment, (4) multimodal fusion and (5) co-learning. The tutorial will also present state-of-the-art <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that were recently proposed to solve multimodal applications such as image captioning, video descriptions and visual question-answer. We will also discuss the current and upcoming challenges.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-5004 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-5004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234950627 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-5004/>Deep Learning for Dialogue Systems</a></strong><br><a href=/people/y/yun-nung-chen/>Yun-Nung Chen</a>
|
<a href=/people/a/asli-celikyilmaz/>Asli Celikyilmaz</a>
|
<a href=/people/d/dilek-hakkani-tur/>Dilek Hakkani-Tür</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-5004><div class="card-body p-3 small">In the past decade, goal-oriented spoken dialogue systems have been the most prominent component in today&#8217;s virtual personal assistants. The classic <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> have rather complex and/or modular pipelines. The advance of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning technologies</a> has recently risen the applications of neural models to dialogue modeling. However, how to successfully apply deep learning based approaches to a <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> is still challenging. Hence, this tutorial is designed to focus on an overview of the <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue system</a> development while describing most recent research for building <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> and summarizing the challenges, in order to allow researchers to study the potential improvements of the state-of-the-art <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a>. The tutorial material is available at http://deepdialogue.miulab.tw.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-5005 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-5005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/P17-5005/>Beyond Words : <a href=https://en.wikipedia.org/wiki/Deep_learning>Deep Learning</a> for Multiword Expressions and Collocations</a></strong><br><a href=/people/v/valia-kordoni/>Valia Kordoni</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-5005><div class="card-body p-3 small">Deep learning has recently shown much promise for NLP applications. Traditionally, in most NLP approaches, documents or sentences are represented by a sparse bag-of-words representation. There is now a lot of work which goes beyond this by adopting a distributed representation of words, by constructing a so-called neural embedding or vector space representation of each word or document. The aim of this tutorial is to go beyond the learning of word vectors and present methods for learning vector representations for <a href=https://en.wikipedia.org/wiki/Multiword_expression>Multiword Expressions</a> and bilingual phrase pairs, all of which are useful for various NLP applications. This tutorial aims to provide attendees with a clear notion of the linguistic and distributional characteristics of <a href=https://en.wikipedia.org/wiki/Multiword_expression>Multiword Expressions (MWEs)</a>, their relevance for the intersection of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep learning</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>, what methods and resources are available to support their use, and what more could be done in the future. Our target audience are researchers and practitioners in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, parsing (syntactic and semantic) and language technology, not necessarily experts in MWEs, who are interested in tasks that involve or could benefit from considering MWEs as a pervasive phenomenon in human language and communication.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/P17-5006.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-P17-5006 data-toggle=collapse aria-expanded=false aria-controls=abstract-P17-5006 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://vimeo.com/234948956 data-toggle=tooltip data-placement=top title=Video><i class="fas fa-video"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/P17-5006/>Tutorial : Making Better Use of the Crowd<span class=acl-fixed-case>T</span>utorial: Making Better Use of the Crowd</a></strong><br><a href=/people/j/jennifer-wortman-vaughan/>Jennifer Wortman Vaughan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-P17-5006><div class="card-body p-3 small">Over the last decade, <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> has been used to harness the power of human computation to solve tasks that are notoriously difficult to solve with computers alone, such as determining whether or not an image contains a <a href=https://en.wikipedia.org/wiki/Tree_(graph_theory)>tree</a>, rating the relevance of a website, or verifying the phone number of a business. The natural language processing community was early to embrace <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> as a tool for quickly and inexpensively obtaining annotated data to train <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>. Once this <a href=https://en.wikipedia.org/wiki/Data>data</a> is collected, it can be handed off to <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> that learn to perform basic NLP tasks such as <a href=https://en.wikipedia.org/wiki/Translation>translation</a> or <a href=https://en.wikipedia.org/wiki/Parsing>parsing</a>. Usually this handoff is where interaction with the crowd ends. The crowd provides the data, but the ultimate goal is to eventually take humans out of the loop. Are there better ways to make use of the crowd?In this tutorial, I will begin with a showcase of innovative uses of <a href=https://en.wikipedia.org/wiki/Crowdsourcing>crowdsourcing</a> that go beyond <a href=https://en.wikipedia.org/wiki/Data_collection>data collection</a> and <a href=https://en.wikipedia.org/wiki/Annotation>annotation</a>. I will discuss applications to <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> and <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a>, hybrid intelligence or human in the loop AI systems that leverage the complementary strengths of humans and machines in order to achieve more than either could achieve alone, and large scale studies of human behavior online. I will then spend the majority of the tutorial diving into recent research aimed at understanding who crowdworkers are, how they behave, and what this should teach us about best practices for interacting with the crowd.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>