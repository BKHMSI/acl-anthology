<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/W17-43.pdf>Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing</a></h2><p class=lead><a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>,
<a href=/people/m/ming-wei-chang/>Ming-Wei Chang</a>,
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a>,
<a href=/people/a/alexander-m-rush/>Alexander M. Rush</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>W17-43</dd><dt>Month:</dt><dd>September</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Copenhagen, Denmark</dd><dt>Venue:</dt><dd><a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/W17-43>https://aclanthology.org/W17-43</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/W17-43 title="To the current version of the paper by DOI">10.18653/v1/W17-43</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/W17-43.pdf>https://aclanthology.org/W17-43.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/W17-43.pdf title="Open PDF of 'Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2nd+Workshop+on+Structured+Prediction+for+Natural+Language+Processing" title="Search for 'Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4300.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4300/>Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing</a></strong><br><a href=/people/k/kai-wei-chang/>Kai-Wei Chang</a>
|
<a href=/people/m/ming-wei-chang/>Ming-Wei Chang</a>
|
<a href=/people/v/vivek-srikumar/>Vivek Srikumar</a>
|
<a href=/people/a/alexander-m-rush/>Alexander M. Rush</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4301.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4301 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4301 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4301/>Dependency Parsing with Dilated Iterated Graph CNNs<span class=acl-fixed-case>CNN</span>s</a></strong><br><a href=/people/e/emma-strubell/>Emma Strubell</a>
|
<a href=/people/a/andrew-mccallum/>Andrew McCallum</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4301><div class="card-body p-3 small">Dependency parses are an effective way to inject linguistic knowledge into many downstream tasks, and many practitioners wish to efficiently parse sentences at scale. Recent advances in <a href=https://en.wikipedia.org/wiki/Graphics_processing_unit>GPU hardware</a> have enabled <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> to achieve significant gains over the previous best models, these models still fail to leverage GPUs&#8217; capability for <a href=https://en.wikipedia.org/wiki/Massively_parallel>massive parallelism</a> due to their requirement of sequential processing of the sentence. In response, we propose Dilated Iterated Graph Convolutional Neural Networks (DIG-CNNs) for graph-based dependency parsing, a graph convolutional architecture that allows for efficient end-to-end GPU parsing. In experiments on the English Penn TreeBank benchmark, we show that DIG-CNNs perform on par with some of the best <a href=https://en.wikipedia.org/wiki/Parsing>neural network parsers</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4302.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4302 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4302 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-4302" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-4302/>Entity Identification as Multitasking</a></strong><br><a href=/people/k/karl-stratos/>Karl Stratos</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4302><div class="card-body p-3 small">Standard approaches in <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity identification</a> hard-code boundary detection and type prediction into labels and perform <a href=https://en.wikipedia.org/wiki/Viterbi_algorithm>Viterbi</a>. This has two disadvantages : 1. the <a href=https://en.wikipedia.org/wiki/Run_time_(program_lifecycle_phase)>runtime complexity</a> grows quadratically in the number of types, and 2. there is no natural segment-level representation. In this paper, we propose a neural architecture that addresses these disadvantages. We frame the problem as <a href=https://en.wikipedia.org/wiki/Computer_multitasking>multitasking</a>, separating boundary detection and type prediction but optimizing them jointly. Despite its simplicity, this <a href=https://en.wikipedia.org/wiki/Computer_architecture>architecture</a> performs competitively with fully structured models such as BiLSTM-CRFs while scaling linearly in the number of types. Furthermore, by construction, the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> induces type-disambiguating embeddings of predicted mentions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4303.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4303 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4303 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/W17-4303.Attachment.zip data-toggle=tooltip data-placement=top title=Attachment><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/W17-4303/>Towards Neural Machine Translation with Latent Tree Attention</a></strong><br><a href=/people/j/james-bradbury/>James Bradbury</a>
|
<a href=/people/r/richard-socher/>Richard Socher</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4303><div class="card-body p-3 small">Building <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> that take advantage of the hierarchical structure of language without a priori annotation is a longstanding goal in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. We introduce such a model for the task of <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>, pairing a recurrent neural network grammar encoder with a novel attentional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target. When trained on character-level datasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4304.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4304 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4304 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4304/>Structured Prediction via Learning to Search under Bandit Feedback</a></strong><br><a href=/people/a/amr-sharaf/>Amr Sharaf</a>
|
<a href=/people/h/hal-daume-iii/>Hal Daumé III</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4304><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for <a href=https://en.wikipedia.org/wiki/Structured_prediction>structured prediction</a> under online bandit feedback. The <a href=https://en.wikipedia.org/wiki/Learning>learner</a> repeatedly predicts a sequence of actions, generating a structured output. It then observes <a href=https://en.wikipedia.org/wiki/Feedback>feedback</a> for that <a href=https://en.wikipedia.org/wiki/Output_(economics)>output</a> and no others. We consider two cases : a pure bandit setting in which it only observes a loss, and more fine-grained feedback in which it observes a loss for every action. We find that the fine-grained feedback is necessary for strong empirical performance, because it allows for a robust variance-reduction strategy. We empirically compare a number of different <a href=https://en.wikipedia.org/wiki/Algorithm>algorithms</a> and exploration methods and show the efficacy of BLS on sequence labeling and dependency parsing tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4305.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4305 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4305 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4305/>Syntax Aware LSTM model for <a href=https://en.wikipedia.org/wiki/Semantic_Role_Labeling>Semantic Role Labeling</a><span class=acl-fixed-case>LSTM</span> model for Semantic Role Labeling</a></strong><br><a href=/people/f/feng-qian/>Feng Qian</a>
|
<a href=/people/l/lei-sha/>Lei Sha</a>
|
<a href=/people/b/baobao-chang/>Baobao Chang</a>
|
<a href=/people/l/lu-chen-liu/>Lu-chen Liu</a>
|
<a href=/people/m/ming-zhang/>Ming Zhang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4305><div class="card-body p-3 small">In Semantic Role Labeling (SRL) task, the tree structured dependency relation is rich in syntax information, but it is not well handled by existing models. In this paper, we propose Syntax Aware Long Short Time Memory (SA-LSTM). The structure of SA-LSTM changes according to dependency structure of each sentence, so that SA-LSTM can model the whole tree structure of dependency relation in an architecture engineering way. Experiments demonstrate that on Chinese Proposition Bank (CPB) 1.0, SA-LSTM improves <a href=https://en.wikipedia.org/wiki/F-number>F1</a> by 2.06 % than ordinary bi-LSTM with feature engineered dependency relation information, and gives state-of-the-art <a href=https://en.wikipedia.org/wiki/F-number>F1</a> of 79.92 %. On English CoNLL 2005 dataset, SA-LSTM brings improvement (2.1 %) to bi-LSTM model and also brings slight improvement (0.3 %) when added to the <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art model</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4307.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4307 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4307 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/W17-4307/>Boosting Information Extraction Systems with Character-level Neural Networks and Free Noisy Supervision</a></strong><br><a href=/people/p/philipp-meerkamp/>Philipp Meerkamp</a>
|
<a href=/people/z/zhengyi-zhou/>Zhengyi Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4307><div class="card-body p-3 small">We present an <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> to boost the <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>precision</a> of existing <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction systems</a>. This is achieved by augmenting the existing <a href=https://en.wikipedia.org/wiki/Parsing>parser</a>, which may be constraint-based or hybrid statistical, with a character-level neural network. Our architecture combines the ability of constraint-based or hybrid extraction systems to easily incorporate <a href=https://en.wikipedia.org/wiki/Domain_knowledge>domain knowledge</a> with the ability of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a> to leverage large amounts of data to learn complex features. The <a href=https://en.wikipedia.org/wiki/Computer_network>network</a> is trained using a measure of consistency between extracted data and existing <a href=https://en.wikipedia.org/wiki/Database>databases</a> as a form of cheap, noisy supervision. Our <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> does not require large scale manual annotation or a <a href=https://en.wikipedia.org/wiki/Code_refactoring>system rewrite</a>. It has led to large precision improvements over an existing, highly-tuned production information extraction system used at <a href=https://en.wikipedia.org/wiki/Bloomberg_L.P.>Bloomberg LP</a> for financial language text.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/W17-4308.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-W17-4308 data-toggle=collapse aria-expanded=false aria-controls=abstract-W17-4308 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=W17-4308" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/W17-4308/>Piecewise Latent Variables for Neural Variational Text Processing</a></strong><br><a href=/people/i/iulian-vlad-serban/>Iulian Vlad Serban</a>
|
<a href=/people/a/alexander-ororbia-ii/>Alexander Ororbia II</a>
|
<a href=/people/j/joelle-pineau/>Joelle Pineau</a>
|
<a href=/people/a/aaron-courville/>Aaron Courville</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-W17-4308><div class="card-body p-3 small">Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> will learn to represent rich, multi-modal latent factors in real-world data, such as <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language text</a>. However, current models often assume simplistic priors on the latent variables-such as the uni-modal Gaussian distribution-which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This <a href=https://en.wikipedia.org/wiki/Probability_distribution>distribution</a> has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and <a href=https://en.wikipedia.org/wiki/Natural-language_generation>natural language generation</a> for <a href=https://en.wikipedia.org/wiki/Dialogue>dialogue</a>.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>