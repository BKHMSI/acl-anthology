<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of Machine Translation Summit XVIII: Research Track - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2021.mtsummit-research.pdf>Proceedings of Machine Translation Summit XVIII: Research Track</a></h2><p class=lead><a href=/people/k/kevin-duh/>Kevin Duh</a>,
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.mtsummit-research</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Virtual</dd><dt>Venue:</dt><dd><a href=/venues/mtsummit/>MTSummit</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Machine Translation in the Americas</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.mtsummit-research>https://aclanthology.org/2021.mtsummit-research</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.mtsummit-research.pdf>https://aclanthology.org/2021.mtsummit-research.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.mtsummit-research.pdf title="Open PDF of 'Proceedings of Machine Translation Summit XVIII: Research Track'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+Machine+Translation+Summit+XVIII%3A+Research+Track" title="Search for 'Proceedings of Machine Translation Summit XVIII: Research Track' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.0/>Proceedings of Machine Translation Summit XVIII: Research Track</a></strong><br><a href=/people/k/kevin-duh/>Kevin Duh</a>
|
<a href=/people/f/francisco-guzman/>Francisco Guzmán</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.1/>Learning Curricula for Multilingual Neural Machine Translation Training</a></strong><br><a href=/people/g/gaurav-kumar/>Gaurav Kumar</a>
|
<a href=/people/p/philipp-koehn/>Philipp Koehn</a>
|
<a href=/people/s/sanjeev-khudanpur/>Sanjeev Khudanpur</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--1><div class="card-body p-3 small">Low-resource Multilingual Neural Machine Translation (MNMT) is typically tasked with improving the <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance on one or more language pairs with the aid of high-resource language pairs. In this paper and we propose two simple search based curricula orderings of the multilingual training data which help improve <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance in conjunction with existing techniques such as <a href=https://en.wikipedia.org/wiki/Fine-tuning>fine-tuning</a>. Additionally and we attempt to learn a <a href=https://en.wikipedia.org/wiki/Curriculum>curriculum</a> for MNMT from scratch jointly with the training of the translation system using contextual multi-arm bandits. We show on the FLORES low-resource translation dataset that these learned curricula can provide better starting points for fine tuning and improve overall performance of the translation system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.5/>Transformers for Low-Resource Languages : Is Fidir Linn !</a></strong><br><a href=/people/s/seamus-lankford/>Seamus Lankford</a>
|
<a href=/people/h/haithem-alfi/>Haithem Alfi</a>
|
<a href=/people/a/andy-way/>Andy Way</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--5><div class="card-body p-3 small">The Transformer model is the state-of-the-art in <a href=https://en.wikipedia.org/wiki/Machine_translation>Machine Translation</a>. However and in general and neural translation models often under perform on language pairs with insufficient training data. As a consequence and relatively few experiments have been carried out using this <a href=https://en.wikipedia.org/wiki/Software_architecture>architecture</a> on low-resource language pairs. In this study and hyperparameter optimization of Transformer models in translating the low-resource English-Irish language pair is evaluated. We demonstrate that choosing appropriate parameters leads to considerable performance improvements. Most importantly and the correct choice of subword model is shown to be the biggest driver of <a href=https://en.wikipedia.org/wiki/Translation>translation</a> performance. SentencePiece models using both unigram and BPE approaches were appraised. Variations on model architectures included modifying the number of layers and testing various <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization techniques</a> and evaluating the optimal number of heads for <a href=https://en.wikipedia.org/wiki/Attention>attention</a>. A generic 55k DGT corpus and an in-domain 88k public admin corpus were used for evaluation. A Transformer optimized model demonstrated a BLEU score improvement of 7.8 points when compared with a baseline <a href=https://en.wikipedia.org/wiki/Real-time_locating_system>RNN model</a>. Improvements were observed across a range of metrics and including TER and indicating a substantially reduced post editing effort for Transformer optimized models with 16k BPE subword models. Bench-marked against <a href=https://en.wikipedia.org/wiki/Google_Translate>Google Translate</a> and our translation engines demonstrated significant improvements. The question of whether or not <a href=https://en.wikipedia.org/wiki/Transformers_(toy_line)>Transformers</a> can be used effectively in a low-resource setting of English-Irish translation has been addressed. Is fidir linn-yes we can.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.6.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--6 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.6 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.6/>The Effect of Domain and Diacritics in YorubaEnglish Neural Machine Translation<span class=acl-fixed-case>Y</span>oruba–<span class=acl-fixed-case>E</span>nglish Neural Machine Translation</a></strong><br><a href=/people/d/david-adelani/>David Adelani</a>
|
<a href=/people/d/dana-ruiter/>Dana Ruiter</a>
|
<a href=/people/j/jesujoba-alabi/>Jesujoba Alabi</a>
|
<a href=/people/d/damilola-adebonojo/>Damilola Adebonojo</a>
|
<a href=/people/a/adesina-ayeni/>Adesina Ayeni</a>
|
<a href=/people/m/mofe-adeyemi/>Mofe Adeyemi</a>
|
<a href=/people/a/ayodele-esther-awokoya/>Ayodele Esther Awokoya</a>
|
<a href=/people/c/cristina-espana-bonet/>Cristina España-Bonet</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--6><div class="card-body p-3 small">Massively multilingual machine translation (MT) has shown impressive capabilities and including zero and few-shot translation between low-resource language pairs. However and these <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due to lack of standardized evaluation datasets. In this paper and we present MENYO-20k and the first multi-domain parallel corpus with a especially curated orthography for YorubaEnglish with standardized train-test splits for benchmarking. We provide several neural MT benchmarks and compare them to the performance of popular pre-trained (massively multilingual) MT models both for the heterogeneous test set and its subdomains. Since these pre-trained models use huge amounts of data with uncertain quality and we also analyze the effect of diacritics and a major characteristic of <a href=https://en.wikipedia.org/wiki/Yoruba_language>Yoruba</a> and in the training data. We investigate how and when this training condition affects the final quality of a translation and its understandability. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> outperform massively multilingual models such as Google (+8.7 BLEU) and Facebook M2 M (+9.1) when translating to <a href=https://en.wikipedia.org/wiki/Yoruba_language>Yoruba</a> and setting a high quality benchmark for future research.<tex-math>+8.7</tex-math> BLEU) and Facebook M2M (<tex-math>+9.1</tex-math>) when translating to Yoruba and setting a high quality benchmark for future research.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.9.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--9 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.9 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.9/>Like Chalk and Cheese? On the Effects of Translationese in MT Training<span class=acl-fixed-case>MT</span> Training</a></strong><br><a href=/people/s/samuel-larkin/>Samuel Larkin</a>
|
<a href=/people/m/michel-simard/>Michel Simard</a>
|
<a href=/people/r/rebecca-knowles/>Rebecca Knowles</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--9><div class="card-body p-3 small">We revisit the topic of translation direction in the data used for training neural machine translation systems and focusing on a real-world scenario with known translation direction and imbalances in translation direction : the Canadian Hansard. According to automatic metrics and we observe that using parallel data that was produced in the matching translation direction (Authentic source and translationese target) improves translation quality. In cases of data imbalance in terms of translation direction and we find that tagging of translation direction can close the performance gap. We perform a human evaluation that differs slightly from the automatic metrics and but nevertheless confirms that for this French-English dataset that is known to contain high-quality translations and authentic or tagged mixed source improves over translationese source for training.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.10/>Investigating Softmax Tempering for Training Neural Machine Translation Models</a></strong><br><a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/atsushi-fujita/>Atsushi Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--10><div class="card-body p-3 small">Neural machine translation (NMT) models are typically trained using a softmax cross-entropy loss where the softmax distribution is compared against the gold labels. In low-resource scenarios and NMT models tend to perform poorly because the model training quickly converges to a point where the softmax distribution computed using <a href=https://en.wikipedia.org/wiki/Logit>logits</a> approaches the gold label distribution. Although label smoothing is a well-known solution to address this issue and we further propose to divide the logits by a <a href=https://en.wikipedia.org/wiki/Temperature_coefficient>temperature coefficient</a> greater than one and forcing the softmax distribution to be smoother during training. This makes it harder for the <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to quickly over-fit. In our experiments on 11 language pairs in the low-resource Asian Language Treebank dataset and we observed significant improvements in translation quality. Our analysis focuses on finding the right balance of label smoothing and softmax tempering which indicates that they are orthogonal methods. Finally and a study of softmax entropies and gradients reveal the impact of our method on the internal behavior of our NMT models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.11.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--11 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.11 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.11/>Scrambled Translation Problem : A Problem of Denoising UNMT<span class=acl-fixed-case>UNMT</span></a></strong><br><a href=/people/t/tamali-banerjee/>Tamali Banerjee</a>
|
<a href=/people/r/rudra-v-murthy/>Rudra V Murthy</a>
|
<a href=/people/p/pushpak-bhattacharyya/>Pushpak Bhattacharya</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--11><div class="card-body p-3 small">In this paper and we identify an interesting kind of error in the output of Unsupervised Neural Machine Translation (UNMT) systems like Undreamt1. We refer to this error type as Scrambled Translation problem. We observe that UNMT models which use word shuffle noise (as in case of Undreamt) can generate correct words and but fail to stitch them together to form phrases. As a result and words of the translated sentence look scrambled and resulting in decreased <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>. We hypothesise that the reason behind scrambled translation problem is&#8217; shuffling noise&#8217; which is introduced in every input sentence as a <a href=https://en.wikipedia.org/wiki/Noise_reduction>denoising strategy</a>. To test our hypothesis and we experiment by retraining UNMT models with a simple retraining strategy. We stop the training of the Denoising UNMT model after a pre-decided number of iterations and resume the training for the remaining iterations- which number is also pre-decided- using original sentence as input without adding any noise. Our proposed <a href=https://en.wikipedia.org/wiki/Solution>solution</a> achieves significant performance improvement UNMT models that train conventionally. We demonstrate these performance gains on four language pairs and viz. and English-French and English-German and English-Spanish and Hindi-Punjabi. Our qualitative and quantitative analysis shows that the retraining strategy helps achieve better alignment as observed by attention heatmap and better phrasal translation and leading to statistically significant improvement in BLEU scores.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mtsummit-research.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.14/>On nature and causes of observed MT errors<span class=acl-fixed-case>MT</span> errors</a></strong><br><a href=/people/m/maja-popovic/>Maja Popovic</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--14><div class="card-body p-3 small">This work describes analysis of nature and causes of MT errors observed by different evaluators under guidance of different quality criteria : <a href=https://en.wikipedia.org/wiki/Adequality>adequacy</a> and <a href=https://en.wikipedia.org/wiki/Comprehension_(logic)>comprehension</a> and and a not specified generic mixture of <a href=https://en.wikipedia.org/wiki/Adequality>adequacy</a> and <a href=https://en.wikipedia.org/wiki/Fluency>fluency</a>. We report results for three language pairs and two domains and eleven MT systems. Our findings indicate that and despite the fact that some of the identified phenomena depend on domain and/or language and the following set of phenomena can be considered as generally challenging for modern MT systems : rephrasing groups of words and translation of ambiguous source words and translating noun phrases and and mistranslations. Furthermore and we show that the quality criterion also has impact on error perception. Our findings indicate that <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension</a> and adequacy can be assessed simultaneously by different evaluators and so that <a href=https://en.wikipedia.org/wiki/Sentence_processing>comprehension</a> and as an important quality criterion and can be included more often in human evaluations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.17.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--17 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.17 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.17/>Studying The Impact Of Document-level Context On Simultaneous Neural Machine Translation</a></strong><br><a href=/people/r/raj-dabre/>Raj Dabre</a>
|
<a href=/people/a/aizhan-imankulova/>Aizhan Imankulova</a>
|
<a href=/people/m/masahiro-kaneko/>Masahiro Kaneko</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--17><div class="card-body p-3 small">In a real-time simultaneous translation setting and neural machine translation (NMT) models start generating target language tokens from incomplete source language sentences and making them harder to translate and leading to poor translation quality. Previous research has shown that document-level NMT and comprising of sentence and context encoders and a decoder and leverages context from neighboring sentences and helps improve translation quality. In simultaneous translation settings and the context from previous sentences should be even more critical. To this end and in this paper and we propose wait-k simultaneous document-level NMT where we keep the context encoder as it is and replace the source sentence encoder and target language decoder with their wait-k equivalents. We experiment with low and high resource settings using the ALT and OpenSubtitles2018 corpora and where we observe minor improvements in translation quality. We then perform an analysis of the translations obtained using our models by focusing on sentences that should benefit from the context where we found out that the model does and in fact and benefit from context but is unable to effectively leverage it and especially in a low-resource setting. This shows that there is a need for further innovation in the way useful context is identified and leveraged.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.18.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--18 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.18 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mtsummit-research.18" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.18/>Attainable Text-to-Text Machine Translation vs. <a href=https://en.wikipedia.org/wiki/Translation>Translation</a> : Issues Beyond Linguistic Processing</a></strong><br><a href=/people/a/atsushi-fujita/>Atsushi Fujita</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--18><div class="card-body p-3 small">Existing approaches for machine translation (MT) mostly translate given text in the source language into the target language and without explicitly referring to information indispensable for producing proper <a href=https://en.wikipedia.org/wiki/Translation>translation</a>. This includes not only information in other textual elements and modalities than texts in the same document and but also extra-document and non-linguistic information and such as <a href=https://en.wikipedia.org/wiki/Social_norm>norms</a> and <a href=https://en.wikipedia.org/wiki/Skopos>skopos</a>. To design better translation production work-flows and we need to distinguish translation issues that could be resolved by the existing text-to-text approaches and those beyond them. To this end and we conducted an analytic assessment of MT outputs and taking an English-to-Japanese news translation task as a case study. First and examples of <a href=https://en.wikipedia.org/wiki/Translation>translation issues</a> and their revisions were collected by a two-stage post-editing (PE) method : performing minimal PE to obtain <a href=https://en.wikipedia.org/wiki/Translation>translation</a> attainable based on the given textual information and further performing full PE to obtain truly acceptable <a href=https://en.wikipedia.org/wiki/Translation>translation</a> referring to any information if necessary. Then and the collected revision examples were manually analyzed. We revealed dominant issues and information indispensable for resolving them and such as fine-grained style specifications and terminology and domain-specific knowledge and and reference documents and delineating a clear distinction between <a href=https://en.wikipedia.org/wiki/Translation>translation</a> and what text-to-text MT can ultimately attain.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.19.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--19 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.19 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.mtsummit-research.19" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.19/>Modeling Target-side Inflection in Placeholder Translation</a></strong><br><a href=/people/r/ryokan-ri/>Ryokan Ri</a>
|
<a href=/people/t/toshiaki-nakazawa/>Toshiaki Nakazawa</a>
|
<a href=/people/y/yoshimasa-tsuruoka/>Yoshimasa Tsuruoka</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--19><div class="card-body p-3 small">Placeholder translation systems enable the users to specify how a specific phrase is translated in the output sentence. The system is trained to output special placeholder tokens and the user-specified term is injected into the output through the context-free replacement of the placeholder token. However and this approach could result in ungrammatical sentences because it is often the case that the specified term needs to be inflected according to the context of the output and which is unknown before the translation. To address this problem and we propose a novel method of placeholder translation that can inflect specified terms according to the <a href=https://en.wikipedia.org/wiki/Grammar>grammatical construction</a> of the output sentence. We extend the seq2seq architecture with a character-level decoder that takes the lemma of a user-specified term and the words generated from the word-level decoder to output a correct inflected form of the lemma. We evaluate our approach with a Japanese-to-English translation task in the scientific writing domain and and show our model can incorporate specified terms in a correct form more successfully than other comparable models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.mtsummit-research.23.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--mtsummit-research--23 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.mtsummit-research.23 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.mtsummit-research.23/>Neural Machine Translation with Inflected Lexicon</a></strong><br><a href=/people/a/artur-nowakowski/>Artur Nowakowski</a>
|
<a href=/people/k/krzysztof-jassem/>Krzysztof Jassem</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--mtsummit-research--23><div class="card-body p-3 small">The paper presents experiments in <a href=https://en.wikipedia.org/wiki/Neural_machine_translation>neural machine translation</a> with <a href=https://en.wikipedia.org/wiki/Lexical_analysis>lexical constraints</a> into a <a href=https://en.wikipedia.org/wiki/Morphological_analysis>morphologically rich language</a>. In particular and we introduce a method and based on constrained decoding and which handles the inflected forms of lexical entries and does not require any modification to the training data or model architecture. To evaluate its effectiveness and we carry out experiments in two different scenarios : general and domain-specific. We compare our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> with baseline translation and i.e. translation without <a href=https://en.wikipedia.org/wiki/Lexical_item>lexical constraints</a> and in terms of translation speed and <a href=https://en.wikipedia.org/wiki/Translation>translation quality</a>. To evaluate how well the method handles the constraints and we propose new evaluation metrics which take into account the presence and placement and duplication and inflectional correctness of lexical terms in the output sentence.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>