<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/2021.acl-short.pdf>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></h2><p class=lead><a href=/people/c/chengqing-zong/>Chengqing Zong</a>,
<a href=/people/f/fei-xia/>Fei Xia</a>,
<a href=/people/w/wenjie-li/>Wenjie Li</a>,
<a href=/people/r/roberto-navigli/>Roberto Navigli</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>2021.acl-short</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2021</dd><dt>Address:</dt><dd>Online</dd><dt>Venues:</dt><dd><a href=/venues/acl/>ACL</a>
| <a href=/venues/ijcnlp/>IJCNLP</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/2021.acl-short>https://aclanthology.org/2021.acl-short</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/2021.acl-short.pdf>https://aclanthology.org/2021.acl-short.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/2021.acl-short.pdf title="Open PDF of 'Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+59th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+and+the+11th+International+Joint+Conference+on+Natural+Language+Processing+%28Volume+2%3A+Short+Papers%29" title="Search for 'Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.0.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.0/>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</a></strong><br><a href=/people/c/chengqing-zong/>Chengqing Zong</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a>
|
<a href=/people/w/wenjie-li/>Wenjie Li</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.1.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--1 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.1 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.1/>Catchphrase : Automatic Detection of Cultural References</a></strong><br><a href=/people/n/nir-sweed/>Nir Sweed</a>
|
<a href=/people/d/dafna-shahaf/>Dafna Shahaf</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--1><div class="card-body p-3 small">A <a href=https://en.wikipedia.org/wiki/Snowclone>snowclone</a> is a customizable phrasal template that can be realized in multiple, instantly recognized variants. For example, * is the new * (Orange is the new black, 40 is the new 30). Snowclones are extensively used in <a href=https://en.wikipedia.org/wiki/Social_media>social media</a>. In this paper, we study snowclones originating from pop-culture quotes ; our goal is to automatically detect cultural references in text. We introduce a new, publicly available data set of pop-culture quotes and their corresponding snowclone usages and train models on them. We publish code for Catchphrase, an internet browser plugin to automatically detect and mark references in real-time, and examine its performance via a user study. Aside from assisting people to better comprehend cultural references, we hope that detecting snowclones can complement work on <a href=https://en.wikipedia.org/wiki/Paraphrase>paraphrasing</a> and help tackling long-standing questions in <a href=https://en.wikipedia.org/wiki/Social_science>social science</a> about the dynamics of information propagation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.5.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--5 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.5 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.5" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.5/>Difficulty-Aware Machine Translation Evaluation</a></strong><br><a href=/people/r/runzhe-zhan/>Runzhe Zhan</a>
|
<a href=/people/x/xuebo-liu/>Xuebo Liu</a>
|
<a href=/people/d/derek-f-wong/>Derek F. Wong</a>
|
<a href=/people/l/lidia-s-chao/>Lidia S. Chao</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--5><div class="card-body p-3 small">The high-quality translation results produced by machine translation (MT) systems still pose a huge challenge for automatic evaluation. Current MT evaluation pays the same attention to each sentence component, while the questions of real-world examinations (e.g., university examinations) have different difficulties and weightings. In this paper, we propose a novel difficulty-aware MT evaluation metric, expanding the evaluation dimension by taking translation difficulty into consideration. A translation that fails to be predicted by most MT systems will be treated as a difficult one and assigned a large weight in the final score function, and conversely. Experimental results on the WMT19 English-German Metrics shared tasks show that our proposed method outperforms commonly used MT metrics in terms of human correlation. In particular, our proposed <a href=https://en.wikipedia.org/wiki/Methodology>method</a> performs well even when all the MT systems are very competitive, which is when most existing <a href=https://en.wikipedia.org/wiki/Performance_metric>metrics</a> fail to distinguish between them. The source code is freely available at https://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.10.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--10 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.10 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.10/>Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions<span class=acl-fixed-case>VQA</span> Systems <span class=acl-fixed-case>RAD</span>? <span class=acl-fixed-case>M</span>easuring Robustness to Augmented Data with Focused Interventions</a></strong><br><a href=/people/d/daniel-rosenberg/>Daniel Rosenberg</a>
|
<a href=/people/i/itai-gat/>Itai Gat</a>
|
<a href=/people/a/amir-feder/>Amir Feder</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--10><div class="card-body p-3 small">Deep learning algorithms have shown promising results in visual question answering (VQA) tasks, but a more careful look reveals that they often do not understand the rich signal they are being fed with. To understand and better measure the generalization capabilities of VQA systems, we look at their <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> to counterfactually augmented data. Our proposed augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes. Using these augmentations, we propose a new robustness measure, Robustness to Augmented Data (RAD), which measures the consistency of model predictions between original and augmented examples. Through extensive experimentation, we show that RAD, unlike classical accuracy measures, can quantify when state-of-the-art systems are not robust to <a href=https://en.wikipedia.org/wiki/Counterfactual_conditional>counterfactuals</a>. We find substantial failure cases which reveal that current VQA systems are still brittle. Finally, we connect between <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> and <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a>, demonstrating the predictive power of RAD for performance on unseen augmentations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.14.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--14 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.14 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.14" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.14/>N-Best ASR Transformer : Enhancing SLU Performance using Multiple ASR Hypotheses<span class=acl-fixed-case>ASR</span> Transformer: Enhancing <span class=acl-fixed-case>SLU</span> Performance using Multiple <span class=acl-fixed-case>ASR</span> Hypotheses</a></strong><br><a href=/people/k/karthik-ganesan/>Karthik Ganesan</a>
|
<a href=/people/p/pakhi-bamdev/>Pakhi Bamdev</a>
|
<a href=/people/j/jaivarsan-b/>Jaivarsan B</a>
|
<a href=/people/a/amresh-venugopal/>Amresh Venugopal</a>
|
<a href=/people/a/abhinav-tushar/>Abhinav Tushar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--14><div class="card-body p-3 small">Spoken Language Understanding (SLU) systems parse speech into semantic structures like <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>dialog acts</a> and <a href=https://en.wikipedia.org/wiki/Action_(philosophy)>slots</a>. This involves the use of an Automatic Speech Recognizer (ASR) to transcribe speech into multiple text alternatives (hypotheses). Transcription errors, ordinary in ASRs, impact downstream SLU performance negatively. Common approaches to mitigate such errors involve using richer information from the ASR, either in form of N-best hypotheses or word-lattices. We hypothesize that transformer models will learn better with a simpler utterance representation using the concatenation of the N-best ASR alternatives, where each alternative is separated by a special delimiter [ SEP ]. In our work, we test our hypothesis by using the concatenated N-best ASR alternatives as the input to the transformer encoder models, namely BERT and XLM-RoBERTa, and achieve equivalent performance to the prior state-of-the-art model on DSTC2 dataset. We also show that our approach significantly outperforms the prior state-of-the-art when subjected to the low data regime. Additionally, this <a href=https://en.wikipedia.org/wiki/Methodology>methodology</a> is accessible to users of third-party ASR APIs which do not provide word-lattice information.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.21.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--21 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.21 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.21/>AND does not mean OR : Using Formal Languages to Study Language Models’ Representations<span class=acl-fixed-case>AND</span> does not mean <span class=acl-fixed-case>OR</span>: Using Formal Languages to Study Language Models’ Representations</a></strong><br><a href=/people/a/aaron-traylor/>Aaron Traylor</a>
|
<a href=/people/r/roman-feiman/>Roman Feiman</a>
|
<a href=/people/e/ellie-pavlick/>Ellie Pavlick</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--21><div class="card-body p-3 small">A current open question in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a> is to what extent language models, which are trained with access only to the form of language, are able to capture the meaning of language. This question is challenging to answer in general, as there is no clear line between <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning</a> and <a href=https://en.wikipedia.org/wiki/Theory_of_forms>form</a>, but rather <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning</a> constrains <a href=https://en.wikipedia.org/wiki/Theory_of_forms>form</a> in consistent ways. The goal of this study is to offer insights into a narrower but critical subquestion : Under what conditions should we expect that <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning</a> and <a href=https://en.wikipedia.org/wiki/Theory_of_forms>form</a> covary sufficiently, such that a language model with access only to <a href=https://en.wikipedia.org/wiki/Theory_of_forms>form</a> might nonetheless succeed in emulating <a href=https://en.wikipedia.org/wiki/Meaning_(linguistics)>meaning</a>? Focusing on several formal languages (propositional logic and a set of programming languages), we generate training corpora using a variety of motivated constraints, and measure a distributional language model&#8217;s ability to differentiate logical symbols (AND, OR, and NOT). Our findings are largely negative : none of our simulated training corpora result in <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> which definitively differentiate meaningfully different symbols (e.g., AND vs. OR), suggesting a limitation to the types of semantic signals that current <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> are able to exploit.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.25.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--25 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.25 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.25/>Continual Quality Estimation with Online Bayesian Meta-Learning<span class=acl-fixed-case>B</span>ayesian Meta-Learning</a></strong><br><a href=/people/a/abiola-obamuyide/>Abiola Obamuyide</a>
|
<a href=/people/m/marina-fomicheva/>Marina Fomicheva</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--25><div class="card-body p-3 small">Most current quality estimation (QE) models for <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> are trained and evaluated in a static setting where <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training and test data</a> are assumed to be from a fixed distribution. However, in real-life settings, the test data that a deployed QE model would be exposed to may differ from its training data. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of end-users, who will employ predictions in practice. To address this challenge, we propose an online Bayesian meta-learning framework for the continuous training of QE models that is able to adapt them to the needs of different users, while being robust to distributional shifts in training and test data. Experiments on <a href=https://en.wikipedia.org/wiki/Data>data</a> with varying number of users and language characteristics validate the effectiveness of the proposed approach.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.31.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--31 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.31 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.31/>Multilingual Agreement for Multilingual Neural Machine Translation</a></strong><br><a href=/people/j/jian-yang/>Jian Yang</a>
|
<a href=/people/y/yuwei-yin/>Yuwei Yin</a>
|
<a href=/people/s/shuming-ma/>Shuming Ma</a>
|
<a href=/people/h/haoyang-huang/>Haoyang Huang</a>
|
<a href=/people/d/dongdong-zhang/>Dongdong Zhang</a>
|
<a href=/people/z/zhoujun-li/>Zhoujun Li</a>
|
<a href=/people/f/furu-wei/>Furu Wei</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--31><div class="card-body p-3 small">Although multilingual neural machine translation (MNMT) enables multiple language translations, the training process is based on independent multilingual objectives. Most multilingual models can not explicitly exploit different language pairs to assist each other, ignoring the relationships among them. In this work, we propose a novel agreement-based method to encourage multilingual agreement among different translation directions, which minimizes the differences among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our <a href=https://en.wikipedia.org/wiki/Methodology>method</a>, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our <a href=https://en.wikipedia.org/wiki/Methodology>method</a> achieves significant improvements over the previous multilingual baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.32.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--32 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.32 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.32/>Higher-order Derivatives of Weighted Finite-state Machines</a></strong><br><a href=/people/r/ran-zmigrod/>Ran Zmigrod</a>
|
<a href=/people/t/tim-vieira/>Tim Vieira</a>
|
<a href=/people/r/ryan-cotterell/>Ryan Cotterell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--32><div class="card-body p-3 small">Weighted finite-state machines are a fundamental building block of <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP systems</a>. They have withstood the test of timefrom their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the <a href=https://en.wikipedia.org/wiki/Normalization_constant>normalization constant</a> for weighted finite-state machines. We provide a general <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for evaluating <a href=https://en.wikipedia.org/wiki/Derivative_(finance)>derivatives</a> of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(A2 N4) time where A is the alphabet size and N is the number of states. Our <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> for computing second-order expectations, such as <a href=https://en.wikipedia.org/wiki/Covariance_matrix>covariance matrices</a> and gradients of first-order expectations.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.33.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--33 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.33 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.33" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.33/>Reinforcement Learning for Abstractive Question Summarization with Question-aware Semantic Rewards</a></strong><br><a href=/people/s/shweta-yadav/>Shweta Yadav</a>
|
<a href=/people/d/deepak-gupta/>Deepak Gupta</a>
|
<a href=/people/a/asma-ben-abacha/>Asma Ben Abacha</a>
|
<a href=/people/d/dina-demner-fushman/>Dina Demner-Fushman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--33><div class="card-body p-3 small">The growth of online consumer health questions has led to the necessity for reliable and accurate <a href=https://en.wikipedia.org/wiki/Question_answering>question answering systems</a>. A recent study showed that manual summarization of consumer health questions brings significant improvement in retrieving relevant answers. However, the automatic summarization of long questions is a challenging task due to the lack of training data and the complexity of the related subtasks, such as the question focus and type recognition. In this paper, we introduce a reinforcement learning-based framework for abstractive question summarization. We propose two novel rewards obtained from the downstream tasks of (i) question-type identification and (ii) question-focus recognition to regularize the question generation model. These rewards ensure the generation of semantically valid questions and encourage the inclusion of key medical entities / foci in the question summary. We evaluated our proposed method on two benchmark datasets and achieved higher performance over <a href=https://en.wikipedia.org/wiki/State-of-the-art>state-of-the-art models</a>. The manual evaluation of the summaries reveals that the generated questions are more diverse and have fewer factual inconsistencies than the baseline summaries. The source code is available here : https://github.com/shwetanlp/CHQ-Summ.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.34.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--34 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.34 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.34/>A Semantics-aware Transformer Model of Relation Linking for Knowledge Base Question Answering</a></strong><br><a href=/people/t/tahira-naseem/>Tahira Naseem</a>
|
<a href=/people/s/srinivas-ravishankar/>Srinivas Ravishankar</a>
|
<a href=/people/n/nandana-mihindukulasooriya/>Nandana Mihindukulasooriya</a>
|
<a href=/people/i/ibrahim-abdelaziz/>Ibrahim Abdelaziz</a>
|
<a href=/people/y/young-suk-lee/>Young-Suk Lee</a>
|
<a href=/people/p/pavan-kapanipathi/>Pavan Kapanipathi</a>
|
<a href=/people/s/salim-roukos/>Salim Roukos</a>
|
<a href=/people/a/alfio-gliozzo/>Alfio Gliozzo</a>
|
<a href=/people/a/alexander-gray/>Alexander Gray</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--34><div class="card-body p-3 small">Relation linking is a crucial component of Knowledge Base Question Answering systems. Existing <a href=https://en.wikipedia.org/wiki/System>systems</a> use a wide variety of <a href=https://en.wikipedia.org/wiki/Heuristic>heuristics</a>, or ensembles of multiple systems, heavily relying on the surface question text. However, the explicit semantic parse of the question is a rich source of relation information that is not taken advantage of. We propose a simple transformer-based neural model for relation linking that leverages the AMR semantic parse of a sentence. Our <a href=https://en.wikipedia.org/wiki/System>system</a> significantly outperforms the <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on 4 popular <a href=https://en.wikipedia.org/wiki/Benchmark_(computing)>benchmark datasets</a>. These are based on either <a href=https://en.wikipedia.org/wiki/DBpedia>DBpedia</a> or <a href=https://en.wikipedia.org/wiki/Wikidata>Wikidata</a>, demonstrating that our approach is effective across KGs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.35.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--35 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.35 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.35/>Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation</a></strong><br><a href=/people/y/yinfei-yang/>Yinfei Yang</a>
|
<a href=/people/n/ning-jin/>Ning Jin</a>
|
<a href=/people/k/kuo-lin/>Kuo Lin</a>
|
<a href=/people/m/mandy-guo/>Mandy Guo</a>
|
<a href=/people/d/daniel-cer/>Daniel Cer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--35><div class="card-body p-3 small">Early fusion models with cross-attention have shown better-than-human performance on some question answer benchmarks, while it is a poor fit for retrieval since it prevents pre-computation of the answer representations. We present a supervised data mining method using an accurate early fusion model to improve the training of an efficient late fusion retrieval model. We first train an accurate <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification model</a> with cross-attention between questions and answers. The cross-attention model is then used to annotate additional passages in order to generate weighted training examples for a neural retrieval model. The resulting retrieval model with additional data significantly outperforms retrieval models directly trained with gold annotations on Precision at N (P@N) and Mean Reciprocal Rank (MRR).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.36.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--36 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.36 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.36" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.36/>Enhancing Descriptive Image Captioning with Natural Language Inference</a></strong><br><a href=/people/z/zhan-shi/>Zhan Shi</a>
|
<a href=/people/h/hui-liu/>Hui Liu</a>
|
<a href=/people/x/xiaodan-zhu/>Xiaodan Zhu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--36><div class="card-body p-3 small">Generating descriptive sentences that convey non-trivial, detailed, and salient information about <a href=https://en.wikipedia.org/wiki/Image>images</a> is an important goal of image captioning. In this paper we propose a novel approach to encourage captioning models to produce more detailed captions using natural language inference, based on the motivation that, among different captions of an image, descriptive captions are more likely to entail less descriptive captions. Specifically, we construct directed inference graphs for <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>reference captions</a> based on <a href=https://en.wikipedia.org/wiki/Natural-language_understanding>natural language inference</a>. A <a href=https://en.wikipedia.org/wiki/PageRank>PageRank algorithm</a> is then employed to estimate the descriptiveness score of each <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>node</a>. Built on that, we use reference sampling and weighted designated rewards to guide <a href=https://en.wikipedia.org/wiki/Closed_captioning>captioning</a> to generate descriptive captions. The results on MSCOCO show that the proposed method outperforms the baselines significantly on a wide range of conventional and descriptiveness-related evaluation metrics.<i>descriptive</i> sentences that convey non-trivial, detailed, and salient information about images is an important goal of image captioning. In this paper we propose a novel approach to encourage captioning models to produce more detailed captions using natural language inference, based on the motivation that, among different captions of an image, descriptive captions are more likely to entail less descriptive captions. Specifically, we construct directed inference graphs for reference captions based on natural language inference. A PageRank algorithm is then employed to estimate the descriptiveness score of each node. Built on that, we use reference sampling and weighted designated rewards to guide captioning to generate descriptive captions. The results on MSCOCO show that the proposed method outperforms the baselines significantly on a wide range of conventional and descriptiveness-related evaluation metrics.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.39.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--39 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.39 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.39" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.39/>On Positivity Bias in Negative Reviews</a></strong><br><a href=/people/m/madhusudhan-aithal/>Madhusudhan Aithal</a>
|
<a href=/people/c/chenhao-tan/>Chenhao Tan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--39><div class="card-body p-3 small">Prior work has revealed that positive words occur more frequently than negative words in human expressions, which is typically attributed to positivity bias, a tendency for people to report positive views of reality. But what about the language used in negative reviews? Consistent with prior work, we show that English negative reviews tend to contain more positive words than negative words, using a variety of datasets. We reconcile this observation with prior findings on the pragmatics of negation, and show that negations are commonly associated with positive words in negative reviews. Furthermore, in negative reviews, the majority of sentences with positive words express negative opinions based on sentiment classifiers, indicating some form of <a href=https://en.wikipedia.org/wiki/Affirmation_and_negation>negation</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.40.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--40 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.40 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.40/>PRAL : A Tailored Pre-Training Model for Task-Oriented Dialog Generation<span class=acl-fixed-case>PRAL</span>: A Tailored Pre-Training Model for Task-Oriented Dialog Generation</a></strong><br><a href=/people/j/jing-gu/>Jing Gu</a>
|
<a href=/people/q/qingyang-wu/>Qingyang Wu</a>
|
<a href=/people/c/chongruo-wu/>Chongruo Wu</a>
|
<a href=/people/w/weiyan-shi/>Weiyan Shi</a>
|
<a href=/people/z/zhou-yu/>Zhou Yu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--40><div class="card-body p-3 small">Large pre-trained language generation models such as GPT-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks. However, the performance of pre-trained <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> on task-oriented dialog tasks is still under-explored. We propose a Pre-trainedRole Alternating Language model (PRAL), explicitly designed for task-oriented conversational systems. We design several techniques : start position randomization, knowledge distillation, and history discount to improve pre-training performance. In addition, we introduce a high-quality large-scale task-oriented dialog pre-training dataset by post-prossessing13 dialog datasets. We effectively adapt PRALon three downstream tasks. The results show that PRAL outperforms or is on par with state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.41.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--41 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.41 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.41/>ROPE : Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction<span class=acl-fixed-case>ROPE</span>: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction</a></strong><br><a href=/people/c/chen-yu-lee/>Chen-Yu Lee</a>
|
<a href=/people/c/chun-liang-li/>Chun-Liang Li</a>
|
<a href=/people/c/chu-wang/>Chu Wang</a>
|
<a href=/people/r/renshen-wang/>Renshen Wang</a>
|
<a href=/people/y/yasuhisa-fujii/>Yasuhisa Fujii</a>
|
<a href=/people/s/siyang-qin/>Siyang Qin</a>
|
<a href=/people/a/ashok-popat/>Ashok Popat</a>
|
<a href=/people/t/tomas-pfister/>Tomas Pfister</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--41><div class="card-body p-3 small">Natural reading orders of words are crucial for <a href=https://en.wikipedia.org/wiki/Information_extraction>information extraction</a> from form-like documents. Despite recent advances in Graph Convolutional Networks (GCNs) on modeling spatial layout patterns of documents, they have limited ability to capture reading orders of given word-level node representations in a <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graph</a>. We propose Reading Order Equivariant Positional Encoding (ROPE), a new positional encoding technique designed to apprehend the sequential presentation of words in documents. ROPE generates unique reading order codes for neighboring words relative to the target word given a word-level graph connectivity. We study two fundamental document entity extraction tasks including word labeling and word grouping on the public FUNSD dataset and a large-scale payment dataset. We show that ROPE consistently improves existing GCNs with a margin up to 8.4 % <a href=https://en.wikipedia.org/wiki/F-number>F1-score</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.44.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--44 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.44 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.44.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-short.44/>Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing</a></strong><br><a href=/people/j/jonathan-k-kummerfeld/>Jonathan K. Kummerfeld</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--44><div class="card-body p-3 small">Extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the U.S. federal minimum wage. Meanwhile, research on collecting high quality annotations suggests using a qualification that requires workers to have previously completed a certain number of tasks. If most requesters who pay fairly require workers to have completed a large number of tasks already then workers need to complete a substantial amount of poorly paid work before they can earn a fair wage. Through analysis of worker discussions and guidance for researchers, we estimate that workers spend approximately 2.25 months of full time effort on poorly paid tasks in order to get the <a href=https://en.wikipedia.org/wiki/Professional_certification>qualifications</a> needed for better paid tasks. We discuss alternatives to this qualification and conduct a study of the correlation between <a href=https://en.wikipedia.org/wiki/Professional_certification>qualifications</a> and work quality on two NLP tasks. We find that it is possible to reduce the burden on workers while still collecting <a href=https://en.wikipedia.org/wiki/Data_quality>high quality data</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.45.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--45 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.45 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.45.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.45" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.45/>Men Are Elected, Women Are Married : Events Gender Bias on Wikipedia<span class=acl-fixed-case>W</span>ikipedia</a></strong><br><a href=/people/j/jiao-sun/>Jiao Sun</a>
|
<a href=/people/n/nanyun-peng/>Nanyun Peng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--45><div class="card-body p-3 small">Human activities can be seen as sequences of events, which are crucial to understanding societies. Disproportional event distribution for different demographic groups can manifest and amplify <a href=https://en.wikipedia.org/wiki/Stereotypes_of_East_Asians_in_the_United_States>social stereotypes</a>, and potentially jeopardize the ability of members in some groups to pursue certain goals. In this paper, we present the first event-centric study of <a href=https://en.wikipedia.org/wiki/Gender_bias>gender biases</a> in a <a href=https://en.wikipedia.org/wiki/Web_of_Science>Wikipedia corpus</a>. To facilitate the study, we curate a corpus of career and personal life descriptions with demographic information consisting of 7,854 fragments from 10,412 celebrities. Then we detect <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> with a state-of-the-art event detection model, calibrate the results using strategically generated templates, and extract <a href=https://en.wikipedia.org/wiki/Event_(probability_theory)>events</a> that have asymmetric associations with genders. Our study discovers that the Wikipedia pages tend to intermingle personal life events with professional events for females but not for males, which calls for the awareness of the Wikipedia community to formalize guidelines and train the editors to mind the implicit biases that contributors carry. Our work also lays the foundation for future works on quantifying and discovering event biases at the corpus level.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.46.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--46 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.46 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.46/>Modeling Task-Aware MIMO Cardinality for Efficient Multilingual Neural Machine Translation<span class=acl-fixed-case>MIMO</span> Cardinality for Efficient Multilingual Neural Machine Translation</a></strong><br><a href=/people/h/hongfei-xu/>Hongfei Xu</a>
|
<a href=/people/q/qiuhui-liu/>Qiuhui Liu</a>
|
<a href=/people/j/josef-van-genabith/>Josef van Genabith</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--46><div class="card-body p-3 small">Neural machine translation has achieved great success in <a href=https://en.wikipedia.org/wiki/Multilingualism>bilingual settings</a>, as well as in <a href=https://en.wikipedia.org/wiki/Multilingualism>multilingual settings</a>. With the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying <a href=https://en.wikipedia.org/wiki/Linguistic_typology>typological characteristics</a>. Previous work increases the modeling capacity by deepening or widening the <a href=https://en.wikipedia.org/wiki/Transformer>Transformer</a>. However, modeling cardinality based on aggregating a set of transformations with the same <a href=https://en.wikipedia.org/wiki/Topological_space>topology</a> has been proven more effective than going deeper or wider when increasing <a href=https://en.wikipedia.org/wiki/Capacity_of_a_set>capacity</a>. In this paper, we propose to efficiently increase the capacity for multilingual NMT by increasing the <a href=https://en.wikipedia.org/wiki/Cardinality>cardinality</a>. Unlike previous work which feeds the same input to several transformations and merges their outputs into one, we present a Multi-Input-Multi-Output (MIMO) architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> surpasses previous work and establishes a new <a href=https://en.wikipedia.org/wiki/State_of_the_art>state-of-the-art</a> on the large scale OPUS-100 corpus while being 1.31 times as fast.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.56.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--56 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.56 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.56.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.56" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.56/>Exploration and Exploitation : Two Ways to Improve Chinese Spelling Correction Models<span class=acl-fixed-case>C</span>hinese Spelling Correction Models</a></strong><br><a href=/people/c/chong-li/>Chong Li</a>
|
<a href=/people/c/cenyuan-zhang/>Cenyuan Zhang</a>
|
<a href=/people/x/xiaoqing-zheng/>Xiaoqing Zheng</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--56><div class="card-body p-3 small">A sequence-to-sequence learning with <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> has empirically proven to be an effective framework for Chinese Spelling Correction (CSC), which takes a sentence with some spelling errors as input and outputs the corrected one. However, CSC models may fail to correct spelling errors covered by the confusion sets, and also will encounter unseen ones. We propose a method, which continually identifies the weak spots of a <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to generate more valuable training instances, and apply a task-specific pre-training strategy to enhance the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a>. The generated adversarial examples are gradually added to the training set. Experimental results show that such an adversarial training method combined with the pre-training strategy can improve both the <a href=https://en.wikipedia.org/wiki/Generalization>generalization</a> and <a href=https://en.wikipedia.org/wiki/Robustness_(computer_science)>robustness</a> of multiple CSC models across three different datasets, achieving state-of-the-art performance for CSC task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.58.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--58 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.58 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.58/>An Empirical Study on Adversarial Attack on NMT : Languages and Positions Matter<span class=acl-fixed-case>NMT</span>: Languages and Positions Matter</a></strong><br><a href=/people/z/zhiyuan-zeng/>Zhiyuan Zeng</a>
|
<a href=/people/d/deyi-xiong/>Deyi Xiong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--58><div class="card-body p-3 small">In this paper, we empirically investigate adversarial attack on <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NMT</a> from two aspects : languages (the source vs. the target language) and positions (front vs. rear). For autoregressive NMT models that generate target words from left to right, we observe that adversarial attack on the source language is more effective than on the target language, and that attacking front positions of target sentences or positions of source sentences aligned to the front positions of corresponding target sentences is more effective than attacking other positions. We further exploit the attention distribution of the victim model to attack source sentences at positions that have a strong association with front target words. Experiment results demonstrate that our attention-based adversarial attack is more effective than adversarial attacks by sampling positions randomly or according to gradients.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.59.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--59 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.59 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.59" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.59/>OntoGUM : Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres<span class=acl-fixed-case>O</span>nto<span class=acl-fixed-case>GUM</span>: Evaluating Contextualized <span class=acl-fixed-case>SOTA</span> Coreference Resolution on 12 More Genres</a></strong><br><a href=/people/y/yilun-zhu/>Yilun Zhu</a>
|
<a href=/people/s/sameer-pradhan/>Sameer Pradhan</a>
|
<a href=/people/a/amir-zeldes/>Amir Zeldes</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--59><div class="card-body p-3 small">SOTA coreference resolution produces increasingly impressive scores on the OntoNotes benchmark. However lack of comparable data following the same <a href=https://en.wikipedia.org/wiki/Scheme_(mathematics)>scheme</a> for more genres makes it difficult to evaluate generalizability to <a href=https://en.wikipedia.org/wiki/Open_domain>open domain data</a>. This paper provides a dataset and comprehensive evaluation showing that the latest neural LM based end-to-end systems degrade very substantially out of domain. We make an OntoNotes-like coreference dataset called OntoGUM publicly available, converted from GUM, an English corpus covering 12 genres, using deterministic rules, which we evaluate. Thanks to the rich syntactic and discourse annotations in GUM, we are able to create the largest human-annotated coreference corpus following the OntoNotes guidelines, and the first to be evaluated for consistency with the OntoNotes scheme. Out-of-domain evaluation across 12 genres shows nearly 15-20 % degradation for both deterministic and deep learning systems, indicating a lack of generalizability or covert <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a> in existing coreference resolution models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.61.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--61 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.61 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.61" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.61/>Zero-shot Fact Verification by Claim Generation</a></strong><br><a href=/people/l/liangming-pan/>Liangming Pan</a>
|
<a href=/people/w/wenhu-chen/>Wenhu Chen</a>
|
<a href=/people/w/wenhan-xiong/>Wenhan Xiong</a>
|
<a href=/people/m/min-yen-kan/>Min-Yen Kan</a>
|
<a href=/people/w/william-yang-wang/>William Yang Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--61><div class="card-body p-3 small">Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires <a href=https://en.wikipedia.org/wiki/Fact-checking>fact verification</a>, creating a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from <a href=https://en.wikipedia.org/wiki/Wikipedia>Wikipedia</a>. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model&#8217;s F1 from 50 % to 77 %, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.62.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--62 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.62 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.62" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.62/>Thank you BART ! Rewarding Pre-Trained Models Improves Formality Style Transfer<span class=acl-fixed-case>BART</span>! Rewarding Pre-Trained Models Improves Formality Style Transfer</a></strong><br><a href=/people/h/huiyuan-lai/>Huiyuan Lai</a>
|
<a href=/people/a/antonio-toral/>Antonio Toral</a>
|
<a href=/people/m/malvina-nissim/>Malvina Nissim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--62><div class="card-body p-3 small">Scarcity of parallel data causes formality style transfer models to have scarce success in <a href=https://en.wikipedia.org/wiki/Data_preservation>preserving content</a>. We show that fine-tuning pre-trained language (GPT-2) and sequence-to-sequence (BART) models boosts content preservation, and that this is possible even with limited amounts of parallel data. Augmenting these models with rewards that target style and content the two core aspects of the task we achieve a new state-of-the-art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.63.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--63 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.63 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.63/>Deep Context- and Relation-Aware Learning for Aspect-based Sentiment Analysis</a></strong><br><a href=/people/s/shinhyeok-oh/>Shinhyeok Oh</a>
|
<a href=/people/d/dongyub-lee/>Dongyub Lee</a>
|
<a href=/people/t/taesun-whang/>Taesun Whang</a>
|
<a href=/people/i/ilnam-park/>IlNam Park</a>
|
<a href=/people/s/seo-gaeun/>Seo Gaeun</a>
|
<a href=/people/e/eunggyun-kim/>EungGyun Kim</a>
|
<a href=/people/h/harksoo-kim/>Harksoo Kim</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--63><div class="card-body p-3 small">Existing works for aspect-based sentiment analysis (ABSA) have adopted a unified approach, which allows the interactive relations among subtasks. However, we observe that these methods tend to predict polarities based on the literal meaning of aspect and opinion terms and mainly consider relations implicitly among subtasks at the word level. In addition, identifying multiple aspectopinion pairs with their polarities is much more challenging. Therefore, a comprehensive understanding of <a href=https://en.wikipedia.org/wiki/Context_(language_use)>contextual information</a> w.r.t. the aspect and opinion are further required in ABSA. In this paper, we propose Deep Contextualized Relation-Aware Network (DCRAN), which allows interactive relations among subtasks with deep contextual information based on two modules (i.e., Aspect and Opinion Propagation and Explicit Self-Supervised Strategies). Especially, we design novel self-supervised strategies for ABSA, which have strengths in dealing with multiple aspects. Experimental results show that DCRAN significantly outperforms previous state-of-the-art methods by large margins on three widely used benchmarks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.67.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--67 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.67 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.67/>TIMERS : Document-level Temporal Relation Extraction<span class=acl-fixed-case>TIMERS</span>: Document-level Temporal Relation Extraction</a></strong><br><a href=/people/p/puneet-mathur/>Puneet Mathur</a>
|
<a href=/people/r/rajiv-jain/>Rajiv Jain</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/v/vlad-morariu/>Vlad Morariu</a>
|
<a href=/people/q/quan-hung-tran/>Quan Hung Tran</a>
|
<a href=/people/d/dinesh-manocha/>Dinesh Manocha</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--67><div class="card-body p-3 small">We present TIMERS-a TIME, Rhetorical and Syntactic-aware model for document-level temporal relation classification in the <a href=https://en.wikipedia.org/wiki/English_language>English language</a>. Our proposed method leverages rhetorical discourse features and temporal arguments from semantic role labels, in addition to traditional local syntactic features, trained through a Gated Relational-GCN. Extensive experiments show that the proposed model outperforms previous methods by 5-18 % on the TDDiscourse, TimeBank-Dense, and MATRES datasets due to our discourse-level modeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.70.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--70 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.70 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.70" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.70/>More than Text : Multi-modal Chinese Word Segmentation<span class=acl-fixed-case>C</span>hinese Word Segmentation</a></strong><br><a href=/people/d/dong-zhang/>Dong Zhang</a>
|
<a href=/people/z/zheng-hu/>Zheng Hu</a>
|
<a href=/people/s/shoushan-li/>Shoushan Li</a>
|
<a href=/people/h/hanqian-wu/>Hanqian Wu</a>
|
<a href=/people/q/qiaoming-zhu/>Qiaoming Zhu</a>
|
<a href=/people/g/guodong-zhou/>Guodong Zhou</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--70><div class="card-body p-3 small">Chinese word segmentation (CWS) is undoubtedly an important basic task in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. Previous works only focus on the textual modality, but there are often audio and video utterances (such as news broadcast and face-to-face dialogues), where textual, acoustic and visual modalities normally exist. To this end, we attempt to combine the multi-modality (mainly the converted text and actual voice information) to perform CWS. In this paper, we annotate a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for <a href=https://en.wikipedia.org/wiki/Content_Scramble_System>CWS</a> containing text and audio. Moreover, we propose a time-dependent multi-modal interactive model based on Transformer framework to integrate multi-modal information for word sequence labeling. The experimental results on three different training sets show the effectiveness of our approach with fusing text and audio.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.71.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--71 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.71 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.71.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.71" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.71/>A Mixture-of-Experts Model for Antonym-Synonym Discrimination</a></strong><br><a href=/people/z/zhipeng-xie/>Zhipeng Xie</a>
|
<a href=/people/n/nan-zeng/>Nan Zeng</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--71><div class="card-body p-3 small">Discrimination between <a href=https://en.wikipedia.org/wiki/Opposite_(semantics)>antonyms</a> and <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> is an important and challenging NLP task. Antonyms and <a href=https://en.wikipedia.org/wiki/Synonym>synonyms</a> often share the same or similar contexts and thus are hard to make a distinction. This paper proposes two underlying hypotheses and employs the mixture-of-experts framework as a solution. It works on the basis of a divide-and-conquer strategy, where a number of localized experts focus on their own domains (or subspaces) to learn their specialties, and a gating mechanism determines the space partitioning and the expert mixture. Experimental results have shown that our <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> achieves the state-of-the-art performance on the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.72.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--72 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.72 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.72" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.72/>Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking</a></strong><br><a href=/people/f/fangyu-liu/>Fangyu Liu</a>
|
<a href=/people/i/ivan-vulic/>Ivan Vulić</a>
|
<a href=/people/a/anna-korhonen/>Anna Korhonen</a>
|
<a href=/people/n/nigel-collier/>Nigel Collier</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--72><div class="card-body p-3 small">Injecting external domain-specific knowledge (e.g., UMLS) into pretrained language models (LMs) advances their capability to handle specialised in-domain tasks such as biomedical entity linking (BEL). However, such abundant expert knowledge is available only for a handful of languages (e.g., English). In this work, by proposing a novel cross-lingual biomedical entity linking task (XL-BEL) and establishing a new XL-BEL benchmark spanning 10 typologically diverse languages, we first investigate the ability of standard knowledge-agnostic as well as knowledge-enhanced monolingual and multilingual LMs beyond the standard monolingual English BEL task. The scores indicate large gaps to English performance. We then address the challenge of transferring domain-specific knowledge in resource-rich languages to resource-poor ones. To this end, we propose and evaluate a series of cross-lingual transfer methods for the XL-BEL task, and demonstrate that general-domain bitext helps propagate the available English knowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.73.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--73 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.73 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.73" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.73/>A Cluster-based Approach for Improving <a href=https://en.wikipedia.org/wiki/Isotropy>Isotropy</a> in Contextual Embedding Space</a></strong><br><a href=/people/s/sara-rajaee/>Sara Rajaee</a>
|
<a href=/people/m/mohammad-taher-pilehvar/>Mohammad Taher Pilehvar</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--73><div class="card-body p-3 small">The representation degeneration problem in Contextual Word Representations (CWRs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study <a href=https://en.wikipedia.org/wiki/Isotropy>isotropy</a>. Our quantitative analysis over <a href=https://en.wikipedia.org/wiki/Isotropy>isotropy</a> shows that a local assessment could be more accurate due to the clustered structure of CWRs. Based on this observation, we propose a local cluster-based method to address the <a href=https://en.wikipedia.org/wiki/Degeneracy_(mathematics)>degeneration issue</a> in contextual embedding spaces. We show that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve CWRs performance on semantic tasks. Moreover, we find that tense information in <a href=https://en.wikipedia.org/wiki/Grammatical_tense>verb representations</a> dominates sense semantics. We show that removing dominant directions of verb representations can transform the <a href=https://en.wikipedia.org/wiki/Semantic_space>space</a> to better suit semantic applications. Our experiments demonstrate that the proposed cluster-based method can mitigate the <a href=https://en.wikipedia.org/wiki/Degeneracy_(mathematics)>degeneration problem</a> on multiple tasks.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.76.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--76 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.76 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.76/>Do n’t Let Discourse Confine Your Model : Sequence Perturbations for Improved Event Language Models</a></strong><br><a href=/people/m/mahnaz-koupaee/>Mahnaz Koupaee</a>
|
<a href=/people/g/greg-durrett/>Greg Durrett</a>
|
<a href=/people/n/nathanael-chambers/>Nathanael Chambers</a>
|
<a href=/people/n/niranjan-balasubramanian/>Niranjan Balasubramanian</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--76><div class="card-body p-3 small">Event language models represent plausible sequences of events. Most existing approaches train <a href=https://en.wikipedia.org/wiki/Autoregressive_model>autoregressive models</a> on text, which successfully capture <a href=https://en.wikipedia.org/wiki/Co-occurrence>event co-occurrence</a> but unfortunately constrain the <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> to follow the <a href=https://en.wikipedia.org/wiki/Discourse_analysis>discourse order</a> in which events are presented. Other domains may employ different discourse orders, and for many applications, we may care about different notions of <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>ordering</a> (e.g., temporal) or not care about <a href=https://en.wikipedia.org/wiki/Partially_ordered_set>ordering</a> at all (e.g., when predicting related events in a schema). We propose a simple yet surprisingly effective strategy for improving event language models by perturbing event sequences so we can relax model dependence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both <a href=https://en.wikipedia.org/wiki/Application_software>applications</a> and out-of-domain events data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.77.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--77 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.77 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.77/>The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes</a></strong><br><a href=/people/n/nils-reimers/>Nils Reimers</a>
|
<a href=/people/i/iryna-gurevych/>Iryna Gurevych</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--77><div class="card-body p-3 small">Information Retrieval using dense low-dimensional representations recently became popular and showed out-performance to traditional sparse-representations like <a href=https://en.wikipedia.org/wiki/BM25>BM25</a>. However, no previous work investigated how dense representations perform with large index sizes. We show theoretically and empirically that the performance for <a href=https://en.wikipedia.org/wiki/Sparse_matrix>dense representations</a> decreases quicker than <a href=https://en.wikipedia.org/wiki/Sparse_matrix>sparse representations</a> for increasing index sizes. In extreme cases, this can even lead to a tipping point where at a certain index size <a href=https://en.wikipedia.org/wiki/Sparse_matrix>sparse representations</a> outperform <a href=https://en.wikipedia.org/wiki/Dense_matrix>dense representations</a>. We show that this behavior is tightly connected to the number of dimensions of the <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> : The lower the <a href=https://en.wikipedia.org/wiki/Dimension>dimension</a>, the higher the chance for <a href=https://en.wikipedia.org/wiki/False_positives_and_false_negatives>false positives</a>, i.e. returning irrelevant documents</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.78.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--78 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.78 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.78" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.78/>Cross-lingual Text Classification with Heterogeneous Graph Neural Network</a></strong><br><a href=/people/z/ziyun-wang/>Ziyun Wang</a>
|
<a href=/people/x/xuan-liu/>Xuan Liu</a>
|
<a href=/people/p/peiji-yang/>Peiji Yang</a>
|
<a href=/people/s/shixing-liu/>Shixing Liu</a>
|
<a href=/people/z/zhisheng-wang/>Zhisheng Wang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--78><div class="card-body p-3 small">Cross-lingual text classification aims at training a <a href=https://en.wikipedia.org/wiki/Statistical_classification>classifier</a> on the source language and transferring the knowledge to target languages, which is very useful for low-resource languages. Recent multilingual pretrained language models (mPLM) achieve impressive results in cross-lingual classification tasks, but rarely consider factors beyond semantic similarity, causing performance degradation between some language pairs. In this paper we propose a simple yet effective method to incorporate heterogeneous information within and across languages for cross-lingual text classification using graph convolutional networks (GCN). In particular, we construct a heterogeneous graph by treating documents and words as <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a>, and linking <a href=https://en.wikipedia.org/wiki/Vertex_(graph_theory)>nodes</a> with different relations, which include part-of-speech roles, <a href=https://en.wikipedia.org/wiki/Semantic_similarity>semantic similarity</a>, and document translations. Extensive experiments show that our graph-based method significantly outperforms state-of-the-art models on all tasks, and also achieves consistent performance gain over baselines in low-resource settings where external tools like translators are unavailable.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.80.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--80 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.80 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.80/>Embedding Time Differences in Context-sensitive Neural Networks for Learning Time to Event</a></strong><br><a href=/people/n/nazanin-dehghani/>Nazanin Dehghani</a>
|
<a href=/people/h/hassan-hajipoor/>Hassan Hajipoor</a>
|
<a href=/people/h/hadi-amiri/>Hadi Amiri</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--80><div class="card-body p-3 small">We propose an effective context-sensitive neural model for time to event (TTE) prediction task, which aims to predict the amount of time to / from the occurrence of given events in <a href=https://en.wikipedia.org/wiki/Streaming_media>streaming content</a>. We investigate this problem in the context of a multi-task learning framework, which we enrich with time difference embeddings. In addition, we develop a multi-genre dataset of English events about <a href=https://en.wikipedia.org/wiki/Association_football>soccer competitions</a> and academy awards ceremonies, and their relevant tweets obtained from <a href=https://en.wikipedia.org/wiki/Twitter>Twitter</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is 1.4 and 3.3 hours more accurate than the current state-of-the-art <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> in estimating <a href=https://en.wikipedia.org/wiki/Time_to_live>TTE</a> on <a href=https://en.wikipedia.org/wiki/Twitter>English and Dutch tweets</a> respectively. We examine different aspects of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> to illustrate its source of improvement.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.84.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--84 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.84 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.84/>Domain-Adaptive Pretraining Methods for Dialogue Understanding</a></strong><br><a href=/people/h/han-wu/>Han Wu</a>
|
<a href=/people/k/kun-xu/>Kun Xu</a>
|
<a href=/people/l/linfeng-song/>Linfeng Song</a>
|
<a href=/people/l/lifeng-jin/>Lifeng Jin</a>
|
<a href=/people/h/haisong-zhang/>Haisong Zhang</a>
|
<a href=/people/l/linqi-song/>Linqi Song</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--84><div class="card-body p-3 small">Language models like <a href=https://en.wikipedia.org/wiki/BERT>BERT</a> and SpanBERT pretrained on open-domain data have obtained impressive gains on various NLP tasks. In this paper, we probe the effectiveness of domain-adaptive pretraining objectives on downstream tasks. In particular, three objectives, including a novel objective focusing on modeling predicate-argument relations, are evaluated on two challenging dialogue understanding tasks. Experimental results demonstrate that domain-adaptive pretraining with proper objectives can significantly improve the performance of a strong baseline on these tasks, achieving the new state-of-the-art performances.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.85.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--85 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.85 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.85/>Targeting the Benchmark : On Methodology in Current Natural Language Processing Research</a></strong><br><a href=/people/d/david-schlangen/>David Schlangen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--85><div class="card-body p-3 small">It has become a common pattern in our field : One group introduces a language task, exemplified by a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>, which they argue is challenging enough to serve as a benchmark. They also provide a <a href=https://en.wikipedia.org/wiki/Baseline_(medicine)>baseline model</a> for it, which then soon is improved upon by other groups. Often, research efforts then move on, and the <a href=https://en.wikipedia.org/wiki/Pattern>pattern</a> repeats itself. What is typically left implicit is the argumentation for why this constitutes progress, and progress towards what. In this paper, we try to step back for a moment from this pattern and work out possible argumentations and their parts.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.87.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--87 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.87 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.87/>nmT5-Is <a href=https://en.wikipedia.org/wiki/Parallel_computing>parallel data</a> still relevant for pre-training massively multilingual language models?<span class=acl-fixed-case>T</span>5 - Is parallel data still relevant for pre-training massively multilingual language models?</a></strong><br><a href=/people/m/mihir-kale/>Mihir Kale</a>
|
<a href=/people/a/aditya-siddhant/>Aditya Siddhant</a>
|
<a href=/people/r/rami-al-rfou/>Rami Al-Rfou</a>
|
<a href=/people/l/linting-xue/>Linting Xue</a>
|
<a href=/people/n/noah-constant/>Noah Constant</a>
|
<a href=/people/m/melvin-johnson/>Melvin Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--87><div class="card-body p-3 small">Recently, mT5-a massively multilingual version of T5-leveraged a unified text-to-text format to attain state-of-the-art results on a wide variety of multilingual NLP tasks. In this paper, we investigate the impact of incorporating parallel data into mT5 pre-training. We find that multi-tasking language modeling with objectives such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a> during pre-training is a straightforward way to improve performance on downstream multilingual and cross-lingual tasks. However, the gains start to diminish as the model capacity increases, suggesting that parallel data might not be as essential for larger <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. At the same time, even at larger model sizes, we find that pre-training with parallel data still provides benefits in the limited labelled data regime</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.93.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--93 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.93 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.93" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.93/>Explicitly Capturing Relations between Entity Mentions via Graph Neural Networks for Domain-specific Named Entity Recognition</a></strong><br><a href=/people/p/pei-chen/>Pei Chen</a>
|
<a href=/people/h/haibo-ding/>Haibo Ding</a>
|
<a href=/people/j/jun-araki/>Jun Araki</a>
|
<a href=/people/r/ruihong-huang/>Ruihong Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--93><div class="card-body p-3 small">Named entity recognition (NER) is well studied for the general domain, and recent systems have achieved human-level performance for identifying common entity types. However, the NER performance is still moderate for <a href=https://en.wikipedia.org/wiki/Domain-specific_language>specialized domains</a> that tend to feature complicated contexts and jargonistic entity types. To address these challenges, we propose explicitly connecting entity mentions based on both global coreference relations and local dependency relations for building better entity mention representations. In our experiments, we incorporate <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity mention relations</a> by Graph Neural Networks and show that our system noticeably improves the NER performance on two <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> from different domains. We further show that the proposed lightweight system can effectively elevate the <a href=https://en.wikipedia.org/wiki/Network_topology>NER</a> performance to a higher level even when only a tiny amount of labeled data is available, which is desirable for domain-specific NER.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.94.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--94 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.94 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.94" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.94/>Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction</a></strong><br><a href=/people/g/gyubok-lee/>Gyubok Lee</a>
|
<a href=/people/s/seongjun-yang/>Seongjun Yang</a>
|
<a href=/people/e/edward-choi/>Edward Choi</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--94><div class="card-body p-3 small">Accurate terminology translation is crucial for ensuring the practicality and reliability of neural machine translation (NMT) systems. To address this, lexically constrained NMT explores various methods to ensure pre-specified words and phrases appear in the translation output. However, in many cases, those methods are studied on general domain corpora, where the terms are mostly uni- and bi-grams (98 %). In this paper, we instead tackle a more challenging setup consisting of domain-specific corpora with much longer <a href=https://en.wikipedia.org/wiki/N-gram>n-gram</a> and highly specialized terms. Inspired by the recent success of masked span prediction models, we propose a simple and effective training strategy that achieves consistent improvements on both terminology and sentence-level translation for three domain-specific corpora in two language pairs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.95.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--95 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.95 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.95" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.95/>Quotation Recommendation and Interpretation Based on Transformation from Queries to Quotations</a></strong><br><a href=/people/l/lingzhi-wang/>Lingzhi Wang</a>
|
<a href=/people/x/xingshan-zeng/>Xingshan Zeng</a>
|
<a href=/people/k/kam-fai-wong/>Kam-Fai Wong</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--95><div class="card-body p-3 small">To help individuals express themselves better, quotation recommendation is receiving growing attention. Nevertheless, most prior efforts focus on modeling <a href=https://en.wikipedia.org/wiki/Quotation>quotations</a> and queries separately and ignore the relationship between the <a href=https://en.wikipedia.org/wiki/Quotation>quotations</a> and the queries. In this work, we introduce a <a href=https://en.wikipedia.org/wiki/Transformation_matrix>transformation matrix</a> that directly maps the query representations to quotation representations. To better learn the mapping relationship, we employ a mapping loss that minimizes the distance of two semantic spaces (one for <a href=https://en.wikipedia.org/wiki/Quotation>quotation</a> and another for mapped-query). Furthermore, we explore using the words in history queries to interpret the figurative language of quotations, where quotation-aware attention is applied on top of history queries to highlight the indicator words. Experiments on two datasets in <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a> show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms previous state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.100.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--100 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.100 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.100/>Neural-Symbolic Commonsense Reasoner with Relation Predictors</a></strong><br><a href=/people/f/farhad-moghimifar/>Farhad Moghimifar</a>
|
<a href=/people/l/lizhen-qu/>Lizhen Qu</a>
|
<a href=/people/t/terry-yue-zhuo/>Terry Yue Zhuo</a>
|
<a href=/people/g/gholamreza-haffari/>Gholamreza Haffari</a>
|
<a href=/people/m/mahsa-baktashmotlagh/>Mahsa Baktashmotlagh</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--100><div class="card-body p-3 small">Commonsense reasoning aims to incorporate sets of commonsense facts, retrieved from Commonsense Knowledge Graphs (CKG), to draw conclusion about ordinary situations. The dynamic nature of <a href=https://en.wikipedia.org/wiki/Commonsense_knowledge>commonsense knowledge</a> postulates <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> capable of performing multi-hop reasoning over new situations. This feature also results in having large-scale sparse Knowledge Graphs, where such reasoning process is needed to predict relations between new events. However, existing approaches in this area are limited by considering CKGs as a limited set of facts, thus rendering them unfit for reasoning over new unseen situations and events. In this paper, we present a neural-symbolic reasoner, which is capable of reasoning over large-scale dynamic CKGs. The <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> for reasoning over CKGs are learned during training by our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a>. In addition to providing interpretable explanation, the learned <a href=https://en.wikipedia.org/wiki/Rule_of_inference>logic rules</a> help to generalise <a href=https://en.wikipedia.org/wiki/Prediction>prediction</a> to newly introduced events. Experimental results on the task of link prediction on CKGs prove the effectiveness of our model by outperforming the state-of-the-art models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.104.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--104 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.104 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.104/>Parameter Selection : Why We Should Pay More Attention to It</a></strong><br><a href=/people/j/jie-jyun-liu/>Jie-Jyun Liu</a>
|
<a href=/people/t/tsung-han-yang/>Tsung-Han Yang</a>
|
<a href=/people/s/si-an-chen/>Si-An Chen</a>
|
<a href=/people/c/chih-jen-lin/>Chih-Jen Lin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--104><div class="card-body p-3 small">The importance of parameter selection in <a href=https://en.wikipedia.org/wiki/Supervised_learning>supervised learning</a> is well known. However, due to the many parameter combinations, an incomplete or an insufficient procedure is often applied. This situation may cause misleading or confusing conclusions. In this opinion paper, through an intriguing example we point out that the seriousness goes beyond what is generally recognized. In the topic of multilabel classification for medical code prediction, one influential paper conducted a proper parameter selection on a set, but when moving to a subset of frequently occurring labels, the authors used the same parameters without a separate tuning. The set of frequent labels became a popular benchmark in subsequent studies, which kept pushing the state of the art. However, we discovered that most of the results in these studies can not surpass the approach in the original paper if a parameter tuning had been conducted at the time. Thus it is unclear how much progress the subsequent developments have actually brought. The lesson clearly indicates that without enough attention on parameter selection, the research progress in our field can be uncertain or even illusive.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.110.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--110 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.110 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.110" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.110/>Modeling Discriminative Representations for Out-of-Domain Detection with Supervised Contrastive Learning</a></strong><br><a href=/people/z/zhiyuan-zeng/>Zhiyuan Zeng</a>
|
<a href=/people/k/keqing-he/>Keqing He</a>
|
<a href=/people/y/yuanmeng-yan/>Yuanmeng Yan</a>
|
<a href=/people/z/zijun-liu/>Zijun Liu</a>
|
<a href=/people/y/yanan-wu/>Yanan Wu</a>
|
<a href=/people/h/hong-xu/>Hong Xu</a>
|
<a href=/people/h/huixing-jiang/>Huixing Jiang</a>
|
<a href=/people/w/weiran-xu/>Weiran Xu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--110><div class="card-body p-3 small">Detecting Out-of-Domain (OOD) or unknown intents from user queries is essential in a task-oriented dialog system. A key challenge of OOD detection is to learn discriminative semantic features. Traditional cross-entropy loss only focuses on whether a sample is correctly classified, and does not explicitly distinguish the margins between categories. In this paper, we propose a supervised contrastive learning objective to minimize intra-class variance by pulling together in-domain intents belonging to the same class and maximize inter-class variance by pushing apart samples from different classes. Besides, we employ an adversarial augmentation mechanism to obtain pseudo diverse views of a sample in the latent space. Experiments on two public datasets prove the effectiveness of our method capturing discriminative representations for OOD detection.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.114.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--114 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.114 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.114/>Exposing the limits of Zero-shot Cross-lingual Hate Speech Detection</a></strong><br><a href=/people/d/debora-nozza/>Debora Nozza</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--114><div class="card-body p-3 small">Reducing and counter-acting hate speech on <a href=https://en.wikipedia.org/wiki/Social_media>Social Media</a> is a significant concern. Most of the proposed automatic methods are conducted exclusively on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and very few consistently labeled, non-English resources have been proposed. Learning to detect <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> on <a href=https://en.wikipedia.org/wiki/English_language>English</a> and transferring to unseen languages seems an immediate solution. This work is the first to shed light on the limits of this zero-shot, cross-lingual transfer learning framework for hate speech detection. We use benchmark data sets in <a href=https://en.wikipedia.org/wiki/English_language>English</a>, <a href=https://en.wikipedia.org/wiki/Italian_language>Italian</a>, and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> to detect <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a> towards immigrants and women. Investigating post-hoc explanations of the model, we discover that non-hateful, language-specific taboo interjections are misinterpreted as signals of <a href=https://en.wikipedia.org/wiki/Hate_speech>hate speech</a>. Our findings demonstrate that zero-shot, cross-lingual models can not be used as they are, but need to be carefully designed.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.118.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--118 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.118 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.118/>Addressing Semantic Drift in Generative Question Answering with Auxiliary Extraction</a></strong><br><a href=/people/c/chenliang-li/>Chenliang Li</a>
|
<a href=/people/b/bin-bi/>Bin Bi</a>
|
<a href=/people/m/ming-yan/>Ming Yan</a>
|
<a href=/people/w/wei-wang/>Wei Wang</a>
|
<a href=/people/s/songfang-huang/>Songfang Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--118><div class="card-body p-3 small">Recently, question answering (QA) based on machine reading comprehension has become popular. This work focuses on generative QA which aims to generate an abstractive answer to a given question instead of extracting an answer span from a provided passage. Generative QA often suffers from two critical problems : (1) summarizing content irrelevant to a given question, (2) drifting away from a correct answer during generation. In this paper, we address these problems by a novel Rationale-Enriched Answer Generator (REAG), which incorporates an extractive mechanism into a generative model. Specifically, we add an extraction task on the <a href=https://en.wikipedia.org/wiki/Encoder>encoder</a> to obtain the rationale for an answer, which is the most relevant piece of text in an input document to a given question. Based on the extracted rationale and original input, the decoder is expected to generate an answer with high confidence. We jointly train REAG on the MS MARCO QA+NLG task and the experimental results show that REAG improves the quality and semantic accuracy of answers over baseline models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.125.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--125 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.125 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.125/>Improving Model Generalization : A Chinese Named Entity Recognition Case Study<span class=acl-fixed-case>C</span>hinese Named Entity Recognition Case Study</a></strong><br><a href=/people/g/guanqing-liang/>Guanqing Liang</a>
|
<a href=/people/c/cane-wing-ki-leung/>Cane Wing-Ki Leung</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--125><div class="card-body p-3 small">Generalization is an important ability that helps to ensure that a machine learning model can perform well on unseen data. In this paper, we study the effect of data bias on model generalization, using Chinese Named Entity Recognition (NER) as a case study. Specifically, we analyzed five benchmarking datasets for Chinese NER, and observed the following two types of data bias that can compromise model generalization ability. Firstly, the test sets of all the five <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> contain a significant proportion of entities that have been seen in the training sets. Such test data would therefore not be able to reflect the true generalization ability of a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>. Secondly, all <a href=https://en.wikipedia.org/wiki/Data_set>datasets</a> are dominated by a few fat-head entities, i.e., entities appearing with particularly high frequency. As a result, a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> might be able to produce high prediction accuracy simply by keyword memorization without leveraging <a href=https://en.wikipedia.org/wiki/Context_(language_use)>context knowledge</a>. To address these data biases, we first refine each test set by excluding seen entities from it, so as to better evaluate a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a>&#8217;s generalization ability. Then, we propose a simple yet effective entity resampling method to make entities within the same category distributed equally, encouraging a model to leverage both name and context knowledge in the training process. Experimental results demonstrate that the proposed entity resampling method significantly improves a model&#8217;s ability in detecting unseen entities, especially for company, organization and position categories.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.128.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--128 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.128 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.128.OptionalSupplementaryMaterial.pdf data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/2021.acl-short.128/>Attentive Multiview Text Representation for <a href=https://en.wikipedia.org/wiki/Differential_diagnosis>Differential Diagnosis</a></a></strong><br><a href=/people/h/hadi-amiri/>Hadi Amiri</a>
|
<a href=/people/m/mitra-mohtarami/>Mitra Mohtarami</a>
|
<a href=/people/i/isaac-kohane/>Isaac Kohane</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--128><div class="card-body p-3 small">We present a text representation approach that can combine different views (representations) of the same input through effective <a href=https://en.wikipedia.org/wiki/Data_fusion>data fusion</a> and attention strategies for ranking purposes. We apply our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> to the problem of <a href=https://en.wikipedia.org/wiki/Differential_diagnosis>differential diagnosis</a>, which aims to find the most probable diseases that match with clinical descriptions of patients, using data from the <a href=https://en.wikipedia.org/wiki/Undiagnosed_Diseases_Network>Undiagnosed Diseases Network</a>. Our model outperforms several ranking approaches (including a commercially-supported system) by effectively prioritizing and combining representations obtained from traditional and recent text representation techniques. We elaborate on several aspects of our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> and shed light on its improved performance.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.129.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--129 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.129 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/2021.acl-short.129.OptionalSupplementaryMaterial.zip data-toggle=tooltip data-placement=top title="Optional supplementary material"><i class="fas fa-file"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.129" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.129/>MedNLI Is Not Immune : Natural Language Inference Artifacts in the Clinical Domain<span class=acl-fixed-case>M</span>ed<span class=acl-fixed-case>NLI</span> Is Not Immune: <span class=acl-fixed-case>N</span>atural Language Inference Artifacts in the Clinical Domain</a></strong><br><a href=/people/c/christine-herlihy/>Christine Herlihy</a>
|
<a href=/people/r/rachel-rudinger/>Rachel Rudinger</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--129><div class="card-body p-3 small">Crowdworker-constructed natural language inference (NLI) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis-only classifiers to achieve better-than-random performance (CITATION). We investigate whether MedNLI, a physician-annotated dataset with premises extracted from clinical notes, contains such artifacts (CITATION). We find that entailed hypotheses contain generic versions of specific concepts in the premise, as well as modifiers related to <a href=https://en.wikipedia.org/wiki/Responsiveness>responsiveness</a>, duration, and <a href=https://en.wikipedia.org/wiki/Probability>probability</a>. Neutral hypotheses feature conditions and behaviors that co-occur with, or cause, the condition(s) in the premise. Contradiction hypotheses feature explicit negation of the premise and implicit negation via assertion of good health. Adversarial filtering demonstrates that performance degrades when evaluated on the difficult subset. We provide <a href=https://en.wikipedia.org/wiki/Partition_of_a_set>partition information</a> and recommendations for alternative dataset construction strategies for knowledge-intensive domains.<i>difficult</i> subset. We provide partition information and recommendations for alternative dataset construction strategies for knowledge-intensive domains.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.130.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--130 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.130 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.130/>Towards a more Robust Evaluation for Conversational Question Answering</a></strong><br><a href=/people/w/wissam-siblini/>Wissam Siblini</a>
|
<a href=/people/b/baris-sayil/>Baris Sayil</a>
|
<a href=/people/y/yacine-kessaci/>Yacine Kessaci</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--130><div class="card-body p-3 small">With the explosion of chatbot applications, Conversational Question Answering (CQA) has generated a lot of interest in recent years. Among proposals, reading comprehension models which take advantage of the conversation history (previous QA) seem to answer better than those which only consider the current question. Nevertheless, we note that the CQA evaluation protocol has a major limitation. In particular, <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> are allowed, at each turn of the conversation, to access the ground truth answers of the previous turns. Not only does this severely prevent their applications in fully autonomous chatbots, it also leads to unsuspected biases in their behavior. In this paper, we highlight this effect and propose new tools for <a href=https://en.wikipedia.org/wiki/Evaluation>evaluation</a> and <a href=https://en.wikipedia.org/wiki/Training>training</a> in order to guard against the noted issues. The new results that we bring come to reinforce <a href=https://en.wikipedia.org/wiki/Methodology>methods</a> of the current state of the art.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.132.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--132 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.132 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.132/>Avoiding Overlap in <a href=https://en.wikipedia.org/wiki/Data_augmentation>Data Augmentation</a> for AMR-to-Text Generation<span class=acl-fixed-case>AMR</span>-to-Text Generation</a></strong><br><a href=/people/w/wenchao-du/>Wenchao Du</a>
|
<a href=/people/j/jeffrey-flanigan/>Jeffrey Flanigan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--132><div class="card-body p-3 small">Leveraging additional unlabeled data to boost model performance is common practice in <a href=https://en.wikipedia.org/wiki/Machine_learning>machine learning</a> and <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing</a>. For generation tasks, if there is overlap between the additional data and the target text evaluation data, then training on the additional <a href=https://en.wikipedia.org/wiki/Data>data</a> is training on answers of the test set. This leads to overly-inflated scores with the additional <a href=https://en.wikipedia.org/wiki/Data>data</a> compared to real-world testing scenarios and problems when comparing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We study the AMR dataset and Gigaword, which is popularly used for improving AMR-to-text generators, and find significant overlap between Gigaword and a subset of the AMR dataset. We propose methods for excluding parts of Gigaword to remove this overlap, and show that our approach leads to a more realistic evaluation of the task of AMR-to-text generation. Going forward, we give simple best-practice recommendations for leveraging additional data in AMR-to-text generation.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.133.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--133 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.133 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.133" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.133/>Weakly-Supervised Methods for <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>Suicide Risk Assessment</a> : Role of Related Domains</a></strong><br><a href=/people/c/chenghao-yang/>Chenghao Yang</a>
|
<a href=/people/y/yudong-zhang/>Yudong Zhang</a>
|
<a href=/people/s/smaranda-muresan/>Smaranda Muresan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--133><div class="card-body p-3 small">Social media has become a valuable resource for the study of <a href=https://en.wikipedia.org/wiki/Suicidal_ideation>suicidal ideation</a> and the <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>assessment of suicide risk</a>. Among social media platforms, <a href=https://en.wikipedia.org/wiki/Reddit>Reddit</a> has emerged as the most promising one due to its anonymity and its focus on topic-based communities (subreddits) that can be indicative of someone&#8217;s state of mind or interest regarding mental health disorders such as <a href=https://en.wikipedia.org/wiki/Reddit>r / SuicideWatch</a>, <a href=https://en.wikipedia.org/wiki/Reddit>r / Anxiety</a>, <a href=https://en.wikipedia.org/wiki/Reddit>r / depression</a>. A challenge for previous work on <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>suicide risk assessment</a> has been the small amount of labeled data. We propose an empirical investigation into several classes of weakly-supervised approaches, and show that using pseudo-labeling based on related issues around <a href=https://en.wikipedia.org/wiki/Mental_health>mental health</a> (e.g., <a href=https://en.wikipedia.org/wiki/Anxiety>anxiety</a>, depression) helps improve model performance for <a href=https://en.wikipedia.org/wiki/Suicide_risk_assessment>suicide risk assessment</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.134.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--134 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.134 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.134/>Can Transformer Models Measure Coherence In Text : Re-Thinking the Shuffle Test</a></strong><br><a href=/people/p/philippe-laban/>Philippe Laban</a>
|
<a href=/people/l/luke-dai/>Luke Dai</a>
|
<a href=/people/l/lucas-bandarkar/>Lucas Bandarkar</a>
|
<a href=/people/m/marti-a-hearst/>Marti A. Hearst</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--134><div class="card-body p-3 small">The Shuffle Test is the most common task to evaluate whether <a href=https://en.wikipedia.org/wiki/Natural_language_processing>NLP models</a> can measure coherence in text. Most recent work uses direct supervision on the task ; we show that by simply finetuning a RoBERTa model, we can achieve a near perfect <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 97.8 %, a state-of-the-art. We argue that this outstanding performance is unlikely to lead to a good model of <a href=https://en.wikipedia.org/wiki/Coherence_(linguistics)>text coherence</a>, and suggest that the Shuffle Test should be approached in a Zero-Shot setting : <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> should be evaluated without being trained on the task itself. We evaluate common models in this setting, such as Generative and Bi-directional Transformers, and find that larger architectures achieve high-performance out-of-the-box. Finally, we suggest the k-Block Shuffle Test, a modification of the original by increasing the size of blocks shuffled. Even though human reader performance remains high (around 95 % accuracy), model performance drops from 94 % to 78 % as block size increases, creating a conceptually simple challenge to benchmark NLP models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/2021.acl-short.136.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-2021--acl-short--136 data-toggle=collapse aria-expanded=false aria-controls=abstract-2021.acl-short.136 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=2021.acl-short.136" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/2021.acl-short.136/>SaRoCo : Detecting Satire in a Novel Romanian Corpus of News Articles<span class=acl-fixed-case>S</span>a<span class=acl-fixed-case>R</span>o<span class=acl-fixed-case>C</span>o: Detecting Satire in a Novel <span class=acl-fixed-case>R</span>omanian Corpus of News Articles</a></strong><br><a href=/people/a/ana-cristina-rogoz/>Ana-Cristina Rogoz</a>
|
<a href=/people/g/gaman-mihaela/>Gaman Mihaela</a>
|
<a href=/people/r/radu-tudor-ionescu/>Radu Tudor Ionescu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-2021--acl-short--136><div class="card-body p-3 small">In this work, we introduce a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a> for satire detection in <a href=https://en.wikipedia.org/wiki/Media_of_Romania>Romanian news</a>. We gathered 55,608 public news articles from multiple real and satirical news sources, composing one of the largest <a href=https://en.wikipedia.org/wiki/Text_corpus>corpora</a> for satire detection regardless of language and the only one for the <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian language</a>. We provide an official split of the text samples, such that training news articles belong to different sources than test news articles, thus ensuring that <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> do not achieve high performance simply due to <a href=https://en.wikipedia.org/wiki/Overfitting>overfitting</a>. We conduct experiments with two state-of-the-art deep neural models, resulting in a set of strong baselines for our novel <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. Our results show that the machine-level accuracy for satire detection in <a href=https://en.wikipedia.org/wiki/Romanian_language>Romanian</a> is quite low (under 73 % on the test set) compared to the human-level accuracy (87 %), leaving enough room for improvement in future research.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>