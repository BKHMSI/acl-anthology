<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 2nd Workshop on New Frontiers in Summarization - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/D19-54.pdf>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></h2><p class=lead><a href=/people/l/lu-wang/>Lu Wang</a>,
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>,
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>,
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>D19-54</dd><dt>Month:</dt><dd>November</dd><dt>Year:</dt><dd>2019</dd><dt>Address:</dt><dd>Hong Kong, China</dd><dt>Venues:</dt><dd><a href=/venues/emnlp/>EMNLP</a>
| <a href=/venues/ws/>WS</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/D19-54>https://aclanthology.org/D19-54</a></dd><dt>DOI:</dt><dd></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/D19-54.pdf>https://aclanthology.org/D19-54.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/D19-54.pdf title="Open PDF of 'Proceedings of the 2nd Workshop on New Frontiers in Summarization'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+2nd+Workshop+on+New+Frontiers+in+Summarization" title="Search for 'Proceedings of the 2nd Workshop on New Frontiers in Summarization' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5400.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5400/>Proceedings of the 2nd Workshop on New Frontiers in Summarization</a></strong><br><a href=/people/l/lu-wang/>Lu Wang</a>
|
<a href=/people/j/jackie-chi-kit-cheung/>Jackie Chi Kit Cheung</a>
|
<a href=/people/g/giuseppe-carenini/>Giuseppe Carenini</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5401.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5401 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5401 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5401/>Answering Naturally : Factoid to Full length Answer Generation</a></strong><br><a href=/people/v/vaishali-pal/>Vaishali Pal</a>
|
<a href=/people/m/manish-shrivastava/>Manish Shrivastava</a>
|
<a href=/people/i/irshad-bhat/>Irshad Bhat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5401><div class="card-body p-3 small">In recent years, the task of Question Answering over passages, also pitched as a <a href=https://en.wikipedia.org/wiki/Reading_comprehension>reading comprehension</a>, has evolved into a very active research area. A reading comprehension system extracts a span of text, comprising of <a href=https://en.wikipedia.org/wiki/Named_entity>named entities</a>, <a href=https://en.wikipedia.org/wiki/Calendar_date>dates</a>, <a href=https://en.wikipedia.org/wiki/Phrase>small phrases</a>, etc., which serve as the answer to a given question. However, these spans of text would result in an unnatural reading experience in a conversational system. Usually, <a href=https://en.wikipedia.org/wiki/Dialogue_system>dialogue systems</a> solve this issue by using template-based language generation. These <a href=https://en.wikipedia.org/wiki/System>systems</a>, though adequate for a domain specific task, are too restrictive and predefined for a domain independent system. In order to present the user with a more conversational experience, we propose a pointer generator based full-length answer generator which can be used with most QA systems. Our system generates a full length answer given a question and the extracted factoid / span answer without relying on the passage from where the answer was extracted. We also present a <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> of 315000 question, factoid answer and full length answer triples. We have evaluated our system using ROUGE-1,2,L and BLEU and achieved 74.05 BLEU score and 86.25 Rogue-L score.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5403.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5403 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5403 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5403/>Abstractive Timeline Summarization</a></strong><br><a href=/people/j/julius-steen/>Julius Steen</a>
|
<a href=/people/k/katja-markert/>Katja Markert</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5403><div class="card-body p-3 small">Timeline summarization (TLS) automatically identifies key dates of major events and provides short descriptions of what happened on these dates. Previous approaches to <a href=https://en.wikipedia.org/wiki/Transport_Layer_Security>TLS</a> have focused on extractive methods. In contrast, we suggest an abstractive timeline summarization system. Our system is entirely unsupervised, which makes it especially suited to TLS where there are very few gold summaries available for training of supervised systems. In addition, we present the first abstractive oracle experiments for <a href=https://en.wikipedia.org/wiki/Transport_Layer_Security>TLS</a>. Our system outperforms extractive competitors in terms of ROUGE when the number of input documents is high and the output requires strong <a href=https://en.wikipedia.org/wiki/Data_compression>compression</a>. In these cases, our oracle experiments confirm that our approach also has a higher <a href=https://en.wikipedia.org/wiki/Upper_and_lower_bounds>upper bound</a> for ROUGE scores than extractive methods. A study with human judges shows that our abstractive system also produces output that is easy to read and understand.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5404.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5404 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5404 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5404/>Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization</a></strong><br><a href=/people/d/diego-antognini/>Diego Antognini</a>
|
<a href=/people/b/boi-faltings/>Boi Faltings</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5404><div class="card-body p-3 small">Linking facts across documents is a challenging <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>, as the language used to express the same information in a sentence can vary significantly, which complicates the task of <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a>. Consequently, existing approaches heavily rely on hand-crafted features, which are domain-dependent and hard to craft, or additional annotated data, which is costly to gather. To overcome these limitations, we present a novel method, which makes use of two types of sentence embeddings : universal embeddings, which are trained on a large unrelated corpus, and domain-specific embeddings, which are learned during training. To this end, we develop SemSentSum, a fully data-driven model able to leverage both types of sentence embeddings by building a sentence semantic relation graph. SemSentSum achieves competitive results on two types of <a href=https://en.wikipedia.org/wiki/Abstract_(summary)>summary</a>, consisting of 665 bytes and 100 words. Unlike other state-of-the-art <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a>, neither hand-crafted features nor additional annotated data are necessary, and the <a href=https://en.wikipedia.org/wiki/Method_(computer_programming)>method</a> is easily adaptable for other tasks. To our knowledge, we are the first to use multiple sentence embeddings for the task of <a href=https://en.wikipedia.org/wiki/Multi-document_summarization>multi-document summarization</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5408.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5408 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5408 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5408/>Towards Annotating and Creating Summary Highlights at Sub-sentence Level</a></strong><br><a href=/people/k/kristjan-arumae/>Kristjan Arumae</a>
|
<a href=/people/p/parminder-bhatia/>Parminder Bhatia</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5408><div class="card-body p-3 small">Highlighting is a powerful tool to pick out important content and emphasize. Creating summary highlights at the sub-sentence level is particularly desirable, because sub-sentences are more concise than whole sentences. They are also better suited than individual words and phrases that can potentially lead to disfluent, fragmented summaries. In this paper we seek to generate summary highlights by annotating summary-worthy sub-sentences and teaching <a href=https://en.wikipedia.org/wiki/Classifier_(linguistics)>classifiers</a> to do the same. We frame the task as jointly selecting important sentences and identifying a single most informative textual unit from each sentence. This <a href=https://en.wikipedia.org/wiki/Formulation>formulation</a> dramatically reduces the <a href=https://en.wikipedia.org/wiki/Complexity>task complexity</a> involved in sentence compression. Our study provides new benchmarks and baselines for generating highlights at the <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sub-sentence level</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5409.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5409 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5409 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5409/>SAMSum Corpus : A Human-annotated Dialogue Dataset for Abstractive Summarization<span class=acl-fixed-case>SAMS</span>um Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization</a></strong><br><a href=/people/b/bogdan-gliwa/>Bogdan Gliwa</a>
|
<a href=/people/i/iwona-mochol/>Iwona Mochol</a>
|
<a href=/people/m/maciej-biesek/>Maciej Biesek</a>
|
<a href=/people/a/aleksander-wawer/>Aleksander Wawer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5409><div class="card-body p-3 small">This paper introduces the SAMSum Corpus, a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> with abstractive dialogue summaries. We investigate the challenges it poses for <a href=https://en.wikipedia.org/wiki/Automatic_summarization>automated summarization</a> by testing several <a href=https://en.wikipedia.org/wiki/Statistical_model>models</a> and comparing their results with those obtained on a <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus of news articles</a>. We show that model-generated summaries of dialogues achieve higher ROUGE scores than the model-generated summaries of news in contrast with human evaluators&#8217; judgement. This suggests that a challenging task of abstractive dialogue summarization requires dedicated <a href=https://en.wikipedia.org/wiki/Conceptual_model>models</a> and non-standard <a href=https://en.wikipedia.org/wiki/Quality_(business)>quality measures</a>. To our knowledge, our study is the first attempt to introduce a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations, which can be used by the research community for further studies.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5410.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5410 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5410 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5410/>A Closer Look at Data Bias in Neural Extractive Summarization Models</a></strong><br><a href=/people/m/ming-zhong/>Ming Zhong</a>
|
<a href=/people/d/danqing-wang/>Danqing Wang</a>
|
<a href=/people/p/pengfei-liu/>Pengfei Liu</a>
|
<a href=/people/x/xipeng-qiu/>Xipeng Qiu</a>
|
<a href=/people/x/xuan-jing-huang/>Xuanjing Huang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5410><div class="card-body p-3 small">In this paper, we take stock of the current state of summarization datasets and explore how different factors of datasets influence the generalization behaviour of neural extractive summarization models. Specifically, we first propose several properties of datasets, which matter for the generalization of summarization models. Then we build the connection between <a href=https://en.wikipedia.org/wiki/Prior_probability>priors</a> residing in datasets and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing state-of-the-art model.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5413.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5413 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5413 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5413/>Analyzing Sentence Fusion in Abstractive Summarization</a></strong><br><a href=/people/l/logan-lebanoff/>Logan Lebanoff</a>
|
<a href=/people/j/john-muchovej/>John Muchovej</a>
|
<a href=/people/f/franck-dernoncourt/>Franck Dernoncourt</a>
|
<a href=/people/d/doo-soon-kim/>Doo Soon Kim</a>
|
<a href=/people/s/seokhwan-kim/>Seokhwan Kim</a>
|
<a href=/people/w/walter-chang/>Walter Chang</a>
|
<a href=/people/f/fei-liu-utdallas/>Fei Liu</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5413><div class="card-body p-3 small">While recent work in abstractive summarization has resulted in higher scores in automatic metrics, there is little understanding on how these systems combine information taken from multiple document sentences. In this paper, we analyze the outputs of five state-of-the-art abstractive summarizers, focusing on summary sentences that are formed by sentence fusion. We ask assessors to judge the <a href=https://en.wikipedia.org/wiki/Grammaticality>grammaticality</a>, <a href=https://en.wikipedia.org/wiki/Faithfulness>faithfulness</a>, and method of fusion for summary sentences. Our analysis reveals that system sentences are mostly grammatical, but often fail to remain faithful to the original article.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/D19-5414.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-D19-5414 data-toggle=collapse aria-expanded=false aria-controls=abstract-D19-5414 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/D19-5414/>Summarizing Relationships for Interactive Concept Map Browsers</a></strong><br><a href=/people/a/abram-handler/>Abram Handler</a>
|
<a href=/people/p/premkumar-ganeshkumar/>Premkumar Ganeshkumar</a>
|
<a href=/people/b/brendan-oconnor/>Brendan O’Connor</a>
|
<a href=/people/m/mohamed-altantawy/>Mohamed AlTantawy</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-D19-5414><div class="card-body p-3 small">Concept maps are visual summaries, structured as <a href=https://en.wikipedia.org/wiki/Directed_graph>directed graphs</a> : important concepts from a dataset are displayed as vertexes, and edges between vertexes show natural language descriptions of the relationships between the concepts on the map. Thus far, preliminary attempts at automatically creating concept maps have focused on building static summaries. However, in interactive settings, users will need to dynamically investigate particular relationships between pairs of concepts. For instance, a historian using a concept map browser might decide to investigate the relationship between two politicians in a <a href=https://en.wikipedia.org/wiki/Archive>news archive</a>. We present a <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> which responds to such queries by returning one or more short, importance-ranked, natural language descriptions of the relationship between two requested concepts, for display in a <a href=https://en.wikipedia.org/wiki/User_interface>visual interface</a>. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> is trained on a new public dataset, collected for this <a href=https://en.wikipedia.org/wiki/Task_(project_management)>task</a>.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>