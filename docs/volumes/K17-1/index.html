<!doctype html><html lang=en-us><head><meta charset=utf-8><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><!--[if IEMobile]><meta http-equiv=cleartype content="on"><![endif]--><title>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017) - ACL Anthology</title><meta name=generator content="Hugo 0.98.0"><link href=/aclicon.ico rel="shortcut icon" type=image/x-icon><link rel=stylesheet href=/css/main.min.b1d14a9a8f6bb9c608ca4de9aad72a6e06945119f97951f2908522dc220e6277.css media=screen><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.7.2/css/all.css integrity=sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.css></head><body><nav class="navbar navbar-expand-sm navbar-dark bg-dark bg-gradient-dark shadow-sm py-0 mb-3 mb-md-4 mb-xl-5"><div id=navbar-container class=container><a class=navbar-brand href=/><img src=/images/acl-logo.svg width=56 alt="ACL Logo">
<span class="d-none d-md-inline pl-md-2">ACL 2022 D&I Special Initiative</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarSupportedContent><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/>Papers<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/terms/>Terminology<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/>Videos<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/corrections/>Corrections<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/info/credits/>Credits<span class=sr-only>(current)</span></a></li><li class=nav-item><a class=nav-link href=/faq/>FAQ<span class=sr-only>(current)</span></a></li></ul><form class="form-inline my-2 my-lg-0" action=/search/? method=get><input id=acl-search-box class="form-control mr-sm-2" name=q type=search placeholder=Search... aria-label=Search>
<button class="btn btn-outline-secondary" type=submit><i class="fas fa-search"></i></button></form></div></div></nav><div id=main-container class=container><section id=main><h2 id=title><a href=https://aclanthology.org/K17-1.pdf>Proceedings of the 21st Conference on Computational Natural Language Learning (<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NLL</span> 2017)</a></h2><p class=lead><a href=/people/r/roger-levy/>Roger Levy</a>,
<a href=/people/l/lucia-specia/>Lucia Specia</a>
<span class=text-muted>(Editors)</span><hr><div class="row acl-paper-details"><div class="col col-lg-10 order-2"><dl><dt>Anthology ID:</dt><dd>K17-1</dd><dt>Month:</dt><dd>August</dd><dt>Year:</dt><dd>2017</dd><dt>Address:</dt><dd>Vancouver, Canada</dd><dt>Venue:</dt><dd><a href=/venues/conll/>CoNLL</a></dd><dt>SIG:</dt><dd><a href=/sigs/signll/>SIGNLL</a></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>URL:</dt><dd><a href=https://aclanthology.org/K17-1>https://aclanthology.org/K17-1</a></dd><dt>DOI:</dt><dd><a href=http://dx.doi.org/10.18653/v1/K17-1 title="To the current version of the paper by DOI">10.18653/v1/K17-1</a></dd><dt class=acl-button-row>Bib Export formats:</dt><dd class=acl-button-row></dd><dt>PDF:</dt><dd><a href=https://aclanthology.org/K17-1.pdf>https://aclanthology.org/K17-1.pdf</a></dd></dl></div><div class=acl-paper-link-block><a class="btn btn-primary" href=https://aclanthology.org/K17-1.pdf title="Open PDF of 'Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)'"><i class="far fa-file-pdf"></i><span class=pl-2>PDF&nbsp;<small>(full)</small></span></a>
<a class="btn btn-secondary" href="https://www.semanticscholar.org/search?q=Proceedings+of+the+21st+Conference+on+Computational+Natural+Language+Learning+%28CoNLL+2017%29" title="Search for 'Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)' on Semantic Scholar"><i class="ai ai-semantic-scholar"></i><span class="pl-sm-2 d-none d-sm-inline">Search</span></a></div></div><hr><div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1000.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1000/>Proceedings of the 21st Conference on Computational Natural Language Learning (<span class=acl-fixed-case>C</span>o<span class=acl-fixed-case>NLL</span> 2017)</a></strong><br><a href=/people/r/roger-levy/>Roger Levy</a>
|
<a href=/people/l/lucia-specia/>Lucia Specia</a></span></p><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1001.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1001 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1001 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1001/>Should Neural Network Architecture Reflect Linguistic Structure?</a></strong><br><a href=/people/c/chris-dyer/>Chris Dyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1001><div class="card-body p-3 small">I explore the hypothesis that conventional neural network models (e.g., recurrent neural networks) are incorrectly biased for making linguistically sensible generalizations when learning, and that a better class of <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> is based on architectures that reflect hierarchical structures for which considerable behavioral evidence exists. I focus on the problem of modeling and representing the meanings of sentences. On the generation front, I introduce recurrent neural network grammars (RNNGs), a joint, generative model of phrase-structure trees and sentences. RNNGs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire (top-down, left-to-right) syntactic derivation history, thus relaxing context-free independence assumptions, while retaining a bias toward explaining decisions via syntactically local conditioning contexts. Experiments show that RNNGs obtain better results in generating language than <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> that do n&#8217;t exploit linguistic structure. On the representation front, I explore unsupervised learning of syntactic structures based on distant semantic supervision using a <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement-learning algorithm</a>. The learner seeks a <a href=https://en.wikipedia.org/wiki/Syntax>syntactic structure</a> that provides a compositional architecture that produces a good <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representation</a> for a downstream semantic task. Although the inferred structures are quite different from traditional syntactic analyses, the performance on the downstream tasks surpasses that of systems that use sequential RNNs and tree-structured RNNs based on treebank dependencies. This is joint work with Adhi Kuncoro, Dani Yogatama, Miguel Ballesteros, Phil Blunsom, Ed Grefenstette, Wang Ling, and Noah A. Smith.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1002.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1002 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1002 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1002/>Rational Distortions of Learners’ Linguistic Input</a></strong><br><a href=/people/n/naomi-feldman/>Naomi Feldman</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1002><div class="card-body p-3 small">Language acquisition can be modeled as a <a href=https://en.wikipedia.org/wiki/Statistical_inference>statistical inference problem</a> : children use <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentences</a> and sounds in their input to infer linguistic structure. However, in many cases, children learn from data whose statistical structure is distorted relative to the language they are learning. Such distortions can arise either in the input itself, or as a result of children&#8217;s immature strategies for encoding their input. This work examines several cases in which the statistical structure of children&#8217;s input differs from the language being learned. Analyses show that these distortions of the input can be accounted for with a statistical learning framework by carefully considering the inference problems that learners solve during <a href=https://en.wikipedia.org/wiki/Language_acquisition>language acquisition</a></div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1003.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1003 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1003 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1003" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1003/>Exploring the Syntactic Abilities of RNNs with <a href=https://en.wikipedia.org/wiki/Multi-task_learning>Multi-task Learning</a><span class=acl-fixed-case>RNN</span>s with Multi-task Learning</a></strong><br><a href=/people/e/emile-enguehard/>Émile Enguehard</a>
|
<a href=/people/y/yoav-goldberg/>Yoav Goldberg</a>
|
<a href=/people/t/tal-linzen/>Tal Linzen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1003><div class="card-body p-3 small">Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>sentence structure</a>. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016). We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single <a href=https://en.wikipedia.org/wiki/Radio-frequency_identification>RNN</a> to perform both the agreement task and an additional task, either CCG supertagging or <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a>. Multi-task training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before. We also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of <a href=https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets>training data</a> is available for those tasks. The multi-task paradigm can also be leveraged to inject <a href=https://en.wikipedia.org/wiki/Grammar>grammatical knowledge</a> into <a href=https://en.wikipedia.org/wiki/Language_model>language models</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1004.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1004 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1004 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1004" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1004/>The Effect of Different Writing Tasks on Linguistic Style : A Case Study of the ROC Story Cloze Task<span class=acl-fixed-case>ROC</span> Story Cloze Task</a></strong><br><a href=/people/r/roy-schwartz/>Roy Schwartz</a>
|
<a href=/people/m/maarten-sap/>Maarten Sap</a>
|
<a href=/people/i/ioannis-konstas/>Ioannis Konstas</a>
|
<a href=/people/l/leila-zilles/>Leila Zilles</a>
|
<a href=/people/y/yejin-choi/>Yejin Choi</a>
|
<a href=/people/n/noah-a-smith/>Noah A. Smith</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1004><div class="card-body p-3 small">A writer&#8217;s style depends not just on <a href=https://en.wikipedia.org/wiki/Trait_theory>personal traits</a> but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in <a href=https://en.wikipedia.org/wiki/Writing_style>writing style</a>. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints : (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple <a href=https://en.wikipedia.org/wiki/Linear_classifier>linear classifier</a> informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the <a href=https://en.wikipedia.org/wiki/Context_(language_use)>story context</a>. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different <a href=https://en.wikipedia.org/wiki/Framing_(social_sciences)>task framings</a> can dramatically affect the way people write.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1005.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1005 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1005 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1005/>Parsing for <a href=https://en.wikipedia.org/wiki/Grammatical_relation>Grammatical Relations</a> via Graph Merging</a></strong><br><a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/y/yantao-du/>Yantao Du</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1005><div class="card-body p-3 small">This paper is concerned with building deep grammatical relation (GR) analysis using data-driven approach. To deal with this problem, we propose graph merging, a new perspective, for building flexible dependency graphs : Constructing complex graphs via constructing simple subgraphs. We discuss two key problems in this perspective : (1) how to decompose a <a href=https://en.wikipedia.org/wiki/Complex_graph>complex graph</a> into simple subgraphs, and (2) how to combine <a href=https://en.wikipedia.org/wiki/Glossary_of_graph_theory_terms>subgraphs</a> into a coherent complex graph. Experiments demonstrate the effectiveness of graph merging. Our <a href=https://en.wikipedia.org/wiki/Parsing>parser</a> reaches state-of-the-art performance and is significantly better than two transition-based parsers.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1007.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1007 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1007 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1007/>Collaborative Partitioning for Coreference Resolution</a></strong><br><a href=/people/o/olga-uryupina/>Olga Uryupina</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1007><div class="card-body p-3 small">This paper presents a collaborative partitioning algorithma novel ensemble-based approach to <a href=https://en.wikipedia.org/wiki/Coreference_resolution>coreference resolution</a>. Starting from the all-singleton partition, we search for a solution close to the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a>&#8217;s outputs in terms of a task-specific similarity measure. Our approach assumes a loose integration of individual components of the <a href=https://en.wikipedia.org/wiki/Statistical_ensemble_(mathematical_physics)>ensemble</a> and can therefore combine arbitrary coreference resolvers, regardless of their models. Our experiments on the CoNLL dataset show that collaborative partitioning yields results superior to those attained by the individual components, for ensembles of both strong and weak systems. Moreover, by applying the collaborative partitioning algorithm on top of three state-of-the-art resolvers, we obtain the best coreference performance reported so far in the literature (MELA v08 score of 64.47).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1009.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1009 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1009 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1009/>Tell Me Why : Using Question Answering as Distant Supervision for Answer Justification</a></strong><br><a href=/people/r/rebecca-sharp/>Rebecca Sharp</a>
|
<a href=/people/m/mihai-surdeanu/>Mihai Surdeanu</a>
|
<a href=/people/p/peter-jansen/>Peter Jansen</a>
|
<a href=/people/m/marco-a-valenzuela-escarcega/>Marco A. Valenzuela-Escárcega</a>
|
<a href=/people/p/peter-clark/>Peter Clark</a>
|
<a href=/people/m/michael-hammond/>Michael Hammond</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1009><div class="card-body p-3 small">For many applications of question answering (QA), being able to explain why a given <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> chose an answer is critical. However, the lack of labeled data for answer justifications makes learning this difficult and expensive. Here we propose an approach that uses answer ranking as distant supervision for learning how to select informative justifications, where justifications serve as inferential connections between the question and the correct answer while often containing little lexical overlap with either. We propose a neural network architecture for <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> that reranks answer justifications as an intermediate (and human-interpretable) step in answer selection. Our approach is informed by a set of <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> designed to combine both learned representations and explicit features to capture the connection between questions, answers, and answer justifications. We show that with this end-to-end approach we are able to significantly improve upon a strong IR baseline in both justification ranking (+9 % rated highly relevant) and answer selection (+6 % P@1).</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1010.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1010 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1010 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1010" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1010/>Learning What is Essential in Questions</a></strong><br><a href=/people/d/daniel-khashabi/>Daniel Khashabi</a>
|
<a href=/people/t/tushar-khot/>Tushar Khot</a>
|
<a href=/people/a/ashish-sabharwal/>Ashish Sabharwal</a>
|
<a href=/people/d/dan-roth/>Dan Roth</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1010><div class="card-body p-3 small">Question answering (QA) systems are easily distracted by irrelevant or redundant words in questions, especially when faced with long or multi-sentence questions in difficult domains. This paper introduces and studies the notion of essential question terms with the goal of improving such QA solvers. We illustrate the importance of essential question terms by showing that humans&#8217; ability to answer questions drops significantly when essential terms are eliminated from questions. We then develop a classifier that reliably (90 % mean average precision) identifies and ranks essential terms in questions. Finally, we use the classifier to demonstrate that the notion of question term essentiality allows state-of-the-art QA solver for elementary-level science questions to make better and more informed decisions, improving performance by up to 5%.We also introduce a new dataset of over 2,200 crowd-sourced essential terms annotated science questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1011.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1011 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1011 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1011/>Top-Rank Enhanced Listwise Optimization for <a href=https://en.wikipedia.org/wiki/Statistical_machine_translation>Statistical Machine Translation</a></a></strong><br><a href=/people/h/huadong-chen/>Huadong Chen</a>
|
<a href=/people/s/shujian-huang/>Shujian Huang</a>
|
<a href=/people/d/david-chiang/>David Chiang</a>
|
<a href=/people/x/xinyu-dai/>Xinyu Dai</a>
|
<a href=/people/j/jiajun-chen/>Jiajun Chen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1011><div class="card-body p-3 small">Pairwise ranking methods are the most widely used discriminative training approaches for structure prediction problems in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>natural language processing (NLP)</a>. Decomposing the problem of ranking hypotheses into <a href=https://en.wikipedia.org/wiki/Pairwise_comparisons>pairwise comparisons</a> enables simple and efficient solutions. However, neglecting the global ordering of the hypothesis list may hinder <a href=https://en.wikipedia.org/wiki/Learning>learning</a>. We propose a listwise learning framework for structure prediction problems such as <a href=https://en.wikipedia.org/wiki/Machine_translation>machine translation</a>. Our framework directly models the entire translation list&#8217;s ordering to learn parameters which may better fit the given listwise samples. Furthermore, we propose top-rank enhanced loss functions, which are more sensitive to ranking errors at higher positions. Experiments on a large-scale Chinese-English translation task show that both our listwise learning framework and top-rank enhanced listwise losses lead to significant improvements in translation quality.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1012.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1012 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1012 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K17-1012.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K17-1012/>Embedding Words and Senses Together via Joint Knowledge-Enhanced Training</a></strong><br><a href=/people/m/massimiliano-mancini/>Massimiliano Mancini</a>
|
<a href=/people/j/jose-camacho-collados/>Jose Camacho-Collados</a>
|
<a href=/people/i/ignacio-iacobacci/>Ignacio Iacobacci</a>
|
<a href=/people/r/roberto-navigli/>Roberto Navigli</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1012><div class="card-body p-3 small">Word embeddings are widely used in <a href=https://en.wikipedia.org/wiki/Natural_language_processing>Natural Language Processing</a>, mainly due to their success in capturing <a href=https://en.wikipedia.org/wiki/Semantics>semantic information</a> from <a href=https://en.wikipedia.org/wiki/Mass_media>massive corpora</a>. However, their creation process does not allow the different meanings of a word to be automatically separated, as it conflates them into a single vector. We address this issue by proposing a new <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> which learns word and sense embeddings jointly. Our model exploits large corpora and knowledge from <a href=https://en.wikipedia.org/wiki/Semantic_network>semantic networks</a> in order to produce a unified vector space of word and sense embeddings. We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to state-of-the-art word- and sense-based models.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1015.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1015 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1015 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1015/>An Artificial Language Evaluation of Distributional Semantic Models</a></strong><br><a href=/people/f/fatemeh-torabi-asr/>Fatemeh Torabi Asr</a>
|
<a href=/people/m/michael-jones/>Michael Jones</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1015><div class="card-body p-3 small">Recent studies of distributional semantic models have set up a competition between <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> obtained from predictive neural networks and <a href=https://en.wikipedia.org/wiki/Word_vector>word vectors</a> obtained from abstractive count-based models. This paper is an attempt to reveal the underlying contribution of additional training data and post-processing steps on each type of <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> in word similarity and relatedness inference tasks. We do so by designing an artificial language framework, training a predictive and a count-based model on data sampled from this <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a>, and evaluating the resulting word vectors in paradigmatic and syntagmatic tasks defined with respect to the <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1016.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1016 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1016 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1016/>Learning Word Representations with Regularization from Prior Knowledge</a></strong><br><a href=/people/y/yan-song/>Yan Song</a>
|
<a href=/people/c/chia-jung-lee/>Chia-Jung Lee</a>
|
<a href=/people/f/fei-xia/>Fei Xia</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1016><div class="card-body p-3 small">Conventional <a href=https://en.wikipedia.org/wiki/Word_embedding>word embeddings</a> are trained with specific <a href=https://en.wikipedia.org/wiki/Statistical_parameter>criteria</a> (e.g., based on <a href=https://en.wikipedia.org/wiki/Language_model>language modeling</a> or <a href=https://en.wikipedia.org/wiki/Co-occurrence>co-occurrence</a>) inside a single information source, disregarding the opportunity for further calibration using external knowledge. This paper presents a unified framework that leverages pre-learned or external priors, in the form of a <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizer</a>, for enhancing conventional language model-based embedding learning. We consider two types of <a href=https://en.wikipedia.org/wiki/Regularization_(physics)>regularizers</a>. The first type is derived from topic distribution by running LDA on unlabeled data. The second type is based on <a href=https://en.wikipedia.org/wiki/Dictionary>dictionaries</a> that are created with human annotation efforts. To effectively learn with the <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularizers</a>, we propose a novel <a href=https://en.wikipedia.org/wiki/Data_structure>data structure</a>, trajectory softmax, in this paper. The resulting <a href=https://en.wikipedia.org/wiki/Embedding>embeddings</a> are evaluated by word similarity and sentiment classification. Experimental results show that our learning framework with <a href=https://en.wikipedia.org/wiki/Regularization_(mathematics)>regularization</a> from prior knowledge improves embedding quality across multiple datasets, compared to a diverse collection of baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1017.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1017 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1017 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1017/>Attention-based Recurrent Convolutional Neural Network for Automatic Essay Scoring</a></strong><br><a href=/people/f/fei-dong/>Fei Dong</a>
|
<a href=/people/y/yue-zhang/>Yue Zhang</a>
|
<a href=/people/j/jie-yang/>Jie Yang</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1017><div class="card-body p-3 small">Neural network models have recently been applied to the task of automatic essay scoring, giving promising results. Existing work used <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>recurrent neural networks</a> and <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>convolutional neural networks</a> to model input essays, giving grades based on a single vector representation of the essay. On the other hand, the relative advantages of <a href=https://en.wikipedia.org/wiki/News_media>RNNs</a> and <a href=https://en.wikipedia.org/wiki/News_media>CNNs</a> have not been compared. In addition, different parts of the essay can contribute differently for scoring, which is not captured by existing <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a>. We address these issues by building a hierarchical sentence-document model to represent essays, using the attention mechanism to automatically decide the relative weights of words and sentences. Results show that our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> outperforms the previous state-of-the-art methods, demonstrating the effectiveness of the attention mechanism.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1018.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1018 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1018 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1018/>Feature Selection as <a href=https://en.wikipedia.org/wiki/Causal_inference>Causal Inference</a> : Experiments with Text Classification</a></strong><br><a href=/people/m/michael-paul/>Michael J. Paul</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1018><div class="card-body p-3 small">This paper proposes a matching technique for learning <a href=https://en.wikipedia.org/wiki/Causality>causal associations</a> between <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>word features</a> and <a href=https://en.wikipedia.org/wiki/Statistical_classification>class labels</a> in <a href=https://en.wikipedia.org/wiki/Document_classification>document classification</a>. The goal is to identify more meaningful and generalizable <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> than with only <a href=https://en.wikipedia.org/wiki/Correlation_and_dependence>correlational approaches</a>. Experiments with sentiment classification show that the proposed method identifies interpretable word associations with sentiment and improves <a href=https://en.wikipedia.org/wiki/Statistical_classification>classification</a> performance in a majority of cases. The proposed feature selection method is particularly effective when applied to out-of-domain data.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1021.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1021 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1021 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1021" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1021/>A Supervised Approach to Extractive Summarisation of Scientific Papers</a></strong><br><a href=/people/e/edward-collins/>Ed Collins</a>
|
<a href=/people/i/isabelle-augenstein/>Isabelle Augenstein</a>
|
<a href=/people/s/sebastian-riedel/>Sebastian Riedel</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1021><div class="card-body p-3 small">Automatic summarisation is a popular approach to reduce a document to its main arguments. Recent research in the area has focused on neural approaches to <a href=https://en.wikipedia.org/wiki/Automatic_summarization>summarisation</a>, which can be very data-hungry. However, few large datasets exist and none for the traditionally popular domain of <a href=https://en.wikipedia.org/wiki/Scientific_literature>scientific publications</a>, which opens up challenging research avenues centered on encoding large, complex documents. In this paper, we introduce a new <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a> for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. We develop <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> on the dataset making use of both neural sentence encoding and traditionally used summarisation features and show that <a href=https://en.wikipedia.org/wiki/Mathematical_model>models</a> which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1022.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1022 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1022 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1022/>An Automatic Approach for Document-level Topic Model Evaluation</a></strong><br><a href=/people/s/shraey-bhatia/>Shraey Bhatia</a>
|
<a href=/people/j/jey-han-lau/>Jey Han Lau</a>
|
<a href=/people/t/timothy-baldwin/>Timothy Baldwin</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1022><div class="card-body p-3 small">Topic models jointly learn <a href=https://en.wikipedia.org/wiki/Topic_and_comment>topics</a> and document-level topic distribution. Extrinsic evaluation of <a href=https://en.wikipedia.org/wiki/Topic_model>topic models</a> tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topic- and document-level model quality, and that basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of document-level topic allocations, and provide empirical evidence for its robustness.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1024.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1024 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1024 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1024/>Cross-language Learning with Adversarial Neural Networks</a></strong><br><a href=/people/s/shafiq-joty/>Shafiq Joty</a>
|
<a href=/people/p/preslav-nakov/>Preslav Nakov</a>
|
<a href=/people/l/lluis-marquez/>Lluís Màrquez</a>
|
<a href=/people/i/israa-jaradat/>Israa Jaradat</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1024><div class="card-body p-3 small">We address the problem of cross-language adaptation for question-question similarity reranking in community question answering, with the objective to port a system trained on one input language to another input language given labeled training data for the first language and only unlabeled data for the second language. In particular, we propose to use adversarial training of neural networks to learn high-level features that are discriminative for the main learning task, and at the same time are invariant across the input languages. The evaluation results show sizable improvements for our cross-language adversarial neural network (CLANN) model over a strong non-adversarial system.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1026.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1026 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1026 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1026" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1026/>A Probabilistic Generative Grammar for Semantic Parsing</a></strong><br><a href=/people/a/abulhair-saparov/>Abulhair Saparov</a>
|
<a href=/people/v/vijay-saraswat/>Vijay Saraswat</a>
|
<a href=/people/t/tom-mitchell/>Tom Mitchell</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1026><div class="card-body p-3 small">We present a generative model of natural language sentences and demonstrate its application to <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. In the generative process, a <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> sampled from a prior, and conditioned on this <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a>, a <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> probabilistically generates the output sentence. Grammar induction using MCMC is applied to learn the <a href=https://en.wikipedia.org/wiki/Grammar>grammar</a> given a set of labeled sentences with corresponding <a href=https://en.wikipedia.org/wiki/Logical_form>logical forms</a>. We develop a <a href=https://en.wikipedia.org/wiki/Semantic_parser>semantic parser</a> that finds the <a href=https://en.wikipedia.org/wiki/Logical_form>logical form</a> with the highest <a href=https://en.wikipedia.org/wiki/Posterior_probability>posterior probability</a> exactly. We obtain strong results on the GeoQuery dataset and achieve state-of-the-art F1 on Jobs.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1027.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1027 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1027 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1027/>Learning Contextual Embeddings for Structural Semantic Similarity using Categorical Information</a></strong><br><a href=/people/m/massimo-nicosia/>Massimo Nicosia</a>
|
<a href=/people/a/alessandro-moschitti/>Alessandro Moschitti</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1027><div class="card-body p-3 small">Tree kernels (TKs) and <a href=https://en.wikipedia.org/wiki/Neural_network>neural networks</a> are two effective approaches for automatic feature engineering. In this paper, we combine them by modeling context word similarity in semantic TKs. This way, the latter can operate subtree matching by applying neural-based similarity on <a href=https://en.wikipedia.org/wiki/Tree_(data_structure)>tree lexical nodes</a>. We study how to learn <a href=https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning>representations</a> for the words in context such that <a href=https://en.wikipedia.org/wiki/Theory_of_Knowledge>TKs</a> can exploit more <a href=https://en.wikipedia.org/wiki/Focus_(linguistics)>focused information</a>. We found that neural embeddings produced by current methods do not provide a suitable contextual similarity. Thus, we define a new approach based on a <a href=https://en.wikipedia.org/wiki/Siamese_network>Siamese Network</a>, which produces word representations while learning a binary text similarity. We set the <a href=https://en.wikipedia.org/wiki/Logical_disjunction>latter</a> considering examples in the same category as similar. The experiments on question and sentiment classification show that our semantic TK highly improves previous results.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1029.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1029 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1029 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K17-1029.Poster.pdf data-toggle=tooltip data-placement=top title=Poster><i class="fas fa-file-image"></i></a></span>
<span class=d-block><strong><a class=align-middle href=/K17-1029/>Neural Domain Adaptation for Biomedical Question Answering</a></strong><br><a href=/people/g/georg-wiese/>Georg Wiese</a>
|
<a href=/people/d/dirk-weissenborn/>Dirk Weissenborn</a>
|
<a href=/people/m/mariana-neves/>Mariana Neves</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1029><div class="card-body p-3 small">Factoid question answering (QA) has recently benefited from the development of deep learning (DL) systems. Neural network models outperform traditional approaches in domains where large datasets exist, such as SQuAD (ca. 100,000 questions) for Wikipedia articles. However, these systems have not yet been applied to <a href=https://en.wikipedia.org/wiki/Quality_assurance>QA</a> in more specific domains, such as <a href=https://en.wikipedia.org/wiki/Biomedicine>biomedicine</a>, because datasets are generally too small to train a DL system from scratch. For example, the BioASQ dataset for biomedical QA comprises less then 900 factoid (single answer) and list (multiple answers) QA instances. In this work, we adapt a neural QA system trained on a large open-domain dataset (SQuAD, source) to a biomedical dataset (BioASQ, target) by employing various transfer learning techniques. Our <a href=https://en.wikipedia.org/wiki/Network_architecture>network architecture</a> is based on a state-of-the-art QA system, extended with biomedical word embeddings and a novel mechanism to answer list questions. In contrast to existing biomedical QA systems, our system does not rely on <a href=https://en.wikipedia.org/wiki/Ontology_(information_science)>domain-specific ontologies</a>, <a href=https://en.wikipedia.org/wiki/Parsing>parsers</a> or <a href=https://en.wikipedia.org/wiki/Entity&#8211;relationship_model>entity taggers</a>, which are expensive to create. Despite this fact, our systems achieve state-of-the-art results on factoid questions and competitive results on list questions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1030.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1030 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1030 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1030" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1030/>A phoneme clustering algorithm based on the obligatory contour principle</a></strong><br><a href=/people/m/mans-hulden/>Mans Hulden</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1030><div class="card-body p-3 small">This paper explores a divisive hierarchical clustering algorithm based on the well-known <a href=https://en.wikipedia.org/wiki/Obligatory_Contour_Principle>Obligatory Contour Principle</a> in <a href=https://en.wikipedia.org/wiki/Phonology>phonology</a>. The purpose is twofold : to see if such an <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> could be used for unsupervised classification of phonemes or graphemes in corpora, and to investigate whether this purported universal constraint really holds for several classes of <a href=https://en.wikipedia.org/wiki/Distinctive_feature>phonological distinctive features</a>. The <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> achieves very high accuracies in an unsupervised setting of inferring a consonant-vowel distinction, and also has a strong tendency to detect coronal phonemes in an unsupervised fashion. Remaining classes, however, do not correspond as neatly to phonological distinctive feature splits. While the results offer only mixed support for a universal Obligatory Contour Principle, the <a href=https://en.wikipedia.org/wiki/Algorithm>algorithm</a> can be very useful for many NLP tasks due to the high accuracy in revealing consonant / vowel / coronal distinctions.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1031.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1031 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1031 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1031/>Learning Stock Market Sentiment Lexicon and Sentiment-Oriented Word Vector from StockTwits<span class=acl-fixed-case>S</span>tock<span class=acl-fixed-case>T</span>wits</a></strong><br><a href=/people/q/quanzhi-li/>Quanzhi Li</a>
|
<a href=/people/s/sameena-shah/>Sameena Shah</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1031><div class="card-body p-3 small">Previous studies have shown that investor sentiment indicators can predict stock market change. A domain-specific sentiment lexicon and sentiment-oriented word embedding model would help the <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>sentiment analysis</a> in financial domain and <a href=https://en.wikipedia.org/wiki/Stock_market>stock market</a>. In this paper, we present a new approach to learning stock market lexicon from <a href=https://en.wikipedia.org/wiki/StockTwits>StockTwits</a>, a popular financial social network for investors to share ideas. It learns word polarity by predicting <a href=https://en.wikipedia.org/wiki/Sentiment_analysis>message sentiment</a>, using a <a href=https://en.wikipedia.org/wiki/Artificial_neural_network>neural net-work</a>. The sentiment-oriented word embeddings are learned from tens of millions of StockTwits posts, and this is the first study presenting sentiment-oriented word embeddings for <a href=https://en.wikipedia.org/wiki/Stock_market>stock market</a>. The experiments of predicting investor sentiment show that our <a href=https://en.wikipedia.org/wiki/Lexicon>lexicon</a> outperformed other lexicons built by the state-of-the-art methods, and the sentiment-oriented word vector was much better than the general word embeddings.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1033.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1033 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1033 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1033/>Idea density for predicting Alzheimer’s disease from transcribed speech<span class=acl-fixed-case>A</span>lzheimer’s disease from transcribed speech</a></strong><br><a href=/people/k/kairit-sirts/>Kairit Sirts</a>
|
<a href=/people/o/olivier-piguet/>Olivier Piguet</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1033><div class="card-body p-3 small">Idea Density (ID) measures the rate at which <a href=https://en.wikipedia.org/wiki/Idea>ideas</a> or elementary predications are expressed in an utterance or in a text. Lower ID is found to be associated with an increased risk of developing Alzheimer&#8217;s disease (AD) (Snowdon et al., 1996 ; Engelman et al., 2010). ID has been used in two different versions : propositional idea density (PID) counts the expressed ideas and can be applied to any text while semantic idea density (SID) counts pre-defined information content units and is naturally more applicable to normative domains, such as picture description tasks. In this paper, we develop DEPID, a novel dependency-based method for computing PID, and its version DEPID-R that enables to exclude repeating ideasa feature characteristic to AD speech. We conduct the first comparison of automatically extracted PID and SID in the diagnostic classification task on two different AD datasets covering both closed-topic and free-recall domains. While SID performs better on the normative dataset, adding PID leads to a small but significant improvement (+1.7 F-score). On the free-topic dataset, PID performs better than SID as expected (77.6 vs 72.3 in F-score) but adding the <a href=https://en.wikipedia.org/wiki/Feature_(machine_learning)>features</a> derived from the word embedding clustering underlying the automatic SID increases the results considerably, leading to an F-score of 84.8.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1034.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1034 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1034 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1034" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1034/>Zero-Shot Relation Extraction via Reading Comprehension</a></strong><br><a href=/people/o/omer-levy/>Omer Levy</a>
|
<a href=/people/m/minjoon-seo/>Minjoon Seo</a>
|
<a href=/people/e/eunsol-choi/>Eunsol Choi</a>
|
<a href=/people/l/luke-zettlemoyer/>Luke Zettlemoyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1034><div class="card-body p-3 small">We show that <a href=https://en.wikipedia.org/wiki/Relation_extraction>relation extraction</a> can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages : we can (1) learn relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relation types that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relation types with high <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a>, and that zero-shot generalization to unseen relation types is possible, at lower <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy levels</a>, setting the bar for future work on this task.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1035.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1035 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1035 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1035/>The Covert Helps Parse the Overt</a></strong><br><a href=/people/x/xun-zhang/>Xun Zhang</a>
|
<a href=/people/w/weiwei-sun/>Weiwei Sun</a>
|
<a href=/people/x/xiaojun-wan/>Xiaojun Wan</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1035><div class="card-body p-3 small">This paper is concerned with whether deep syntactic information can help surface parsing, with a particular focus on <a href=https://en.wikipedia.org/wiki/Empty_category>empty categories</a>. We design new algorithms to produce dependency trees in which empty elements are allowed, and evaluate the impact of information about <a href=https://en.wikipedia.org/wiki/Empty_category>empty category</a> on parsing overt elements. Such information is helpful to reduce the <a href=https://en.wikipedia.org/wiki/Approximation_error>approximation error</a> in a structured parsing model, but increases the <a href=https://en.wikipedia.org/wiki/Feasible_region>search space</a> for <a href=https://en.wikipedia.org/wiki/Statistical_inference>inference</a> and accordingly the estimation error. To deal with structure-based overfitting, we propose to integrate disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1036.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1036 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1036 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-attachment align-middle mr-1" href=https://aclanthology.org/attachments/K17-1036.Presentation.pdf data-toggle=tooltip data-placement=top title=Presentation><i class="fas fa-file-powerpoint"></i>
</a><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1036" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1036/>German in Flux : Detecting Metaphoric Change via Word Entropy<span class=acl-fixed-case>G</span>erman in Flux: Detecting Metaphoric Change via Word Entropy</a></strong><br><a href=/people/d/dominik-schlechtweg/>Dominik Schlechtweg</a>
|
<a href=/people/s/stefanie-eckmann/>Stefanie Eckmann</a>
|
<a href=/people/e/enrico-santus/>Enrico Santus</a>
|
<a href=/people/s/sabine-schulte-im-walde/>Sabine Schulte im Walde</a>
|
<a href=/people/d/daniel-hole/>Daniel Hole</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1036><div class="card-body p-3 small">This paper explores the information-theoretic measure entropy to detect metaphoric change, transferring ideas from hypernym detection to research on <a href=https://en.wikipedia.org/wiki/Language_change>language change</a>. We build the first diachronic test set for <a href=https://en.wikipedia.org/wiki/German_language>German</a> as a standard for metaphoric change annotation. Our <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> is unsupervised, language-independent and generalizable to other processes of <a href=https://en.wikipedia.org/wiki/Semantic_change>semantic change</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1038.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1038 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1038 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1038" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1038/>Multilingual Semantic Parsing And Code-Switching</a></strong><br><a href=/people/l/long-duong/>Long Duong</a>
|
<a href=/people/h/hadi-afshar/>Hadi Afshar</a>
|
<a href=/people/d/dominique-estival/>Dominique Estival</a>
|
<a href=/people/g/glen-pink/>Glen Pink</a>
|
<a href=/people/p/philip-r-cohen/>Philip Cohen</a>
|
<a href=/people/m/mark-johnson/>Mark Johnson</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1038><div class="card-body p-3 small">Extending semantic parsing systems to new domains and languages is a highly expensive, time-consuming process, so making effective use of existing resources is critical. In this paper, we describe a transfer learning method using crosslingual word embeddings in a sequence-to-sequence model. On the NLmaps corpus, our approach achieves state-of-the-art <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 85.7 % for <a href=https://en.wikipedia.org/wiki/English_language>English</a>. Most importantly, we observed a consistent improvement for <a href=https://en.wikipedia.org/wiki/German_language>German</a> compared with several baseline domain adaptation techniques. As a by-product of this approach, our models that are trained on a combination of <a href=https://en.wikipedia.org/wiki/English_language>English and German utterances</a> perform reasonably well on code-switching utterances which contain a mixture of <a href=https://en.wikipedia.org/wiki/English_language>English</a> and <a href=https://en.wikipedia.org/wiki/German_language>German</a>, even though the training data does not contain any such. As far as we know, this is the first study of <a href=https://en.wikipedia.org/wiki/Code-switching>code-switching</a> in <a href=https://en.wikipedia.org/wiki/Semantic_parsing>semantic parsing</a>. We manually constructed the set of code-switching test utterances for the NLmaps corpus and achieve 78.3 % <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> on this <a href=https://en.wikipedia.org/wiki/Data_set>dataset</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1039.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1039 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1039 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1039" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1039/>Optimizing Differentiable Relaxations of Coreference Evaluation Metrics</a></strong><br><a href=/people/p/phong-le/>Phong Le</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1039><div class="card-body p-3 small">Coreference evaluation metrics are hard to optimize directly as they are <a href=https://en.wikipedia.org/wiki/Differentiable_function>non-differentiable functions</a>, not easily decomposable into elementary decisions. Consequently, most approaches optimize objectives only indirectly related to the end goal, resulting in suboptimal performance. Instead, we propose a differentiable relaxation that lends itself to gradient-based optimisation, thus bypassing the need for <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> or heuristic modification of cross-entropy. We show that by modifying the training objective of a competitive neural coreference system, we obtain a substantial gain in performance. This suggests that our approach can be regarded as a viable alternative to using <a href=https://en.wikipedia.org/wiki/Reinforcement_learning>reinforcement learning</a> or more computationally expensive imitation learning.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1040.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1040 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1040 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1040" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1040/>Neural Structural Correspondence Learning for Domain Adaptation</a></strong><br><a href=/people/y/yftah-ziser/>Yftah Ziser</a>
|
<a href=/people/r/roi-reichart/>Roi Reichart</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1040><div class="card-body p-3 small">We introduce a neural network model that marries together ideas from two prominent strands of research on domain adaptation through <a href=https://en.wikipedia.org/wiki/Representation_learning>representation learning</a> : structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks (NNs). Our model is a three-layer NN that learns to encode the non-pivot features of an input example into a low dimensional representation, so that the existence of pivot features (features that are prominent in both domains and convey useful information for the NLP task) in the example can be decoded from that representation. The low-dimensional representation is then employed in a <a href=https://en.wikipedia.org/wiki/Machine_learning>learning algorithm</a> for the <a href=https://en.wikipedia.org/wiki/Computational_complexity_theory>task</a>. Moreover, we show how to inject pre-trained word embeddings into our model in order to improve generalization across examples with similar pivot features. We experiment with the task of cross-domain sentiment classification on 16 domain pairs and show substantial improvements over strong baselines.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1041.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1041 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1041 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"><a class="badge badge-secondary align-middle mr-1 pwc-reduced-padding" href="https://paperswithcode.com/paper/?acl=K17-1041" data-toggle=tooltip data-placement=top title=Code><svg xmlns="http://www.w3.org/2000/svg" class="pwc-icon-small" viewBox="0 0 512 512"><path stroke="#4d8093" fill="#4d8093" d="M88 128h48v256H88zm144 0h48v256h-48zm-72 16h48v224h-48zm144 0h48v224h-48zm72-16h48v256h-48z"/><path stroke="#4d8093" fill="#4d8093" d="M104 104V56H16v4e2h88v-48H64V104zM408 56v48h40v304h-40v48h88V56z"/></svg></a></span><span class=d-block><strong><a class=align-middle href=/K17-1041/>A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling</a></strong><br><a href=/people/d/diego-marcheggiani/>Diego Marcheggiani</a>
|
<a href=/people/a/anton-frolov/>Anton Frolov</a>
|
<a href=/people/i/ivan-titov/>Ivan Titov</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1041><div class="card-body p-3 small">We introduce a simple and accurate neural model for dependency-based semantic role labeling. Our <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder. The semantic role labeler achieves competitive performance on <a href=https://en.wikipedia.org/wiki/English_language>English</a>, even without any kind of <a href=https://en.wikipedia.org/wiki/Syntax>syntactic information</a> and only using local inference. However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the English CoNLL-2009 dataset. We also consider <a href=https://en.wikipedia.org/wiki/Chinese_language>Chinese</a>, <a href=https://en.wikipedia.org/wiki/Czech_language>Czech</a> and <a href=https://en.wikipedia.org/wiki/Spanish_language>Spanish</a> where our approach also achieves competitive results. Syntactic parsers are unreliable on out-of-domain data, so standard (i.e., syntactically-informed) SRL models are hindered when tested in this setting. Our syntax-agnostic model appears more robust, resulting in the best reported results on standard out-of-domain test sets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1042.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1042 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1042 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1042/>Joint Prediction of Morphosyntactic Categories for Fine-Grained Arabic Part-of-Speech Tagging Exploiting Tag Dictionary Information<span class=acl-fixed-case>A</span>rabic Part-of-Speech Tagging Exploiting Tag Dictionary Information</a></strong><br><a href=/people/g/go-inoue/>Go Inoue</a>
|
<a href=/people/h/hiroyuki-shindo/>Hiroyuki Shindo</a>
|
<a href=/people/y/yuji-matsumoto/>Yuji Matsumoto</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1042><div class="card-body p-3 small">Part-of-speech (POS) tagging for morphologically rich languages such as <a href=https://en.wikipedia.org/wiki/Arabic>Arabic</a> is a challenging problem because of their enormous tag sets. One reason for this is that in the <a href=https://en.wikipedia.org/wiki/Tag_(metadata)>tagging scheme</a> for such languages, a complete POS tag is formed by combining tags from multiple tag sets defined for each morphosyntactic category. Previous approaches in Arabic POS tagging applied one <a href=https://en.wikipedia.org/wiki/Conceptual_model>model</a> for each morphosyntactic tagging task, without utilizing shared information between the <a href=https://en.wikipedia.org/wiki/Task_(project_management)>tasks</a>. In this paper, we propose an approach that utilizes this information by jointly modeling multiple morphosyntactic tagging tasks with a multi-task learning framework. We also propose a method of incorporating tag dictionary information into our neural models by combining word representations with representations of the sets of possible tags. Our experiments showed that the joint model with tag dictionary information results in an <a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a> of 91.38 % on the Penn Arabic Treebank data set, with an absolute improvement of 2.11 % over the current state-of-the-art <a href=https://en.wikipedia.org/wiki/Tagger>tagger</a>.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1043.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1043 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1043 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1043/>Learning from Relatives : Unified Dialectal Arabic Segmentation<span class=acl-fixed-case>A</span>rabic Segmentation</a></strong><br><a href=/people/y/younes-samih/>Younes Samih</a>
|
<a href=/people/m/mohamed-eldesouki/>Mohamed Eldesouki</a>
|
<a href=/people/m/mohammed-attia/>Mohammed Attia</a>
|
<a href=/people/k/kareem-darwish/>Kareem Darwish</a>
|
<a href=/people/a/ahmed-abdelali/>Ahmed Abdelali</a>
|
<a href=/people/h/hamdy-mubarak/>Hamdy Mubarak</a>
|
<a href=/people/l/laura-kallmeyer/>Laura Kallmeyer</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1043><div class="card-body p-3 small">Arabic dialects do not just share a common koin, but there are shared pan-dialectal linguistic phenomena that allow <a href=https://en.wikipedia.org/wiki/Computational_linguistics>computational models</a> for <a href=https://en.wikipedia.org/wiki/Dialect>dialects</a> to learn from each other. In this paper we build a unified segmentation model where the training data for different dialects are combined and a single <a href=https://en.wikipedia.org/wiki/Statistical_model>model</a> is trained. The model yields higher accuracies than dialect-specific models, eliminating the need for dialect identification before segmentation. We also measure the degree of <a href=https://en.wikipedia.org/wiki/Coefficient_of_relationship>relatedness</a> between four major <a href=https://en.wikipedia.org/wiki/Varieties_of_Arabic>Arabic dialects</a> by testing how a segmentation model trained on one dialect performs on the other dialects. We found that linguistic relatedness is contingent with <a href=https://en.wikipedia.org/wiki/Proxemics>geographical proximity</a>. In our experiments we use SVM-based ranking and bi-LSTM-CRF sequence labeling.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1044.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1044 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1044 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1044/>Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks<span class=acl-fixed-case>RNN</span> Encoder-Decoder Networks</a></strong><br><a href=/people/v/van-khanh-tran/>Van-Khanh Tran</a>
|
<a href=/people/m/minh-le-nguyen/>Le-Minh Nguyen</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1044><div class="card-body p-3 small">Natural language generation (NLG) is a critical component in a <a href=https://en.wikipedia.org/wiki/Spoken_dialogue_system>spoken dialogue system</a>. This paper presents a Recurrent Neural Network based Encoder-Decoder architecture, in which an LSTM-based decoder is introduced to select, aggregate semantic elements produced by an attention mechanism over the input elements, and to produce the required utterances. The proposed generator can be jointly trained both sentence planning and surface realization to produce <a href=https://en.wikipedia.org/wiki/Sentence_(linguistics)>natural language sentences</a>. The proposed <a href=https://en.wikipedia.org/wiki/Mathematical_model>model</a> was extensively evaluated on four different NLG datasets. The experimental results showed that the proposed generators not only consistently outperform the previous methods across all the NLG domains but also show an ability to generalize from a new, unseen domain and learn from multi-domain datasets.</div></div><p class="d-sm-flex align-items-stretch"><span class="d-block mr-2 text-nowrap list-button-row"><a class="badge badge-primary align-middle mr-1" href=https://aclanthology.org/K17-1045.pdf data-toggle=tooltip data-placement=top title="Open PDF">pdf
</a><a class="badge badge-info align-middle mr-1" href=#abstract-K17-1045 data-toggle=collapse aria-expanded=false aria-controls=abstract-K17-1045 title="Show Abstract">abs</a><br class="d-none d-sm-inline-block"></span><span class=d-block><strong><a class=align-middle href=/K17-1045/>Graph-based Neural Multi-Document Summarization</a></strong><br><a href=/people/m/michihiro-yasunaga/>Michihiro Yasunaga</a>
|
<a href=/people/r/rui-zhang/>Rui Zhang</a>
|
<a href=/people/k/kshitijh-meelu/>Kshitijh Meelu</a>
|
<a href=/people/a/ayush-pareek/>Ayush Pareek</a>
|
<a href=/people/k/krishnan-srinivasan/>Krishnan Srinivasan</a>
|
<a href=/people/d/dragomir-radev/>Dragomir Radev</a></span></p><div class="card bg-light mb-2 mb-lg-3 collapse abstract-collapse" id=abstract-K17-1045><div class="card-body p-3 small">We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>Recurrent Neural Networks</a> as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a <a href=https://en.wikipedia.org/wiki/Greedy_heuristic>greedy heuristic</a> to extract salient sentences that avoid <a href=https://en.wikipedia.org/wiki/Redundancy_(information_theory)>redundancy</a>. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in <a href=https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)>graphs</a> with the representation power of <a href=https://en.wikipedia.org/wiki/Deep_learning>deep neural networks</a>. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems.</div></div></div></section></div><footer class="bg-gradient-dark py-2 py-xl-3 mt-3 mt-md-4 mt-xl-5" style=color:#fff><div class=container><p class="small px-1"><span class="float-right mt-2 ml-2"><a rel=license href=http://creativecommons.org/licenses/by/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by/4.0/88x31.png></a></span>
ACL materials are Copyright ©&nbsp;1963&ndash;2022 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/>Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License</a>. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a <a href=https://creativecommons.org/licenses/by/4.0/>Creative Commons Attribution 4.0 International License</a>.</p><p class="small px-1">The ACL Anthology is managed and built by the <a href=/info/credits/>ACL Anthology team</a> of volunteers.</p><p class="small px-1"><i>Site last built on 23 May 2022 at 01:26 UTC with <a href=https://github.com/acl-org/acl-anthology/tree/6b5537cdc480294721047e4e67eca5cb2bec4ab4>commit 6b5537cd</a>.</i></p></div></footer><script src=https://code.jquery.com/jquery-3.3.1.slim.min.js integrity=sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js integrity=sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut crossorigin=anonymous></script>
<script src=https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js integrity=sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k crossorigin=anonymous></script>
<script>$(function(){$('[data-toggle="tooltip"]').tooltip(),$("#toggle-all-abstracts")&&($("#toggle-all-abstracts").click(function(){var e=$("#toggle-all-abstracts");e.attr("disabled",!0),e.attr("data-toggle-state")=="hide"?($(".abstract-collapse").collapse("show"),e.attr("data-toggle-state","show")):($(".abstract-collapse").collapse("hide"),e.attr("data-toggle-state","hide")),e.attr("disabled",!1)}),$("#toggle-all-abstracts").attr("disabled",!1))})</script></body></html>